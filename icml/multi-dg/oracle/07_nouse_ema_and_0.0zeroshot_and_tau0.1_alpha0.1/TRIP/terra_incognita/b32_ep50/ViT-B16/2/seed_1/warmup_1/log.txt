Loading trainer: TRIP
Loading dataset: SPG_TerraIncognita
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------------------------------------------
Dataset    SPG_TerraIncognita
Source     ['location_100', 'location_43', 'location_46']
Target     ['location_38']
# classes  10
# train_x  10,216
# val      4,378
# test     9,736
---------  ----------------------------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,195
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/oracle/07_nouse_ema_and_0.0zeroshot_and_tau0.1_alpha0.1/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/319] time 0.067 (0.180) data 0.000 (0.041) loss 1.9617 (2.7144) ce_loss 1.2549 (2.0158) teacher_loss 1.2528 (2.0152) loss_zs_kd 0.0002 (0.0001) loss_oracle 0.7090 (0.6992) acc 59.3750 (36.8750) kd_loss 0.9628 (0.9466) lr 1.0000e-05 eta 0:47:43
epoch [1/50] batch [40/319] time 0.175 (0.145) data 0.000 (0.021) loss 2.2435 (2.5982) ce_loss 1.5752 (1.9127) teacher_loss 1.5746 (1.9121) loss_zs_kd 0.0015 (0.0003) loss_oracle 0.6689 (0.6861) acc 56.2500 (39.7656) kd_loss 0.9064 (0.9283) lr 1.0000e-05 eta 0:38:31
epoch [1/50] batch [60/319] time 0.078 (0.129) data 0.000 (0.014) loss 2.8721 (2.6305) ce_loss 2.1699 (1.9407) teacher_loss 2.1716 (1.9404) loss_zs_kd 0.0026 (0.0008) loss_oracle 0.7005 (0.6901) acc 31.2500 (39.2188) kd_loss 0.9491 (0.9336) lr 1.0000e-05 eta 0:34:15
epoch [1/50] batch [80/319] time 0.179 (0.124) data 0.000 (0.010) loss 2.6623 (2.6118) ce_loss 1.9766 (1.9221) teacher_loss 1.9702 (1.9215) loss_zs_kd 0.0041 (0.0015) loss_oracle 0.6921 (0.6903) acc 43.7500 (38.8281) kd_loss 0.9235 (0.9324) lr 1.0000e-05 eta 0:32:52
epoch [1/50] batch [100/319] time 0.096 (0.117) data 0.000 (0.008) loss 2.9021 (2.5728) ce_loss 2.1680 (1.8832) teacher_loss 2.1710 (1.8828) loss_zs_kd 0.0083 (0.0025) loss_oracle 0.7311 (0.6900) acc 25.0000 (39.4375) kd_loss 1.0048 (0.9303) lr 1.0000e-05 eta 0:31:01
epoch [1/50] batch [120/319] time 0.094 (0.114) data 0.000 (0.007) loss 2.5204 (2.5706) ce_loss 1.8662 (1.8809) teacher_loss 1.8661 (1.8809) loss_zs_kd 0.0065 (0.0035) loss_oracle 0.6543 (0.6897) acc 40.6250 (39.6094) kd_loss 0.8698 (0.9287) lr 1.0000e-05 eta 0:30:07
epoch [1/50] batch [140/319] time 0.102 (0.113) data 0.000 (0.006) loss 2.6550 (2.5689) ce_loss 2.0508 (1.8807) teacher_loss 2.0505 (1.8808) loss_zs_kd 0.0103 (0.0045) loss_oracle 0.6045 (0.6880) acc 37.5000 (39.5536) kd_loss 0.8176 (0.9264) lr 1.0000e-05 eta 0:29:40
epoch [1/50] batch [160/319] time 0.090 (0.111) data 0.000 (0.005) loss 2.4977 (2.5632) ce_loss 1.8291 (1.8767) teacher_loss 1.8241 (1.8768) loss_zs_kd 0.0138 (0.0058) loss_oracle 0.6736 (0.6864) acc 34.3750 (39.6875) kd_loss 0.9074 (0.9246) lr 1.0000e-05 eta 0:29:13
epoch [1/50] batch [180/319] time 0.082 (0.110) data 0.000 (0.005) loss 2.8037 (2.5585) ce_loss 2.1719 (1.8733) teacher_loss 2.1712 (1.8734) loss_zs_kd 0.0238 (0.0073) loss_oracle 0.6326 (0.6852) acc 28.1250 (39.6007) kd_loss 0.8519 (0.9231) lr 1.0000e-05 eta 0:28:48
epoch [1/50] batch [200/319] time 0.089 (0.108) data 0.000 (0.004) loss 2.7186 (2.5540) ce_loss 1.9932 (1.8687) teacher_loss 1.9913 (1.8687) loss_zs_kd 0.0248 (0.0089) loss_oracle 0.7272 (0.6854) acc 31.2500 (39.5469) kd_loss 0.9751 (0.9231) lr 1.0000e-05 eta 0:28:24
epoch [1/50] batch [220/319] time 0.094 (0.107) data 0.000 (0.004) loss 2.4278 (2.5607) ce_loss 1.7139 (1.8743) teacher_loss 1.7087 (1.8743) loss_zs_kd 0.0326 (0.0108) loss_oracle 0.7191 (0.6863) acc 50.0000 (39.1761) kd_loss 0.9470 (0.9235) lr 1.0000e-05 eta 0:28:10
epoch [1/50] batch [240/319] time 0.094 (0.106) data 0.000 (0.004) loss 2.6047 (2.5691) ce_loss 1.9326 (1.8807) teacher_loss 1.9297 (1.8808) loss_zs_kd 0.0452 (0.0128) loss_oracle 0.6750 (0.6883) acc 28.1250 (38.8672) kd_loss 0.8989 (0.9254) lr 1.0000e-05 eta 0:27:46
epoch [1/50] batch [260/319] time 0.119 (0.106) data 0.000 (0.003) loss 2.6606 (2.5669) ce_loss 1.9736 (1.8779) teacher_loss 1.9806 (1.8779) loss_zs_kd 0.0415 (0.0149) loss_oracle 0.6800 (0.6890) acc 40.6250 (38.7981) kd_loss 0.8925 (0.9255) lr 1.0000e-05 eta 0:27:39
epoch [1/50] batch [280/319] time 0.093 (0.105) data 0.000 (0.003) loss 2.8070 (2.5640) ce_loss 2.0684 (1.8738) teacher_loss 2.0918 (1.8740) loss_zs_kd 0.0606 (0.0172) loss_oracle 0.7152 (0.6900) acc 31.2500 (38.8170) kd_loss 0.9579 (0.9256) lr 1.0000e-05 eta 0:27:32
epoch [1/50] batch [300/319] time 0.091 (0.105) data 0.000 (0.003) loss 2.9719 (2.5603) ce_loss 2.2773 (1.8695) teacher_loss 2.2817 (1.8698) loss_zs_kd 0.0592 (0.0197) loss_oracle 0.6902 (0.6906) acc 31.2500 (38.8021) kd_loss 0.9081 (0.9255) lr 1.0000e-05 eta 0:27:27
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,773
* accuracy: 40.5%
* error: 59.5%
* macro_f1: 26.1%
Checkpoint saved to icml/multi-dg/oracle/07_nouse_ema_and_0.0zeroshot_and_tau0.1_alpha0.1/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 3,207
* accuracy: 32.9%
* error: 67.1%
* macro_f1: 14.2%
Checkpoint saved to icml/multi-dg/oracle/07_nouse_ema_and_0.0zeroshot_and_tau0.1_alpha0.1/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      40.5%, epoch: 1 *******
******* Domain 2 best val test acc: 32.9%, epoch: 1 *******
******* Domain 2 best test acc:     32.9%, epoch: 1 *******
[TensorBoard] Initialized writer in: icml/multi-dg/oracle/07_nouse_ema_and_0.0zeroshot_and_tau0.1_alpha0.1/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/ana
epoch [2/50] batch [20/319] time 0.082 (0.130) data 0.000 (0.025) loss 2.5690 (2.5257) ce_loss 1.7559 (1.7053) teacher_loss 1.5941 (1.6725) loss_zs_kd 0.6244 (0.4990) loss_oracle 0.9749 (0.8532) acc 28.1250 (37.1875) kd_loss 1.0187 (0.9911) lr 2.0000e-03 eta 0:33:50
epoch [2/50] batch [40/319] time 0.071 (0.112) data 0.000 (0.013) loss 2.8411 (2.5498) ce_loss 2.0430 (1.7005) teacher_loss 1.8791 (1.6267) loss_zs_kd 0.4792 (0.6267) loss_oracle 0.9620 (0.9231) acc 28.1250 (37.3438) kd_loss 1.0315 (1.0170) lr 2.0000e-03 eta 0:29:10
epoch [2/50] batch [60/319] time 0.070 (0.103) data 0.000 (0.008) loss 2.5157 (2.5322) ce_loss 1.7666 (1.6683) teacher_loss 1.4907 (1.5795) loss_zs_kd 0.3677 (0.5531) loss_oracle 1.0250 (0.9527) acc 34.3750 (38.1250) kd_loss 1.0545 (1.0268) lr 2.0000e-03 eta 0:26:50
epoch [2/50] batch [80/319] time 0.093 (0.101) data 0.000 (0.006) loss 2.2461 (2.4869) ce_loss 1.4912 (1.6222) teacher_loss 1.4039 (1.5474) loss_zs_kd 0.4397 (0.4996) loss_oracle 0.8422 (0.9395) acc 53.1250 (39.8047) kd_loss 1.0130 (1.0299) lr 2.0000e-03 eta 0:26:09
epoch [2/50] batch [100/319] time 0.103 (0.099) data 0.000 (0.005) loss 2.3914 (2.4526) ce_loss 1.6982 (1.6028) teacher_loss 1.5053 (1.5340) loss_zs_kd 0.3950 (0.4825) loss_oracle 0.8861 (0.9186) acc 34.3750 (40.2500) kd_loss 1.0469 (1.0371) lr 2.0000e-03 eta 0:25:43
epoch [2/50] batch [120/319] time 0.113 (0.100) data 0.000 (0.004) loss 1.9622 (2.4250) ce_loss 1.2080 (1.5822) teacher_loss 1.0762 (1.5135) loss_zs_kd 0.3804 (0.4770) loss_oracle 0.8860 (0.9115) acc 62.5000 (40.7031) kd_loss 1.0561 (1.0363) lr 2.0000e-03 eta 0:25:52
epoch [2/50] batch [140/319] time 0.105 (0.100) data 0.000 (0.004) loss 2.4831 (2.4065) ce_loss 1.7295 (1.5759) teacher_loss 1.5408 (1.4995) loss_zs_kd 0.5189 (0.4757) loss_oracle 0.9423 (0.9070) acc 37.5000 (40.7366) kd_loss 1.0523 (1.0357) lr 2.0000e-03 eta 0:25:41
epoch [2/50] batch [160/319] time 0.089 (0.099) data 0.000 (0.003) loss 2.3260 (2.3870) ce_loss 1.4619 (1.5603) teacher_loss 1.3376 (1.4779) loss_zs_kd 0.4631 (0.4842) loss_oracle 0.9884 (0.9091) acc 46.8750 (41.6406) kd_loss 1.0610 (1.0370) lr 2.0000e-03 eta 0:25:38
epoch [2/50] batch [180/319] time 0.097 (0.099) data 0.000 (0.003) loss 2.5321 (2.3876) ce_loss 1.6318 (1.5610) teacher_loss 1.4945 (1.4709) loss_zs_kd 0.6379 (0.5004) loss_oracle 1.0376 (0.9167) acc 31.2500 (41.6667) kd_loss 1.0366 (1.0359) lr 2.0000e-03 eta 0:25:30
epoch [2/50] batch [200/319] time 0.105 (0.099) data 0.000 (0.003) loss 2.3269 (2.3855) ce_loss 1.4824 (1.5624) teacher_loss 1.3736 (1.4652) loss_zs_kd 0.6674 (0.5202) loss_oracle 0.9533 (0.9203) acc 53.1250 (41.6875) kd_loss 1.0116 (1.0332) lr 2.0000e-03 eta 0:25:32
epoch [2/50] batch [220/319] time 0.096 (0.100) data 0.000 (0.003) loss 2.6070 (2.3764) ce_loss 1.7490 (1.5583) teacher_loss 1.6519 (1.4562) loss_zs_kd 0.3932 (0.5216) loss_oracle 0.9551 (0.9202) acc 31.2500 (41.5199) kd_loss 0.9992 (1.0295) lr 2.0000e-03 eta 0:25:34
epoch [2/50] batch [240/319] time 0.098 (0.100) data 0.000 (0.002) loss 1.9156 (2.3596) ce_loss 1.1367 (1.5462) teacher_loss 0.9756 (1.4395) loss_zs_kd 0.4891 (0.5131) loss_oracle 0.9400 (0.9202) acc 56.2500 (41.9661) kd_loss 1.0045 (1.0268) lr 2.0000e-03 eta 0:25:37
epoch [2/50] batch [260/319] time 0.108 (0.100) data 0.000 (0.002) loss 2.2030 (2.3381) ce_loss 1.4434 (1.5342) teacher_loss 1.2935 (1.4174) loss_zs_kd 0.4363 (0.5144) loss_oracle 0.9096 (0.9207) acc 46.8750 (42.3317) kd_loss 0.9881 (1.0227) lr 2.0000e-03 eta 0:25:40
epoch [2/50] batch [280/319] time 0.114 (0.100) data 0.000 (0.002) loss 2.2014 (2.3279) ce_loss 1.4004 (1.5303) teacher_loss 1.3513 (1.4105) loss_zs_kd 0.4167 (0.5123) loss_oracle 0.8501 (0.9175) acc 46.8750 (42.4442) kd_loss 0.9726 (1.0190) lr 2.0000e-03 eta 0:25:41
epoch [2/50] batch [300/319] time 0.110 (0.101) data 0.000 (0.002) loss 1.9681 (2.3192) ce_loss 1.2959 (1.5275) teacher_loss 1.1027 (1.4058) loss_zs_kd 0.4758 (0.5136) loss_oracle 0.8654 (0.9135) acc 50.0000 (42.4167) kd_loss 0.9590 (1.0152) lr 2.0000e-03 eta 0:25:43
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,107
* accuracy: 48.1%
* error: 51.9%
* macro_f1: 36.2%
Checkpoint saved to icml/multi-dg/oracle/07_nouse_ema_and_0.0zeroshot_and_tau0.1_alpha0.1/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 4,452
* accuracy: 45.7%
* error: 54.3%
* macro_f1: 17.5%
Checkpoint saved to icml/multi-dg/oracle/07_nouse_ema_and_0.0zeroshot_and_tau0.1_alpha0.1/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      48.1%, epoch: 2 *******
******* Domain 2 best val test acc: 45.7%, epoch: 2 *******
******* Domain 2 best test acc:     45.7%, epoch: 2 *******
epoch [3/50] batch [20/319] time 0.120 (0.145) data 0.000 (0.031) loss 1.8314 (2.0150) ce_loss 1.1025 (1.3163) teacher_loss 1.0089 (1.1996) loss_zs_kd 0.4173 (0.5066) loss_oracle 0.8225 (0.8154) acc 56.2500 (50.4688) kd_loss 0.9240 (0.9435) lr 1.9980e-03 eta 0:36:54
epoch [3/50] batch [40/319] time 0.084 (0.126) data 0.000 (0.016) loss 1.9657 (2.0563) ce_loss 1.2920 (1.3445) teacher_loss 1.1386 (1.2312) loss_zs_kd 0.5511 (0.5177) loss_oracle 0.8270 (0.8251) acc 53.1250 (48.9844) kd_loss 0.9252 (0.9393) lr 1.9980e-03 eta 0:32:04
epoch [3/50] batch [60/319] time 0.098 (0.113) data 0.001 (0.010) loss 1.8952 (2.0391) ce_loss 1.2021 (1.3466) teacher_loss 1.0183 (1.2170) loss_zs_kd 0.5972 (0.5319) loss_oracle 0.8769 (0.8221) acc 53.1250 (48.8021) kd_loss 0.9150 (0.9350) lr 1.9980e-03 eta 0:28:37
epoch [3/50] batch [80/319] time 0.097 (0.110) data 0.000 (0.008) loss 1.8935 (2.0476) ce_loss 1.1484 (1.3531) teacher_loss 1.0648 (1.2203) loss_zs_kd 0.4949 (0.5477) loss_oracle 0.8287 (0.8274) acc 62.5000 (48.3203) kd_loss 0.9321 (0.9344) lr 1.9980e-03 eta 0:27:55
epoch [3/50] batch [100/319] time 0.100 (0.109) data 0.000 (0.006) loss 2.0238 (2.0549) ce_loss 1.1699 (1.3564) teacher_loss 1.1631 (1.2232) loss_zs_kd 0.5140 (0.5394) loss_oracle 0.8607 (0.8317) acc 65.6250 (48.5625) kd_loss 0.8885 (0.9334) lr 1.9980e-03 eta 0:27:40
epoch [3/50] batch [120/319] time 0.104 (0.109) data 0.000 (0.005) loss 2.2934 (2.0596) ce_loss 1.4873 (1.3538) teacher_loss 1.4155 (1.2221) loss_zs_kd 0.5278 (0.5376) loss_oracle 0.8779 (0.8376) acc 43.7500 (49.0625) kd_loss 0.9802 (0.9385) lr 1.9980e-03 eta 0:27:34
epoch [3/50] batch [140/319] time 0.104 (0.108) data 0.000 (0.005) loss 1.8033 (2.0530) ce_loss 1.0928 (1.3488) teacher_loss 0.9843 (1.2162) loss_zs_kd 0.5412 (0.5402) loss_oracle 0.8190 (0.8367) acc 65.6250 (49.2411) kd_loss 0.9295 (0.9378) lr 1.9980e-03 eta 0:27:23
epoch [3/50] batch [160/319] time 0.096 (0.108) data 0.000 (0.004) loss 1.9140 (2.0413) ce_loss 1.1885 (1.3465) teacher_loss 1.0907 (1.2056) loss_zs_kd 0.6872 (0.5506) loss_oracle 0.8233 (0.8357) acc 59.3750 (49.1602) kd_loss 0.9135 (0.9357) lr 1.9980e-03 eta 0:27:13
epoch [3/50] batch [180/319] time 0.108 (0.107) data 0.000 (0.004) loss 2.2163 (2.0394) ce_loss 1.3584 (1.3459) teacher_loss 1.3458 (1.2072) loss_zs_kd 0.4451 (0.5584) loss_oracle 0.8704 (0.8323) acc 50.0000 (49.1146) kd_loss 0.9429 (0.9346) lr 1.9980e-03 eta 0:27:05
epoch [3/50] batch [200/319] time 0.106 (0.107) data 0.000 (0.003) loss 2.0792 (2.0367) ce_loss 1.3447 (1.3439) teacher_loss 1.2523 (1.2038) loss_zs_kd 0.6085 (0.5541) loss_oracle 0.8269 (0.8329) acc 56.2500 (49.1094) kd_loss 0.9205 (0.9317) lr 1.9980e-03 eta 0:26:59
epoch [3/50] batch [220/319] time 0.105 (0.107) data 0.000 (0.003) loss 1.9210 (2.0329) ce_loss 1.3896 (1.3457) teacher_loss 1.0375 (1.1981) loss_zs_kd 0.5229 (0.5610) loss_oracle 0.8835 (0.8348) acc 31.2500 (48.9062) kd_loss 0.9749 (0.9313) lr 1.9980e-03 eta 0:26:53
epoch [3/50] batch [240/319] time 0.099 (0.107) data 0.000 (0.003) loss 2.1813 (2.0289) ce_loss 1.5459 (1.3420) teacher_loss 1.2734 (1.1913) loss_zs_kd 0.7482 (0.5682) loss_oracle 0.9079 (0.8376) acc 43.7500 (49.1016) kd_loss 0.9420 (0.9309) lr 1.9980e-03 eta 0:26:47
epoch [3/50] batch [260/319] time 0.096 (0.107) data 0.000 (0.003) loss 2.0334 (2.0320) ce_loss 1.2441 (1.3410) teacher_loss 1.1891 (1.1940) loss_zs_kd 0.6348 (0.5755) loss_oracle 0.8443 (0.8380) acc 53.1250 (49.2668) kd_loss 0.9201 (0.9319) lr 1.9980e-03 eta 0:26:43
epoch [3/50] batch [280/319] time 0.117 (0.106) data 0.000 (0.002) loss 1.9069 (2.0264) ce_loss 1.3027 (1.3388) teacher_loss 1.0264 (1.1880) loss_zs_kd 0.7511 (0.5862) loss_oracle 0.8805 (0.8384) acc 50.0000 (49.2857) kd_loss 0.9191 (0.9311) lr 1.9980e-03 eta 0:26:38
epoch [3/50] batch [300/319] time 0.097 (0.106) data 0.000 (0.002) loss 1.7640 (2.0231) ce_loss 1.1855 (1.3394) teacher_loss 0.8957 (1.1851) loss_zs_kd 0.8643 (0.6008) loss_oracle 0.8683 (0.8380) acc 56.2500 (49.4896) kd_loss 0.9503 (0.9309) lr 1.9980e-03 eta 0:26:34
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,147
* accuracy: 49.0%
* error: 51.0%
* macro_f1: 38.5%
Checkpoint saved to icml/multi-dg/oracle/07_nouse_ema_and_0.0zeroshot_and_tau0.1_alpha0.1/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,134
* accuracy: 52.7%
* error: 47.3%
* macro_f1: 20.9%
Checkpoint saved to icml/multi-dg/oracle/07_nouse_ema_and_0.0zeroshot_and_tau0.1_alpha0.1/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      49.0%, epoch: 3 *******
******* Domain 2 best val test acc: 52.7%, epoch: 3 *******
******* Domain 2 best test acc:     52.7%, epoch: 3 *******
epoch [4/50] batch [20/319] time 0.082 (0.134) data 0.000 (0.024) loss 1.9917 (1.9345) ce_loss 1.4736 (1.3802) teacher_loss 1.1251 (1.1037) loss_zs_kd 0.9057 (0.8968) loss_oracle 0.8666 (0.8308) acc 37.5000 (45.3125) kd_loss 0.8899 (0.8987) lr 1.9921e-03 eta 0:33:30
epoch [4/50] batch [40/319] time 0.091 (0.124) data 0.000 (0.012) loss 1.9636 (1.9369) ce_loss 1.2822 (1.3464) teacher_loss 1.1174 (1.0986) loss_zs_kd 0.7519 (0.9202) loss_oracle 0.8462 (0.8383) acc 46.8750 (46.8750) kd_loss 0.9670 (0.9098) lr 1.9921e-03 eta 0:30:55
