Loading trainer: TRIP
Loading dataset: SPG_OfficeHome
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------------------------------
Dataset    SPG_OfficeHome
Source     ['clipart', 'product', 'real_world']
Target     ['art']
# classes  65
# train_x  9,222
# val      3,939
# test     2,427
---------  ------------------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
alpha_logit                                        1
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,196
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/oracle/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/288] time 0.157 (0.214) data 0.000 (0.018) loss 1.4444 (1.5895) teacher_loss 1.1096 (1.2382) loss_zs_kd 0.0000 (0.0000) loss_oracle -0.0000 (-0.0001) acc 68.7500 (68.2812) lr 1.0000e-05 eta 0:51:23
epoch [1/50] batch [40/288] time 0.152 (0.187) data 0.000 (0.009) loss 1.6044 (1.6022) teacher_loss 1.2142 (1.2594) loss_zs_kd 0.0000 (0.0000) loss_oracle -0.0001 (-0.0001) acc 65.6250 (68.2031) lr 1.0000e-05 eta 0:44:41
epoch [1/50] batch [60/288] time 0.158 (0.178) data 0.000 (0.006) loss 1.6998 (1.6235) teacher_loss 1.4278 (1.2662) loss_zs_kd 0.0001 (0.0000) loss_oracle 0.0001 (-0.0001) acc 59.3750 (67.6042) lr 1.0000e-05 eta 0:42:38
epoch [1/50] batch [80/288] time 0.167 (0.176) data 0.000 (0.005) loss 1.5932 (1.6448) teacher_loss 1.2582 (1.2702) loss_zs_kd 0.0002 (0.0000) loss_oracle 0.0001 (-0.0000) acc 62.5000 (67.3438) lr 1.0000e-05 eta 0:41:56
epoch [1/50] batch [100/288] time 0.152 (0.173) data 0.000 (0.004) loss 1.4801 (1.6126) teacher_loss 1.0901 (1.2424) loss_zs_kd 0.0001 (0.0000) loss_oracle 0.0000 (-0.0000) acc 62.5000 (68.0625) lr 1.0000e-05 eta 0:41:17
epoch [1/50] batch [120/288] time 0.178 (0.172) data 0.000 (0.003) loss 1.4391 (1.6102) teacher_loss 1.0327 (1.2361) loss_zs_kd 0.0001 (0.0000) loss_oracle 0.0000 (-0.0000) acc 75.0000 (68.1250) lr 1.0000e-05 eta 0:40:54
epoch [1/50] batch [140/288] time 0.094 (0.170) data 0.000 (0.003) loss 1.9085 (1.6008) teacher_loss 1.4852 (1.2221) loss_zs_kd 0.0001 (0.0001) loss_oracle 0.0001 (0.0000) acc 56.2500 (68.2589) lr 1.0000e-05 eta 0:40:19
epoch [1/50] batch [160/288] time 0.103 (0.165) data 0.000 (0.002) loss 1.2097 (1.5955) teacher_loss 1.0586 (1.2196) loss_zs_kd 0.0006 (0.0001) loss_oracle 0.0001 (0.0000) acc 71.8750 (68.5938) lr 1.0000e-05 eta 0:39:14
epoch [1/50] batch [180/288] time 0.124 (0.166) data 0.000 (0.002) loss 1.3086 (1.5963) teacher_loss 0.9942 (1.2231) loss_zs_kd 0.0004 (0.0001) loss_oracle 0.0001 (0.0000) acc 71.8750 (68.4375) lr 1.0000e-05 eta 0:39:19
epoch [1/50] batch [200/288] time 0.093 (0.161) data 0.000 (0.002) loss 1.7903 (1.5922) teacher_loss 1.3385 (1.2198) loss_zs_kd 0.0001 (0.0001) loss_oracle 0.0000 (0.0000) acc 62.5000 (68.3281) lr 1.0000e-05 eta 0:38:10
epoch [1/50] batch [220/288] time 0.093 (0.162) data 0.000 (0.002) loss 1.4844 (1.5851) teacher_loss 1.4154 (1.2154) loss_zs_kd 0.0004 (0.0001) loss_oracle 0.0000 (0.0000) acc 65.6250 (68.5938) lr 1.0000e-05 eta 0:38:17
epoch [1/50] batch [240/288] time 0.289 (0.163) data 0.000 (0.002) loss 1.3220 (1.5806) teacher_loss 0.9828 (1.2129) loss_zs_kd 0.0000 (0.0002) loss_oracle 0.0000 (0.0000) acc 81.2500 (68.8672) lr 1.0000e-05 eta 0:38:30
epoch [1/50] batch [260/288] time 0.095 (0.163) data 0.000 (0.002) loss 1.8035 (1.5859) teacher_loss 1.2973 (1.2183) loss_zs_kd 0.0000 (0.0002) loss_oracle 0.0000 (0.0000) acc 71.8750 (68.7380) lr 1.0000e-05 eta 0:38:24
epoch [1/50] batch [280/288] time 0.085 (0.163) data 0.000 (0.001) loss 1.7386 (1.5855) teacher_loss 1.5192 (1.2181) loss_zs_kd 0.0020 (0.0002) loss_oracle 0.0000 (0.0000) acc 65.6250 (68.8058) lr 1.0000e-05 eta 0:38:24
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,269
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 82.0%
Checkpoint saved to icml/multi-dg/oracle/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,966
* accuracy: 81.0%
* error: 19.0%
* macro_f1: 76.8%
Checkpoint saved to icml/multi-dg/oracle/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      83.0%, epoch: 1 *******
******* Domain a best val test acc: 81.0%, epoch: 1 *******
******* Domain a best test acc:     81.0%, epoch: 1 *******
epoch [2/50] batch [20/288] time 0.146 (0.174) data 0.000 (0.015) loss 1.5994 (1.7237) teacher_loss 0.9725 (1.1857) loss_zs_kd 0.0075 (0.0098) loss_oracle 0.2214 (0.1044) acc 71.8750 (68.2812) lr 2.0000e-03 eta 0:40:49
epoch [2/50] batch [40/288] time 0.149 (0.162) data 0.000 (0.008) loss 1.2788 (1.6994) teacher_loss 0.7095 (1.1129) loss_zs_kd 0.0183 (0.0138) loss_oracle 0.1751 (0.1502) acc 78.1250 (70.7031) lr 2.0000e-03 eta 0:37:59
epoch [2/50] batch [60/288] time 0.148 (0.162) data 0.000 (0.005) loss 2.2981 (1.7841) teacher_loss 1.4022 (1.1108) loss_zs_kd 0.0174 (0.0153) loss_oracle 0.4002 (0.2292) acc 75.0000 (71.5625) lr 2.0000e-03 eta 0:37:50
epoch [2/50] batch [80/288] time 0.177 (0.164) data 0.000 (0.004) loss 1.5773 (1.8177) teacher_loss 0.8080 (1.0930) loss_zs_kd 0.0115 (0.0161) loss_oracle 0.2969 (0.2715) acc 71.8750 (71.9531) lr 2.0000e-03 eta 0:38:14
epoch [2/50] batch [100/288] time 0.178 (0.164) data 0.000 (0.003) loss 2.1752 (1.8240) teacher_loss 1.3088 (1.0807) loss_zs_kd 0.0292 (0.0171) loss_oracle 0.4038 (0.2888) acc 71.8750 (72.1875) lr 2.0000e-03 eta 0:38:19
epoch [2/50] batch [120/288] time 0.146 (0.164) data 0.000 (0.003) loss 1.9740 (1.8371) teacher_loss 1.1030 (1.0733) loss_zs_kd 0.0112 (0.0177) loss_oracle 0.3907 (0.3080) acc 71.8750 (72.3177) lr 2.0000e-03 eta 0:38:16
epoch [2/50] batch [140/288] time 0.172 (0.162) data 0.000 (0.002) loss 1.8290 (1.8458) teacher_loss 1.2178 (1.0779) loss_zs_kd 0.0283 (0.0182) loss_oracle 0.3374 (0.3134) acc 71.8750 (72.2768) lr 2.0000e-03 eta 0:37:48
epoch [2/50] batch [160/288] time 0.152 (0.161) data 0.000 (0.002) loss 1.7895 (1.8502) teacher_loss 1.0794 (1.0795) loss_zs_kd 0.0471 (0.0185) loss_oracle 0.3117 (0.3128) acc 68.7500 (72.0312) lr 2.0000e-03 eta 0:37:25
epoch [2/50] batch [180/288] time 0.142 (0.160) data 0.000 (0.002) loss 1.7358 (1.8393) teacher_loss 0.9467 (1.0756) loss_zs_kd 0.0106 (0.0184) loss_oracle 0.2903 (0.3058) acc 78.1250 (72.1354) lr 2.0000e-03 eta 0:37:12
epoch [2/50] batch [200/288] time 0.167 (0.160) data 0.000 (0.002) loss 2.7072 (1.8407) teacher_loss 1.8365 (1.0790) loss_zs_kd 0.0355 (0.0189) loss_oracle 0.2928 (0.3045) acc 50.0000 (71.9219) lr 2.0000e-03 eta 0:37:04
epoch [2/50] batch [220/288] time 0.097 (0.155) data 0.000 (0.002) loss 2.0258 (1.8297) teacher_loss 1.2067 (1.0680) loss_zs_kd 0.0143 (0.0188) loss_oracle 0.3356 (0.3045) acc 71.8750 (72.0881) lr 2.0000e-03 eta 0:35:54
epoch [2/50] batch [240/288] time 0.098 (0.156) data 0.000 (0.001) loss 1.9766 (1.8246) teacher_loss 1.2605 (1.0633) loss_zs_kd 0.0236 (0.0188) loss_oracle 0.3450 (0.3052) acc 65.6250 (72.2135) lr 2.0000e-03 eta 0:36:09
epoch [2/50] batch [260/288] time 0.105 (0.156) data 0.000 (0.001) loss 1.7007 (1.8256) teacher_loss 0.9595 (1.0596) loss_zs_kd 0.0437 (0.0191) loss_oracle 0.2689 (0.3090) acc 78.1250 (72.2115) lr 2.0000e-03 eta 0:36:07
epoch [2/50] batch [280/288] time 0.375 (0.155) data 0.000 (0.001) loss 1.8462 (1.8271) teacher_loss 0.8380 (1.0559) loss_zs_kd 0.0198 (0.0193) loss_oracle 0.4774 (0.3125) acc 71.8750 (72.2879) lr 2.0000e-03 eta 0:35:45
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,395
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Checkpoint saved to icml/multi-dg/oracle/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,032
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 80.4%
Checkpoint saved to icml/multi-dg/oracle/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.2%, epoch: 2 *******
******* Domain a best val test acc: 83.7%, epoch: 2 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [3/50] batch [20/288] time 0.147 (0.135) data 0.000 (0.013) loss 2.2371 (2.0103) teacher_loss 1.4368 (1.0361) loss_zs_kd 0.0386 (0.0225) loss_oracle 0.4026 (0.4899) acc 56.2500 (73.7500) lr 1.9980e-03 eta 0:31:01
epoch [3/50] batch [40/288] time 0.151 (0.147) data 0.000 (0.007) loss 1.8822 (1.9341) teacher_loss 0.8629 (0.9732) loss_zs_kd 0.0126 (0.0202) loss_oracle 0.5039 (0.4862) acc 78.1250 (74.2969) lr 1.9980e-03 eta 0:33:45
epoch [3/50] batch [60/288] time 0.151 (0.148) data 0.000 (0.005) loss 1.8905 (2.0169) teacher_loss 0.9423 (1.0259) loss_zs_kd 0.0180 (0.0201) loss_oracle 0.5306 (0.5055) acc 75.0000 (73.0208) lr 1.9980e-03 eta 0:33:58
epoch [3/50] batch [80/288] time 0.154 (0.149) data 0.000 (0.003) loss 1.9901 (2.0222) teacher_loss 1.0182 (1.0266) loss_zs_kd 0.0220 (0.0202) loss_oracle 0.4963 (0.5078) acc 71.8750 (72.6562) lr 1.9980e-03 eta 0:34:09
epoch [3/50] batch [100/288] time 0.150 (0.150) data 0.000 (0.003) loss 1.8040 (2.0115) teacher_loss 0.9792 (1.0310) loss_zs_kd 0.0138 (0.0204) loss_oracle 0.3611 (0.4913) acc 71.8750 (72.5312) lr 1.9980e-03 eta 0:34:12
epoch [3/50] batch [120/288] time 0.160 (0.150) data 0.000 (0.002) loss 1.5743 (1.9788) teacher_loss 0.7276 (1.0176) loss_zs_kd 0.0203 (0.0208) loss_oracle 0.3421 (0.4755) acc 78.1250 (72.9427) lr 1.9980e-03 eta 0:34:16
epoch [3/50] batch [140/288] time 0.151 (0.150) data 0.000 (0.002) loss 1.8745 (1.9653) teacher_loss 1.0258 (1.0140) loss_zs_kd 0.0130 (0.0210) loss_oracle 0.4053 (0.4674) acc 71.8750 (72.8571) lr 1.9980e-03 eta 0:34:15
epoch [3/50] batch [160/288] time 0.155 (0.151) data 0.000 (0.002) loss 2.3090 (1.9583) teacher_loss 1.4996 (1.0184) loss_zs_kd 0.0359 (0.0216) loss_oracle 0.3942 (0.4569) acc 59.3750 (72.7734) lr 1.9980e-03 eta 0:34:18
epoch [3/50] batch [180/288] time 0.181 (0.152) data 0.000 (0.002) loss 1.8771 (1.9505) teacher_loss 1.1655 (1.0177) loss_zs_kd 0.0285 (0.0215) loss_oracle 0.2816 (0.4487) acc 68.7500 (72.8646) lr 1.9980e-03 eta 0:34:30
epoch [3/50] batch [200/288] time 0.148 (0.154) data 0.000 (0.002) loss 1.7533 (1.9357) teacher_loss 0.9771 (1.0133) loss_zs_kd 0.0136 (0.0216) loss_oracle 0.3435 (0.4388) acc 81.2500 (73.1406) lr 1.9980e-03 eta 0:34:55
epoch [3/50] batch [220/288] time 0.148 (0.154) data 0.000 (0.001) loss 1.6563 (1.9432) teacher_loss 0.7689 (1.0248) loss_zs_kd 0.0055 (0.0218) loss_oracle 0.3391 (0.4358) acc 78.1250 (72.9830) lr 1.9980e-03 eta 0:35:01
epoch [3/50] batch [240/288] time 0.150 (0.155) data 0.000 (0.001) loss 2.4629 (1.9429) teacher_loss 1.5320 (1.0320) loss_zs_kd 0.0184 (0.0218) loss_oracle 0.4186 (0.4302) acc 59.3750 (72.8255) lr 1.9980e-03 eta 0:35:07
epoch [3/50] batch [260/288] time 0.153 (0.156) data 0.000 (0.001) loss 1.3045 (1.9278) teacher_loss 0.6064 (1.0273) loss_zs_kd 0.0281 (0.0217) loss_oracle 0.3110 (0.4231) acc 84.3750 (72.9567) lr 1.9980e-03 eta 0:35:10
epoch [3/50] batch [280/288] time 0.175 (0.155) data 0.000 (0.001) loss 1.8692 (1.9165) teacher_loss 0.9653 (1.0226) loss_zs_kd 0.0517 (0.0221) loss_oracle 0.3525 (0.4175) acc 68.7500 (73.0134) lr 1.9980e-03 eta 0:35:04
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,408
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/oracle/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.9%
******* Domain a best val acc:      86.5%, epoch: 3 *******
******* Domain a best val test acc: 83.3%, epoch: 3 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [4/50] batch [20/288] time 0.228 (0.186) data 0.000 (0.014) loss 1.6524 (1.8289) teacher_loss 0.7443 (0.9619) loss_zs_kd 0.0320 (0.0222) loss_oracle 0.4469 (0.3977) acc 81.2500 (74.2188) lr 1.9921e-03 eta 0:41:54
epoch [4/50] batch [40/288] time 0.084 (0.169) data 0.000 (0.007) loss 1.8247 (1.8724) teacher_loss 1.0837 (0.9876) loss_zs_kd 0.0398 (0.0216) loss_oracle 0.3425 (0.4131) acc 75.0000 (74.2188) lr 1.9921e-03 eta 0:38:05
epoch [4/50] batch [60/288] time 0.378 (0.164) data 0.000 (0.005) loss 2.1276 (1.8449) teacher_loss 1.2798 (0.9827) loss_zs_kd 0.0344 (0.0225) loss_oracle 0.3176 (0.3880) acc 65.6250 (73.7500) lr 1.9921e-03 eta 0:36:51
epoch [4/50] batch [80/288] time 0.127 (0.161) data 0.000 (0.004) loss 2.1094 (1.8516) teacher_loss 1.3866 (0.9989) loss_zs_kd 0.0130 (0.0221) loss_oracle 0.3359 (0.3822) acc 68.7500 (73.7109) lr 1.9921e-03 eta 0:36:12
epoch [4/50] batch [100/288] time 0.148 (0.159) data 0.000 (0.003) loss 1.6645 (1.8639) teacher_loss 0.8627 (1.0136) loss_zs_kd 0.0062 (0.0219) loss_oracle 0.3318 (0.3812) acc 81.2500 (73.4375) lr 1.9921e-03 eta 0:35:35
epoch [4/50] batch [120/288] time 0.166 (0.159) data 0.000 (0.003) loss 1.9364 (1.8698) teacher_loss 0.8901 (1.0189) loss_zs_kd 0.0210 (0.0217) loss_oracle 0.3680 (0.3798) acc 81.2500 (73.2552) lr 1.9921e-03 eta 0:35:27
epoch [4/50] batch [140/288] time 0.149 (0.159) data 0.000 (0.002) loss 1.6479 (1.8791) teacher_loss 0.7643 (1.0232) loss_zs_kd 0.0210 (0.0212) loss_oracle 0.3456 (0.3771) acc 81.2500 (73.1027) lr 1.9921e-03 eta 0:35:23
epoch [4/50] batch [160/288] time 0.159 (0.158) data 0.000 (0.002) loss 1.9344 (1.8686) teacher_loss 1.2332 (1.0149) loss_zs_kd 0.0217 (0.0216) loss_oracle 0.2872 (0.3759) acc 71.8750 (73.1641) lr 1.9921e-03 eta 0:35:18
epoch [4/50] batch [180/288] time 0.153 (0.159) data 0.000 (0.002) loss 2.2576 (1.8666) teacher_loss 1.4230 (1.0103) loss_zs_kd 0.0283 (0.0218) loss_oracle 0.3835 (0.3786) acc 62.5000 (73.2465) lr 1.9921e-03 eta 0:35:22
epoch [4/50] batch [200/288] time 0.151 (0.159) data 0.000 (0.002) loss 2.1063 (1.8663) teacher_loss 1.0993 (1.0072) loss_zs_kd 0.0109 (0.0222) loss_oracle 0.4282 (0.3813) acc 71.8750 (73.3438) lr 1.9921e-03 eta 0:35:15
epoch [4/50] batch [220/288] time 0.152 (0.158) data 0.000 (0.001) loss 2.1237 (1.8654) teacher_loss 1.2443 (1.0064) loss_zs_kd 0.0234 (0.0226) loss_oracle 0.3661 (0.3817) acc 62.5000 (73.3239) lr 1.9921e-03 eta 0:35:05
epoch [4/50] batch [240/288] time 0.148 (0.159) data 0.000 (0.001) loss 2.1779 (1.8640) teacher_loss 1.1605 (1.0030) loss_zs_kd 0.0199 (0.0225) loss_oracle 0.3970 (0.3835) acc 75.0000 (73.4505) lr 1.9921e-03 eta 0:35:14
epoch [4/50] batch [260/288] time 0.152 (0.159) data 0.000 (0.001) loss 1.5959 (1.8688) teacher_loss 0.7446 (1.0082) loss_zs_kd 0.0252 (0.0225) loss_oracle 0.3627 (0.3827) acc 71.8750 (73.3413) lr 1.9921e-03 eta 0:35:07
epoch [4/50] batch [280/288] time 0.150 (0.158) data 0.000 (0.001) loss 1.6272 (1.8695) teacher_loss 0.6676 (1.0024) loss_zs_kd 0.0191 (0.0228) loss_oracle 0.4090 (0.3872) acc 81.2500 (73.6384) lr 1.9921e-03 eta 0:34:58
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,397
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,014
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.5%
******* Domain a best val acc:      86.5%, epoch: 3 *******
******* Domain a best val test acc: 83.3%, epoch: 3 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [5/50] batch [20/288] time 0.093 (0.158) data 0.000 (0.013) loss 2.2614 (2.0572) teacher_loss 1.3677 (1.1269) loss_zs_kd 0.0312 (0.0265) loss_oracle 0.3627 (0.4316) acc 68.7500 (70.3125) lr 1.9823e-03 eta 0:34:46
epoch [5/50] batch [40/288] time 0.084 (0.137) data 0.000 (0.006) loss 1.4823 (1.9696) teacher_loss 0.4635 (1.0428) loss_zs_kd 0.0296 (0.0262) loss_oracle 0.4533 (0.4230) acc 93.7500 (72.9688) lr 1.9823e-03 eta 0:30:13
epoch [5/50] batch [60/288] time 0.101 (0.140) data 0.000 (0.004) loss 1.9474 (1.9351) teacher_loss 1.2101 (1.0023) loss_zs_kd 0.0109 (0.0248) loss_oracle 0.4050 (0.4226) acc 59.3750 (73.7500) lr 1.9823e-03 eta 0:30:40
epoch [5/50] batch [80/288] time 0.091 (0.141) data 0.000 (0.003) loss 2.5100 (1.9219) teacher_loss 1.4394 (0.9856) loss_zs_kd 0.0222 (0.0252) loss_oracle 0.5783 (0.4310) acc 71.8750 (73.9062) lr 1.9823e-03 eta 0:30:54
epoch [5/50] batch [100/288] time 0.105 (0.135) data 0.000 (0.003) loss 1.5352 (1.9325) teacher_loss 0.6603 (0.9962) loss_zs_kd 0.0241 (0.0262) loss_oracle 0.4035 (0.4388) acc 84.3750 (73.6875) lr 1.9823e-03 eta 0:29:30
epoch [5/50] batch [120/288] time 0.091 (0.138) data 0.000 (0.002) loss 2.0236 (1.9176) teacher_loss 1.0588 (0.9847) loss_zs_kd 0.0361 (0.0263) loss_oracle 0.4467 (0.4388) acc 75.0000 (74.0365) lr 1.9823e-03 eta 0:30:07
epoch [5/50] batch [140/288] time 0.109 (0.143) data 0.000 (0.002) loss 1.7435 (1.9237) teacher_loss 1.0369 (0.9886) loss_zs_kd 0.0312 (0.0266) loss_oracle 0.3721 (0.4402) acc 75.0000 (74.0402) lr 1.9823e-03 eta 0:31:09
epoch [5/50] batch [160/288] time 0.302 (0.146) data 0.000 (0.002) loss 2.2057 (1.9245) teacher_loss 1.2267 (0.9888) loss_zs_kd 0.0249 (0.0262) loss_oracle 0.4518 (0.4426) acc 65.6250 (73.7891) lr 1.9823e-03 eta 0:31:53
epoch [5/50] batch [180/288] time 0.085 (0.148) data 0.000 (0.002) loss 1.7769 (1.9289) teacher_loss 0.8800 (0.9948) loss_zs_kd 0.0337 (0.0261) loss_oracle 0.4006 (0.4386) acc 75.0000 (73.5243) lr 1.9823e-03 eta 0:32:17
epoch [5/50] batch [200/288] time 0.150 (0.147) data 0.000 (0.001) loss 1.7917 (1.9247) teacher_loss 0.9201 (0.9964) loss_zs_kd 0.0198 (0.0258) loss_oracle 0.4182 (0.4322) acc 78.1250 (73.7500) lr 1.9823e-03 eta 0:32:03
epoch [5/50] batch [220/288] time 0.151 (0.148) data 0.000 (0.001) loss 2.2121 (1.9275) teacher_loss 1.1992 (1.0017) loss_zs_kd 0.0294 (0.0258) loss_oracle 0.4224 (0.4298) acc 75.0000 (73.5369) lr 1.9823e-03 eta 0:32:08
epoch [5/50] batch [240/288] time 0.146 (0.148) data 0.000 (0.001) loss 1.6438 (1.9218) teacher_loss 0.7963 (0.9988) loss_zs_kd 0.0255 (0.0256) loss_oracle 0.3995 (0.4299) acc 84.3750 (73.4896) lr 1.9823e-03 eta 0:32:08
epoch [5/50] batch [260/288] time 0.150 (0.149) data 0.000 (0.001) loss 2.0137 (1.9162) teacher_loss 1.0125 (0.9946) loss_zs_kd 0.0154 (0.0255) loss_oracle 0.4366 (0.4282) acc 65.6250 (73.5337) lr 1.9823e-03 eta 0:32:09
epoch [5/50] batch [280/288] time 0.147 (0.149) data 0.000 (0.001) loss 1.7199 (1.9083) teacher_loss 0.7569 (0.9858) loss_zs_kd 0.0327 (0.0255) loss_oracle 0.4682 (0.4308) acc 78.1250 (73.6942) lr 1.9823e-03 eta 0:32:08
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Checkpoint saved to icml/multi-dg/oracle/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,028
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.4%
******* Domain a best val acc:      86.6%, epoch: 5 *******
******* Domain a best val test acc: 83.6%, epoch: 5 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [6/50] batch [20/288] time 0.156 (0.168) data 0.000 (0.012) loss 2.0444 (1.8653) teacher_loss 0.9857 (0.8762) loss_zs_kd 0.0179 (0.0232) loss_oracle 0.5394 (0.5031) acc 71.8750 (74.2188) lr 1.9686e-03 eta 0:36:18
epoch [6/50] batch [40/288] time 0.171 (0.168) data 0.000 (0.006) loss 2.0874 (1.9183) teacher_loss 0.9807 (0.9400) loss_zs_kd 0.0318 (0.0235) loss_oracle 0.4929 (0.4866) acc 75.0000 (73.5938) lr 1.9686e-03 eta 0:36:09
epoch [6/50] batch [60/288] time 0.155 (0.164) data 0.000 (0.004) loss 1.9728 (1.9181) teacher_loss 0.9627 (0.9305) loss_zs_kd 0.0343 (0.0240) loss_oracle 0.4961 (0.4969) acc 71.8750 (73.7500) lr 1.9686e-03 eta 0:35:13
epoch [6/50] batch [80/288] time 0.153 (0.162) data 0.000 (0.003) loss 1.7880 (1.9158) teacher_loss 0.9395 (0.9408) loss_zs_kd 0.0452 (0.0248) loss_oracle 0.4055 (0.4854) acc 71.8750 (73.6719) lr 1.9686e-03 eta 0:34:50
epoch [6/50] batch [100/288] time 0.147 (0.164) data 0.000 (0.003) loss 2.2036 (1.9246) teacher_loss 1.3033 (0.9555) loss_zs_kd 0.0224 (0.0248) loss_oracle 0.3648 (0.4726) acc 65.6250 (73.8438) lr 1.9686e-03 eta 0:35:08
epoch [6/50] batch [120/288] time 0.117 (0.163) data 0.000 (0.002) loss 2.0105 (1.9100) teacher_loss 1.1061 (0.9522) loss_zs_kd 0.0423 (0.0248) loss_oracle 0.4203 (0.4639) acc 78.1250 (74.2448) lr 1.9686e-03 eta 0:34:50
epoch [6/50] batch [140/288] time 0.409 (0.156) data 0.000 (0.002) loss 1.6457 (1.8937) teacher_loss 0.7601 (0.9489) loss_zs_kd 0.0246 (0.0247) loss_oracle 0.4528 (0.4587) acc 84.3750 (74.3750) lr 1.9686e-03 eta 0:33:26
epoch [6/50] batch [160/288] time 0.302 (0.155) data 0.000 (0.002) loss 1.5006 (1.8908) teacher_loss 0.5671 (0.9472) loss_zs_kd 0.0275 (0.0253) loss_oracle 0.5002 (0.4550) acc 78.1250 (74.3750) lr 1.9686e-03 eta 0:33:05
epoch [6/50] batch [180/288] time 0.094 (0.154) data 0.000 (0.002) loss 1.8238 (1.8948) teacher_loss 0.7786 (0.9508) loss_zs_kd 0.0412 (0.0258) loss_oracle 0.5262 (0.4568) acc 78.1250 (74.2708) lr 1.9686e-03 eta 0:32:48
epoch [6/50] batch [200/288] time 0.092 (0.149) data 0.000 (0.001) loss 1.6571 (1.8999) teacher_loss 0.9298 (0.9569) loss_zs_kd 0.0203 (0.0257) loss_oracle 0.3978 (0.4588) acc 68.7500 (74.2344) lr 1.9686e-03 eta 0:31:45
epoch [6/50] batch [220/288] time 0.085 (0.149) data 0.000 (0.001) loss 2.0685 (1.9025) teacher_loss 1.1655 (0.9594) loss_zs_kd 0.0468 (0.0261) loss_oracle 0.4425 (0.4585) acc 78.1250 (74.2188) lr 1.9686e-03 eta 0:31:35
epoch [6/50] batch [240/288] time 0.109 (0.150) data 0.000 (0.001) loss 1.6948 (1.9064) teacher_loss 0.9566 (0.9640) loss_zs_kd 0.0102 (0.0260) loss_oracle 0.3909 (0.4561) acc 71.8750 (74.0885) lr 1.9686e-03 eta 0:31:54
epoch [6/50] batch [260/288] time 0.106 (0.152) data 0.000 (0.001) loss 2.2596 (1.9142) teacher_loss 1.2726 (0.9663) loss_zs_kd 0.0171 (0.0263) loss_oracle 0.4423 (0.4556) acc 71.8750 (74.0385) lr 1.9686e-03 eta 0:32:07
epoch [6/50] batch [280/288] time 0.374 (0.154) data 0.000 (0.001) loss 2.1129 (1.9152) teacher_loss 1.2033 (0.9680) loss_zs_kd 0.0628 (0.0266) loss_oracle 0.4268 (0.4528) acc 75.0000 (74.0513) lr 1.9686e-03 eta 0:32:28
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,419
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.2%
Checkpoint saved to icml/multi-dg/oracle/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,026
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.1%
******* Domain a best val acc:      86.8%, epoch: 6 *******
******* Domain a best val test acc: 83.5%, epoch: 6 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [7/50] batch [20/288] time 0.150 (0.185) data 0.000 (0.015) loss 2.2023 (1.9492) teacher_loss 1.2063 (1.0221) loss_zs_kd 0.0225 (0.0274) loss_oracle 0.4005 (0.3988) acc 68.7500 (73.9062) lr 1.9511e-03 eta 0:39:05
epoch [7/50] batch [40/288] time 0.177 (0.175) data 0.000 (0.008) loss 1.6887 (1.9054) teacher_loss 0.6642 (0.9557) loss_zs_kd 0.0080 (0.0255) loss_oracle 0.4243 (0.4142) acc 87.5000 (75.2344) lr 1.9511e-03 eta 0:36:53
epoch [7/50] batch [60/288] time 0.154 (0.169) data 0.000 (0.005) loss 2.0750 (1.8906) teacher_loss 1.1430 (0.9329) loss_zs_kd 0.0393 (0.0259) loss_oracle 0.3964 (0.4201) acc 68.7500 (75.2083) lr 1.9511e-03 eta 0:35:35
epoch [7/50] batch [80/288] time 0.155 (0.165) data 0.000 (0.004) loss 1.7937 (1.8861) teacher_loss 0.8413 (0.9335) loss_zs_kd 0.0287 (0.0261) loss_oracle 0.4329 (0.4207) acc 75.0000 (74.8438) lr 1.9511e-03 eta 0:34:39
epoch [7/50] batch [100/288] time 0.152 (0.162) data 0.000 (0.003) loss 1.6369 (1.8867) teacher_loss 0.6352 (0.9332) loss_zs_kd 0.0156 (0.0260) loss_oracle 0.4459 (0.4232) acc 81.2500 (74.5938) lr 1.9511e-03 eta 0:34:00
epoch [7/50] batch [120/288] time 0.174 (0.162) data 0.000 (0.003) loss 2.4457 (1.8983) teacher_loss 1.3255 (0.9306) loss_zs_kd 0.0230 (0.0257) loss_oracle 0.5413 (0.4330) acc 68.7500 (74.6094) lr 1.9511e-03 eta 0:33:48
epoch [7/50] batch [140/288] time 0.147 (0.162) data 0.000 (0.002) loss 2.0760 (1.9190) teacher_loss 1.0191 (0.9437) loss_zs_kd 0.0212 (0.0261) loss_oracle 0.4652 (0.4417) acc 71.8750 (74.2634) lr 1.9511e-03 eta 0:33:48
epoch [7/50] batch [160/288] time 0.148 (0.161) data 0.000 (0.002) loss 2.0026 (1.9346) teacher_loss 0.9539 (0.9600) loss_zs_kd 0.0233 (0.0261) loss_oracle 0.4377 (0.4406) acc 75.0000 (73.9453) lr 1.9511e-03 eta 0:33:30
epoch [7/50] batch [180/288] time 0.153 (0.160) data 0.000 (0.002) loss 1.9407 (1.9346) teacher_loss 1.0251 (0.9649) loss_zs_kd 0.0293 (0.0259) loss_oracle 0.4382 (0.4368) acc 78.1250 (73.9931) lr 1.9511e-03 eta 0:33:16
epoch [7/50] batch [200/288] time 0.154 (0.160) data 0.000 (0.002) loss 1.4985 (1.9239) teacher_loss 0.6609 (0.9557) loss_zs_kd 0.0251 (0.0263) loss_oracle 0.3596 (0.4355) acc 75.0000 (74.1562) lr 1.9511e-03 eta 0:33:10
epoch [7/50] batch [220/288] time 0.153 (0.160) data 0.000 (0.002) loss 2.1659 (1.9236) teacher_loss 1.1626 (0.9568) loss_zs_kd 0.0209 (0.0262) loss_oracle 0.4717 (0.4346) acc 68.7500 (74.0341) lr 1.9511e-03 eta 0:33:12
epoch [7/50] batch [240/288] time 0.094 (0.155) data 0.000 (0.001) loss 2.3454 (1.9281) teacher_loss 1.3053 (0.9628) loss_zs_kd 0.0286 (0.0264) loss_oracle 0.4994 (0.4349) acc 62.5000 (73.9714) lr 1.9511e-03 eta 0:32:01
epoch [7/50] batch [260/288] time 0.099 (0.156) data 0.000 (0.001) loss 1.9465 (1.9323) teacher_loss 1.0002 (0.9645) loss_zs_kd 0.0145 (0.0264) loss_oracle 0.4929 (0.4386) acc 71.8750 (73.9663) lr 1.9511e-03 eta 0:32:11
epoch [7/50] batch [280/288] time 0.081 (0.155) data 0.000 (0.001) loss 1.5539 (1.9283) teacher_loss 0.6485 (0.9583) loss_zs_kd 0.0446 (0.0262) loss_oracle 0.4489 (0.4410) acc 84.3750 (74.2076) lr 1.9511e-03 eta 0:32:04
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,419
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.1%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,011
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.4%
******* Domain a best val acc:      86.8%, epoch: 6 *******
******* Domain a best val test acc: 83.5%, epoch: 6 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [8/50] batch [20/288] time 0.366 (0.178) data 0.000 (0.013) loss 1.9092 (1.9200) teacher_loss 0.8410 (0.9360) loss_zs_kd 0.0467 (0.0295) loss_oracle 0.5619 (0.4638) acc 81.2500 (74.6875) lr 1.9298e-03 eta 0:36:44
epoch [8/50] batch [40/288] time 0.093 (0.165) data 0.000 (0.007) loss 1.7127 (1.8962) teacher_loss 0.8191 (0.9313) loss_zs_kd 0.0356 (0.0268) loss_oracle 0.4404 (0.4621) acc 78.1250 (74.4531) lr 1.9298e-03 eta 0:33:59
epoch [8/50] batch [60/288] time 0.144 (0.159) data 0.000 (0.005) loss 2.6574 (1.8813) teacher_loss 1.4800 (0.9142) loss_zs_kd 0.0290 (0.0266) loss_oracle 0.5418 (0.4594) acc 65.6250 (74.5833) lr 1.9298e-03 eta 0:32:41
epoch [8/50] batch [80/288] time 0.148 (0.157) data 0.000 (0.004) loss 2.1638 (1.8637) teacher_loss 1.2094 (0.9054) loss_zs_kd 0.0151 (0.0270) loss_oracle 0.4392 (0.4571) acc 68.7500 (75.0391) lr 1.9298e-03 eta 0:32:14
epoch [8/50] batch [100/288] time 0.152 (0.156) data 0.000 (0.003) loss 1.6645 (1.8623) teacher_loss 0.7501 (0.9135) loss_zs_kd 0.0293 (0.0270) loss_oracle 0.4628 (0.4523) acc 81.2500 (75.1875) lr 1.9298e-03 eta 0:31:53
epoch [8/50] batch [120/288] time 0.153 (0.155) data 0.000 (0.002) loss 1.8381 (1.8411) teacher_loss 0.8581 (0.9044) loss_zs_kd 0.0331 (0.0275) loss_oracle 0.3656 (0.4429) acc 75.0000 (75.5729) lr 1.9298e-03 eta 0:31:40
epoch [8/50] batch [140/288] time 0.147 (0.154) data 0.000 (0.002) loss 1.5783 (1.8460) teacher_loss 0.8359 (0.9139) loss_zs_kd 0.0505 (0.0282) loss_oracle 0.4277 (0.4362) acc 71.8750 (75.3348) lr 1.9298e-03 eta 0:31:26
epoch [8/50] batch [160/288] time 0.154 (0.154) data 0.000 (0.002) loss 1.8026 (1.8597) teacher_loss 1.0403 (0.9302) loss_zs_kd 0.0499 (0.0284) loss_oracle 0.3520 (0.4347) acc 68.7500 (74.9414) lr 1.9298e-03 eta 0:31:17
epoch [8/50] batch [180/288] time 0.153 (0.153) data 0.000 (0.002) loss 2.0412 (1.8560) teacher_loss 1.3528 (0.9336) loss_zs_kd 0.0146 (0.0282) loss_oracle 0.3324 (0.4295) acc 68.7500 (74.9306) lr 1.9298e-03 eta 0:31:10
epoch [8/50] batch [200/288] time 0.150 (0.153) data 0.000 (0.002) loss 2.2665 (1.8547) teacher_loss 1.2649 (0.9381) loss_zs_kd 0.0188 (0.0279) loss_oracle 0.4563 (0.4238) acc 65.6250 (74.7031) lr 1.9298e-03 eta 0:31:05
epoch [8/50] batch [220/288] time 0.153 (0.153) data 0.000 (0.001) loss 1.5020 (1.8468) teacher_loss 0.6816 (0.9332) loss_zs_kd 0.0182 (0.0279) loss_oracle 0.4018 (0.4205) acc 84.3750 (74.9148) lr 1.9298e-03 eta 0:31:00
epoch [8/50] batch [240/288] time 0.155 (0.153) data 0.000 (0.001) loss 1.6444 (1.8477) teacher_loss 0.8589 (0.9385) loss_zs_kd 0.0323 (0.0281) loss_oracle 0.3757 (0.4174) acc 75.0000 (74.6224) lr 1.9298e-03 eta 0:30:55
epoch [8/50] batch [260/288] time 0.151 (0.153) data 0.000 (0.001) loss 1.7761 (1.8462) teacher_loss 0.9062 (0.9383) loss_zs_kd 0.0322 (0.0280) loss_oracle 0.4526 (0.4149) acc 75.0000 (74.7356) lr 1.9298e-03 eta 0:30:50
epoch [8/50] batch [280/288] time 0.150 (0.152) data 0.000 (0.001) loss 2.1579 (1.8497) teacher_loss 1.2498 (0.9442) loss_zs_kd 0.0433 (0.0277) loss_oracle 0.4132 (0.4135) acc 62.5000 (74.5759) lr 1.9298e-03 eta 0:30:44
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,430
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.4%
Checkpoint saved to icml/multi-dg/oracle/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,001
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 78.7%
******* Domain a best val acc:      87.1%, epoch: 8 *******
******* Domain a best val test acc: 82.4%, epoch: 8 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [9/50] batch [20/288] time 0.128 (0.136) data 0.000 (0.013) loss 1.8400 (1.8194) teacher_loss 1.0459 (0.9431) loss_zs_kd 0.0355 (0.0276) loss_oracle 0.3929 (0.3830) acc 71.8750 (75.1562) lr 1.9048e-03 eta 0:27:20
epoch [9/50] batch [40/288] time 0.226 (0.161) data 0.000 (0.007) loss 1.3982 (1.7687) teacher_loss 0.6446 (0.9039) loss_zs_kd 0.0179 (0.0259) loss_oracle 0.3735 (0.3725) acc 84.3750 (76.1719) lr 1.9048e-03 eta 0:32:21
epoch [9/50] batch [60/288] time 0.103 (0.157) data 0.001 (0.005) loss 1.9009 (1.7601) teacher_loss 1.0640 (0.9086) loss_zs_kd 0.0242 (0.0265) loss_oracle 0.3656 (0.3643) acc 71.8750 (76.2500) lr 1.9048e-03 eta 0:31:26
epoch [9/50] batch [80/288] time 0.116 (0.150) data 0.000 (0.003) loss 1.6486 (1.8073) teacher_loss 0.7537 (0.9421) loss_zs_kd 0.0143 (0.0267) loss_oracle 0.4199 (0.3772) acc 84.3750 (75.3516) lr 1.9048e-03 eta 0:30:01
epoch [9/50] batch [100/288] time 0.087 (0.156) data 0.000 (0.003) loss 1.7419 (1.8162) teacher_loss 0.8327 (0.9424) loss_zs_kd 0.0156 (0.0264) loss_oracle 0.4221 (0.3870) acc 71.8750 (75.2188) lr 1.9048e-03 eta 0:31:13
epoch [9/50] batch [120/288] time 0.366 (0.158) data 0.000 (0.002) loss 1.6938 (1.8314) teacher_loss 0.9665 (0.9526) loss_zs_kd 0.0330 (0.0272) loss_oracle 0.3466 (0.3938) acc 75.0000 (75.0000) lr 1.9048e-03 eta 0:31:30
epoch [9/50] batch [140/288] time 0.086 (0.156) data 0.000 (0.002) loss 1.7255 (1.8420) teacher_loss 0.9332 (0.9586) loss_zs_kd 0.0181 (0.0274) loss_oracle 0.3605 (0.3985) acc 81.2500 (75.1562) lr 1.9048e-03 eta 0:31:01
epoch [9/50] batch [160/288] time 0.092 (0.156) data 0.000 (0.002) loss 1.9184 (1.8439) teacher_loss 0.7851 (0.9530) loss_zs_kd 0.0212 (0.0277) loss_oracle 0.4551 (0.4037) acc 81.2500 (75.1367) lr 1.9048e-03 eta 0:30:57
epoch [9/50] batch [180/288] time 0.152 (0.154) data 0.000 (0.002) loss 2.4014 (1.8461) teacher_loss 1.6196 (0.9554) loss_zs_kd 0.0374 (0.0274) loss_oracle 0.4048 (0.4082) acc 65.6250 (75.0868) lr 1.9048e-03 eta 0:30:40
epoch [9/50] batch [200/288] time 0.153 (0.156) data 0.000 (0.002) loss 2.1062 (1.8497) teacher_loss 1.2848 (0.9634) loss_zs_kd 0.0131 (0.0277) loss_oracle 0.4689 (0.4101) acc 68.7500 (74.8906) lr 1.9048e-03 eta 0:30:50
epoch [9/50] batch [220/288] time 0.153 (0.156) data 0.000 (0.001) loss 1.6015 (1.8506) teacher_loss 0.6550 (0.9634) loss_zs_kd 0.0346 (0.0278) loss_oracle 0.4237 (0.4138) acc 78.1250 (74.7585) lr 1.9048e-03 eta 0:30:54
epoch [9/50] batch [240/288] time 0.187 (0.157) data 0.000 (0.001) loss 1.7403 (1.8506) teacher_loss 0.9192 (0.9614) loss_zs_kd 0.0491 (0.0278) loss_oracle 0.4700 (0.4173) acc 78.1250 (74.8177) lr 1.9048e-03 eta 0:31:03
epoch [9/50] batch [260/288] time 0.153 (0.158) data 0.000 (0.001) loss 1.7084 (1.8522) teacher_loss 0.8489 (0.9650) loss_zs_kd 0.0149 (0.0279) loss_oracle 0.4361 (0.4178) acc 71.8750 (74.5793) lr 1.9048e-03 eta 0:31:10
epoch [9/50] batch [280/288] time 0.169 (0.159) data 0.000 (0.001) loss 1.6870 (1.8434) teacher_loss 0.8972 (0.9590) loss_zs_kd 0.0077 (0.0278) loss_oracle 0.3733 (0.4158) acc 75.0000 (74.6652) lr 1.9048e-03 eta 0:31:14
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,426
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,016
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.6%
******* Domain a best val acc:      87.1%, epoch: 8 *******
******* Domain a best val test acc: 82.4%, epoch: 8 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [10/50] batch [20/288] time 0.170 (0.181) data 0.000 (0.013) loss 1.7697 (1.8280) teacher_loss 0.7443 (0.9612) loss_zs_kd 0.0130 (0.0241) loss_oracle 0.4358 (0.4206) acc 81.2500 (75.3125) lr 1.8763e-03 eta 0:35:31
epoch [10/50] batch [40/288] time 0.170 (0.171) data 0.000 (0.007) loss 2.2033 (1.8060) teacher_loss 1.2724 (0.9241) loss_zs_kd 0.0414 (0.0271) loss_oracle 0.3804 (0.4264) acc 59.3750 (76.0938) lr 1.8763e-03 eta 0:33:29
epoch [10/50] batch [60/288] time 0.083 (0.161) data 0.000 (0.005) loss 1.6376 (1.7874) teacher_loss 0.8807 (0.8965) loss_zs_kd 0.0262 (0.0271) loss_oracle 0.3085 (0.4200) acc 75.0000 (76.3542) lr 1.8763e-03 eta 0:31:30
epoch [10/50] batch [80/288] time 0.089 (0.156) data 0.000 (0.004) loss 2.0457 (1.8110) teacher_loss 1.1125 (0.9171) loss_zs_kd 0.0162 (0.0282) loss_oracle 0.4166 (0.4143) acc 78.1250 (75.6641) lr 1.8763e-03 eta 0:30:24
epoch [10/50] batch [100/288] time 0.085 (0.158) data 0.000 (0.003) loss 2.2008 (1.8006) teacher_loss 1.3181 (0.9088) loss_zs_kd 0.0430 (0.0274) loss_oracle 0.3906 (0.4127) acc 75.0000 (75.9375) lr 1.8763e-03 eta 0:30:51
epoch [10/50] batch [120/288] time 0.098 (0.150) data 0.000 (0.002) loss 1.7646 (1.8101) teacher_loss 0.8153 (0.9060) loss_zs_kd 0.0192 (0.0274) loss_oracle 0.4554 (0.4145) acc 81.2500 (76.1458) lr 1.8763e-03 eta 0:29:08
epoch [10/50] batch [140/288] time 0.091 (0.149) data 0.000 (0.002) loss 2.0838 (1.8305) teacher_loss 1.1864 (0.9212) loss_zs_kd 0.0199 (0.0277) loss_oracle 0.4092 (0.4165) acc 71.8750 (75.6920) lr 1.8763e-03 eta 0:29:01
epoch [10/50] batch [160/288] time 0.091 (0.151) data 0.000 (0.002) loss 1.3657 (1.8342) teacher_loss 0.5514 (0.9206) loss_zs_kd 0.0188 (0.0277) loss_oracle 0.4343 (0.4175) acc 90.6250 (75.6641) lr 1.8763e-03 eta 0:29:22
epoch [10/50] batch [180/288] time 0.096 (0.153) data 0.000 (0.002) loss 1.8651 (1.8476) teacher_loss 1.0504 (0.9340) loss_zs_kd 0.0229 (0.0280) loss_oracle 0.3528 (0.4150) acc 75.0000 (75.3819) lr 1.8763e-03 eta 0:29:41
epoch [10/50] batch [200/288] time 0.330 (0.156) data 0.000 (0.002) loss 2.2295 (1.8528) teacher_loss 1.1953 (0.9395) loss_zs_kd 0.0169 (0.0276) loss_oracle 0.3365 (0.4113) acc 65.6250 (75.2344) lr 1.8763e-03 eta 0:30:12
epoch [10/50] batch [220/288] time 0.149 (0.153) data 0.000 (0.001) loss 1.7489 (1.8519) teacher_loss 1.0453 (0.9410) loss_zs_kd 0.0309 (0.0279) loss_oracle 0.3235 (0.4079) acc 68.7500 (75.2699) lr 1.8763e-03 eta 0:29:37
epoch [10/50] batch [240/288] time 0.155 (0.153) data 0.000 (0.001) loss 1.7084 (1.8527) teacher_loss 0.8557 (0.9446) loss_zs_kd 0.0309 (0.0277) loss_oracle 0.3779 (0.4033) acc 78.1250 (75.0911) lr 1.8763e-03 eta 0:29:30
epoch [10/50] batch [260/288] time 0.160 (0.153) data 0.000 (0.001) loss 1.6560 (1.8455) teacher_loss 0.9825 (0.9427) loss_zs_kd 0.0208 (0.0280) loss_oracle 0.4072 (0.3993) acc 75.0000 (75.1803) lr 1.8763e-03 eta 0:29:26
epoch [10/50] batch [280/288] time 0.151 (0.153) data 0.000 (0.001) loss 2.4991 (1.8533) teacher_loss 1.5105 (0.9504) loss_zs_kd 0.0497 (0.0284) loss_oracle 0.4409 (0.3991) acc 59.3750 (75.0112) lr 1.8763e-03 eta 0:29:23
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,427
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,979
* accuracy: 81.5%
* error: 18.5%
* macro_f1: 78.4%
******* Domain a best val acc:      87.1%, epoch: 8 *******
******* Domain a best val test acc: 82.4%, epoch: 8 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [11/50] batch [20/288] time 0.153 (0.168) data 0.000 (0.014) loss 2.3948 (1.8780) teacher_loss 1.3772 (0.9615) loss_zs_kd 0.0294 (0.0286) loss_oracle 0.4934 (0.4319) acc 62.5000 (73.7500) lr 1.8443e-03 eta 0:32:09
epoch [11/50] batch [40/288] time 0.147 (0.159) data 0.000 (0.007) loss 1.9800 (1.8879) teacher_loss 1.0208 (0.9556) loss_zs_kd 0.0253 (0.0288) loss_oracle 0.4785 (0.4293) acc 68.7500 (73.0469) lr 1.8443e-03 eta 0:30:27
epoch [11/50] batch [60/288] time 0.170 (0.157) data 0.000 (0.005) loss 1.5832 (1.8991) teacher_loss 0.6888 (0.9578) loss_zs_kd 0.0404 (0.0282) loss_oracle 0.5027 (0.4461) acc 84.3750 (73.4375) lr 1.8443e-03 eta 0:29:59
epoch [11/50] batch [80/288] time 0.156 (0.156) data 0.000 (0.004) loss 2.1196 (1.8909) teacher_loss 1.1809 (0.9358) loss_zs_kd 0.0294 (0.0281) loss_oracle 0.4598 (0.4596) acc 65.6250 (74.3359) lr 1.8443e-03 eta 0:29:46
epoch [11/50] batch [100/288] time 0.154 (0.156) data 0.000 (0.003) loss 1.9948 (1.8731) teacher_loss 1.0521 (0.9182) loss_zs_kd 0.0436 (0.0285) loss_oracle 0.5194 (0.4647) acc 78.1250 (75.0000) lr 1.8443e-03 eta 0:29:36
epoch [11/50] batch [120/288] time 0.147 (0.155) data 0.000 (0.002) loss 2.2570 (1.8686) teacher_loss 1.3419 (0.9180) loss_zs_kd 0.0327 (0.0286) loss_oracle 0.5079 (0.4681) acc 56.2500 (74.8438) lr 1.8443e-03 eta 0:29:25
epoch [11/50] batch [140/288] time 0.150 (0.155) data 0.000 (0.002) loss 1.7487 (1.8627) teacher_loss 0.7912 (0.9219) loss_zs_kd 0.0051 (0.0286) loss_oracle 0.4755 (0.4632) acc 84.3750 (74.9107) lr 1.8443e-03 eta 0:29:18
epoch [11/50] batch [160/288] time 0.148 (0.154) data 0.000 (0.002) loss 2.1085 (1.8741) teacher_loss 1.1523 (0.9390) loss_zs_kd 0.0240 (0.0285) loss_oracle 0.5208 (0.4585) acc 68.7500 (74.5508) lr 1.8443e-03 eta 0:29:08
epoch [11/50] batch [180/288] time 0.098 (0.150) data 0.000 (0.002) loss 1.7709 (1.8619) teacher_loss 0.9673 (0.9323) loss_zs_kd 0.0355 (0.0285) loss_oracle 0.3616 (0.4539) acc 78.1250 (74.5660) lr 1.8443e-03 eta 0:28:23
epoch [11/50] batch [200/288] time 0.366 (0.151) data 0.000 (0.002) loss 1.6895 (1.8535) teacher_loss 0.9065 (0.9287) loss_zs_kd 0.0462 (0.0285) loss_oracle 0.3154 (0.4481) acc 78.1250 (74.7812) lr 1.8443e-03 eta 0:28:30
epoch [11/50] batch [220/288] time 0.290 (0.153) data 0.000 (0.001) loss 2.0336 (1.8550) teacher_loss 1.0054 (0.9286) loss_zs_kd 0.0275 (0.0294) loss_oracle 0.5052 (0.4453) acc 84.3750 (74.8438) lr 1.8443e-03 eta 0:28:43
epoch [11/50] batch [240/288] time 0.082 (0.149) data 0.000 (0.001) loss 1.9311 (1.8517) teacher_loss 0.9098 (0.9244) loss_zs_kd 0.0210 (0.0294) loss_oracle 0.4793 (0.4444) acc 75.0000 (74.9479) lr 1.8443e-03 eta 0:28:05
epoch [11/50] batch [260/288] time 0.083 (0.148) data 0.000 (0.001) loss 1.7490 (1.8521) teacher_loss 0.8681 (0.9264) loss_zs_kd 0.0322 (0.0292) loss_oracle 0.4574 (0.4439) acc 71.8750 (74.9038) lr 1.8443e-03 eta 0:27:51
epoch [11/50] batch [280/288] time 0.083 (0.150) data 0.000 (0.001) loss 1.3938 (1.8424) teacher_loss 0.6286 (0.9192) loss_zs_kd 0.0097 (0.0288) loss_oracle 0.3954 (0.4433) acc 84.3750 (75.1228) lr 1.8443e-03 eta 0:28:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,428
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,009
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.2%
******* Domain a best val acc:      87.1%, epoch: 8 *******
******* Domain a best val test acc: 82.4%, epoch: 8 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [12/50] batch [20/288] time 0.178 (0.181) data 0.000 (0.015) loss 1.5528 (1.8707) teacher_loss 0.8932 (0.9651) loss_zs_kd 0.0078 (0.0293) loss_oracle 0.3504 (0.4188) acc 81.2500 (73.4375) lr 1.8090e-03 eta 0:33:45
epoch [12/50] batch [40/288] time 0.160 (0.169) data 0.000 (0.008) loss 1.9688 (1.8377) teacher_loss 1.0006 (0.9264) loss_zs_kd 0.0231 (0.0289) loss_oracle 0.4475 (0.4207) acc 78.1250 (75.1562) lr 1.8090e-03 eta 0:31:34
epoch [12/50] batch [60/288] time 0.155 (0.163) data 0.000 (0.005) loss 1.9800 (1.8633) teacher_loss 1.1157 (0.9453) loss_zs_kd 0.0339 (0.0279) loss_oracle 0.4252 (0.4241) acc 71.8750 (74.5833) lr 1.8090e-03 eta 0:30:21
epoch [12/50] batch [80/288] time 0.153 (0.160) data 0.000 (0.004) loss 2.8839 (1.8785) teacher_loss 1.8739 (0.9563) loss_zs_kd 0.0320 (0.0287) loss_oracle 0.4191 (0.4280) acc 56.2500 (74.9219) lr 1.8090e-03 eta 0:29:42
epoch [12/50] batch [100/288] time 0.162 (0.159) data 0.000 (0.003) loss 1.9314 (1.8630) teacher_loss 1.0768 (0.9371) loss_zs_kd 0.0121 (0.0275) loss_oracle 0.4045 (0.4303) acc 59.3750 (75.1875) lr 1.8090e-03 eta 0:29:32
epoch [12/50] batch [120/288] time 0.172 (0.161) data 0.000 (0.003) loss 2.2430 (1.8574) teacher_loss 1.1440 (0.9304) loss_zs_kd 0.0325 (0.0275) loss_oracle 0.4791 (0.4341) acc 71.8750 (75.2865) lr 1.8090e-03 eta 0:29:48
epoch [12/50] batch [140/288] time 0.154 (0.161) data 0.000 (0.002) loss 1.3511 (1.8580) teacher_loss 0.4817 (0.9332) loss_zs_kd 0.0074 (0.0282) loss_oracle 0.4363 (0.4347) acc 87.5000 (75.0893) lr 1.8090e-03 eta 0:29:46
epoch [12/50] batch [160/288] time 0.152 (0.161) data 0.000 (0.002) loss 1.8741 (1.8549) teacher_loss 0.9763 (0.9287) loss_zs_kd 0.0361 (0.0286) loss_oracle 0.4317 (0.4381) acc 78.1250 (75.0977) lr 1.8090e-03 eta 0:29:42
epoch [12/50] batch [180/288] time 0.155 (0.161) data 0.000 (0.002) loss 1.7699 (1.8703) teacher_loss 0.8464 (0.9406) loss_zs_kd 0.0369 (0.0291) loss_oracle 0.4425 (0.4410) acc 81.2500 (74.7743) lr 1.8090e-03 eta 0:29:35
epoch [12/50] batch [200/288] time 0.144 (0.160) data 0.000 (0.002) loss 2.2209 (1.8733) teacher_loss 1.0644 (0.9407) loss_zs_kd 0.0285 (0.0291) loss_oracle 0.4749 (0.4413) acc 75.0000 (74.7812) lr 1.8090e-03 eta 0:29:27
epoch [12/50] batch [220/288] time 0.150 (0.159) data 0.000 (0.002) loss 1.7522 (1.8706) teacher_loss 0.6853 (0.9356) loss_zs_kd 0.0467 (0.0293) loss_oracle 0.4769 (0.4399) acc 75.0000 (74.9716) lr 1.8090e-03 eta 0:29:15
epoch [12/50] batch [240/288] time 0.188 (0.160) data 0.000 (0.001) loss 2.3939 (1.8771) teacher_loss 1.3490 (0.9375) loss_zs_kd 0.0345 (0.0296) loss_oracle 0.6077 (0.4426) acc 65.6250 (75.0000) lr 1.8090e-03 eta 0:29:17
epoch [12/50] batch [260/288] time 0.094 (0.157) data 0.000 (0.001) loss 1.6954 (1.8743) teacher_loss 0.6881 (0.9337) loss_zs_kd 0.0392 (0.0297) loss_oracle 0.4731 (0.4442) acc 84.3750 (75.0841) lr 1.8090e-03 eta 0:28:43
epoch [12/50] batch [280/288] time 0.091 (0.157) data 0.000 (0.001) loss 2.0337 (1.8768) teacher_loss 1.0142 (0.9326) loss_zs_kd 0.0174 (0.0297) loss_oracle 0.4851 (0.4493) acc 71.8750 (75.0670) lr 1.8090e-03 eta 0:28:39
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,424
* accuracy: 86.9%
* error: 13.1%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,017
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.7%
******* Domain a best val acc:      87.1%, epoch: 8 *******
******* Domain a best val test acc: 82.4%, epoch: 8 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [13/50] batch [20/288] time 0.091 (0.185) data 0.000 (0.015) loss 1.5322 (1.8409) teacher_loss 0.6126 (0.8938) loss_zs_kd 0.0381 (0.0323) loss_oracle 0.4812 (0.4967) acc 84.3750 (76.0938) lr 1.7705e-03 eta 0:33:41
epoch [13/50] batch [40/288] time 0.085 (0.177) data 0.000 (0.008) loss 1.6676 (1.9153) teacher_loss 0.7231 (0.9638) loss_zs_kd 0.0427 (0.0308) loss_oracle 0.4905 (0.4916) acc 87.5000 (74.4531) lr 1.7705e-03 eta 0:32:11
epoch [13/50] batch [60/288] time 0.083 (0.165) data 0.000 (0.005) loss 2.6385 (1.9785) teacher_loss 1.5816 (1.0168) loss_zs_kd 0.0538 (0.0319) loss_oracle 0.4603 (0.4852) acc 59.3750 (72.9167) lr 1.7705e-03 eta 0:29:55
epoch [13/50] batch [80/288] time 0.157 (0.155) data 0.000 (0.004) loss 2.2264 (1.9428) teacher_loss 1.3558 (0.9933) loss_zs_kd 0.0375 (0.0309) loss_oracle 0.4647 (0.4738) acc 65.6250 (73.6328) lr 1.7705e-03 eta 0:28:02
epoch [13/50] batch [100/288] time 0.153 (0.155) data 0.000 (0.003) loss 1.5779 (1.9178) teacher_loss 0.6262 (0.9655) loss_zs_kd 0.0031 (0.0294) loss_oracle 0.5131 (0.4723) acc 84.3750 (74.2812) lr 1.7705e-03 eta 0:28:02
epoch [13/50] batch [120/288] time 0.152 (0.156) data 0.000 (0.003) loss 2.0231 (1.9024) teacher_loss 0.9786 (0.9546) loss_zs_kd 0.0338 (0.0296) loss_oracle 0.4560 (0.4701) acc 75.0000 (74.6094) lr 1.7705e-03 eta 0:28:13
epoch [13/50] batch [140/288] time 0.146 (0.156) data 0.000 (0.002) loss 1.6709 (1.9021) teacher_loss 0.8215 (0.9628) loss_zs_kd 0.0291 (0.0294) loss_oracle 0.4471 (0.4657) acc 71.8750 (74.2857) lr 1.7705e-03 eta 0:28:01
epoch [13/50] batch [160/288] time 0.155 (0.155) data 0.000 (0.002) loss 1.9447 (1.8870) teacher_loss 1.0276 (0.9479) loss_zs_kd 0.0187 (0.0293) loss_oracle 0.4685 (0.4663) acc 71.8750 (74.6289) lr 1.7705e-03 eta 0:27:54
epoch [13/50] batch [180/288] time 0.186 (0.157) data 0.000 (0.002) loss 1.3753 (1.8927) teacher_loss 0.6081 (0.9557) loss_zs_kd 0.0177 (0.0296) loss_oracle 0.4023 (0.4644) acc 84.3750 (74.4097) lr 1.7705e-03 eta 0:28:13
epoch [13/50] batch [200/288] time 0.169 (0.157) data 0.000 (0.002) loss 2.3792 (1.8879) teacher_loss 1.4492 (0.9516) loss_zs_kd 0.0285 (0.0298) loss_oracle 0.4862 (0.4640) acc 68.7500 (74.4688) lr 1.7705e-03 eta 0:28:11
epoch [13/50] batch [220/288] time 0.151 (0.157) data 0.000 (0.002) loss 1.8570 (1.8818) teacher_loss 0.8134 (0.9444) loss_zs_kd 0.0388 (0.0303) loss_oracle 0.4575 (0.4631) acc 81.2500 (74.6733) lr 1.7705e-03 eta 0:28:03
epoch [13/50] batch [240/288] time 0.154 (0.157) data 0.000 (0.001) loss 2.0654 (1.8765) teacher_loss 1.1822 (0.9410) loss_zs_kd 0.0327 (0.0303) loss_oracle 0.4994 (0.4629) acc 75.0000 (74.7917) lr 1.7705e-03 eta 0:28:00
epoch [13/50] batch [260/288] time 0.155 (0.157) data 0.000 (0.001) loss 1.8035 (1.8646) teacher_loss 0.9323 (0.9294) loss_zs_kd 0.0237 (0.0299) loss_oracle 0.4177 (0.4634) acc 75.0000 (75.1082) lr 1.7705e-03 eta 0:27:58
epoch [13/50] batch [280/288] time 0.151 (0.157) data 0.000 (0.001) loss 1.4967 (1.8640) teacher_loss 0.5713 (0.9306) loss_zs_kd 0.0082 (0.0295) loss_oracle 0.4124 (0.4613) acc 87.5000 (74.9777) lr 1.7705e-03 eta 0:27:51
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,426
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,006
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 79.6%
******* Domain a best val acc:      87.1%, epoch: 8 *******
******* Domain a best val test acc: 82.4%, epoch: 8 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [14/50] batch [20/288] time 0.102 (0.129) data 0.000 (0.014) loss 1.8459 (1.9443) teacher_loss 1.0334 (0.9664) loss_zs_kd 0.0460 (0.0288) loss_oracle 0.3750 (0.4339) acc 78.1250 (75.7812) lr 1.7290e-03 eta 0:22:49
epoch [14/50] batch [40/288] time 0.084 (0.149) data 0.000 (0.007) loss 1.6078 (1.8830) teacher_loss 0.6162 (0.9142) loss_zs_kd 0.0395 (0.0292) loss_oracle 0.3887 (0.4340) acc 84.3750 (76.3281) lr 1.7290e-03 eta 0:26:25
epoch [14/50] batch [60/288] time 0.422 (0.147) data 0.000 (0.005) loss 1.7053 (1.8478) teacher_loss 0.8293 (0.8931) loss_zs_kd 0.0128 (0.0300) loss_oracle 0.3846 (0.4216) acc 78.1250 (76.6667) lr 1.7290e-03 eta 0:26:00
epoch [14/50] batch [80/288] time 0.108 (0.138) data 0.000 (0.004) loss 1.9932 (1.8628) teacher_loss 0.9648 (0.9096) loss_zs_kd 0.0345 (0.0308) loss_oracle 0.4374 (0.4258) acc 78.1250 (76.2109) lr 1.7290e-03 eta 0:24:20
epoch [14/50] batch [100/288] time 0.293 (0.140) data 0.000 (0.003) loss 2.0445 (1.8628) teacher_loss 1.0856 (0.9083) loss_zs_kd 0.0417 (0.0315) loss_oracle 0.4674 (0.4310) acc 78.1250 (76.0625) lr 1.7290e-03 eta 0:24:42
epoch [14/50] batch [120/288] time 0.393 (0.143) data 0.000 (0.003) loss 1.5535 (1.8811) teacher_loss 0.7090 (0.9216) loss_zs_kd 0.0132 (0.0310) loss_oracle 0.4900 (0.4370) acc 78.1250 (75.8854) lr 1.7290e-03 eta 0:25:05
epoch [14/50] batch [140/288] time 0.094 (0.144) data 0.000 (0.002) loss 1.6425 (1.8943) teacher_loss 0.7790 (0.9332) loss_zs_kd 0.0197 (0.0308) loss_oracle 0.3934 (0.4387) acc 78.1250 (75.7812) lr 1.7290e-03 eta 0:25:09
epoch [14/50] batch [160/288] time 0.085 (0.146) data 0.000 (0.002) loss 1.4985 (1.8795) teacher_loss 0.6432 (0.9215) loss_zs_kd 0.0176 (0.0303) loss_oracle 0.4001 (0.4361) acc 81.2500 (76.0742) lr 1.7290e-03 eta 0:25:31
epoch [14/50] batch [180/288] time 0.095 (0.145) data 0.000 (0.002) loss 1.5014 (1.8698) teacher_loss 0.6899 (0.9124) loss_zs_kd 0.0189 (0.0300) loss_oracle 0.4466 (0.4386) acc 81.2500 (76.1806) lr 1.7290e-03 eta 0:25:21
epoch [14/50] batch [200/288] time 0.141 (0.145) data 0.000 (0.002) loss 2.1788 (1.8716) teacher_loss 1.0572 (0.9168) loss_zs_kd 0.0199 (0.0301) loss_oracle 0.5110 (0.4418) acc 68.7500 (76.0312) lr 1.7290e-03 eta 0:25:12
epoch [14/50] batch [220/288] time 0.152 (0.145) data 0.000 (0.001) loss 1.2349 (1.8649) teacher_loss 0.3587 (0.9102) loss_zs_kd 0.0175 (0.0300) loss_oracle 0.3915 (0.4428) acc 87.5000 (76.1222) lr 1.7290e-03 eta 0:25:14
epoch [14/50] batch [240/288] time 0.150 (0.146) data 0.000 (0.001) loss 1.7297 (1.8592) teacher_loss 0.8365 (0.9039) loss_zs_kd 0.0201 (0.0301) loss_oracle 0.3921 (0.4433) acc 78.1250 (76.2500) lr 1.7290e-03 eta 0:25:18
epoch [14/50] batch [260/288] time 0.152 (0.147) data 0.000 (0.001) loss 2.1359 (1.8600) teacher_loss 1.2700 (0.9041) loss_zs_kd 0.0304 (0.0301) loss_oracle 0.4546 (0.4455) acc 68.7500 (76.2500) lr 1.7290e-03 eta 0:25:28
epoch [14/50] batch [280/288] time 0.151 (0.147) data 0.000 (0.001) loss 1.4396 (1.8585) teacher_loss 0.5590 (0.9044) loss_zs_kd 0.0201 (0.0299) loss_oracle 0.4391 (0.4456) acc 84.3750 (76.1830) lr 1.7290e-03 eta 0:25:29
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,411
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,998
* accuracy: 82.3%
* error: 17.7%
* macro_f1: 79.1%
******* Domain a best val acc:      87.1%, epoch: 8 *******
******* Domain a best val test acc: 82.4%, epoch: 8 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [15/50] batch [20/288] time 0.161 (0.187) data 0.000 (0.014) loss 2.0924 (1.8657) teacher_loss 1.0422 (0.9157) loss_zs_kd 0.0265 (0.0235) loss_oracle 0.5352 (0.4650) acc 78.1250 (75.6250) lr 1.6845e-03 eta 0:32:11
epoch [15/50] batch [40/288] time 0.153 (0.173) data 0.000 (0.007) loss 1.9827 (1.9049) teacher_loss 0.9878 (0.9751) loss_zs_kd 0.0247 (0.0254) loss_oracle 0.4328 (0.4575) acc 81.2500 (75.1562) lr 1.6845e-03 eta 0:29:51
epoch [15/50] batch [60/288] time 0.152 (0.167) data 0.000 (0.005) loss 1.4470 (1.8592) teacher_loss 0.5417 (0.9323) loss_zs_kd 0.0147 (0.0255) loss_oracle 0.4601 (0.4564) acc 84.3750 (75.9896) lr 1.6845e-03 eta 0:28:37
epoch [15/50] batch [80/288] time 0.151 (0.165) data 0.000 (0.004) loss 1.9667 (1.8269) teacher_loss 0.9418 (0.8991) loss_zs_kd 0.0202 (0.0262) loss_oracle 0.5313 (0.4542) acc 84.3750 (76.6016) lr 1.6845e-03 eta 0:28:13
epoch [15/50] batch [100/288] time 0.168 (0.165) data 0.000 (0.003) loss 1.8286 (1.8149) teacher_loss 0.8585 (0.8878) loss_zs_kd 0.0260 (0.0271) loss_oracle 0.4017 (0.4503) acc 78.1250 (76.7500) lr 1.6845e-03 eta 0:28:14
epoch [15/50] batch [120/288] time 0.102 (0.160) data 0.000 (0.002) loss 2.2265 (1.8259) teacher_loss 1.2034 (0.8923) loss_zs_kd 0.0192 (0.0269) loss_oracle 0.5489 (0.4513) acc 71.8750 (76.5885) lr 1.6845e-03 eta 0:27:15
epoch [15/50] batch [140/288] time 0.099 (0.158) data 0.000 (0.002) loss 1.9386 (1.8412) teacher_loss 1.0394 (0.9030) loss_zs_kd 0.0249 (0.0275) loss_oracle 0.4671 (0.4516) acc 78.1250 (76.1384) lr 1.6845e-03 eta 0:26:57
epoch [15/50] batch [160/288] time 0.083 (0.159) data 0.000 (0.002) loss 2.0270 (1.8485) teacher_loss 1.0152 (0.9099) loss_zs_kd 0.0184 (0.0281) loss_oracle 0.4711 (0.4517) acc 75.0000 (75.8398) lr 1.6845e-03 eta 0:27:03
epoch [15/50] batch [180/288] time 0.096 (0.153) data 0.000 (0.002) loss 2.3577 (1.8554) teacher_loss 1.3481 (0.9200) loss_zs_kd 0.0571 (0.0281) loss_oracle 0.5065 (0.4513) acc 65.6250 (75.6076) lr 1.6845e-03 eta 0:26:02
epoch [15/50] batch [200/288] time 0.120 (0.156) data 0.000 (0.002) loss 1.5406 (1.8573) teacher_loss 0.7407 (0.9252) loss_zs_kd 0.0173 (0.0282) loss_oracle 0.4045 (0.4497) acc 78.1250 (75.4688) lr 1.6845e-03 eta 0:26:21
epoch [15/50] batch [220/288] time 0.324 (0.157) data 0.000 (0.001) loss 2.0024 (1.8600) teacher_loss 1.0861 (0.9253) loss_zs_kd 0.0168 (0.0282) loss_oracle 0.3829 (0.4487) acc 75.0000 (75.5114) lr 1.6845e-03 eta 0:26:34
epoch [15/50] batch [240/288] time 0.190 (0.158) data 0.000 (0.001) loss 1.6624 (1.8536) teacher_loss 0.6895 (0.9210) loss_zs_kd 0.0591 (0.0284) loss_oracle 0.4222 (0.4464) acc 78.1250 (75.6120) lr 1.6845e-03 eta 0:26:42
epoch [15/50] batch [260/288] time 0.127 (0.158) data 0.000 (0.001) loss 1.8302 (1.8517) teacher_loss 0.9862 (0.9218) loss_zs_kd 0.0253 (0.0282) loss_oracle 0.4047 (0.4441) acc 71.8750 (75.6250) lr 1.6845e-03 eta 0:26:39
epoch [15/50] batch [280/288] time 0.151 (0.158) data 0.000 (0.001) loss 1.6005 (1.8487) teacher_loss 0.8079 (0.9208) loss_zs_kd 0.0405 (0.0280) loss_oracle 0.3989 (0.4415) acc 75.0000 (75.6138) lr 1.6845e-03 eta 0:26:35
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,433
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.5%
Checkpoint saved to icml/multi-dg/oracle/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,005
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.2%
******* Domain a best val acc:      87.2%, epoch: 15 *******
******* Domain a best val test acc: 82.6%, epoch: 15 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [16/50] batch [20/288] time 0.183 (0.203) data 0.000 (0.017) loss 2.3968 (1.8528) teacher_loss 1.4105 (0.9754) loss_zs_kd 0.0253 (0.0278) loss_oracle 0.4282 (0.4028) acc 71.8750 (74.5312) lr 1.6374e-03 eta 0:33:59
epoch [16/50] batch [40/288] time 0.159 (0.187) data 0.000 (0.008) loss 2.0204 (1.8063) teacher_loss 1.1074 (0.9316) loss_zs_kd 0.0316 (0.0287) loss_oracle 0.4341 (0.3960) acc 68.7500 (74.9219) lr 1.6374e-03 eta 0:31:17
epoch [16/50] batch [60/288] time 0.155 (0.180) data 0.001 (0.006) loss 2.1878 (1.8081) teacher_loss 1.0084 (0.9251) loss_zs_kd 0.0416 (0.0283) loss_oracle 0.3804 (0.3958) acc 78.1250 (75.3125) lr 1.6374e-03 eta 0:30:08
epoch [16/50] batch [80/288] time 0.134 (0.179) data 0.000 (0.004) loss 2.1870 (1.8077) teacher_loss 1.1743 (0.9186) loss_zs_kd 0.0239 (0.0283) loss_oracle 0.4391 (0.3941) acc 71.8750 (75.4297) lr 1.6374e-03 eta 0:29:47
epoch [16/50] batch [100/288] time 0.180 (0.176) data 0.000 (0.003) loss 1.8743 (1.8289) teacher_loss 1.0404 (0.9321) loss_zs_kd 0.0240 (0.0287) loss_oracle 0.3818 (0.3994) acc 65.6250 (74.7500) lr 1.6374e-03 eta 0:29:17
epoch [16/50] batch [120/288] time 0.181 (0.176) data 0.000 (0.003) loss 1.7005 (1.8280) teacher_loss 0.6792 (0.9266) loss_zs_kd 0.0340 (0.0281) loss_oracle 0.4422 (0.3992) acc 81.2500 (74.6875) lr 1.6374e-03 eta 0:29:14
epoch [16/50] batch [140/288] time 0.100 (0.174) data 0.000 (0.003) loss 1.9011 (1.8000) teacher_loss 1.0132 (0.9053) loss_zs_kd 0.0296 (0.0280) loss_oracle 0.3719 (0.3962) acc 75.0000 (75.2455) lr 1.6374e-03 eta 0:28:47
epoch [16/50] batch [160/288] time 0.449 (0.171) data 0.000 (0.002) loss 2.0457 (1.7994) teacher_loss 1.0418 (0.9029) loss_zs_kd 0.0287 (0.0279) loss_oracle 0.4756 (0.3956) acc 78.1250 (75.4688) lr 1.6374e-03 eta 0:28:15
epoch [16/50] batch [180/288] time 0.231 (0.172) data 0.000 (0.002) loss 1.5657 (1.8067) teacher_loss 0.6938 (0.9127) loss_zs_kd 0.0127 (0.0283) loss_oracle 0.4187 (0.3940) acc 81.2500 (75.4688) lr 1.6374e-03 eta 0:28:21
epoch [16/50] batch [200/288] time 0.102 (0.166) data 0.000 (0.002) loss 1.7359 (1.8096) teacher_loss 0.7959 (0.9184) loss_zs_kd 0.0448 (0.0281) loss_oracle 0.4330 (0.3914) acc 75.0000 (75.2188) lr 1.6374e-03 eta 0:27:15
epoch [16/50] batch [220/288] time 0.087 (0.166) data 0.000 (0.002) loss 1.3932 (1.8086) teacher_loss 0.6229 (0.9147) loss_zs_kd 0.0136 (0.0282) loss_oracle 0.4032 (0.3896) acc 84.3750 (75.2841) lr 1.6374e-03 eta 0:27:16
epoch [16/50] batch [240/288] time 0.099 (0.164) data 0.000 (0.002) loss 1.6276 (1.8004) teacher_loss 0.6657 (0.9055) loss_zs_kd 0.0422 (0.0288) loss_oracle 0.4320 (0.3917) acc 81.2500 (75.5208) lr 1.6374e-03 eta 0:26:58
epoch [16/50] batch [260/288] time 0.092 (0.165) data 0.000 (0.002) loss 1.8218 (1.8069) teacher_loss 0.8526 (0.9097) loss_zs_kd 0.0196 (0.0289) loss_oracle 0.5065 (0.3940) acc 78.1250 (75.3245) lr 1.6374e-03 eta 0:26:59
epoch [16/50] batch [280/288] time 0.083 (0.165) data 0.000 (0.001) loss 2.3745 (1.8189) teacher_loss 1.4132 (0.9189) loss_zs_kd 0.0363 (0.0292) loss_oracle 0.4247 (0.3951) acc 59.3750 (75.1562) lr 1.6374e-03 eta 0:26:58
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,429
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,014
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.7%
******* Domain a best val acc:      87.2%, epoch: 15 *******
******* Domain a best val test acc: 82.6%, epoch: 15 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [17/50] batch [20/288] time 0.151 (0.185) data 0.000 (0.015) loss 2.3760 (1.7444) teacher_loss 1.4276 (0.7956) loss_zs_kd 0.0161 (0.0244) loss_oracle 0.5008 (0.4282) acc 78.1250 (78.7500) lr 1.5878e-03 eta 0:30:03
epoch [17/50] batch [40/288] time 0.150 (0.175) data 0.000 (0.008) loss 1.7317 (1.8223) teacher_loss 0.8567 (0.8821) loss_zs_kd 0.0394 (0.0283) loss_oracle 0.5023 (0.4299) acc 84.3750 (77.4219) lr 1.5878e-03 eta 0:28:25
epoch [17/50] batch [60/288] time 0.153 (0.170) data 0.000 (0.005) loss 2.4518 (1.8309) teacher_loss 1.3902 (0.8953) loss_zs_kd 0.0306 (0.0288) loss_oracle 0.4746 (0.4288) acc 65.6250 (76.8229) lr 1.5878e-03 eta 0:27:36
epoch [17/50] batch [80/288] time 0.184 (0.171) data 0.000 (0.004) loss 1.6640 (1.8242) teacher_loss 0.6931 (0.8921) loss_zs_kd 0.0350 (0.0288) loss_oracle 0.4551 (0.4217) acc 81.2500 (76.5234) lr 1.5878e-03 eta 0:27:38
epoch [17/50] batch [100/288] time 0.182 (0.172) data 0.000 (0.003) loss 1.8033 (1.8129) teacher_loss 0.9780 (0.8899) loss_zs_kd 0.0275 (0.0288) loss_oracle 0.4014 (0.4209) acc 68.7500 (76.5625) lr 1.5878e-03 eta 0:27:47
epoch [17/50] batch [120/288] time 0.181 (0.172) data 0.000 (0.003) loss 1.6959 (1.8157) teacher_loss 0.9082 (0.8973) loss_zs_kd 0.0207 (0.0286) loss_oracle 0.4437 (0.4167) acc 81.2500 (76.2760) lr 1.5878e-03 eta 0:27:43
epoch [17/50] batch [140/288] time 0.194 (0.172) data 0.001 (0.002) loss 2.0220 (1.8332) teacher_loss 1.1400 (0.9169) loss_zs_kd 0.0262 (0.0299) loss_oracle 0.4013 (0.4165) acc 65.6250 (75.6920) lr 1.5878e-03 eta 0:27:43
epoch [17/50] batch [160/288] time 0.181 (0.173) data 0.000 (0.002) loss 1.9022 (1.8342) teacher_loss 0.9970 (0.9181) loss_zs_kd 0.0199 (0.0299) loss_oracle 0.3742 (0.4165) acc 62.5000 (75.6836) lr 1.5878e-03 eta 0:27:42
epoch [17/50] batch [180/288] time 0.389 (0.168) data 0.000 (0.002) loss 1.7798 (1.8237) teacher_loss 0.7094 (0.9061) loss_zs_kd 0.0258 (0.0293) loss_oracle 0.4968 (0.4149) acc 81.2500 (76.0417) lr 1.5878e-03 eta 0:26:55
epoch [17/50] batch [200/288] time 0.393 (0.168) data 0.000 (0.002) loss 1.6899 (1.8260) teacher_loss 0.7668 (0.9035) loss_zs_kd 0.0373 (0.0295) loss_oracle 0.4669 (0.4172) acc 75.0000 (75.9688) lr 1.5878e-03 eta 0:26:52
epoch [17/50] batch [220/288] time 0.100 (0.166) data 0.000 (0.002) loss 1.7087 (1.8301) teacher_loss 0.6920 (0.9015) loss_zs_kd 0.0099 (0.0298) loss_oracle 0.5013 (0.4184) acc 84.3750 (76.0938) lr 1.5878e-03 eta 0:26:27
epoch [17/50] batch [240/288] time 0.137 (0.161) data 0.000 (0.001) loss 1.7515 (1.8327) teacher_loss 0.7651 (0.8999) loss_zs_kd 0.0319 (0.0297) loss_oracle 0.4847 (0.4185) acc 81.2500 (76.1979) lr 1.5878e-03 eta 0:25:37
epoch [17/50] batch [260/288] time 0.102 (0.161) data 0.000 (0.001) loss 2.3184 (1.8409) teacher_loss 1.2654 (0.9025) loss_zs_kd 0.0289 (0.0299) loss_oracle 0.4539 (0.4206) acc 65.6250 (76.1298) lr 1.5878e-03 eta 0:25:38
epoch [17/50] batch [280/288] time 0.328 (0.163) data 0.000 (0.001) loss 2.0910 (1.8425) teacher_loss 1.1023 (0.9002) loss_zs_kd 0.0387 (0.0300) loss_oracle 0.4216 (0.4221) acc 71.8750 (76.1496) lr 1.5878e-03 eta 0:25:47
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,449
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 87.0%
Checkpoint saved to icml/multi-dg/oracle/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,003
* accuracy: 82.5%
* error: 17.5%
* macro_f1: 79.2%
******* Domain a best val acc:      87.6%, epoch: 17 *******
******* Domain a best val test acc: 82.5%, epoch: 17 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [18/50] batch [20/288] time 0.141 (0.172) data 0.000 (0.011) loss 2.2700 (1.9053) teacher_loss 1.1341 (0.8866) loss_zs_kd 0.0238 (0.0298) loss_oracle 0.3841 (0.4658) acc 68.7500 (76.4062) lr 1.5358e-03 eta 0:27:09
epoch [18/50] batch [40/288] time 0.165 (0.165) data 0.000 (0.005) loss 2.2796 (1.9456) teacher_loss 1.3240 (0.9209) loss_zs_kd 0.0854 (0.0332) loss_oracle 0.4161 (0.4750) acc 68.7500 (75.6250) lr 1.5358e-03 eta 0:26:02
epoch [18/50] batch [60/288] time 0.172 (0.167) data 0.000 (0.004) loss 1.5547 (1.9442) teacher_loss 0.5678 (0.9233) loss_zs_kd 0.0241 (0.0318) loss_oracle 0.3851 (0.4663) acc 87.5000 (75.4167) lr 1.5358e-03 eta 0:26:13
epoch [18/50] batch [80/288] time 0.172 (0.166) data 0.000 (0.003) loss 2.8535 (1.9550) teacher_loss 1.7743 (0.9460) loss_zs_kd 0.0432 (0.0318) loss_oracle 0.5001 (0.4601) acc 53.1250 (74.8828) lr 1.5358e-03 eta 0:26:07
epoch [18/50] batch [100/288] time 0.155 (0.166) data 0.000 (0.002) loss 1.8248 (1.9411) teacher_loss 0.7884 (0.9376) loss_zs_kd 0.0351 (0.0319) loss_oracle 0.4257 (0.4573) acc 87.5000 (75.0625) lr 1.5358e-03 eta 0:26:03
epoch [18/50] batch [120/288] time 0.182 (0.168) data 0.000 (0.002) loss 1.9079 (1.9221) teacher_loss 1.0471 (0.9234) loss_zs_kd 0.0393 (0.0319) loss_oracle 0.4461 (0.4572) acc 75.0000 (75.5208) lr 1.5358e-03 eta 0:26:18
epoch [18/50] batch [140/288] time 0.152 (0.169) data 0.000 (0.002) loss 2.0104 (1.9266) teacher_loss 1.0601 (0.9308) loss_zs_kd 0.0250 (0.0319) loss_oracle 0.5063 (0.4584) acc 68.7500 (75.2455) lr 1.5358e-03 eta 0:26:22
epoch [18/50] batch [160/288] time 0.160 (0.168) data 0.000 (0.002) loss 1.9441 (1.9225) teacher_loss 0.8631 (0.9247) loss_zs_kd 0.0766 (0.0322) loss_oracle 0.5294 (0.4616) acc 78.1250 (75.2344) lr 1.5358e-03 eta 0:26:13
epoch [18/50] batch [180/288] time 0.154 (0.167) data 0.000 (0.001) loss 1.6571 (1.9114) teacher_loss 0.7177 (0.9175) loss_zs_kd 0.0248 (0.0326) loss_oracle 0.4315 (0.4614) acc 71.8750 (75.3299) lr 1.5358e-03 eta 0:25:57
epoch [18/50] batch [200/288] time 0.183 (0.166) data 0.000 (0.001) loss 2.2752 (1.8968) teacher_loss 1.2840 (0.9089) loss_zs_kd 0.0532 (0.0321) loss_oracle 0.4512 (0.4591) acc 59.3750 (75.4531) lr 1.5358e-03 eta 0:25:46
epoch [18/50] batch [220/288] time 0.166 (0.163) data 0.000 (0.001) loss 2.3827 (1.8948) teacher_loss 1.4224 (0.9103) loss_zs_kd 0.0283 (0.0323) loss_oracle 0.4601 (0.4581) acc 68.7500 (75.4688) lr 1.5358e-03 eta 0:25:12
epoch [18/50] batch [240/288] time 0.086 (0.161) data 0.000 (0.001) loss 1.5130 (1.8933) teacher_loss 0.6913 (0.9115) loss_zs_kd 0.0350 (0.0322) loss_oracle 0.4035 (0.4556) acc 81.2500 (75.5208) lr 1.5358e-03 eta 0:24:53
epoch [18/50] batch [260/288] time 0.216 (0.161) data 0.000 (0.001) loss 1.9163 (1.8894) teacher_loss 0.9612 (0.9098) loss_zs_kd 0.0119 (0.0317) loss_oracle 0.4438 (0.4543) acc 84.3750 (75.7212) lr 1.5358e-03 eta 0:24:44
epoch [18/50] batch [280/288] time 0.089 (0.158) data 0.000 (0.001) loss 2.0965 (1.8921) teacher_loss 1.0553 (0.9146) loss_zs_kd 0.0390 (0.0317) loss_oracle 0.4635 (0.4528) acc 68.7500 (75.6920) lr 1.5358e-03 eta 0:24:16
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,450
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 87.0%
Checkpoint saved to icml/multi-dg/oracle/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,009
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.6%
******* Domain a best val acc:      87.6%, epoch: 18 *******
******* Domain a best val test acc: 82.8%, epoch: 18 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [19/50] batch [20/288] time 0.152 (0.159) data 0.000 (0.016) loss 2.0298 (1.8645) teacher_loss 1.0369 (0.8485) loss_zs_kd 0.0264 (0.0275) loss_oracle 0.4371 (0.4543) acc 71.8750 (77.1875) lr 1.4818e-03 eta 0:24:20
epoch [19/50] batch [40/288] time 0.181 (0.159) data 0.000 (0.008) loss 2.0562 (1.9155) teacher_loss 0.9564 (0.9234) loss_zs_kd 0.0285 (0.0301) loss_oracle 0.4645 (0.4418) acc 65.6250 (74.7656) lr 1.4818e-03 eta 0:24:22
epoch [19/50] batch [60/288] time 0.179 (0.162) data 0.000 (0.005) loss 2.2216 (1.9441) teacher_loss 1.2272 (0.9639) loss_zs_kd 0.0450 (0.0315) loss_oracle 0.4395 (0.4312) acc 65.6250 (74.1146) lr 1.4818e-03 eta 0:24:40
epoch [19/50] batch [80/288] time 0.151 (0.164) data 0.000 (0.004) loss 1.8641 (1.8963) teacher_loss 0.8619 (0.9203) loss_zs_kd 0.0257 (0.0314) loss_oracle 0.3675 (0.4281) acc 71.8750 (75.0781) lr 1.4818e-03 eta 0:24:57
epoch [19/50] batch [100/288] time 0.180 (0.165) data 0.000 (0.003) loss 1.8082 (1.8788) teacher_loss 1.0403 (0.9153) loss_zs_kd 0.0506 (0.0317) loss_oracle 0.4125 (0.4259) acc 71.8750 (75.1562) lr 1.4818e-03 eta 0:25:04
epoch [19/50] batch [120/288] time 0.164 (0.165) data 0.000 (0.003) loss 2.3015 (1.8751) teacher_loss 1.4146 (0.9150) loss_zs_kd 0.0290 (0.0317) loss_oracle 0.4416 (0.4264) acc 62.5000 (75.1823) lr 1.4818e-03 eta 0:25:00
epoch [19/50] batch [140/288] time 0.157 (0.166) data 0.000 (0.002) loss 2.1767 (1.8750) teacher_loss 1.3180 (0.9152) loss_zs_kd 0.0152 (0.0322) loss_oracle 0.3657 (0.4233) acc 75.0000 (75.3125) lr 1.4818e-03 eta 0:25:05
epoch [19/50] batch [160/288] time 0.160 (0.165) data 0.000 (0.002) loss 1.8041 (1.8798) teacher_loss 0.8607 (0.9194) loss_zs_kd 0.0270 (0.0315) loss_oracle 0.3769 (0.4217) acc 78.1250 (75.1953) lr 1.4818e-03 eta 0:24:57
epoch [19/50] batch [180/288] time 0.154 (0.165) data 0.000 (0.002) loss 2.0780 (1.8834) teacher_loss 0.9678 (0.9188) loss_zs_kd 0.0362 (0.0315) loss_oracle 0.4721 (0.4233) acc 71.8750 (75.0174) lr 1.4818e-03 eta 0:24:51
epoch [19/50] batch [200/288] time 0.154 (0.164) data 0.000 (0.002) loss 1.3167 (1.8721) teacher_loss 0.5463 (0.9098) loss_zs_kd 0.0169 (0.0314) loss_oracle 0.3607 (0.4219) acc 81.2500 (75.2031) lr 1.4818e-03 eta 0:24:36
epoch [19/50] batch [220/288] time 0.152 (0.163) data 0.000 (0.002) loss 1.6376 (1.8757) teacher_loss 0.8994 (0.9161) loss_zs_kd 0.0588 (0.0323) loss_oracle 0.3632 (0.4210) acc 78.1250 (75.2131) lr 1.4818e-03 eta 0:24:23
epoch [19/50] batch [240/288] time 0.179 (0.162) data 0.000 (0.001) loss 1.6191 (1.8758) teacher_loss 0.6330 (0.9140) loss_zs_kd 0.0615 (0.0324) loss_oracle 0.3877 (0.4212) acc 87.5000 (75.2214) lr 1.4818e-03 eta 0:24:18
epoch [19/50] batch [260/288] time 0.106 (0.158) data 0.000 (0.001) loss 2.0336 (1.8713) teacher_loss 1.0093 (0.9096) loss_zs_kd 0.0495 (0.0324) loss_oracle 0.5024 (0.4226) acc 78.1250 (75.3606) lr 1.4818e-03 eta 0:23:39
epoch [19/50] batch [280/288] time 0.084 (0.159) data 0.000 (0.001) loss 2.5022 (1.8729) teacher_loss 1.4884 (0.9106) loss_zs_kd 0.0494 (0.0326) loss_oracle 0.4064 (0.4242) acc 62.5000 (75.4241) lr 1.4818e-03 eta 0:23:42
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,442
* accuracy: 87.4%
* error: 12.6%
* macro_f1: 86.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,005
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.2%
******* Domain a best val acc:      87.6%, epoch: 18 *******
******* Domain a best val test acc: 82.8%, epoch: 18 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [20/50] batch [20/288] time 0.094 (0.167) data 0.000 (0.011) loss 2.5412 (1.8326) teacher_loss 1.3759 (0.8640) loss_zs_kd 0.0466 (0.0357) loss_oracle 0.4539 (0.4469) acc 59.3750 (76.4062) lr 1.4258e-03 eta 0:24:45
epoch [20/50] batch [40/288] time 0.306 (0.162) data 0.000 (0.006) loss 1.8588 (1.8347) teacher_loss 0.8035 (0.8697) loss_zs_kd 0.0721 (0.0356) loss_oracle 0.5577 (0.4582) acc 78.1250 (75.7812) lr 1.4258e-03 eta 0:24:00
epoch [20/50] batch [60/288] time 0.086 (0.153) data 0.000 (0.004) loss 1.7484 (1.8497) teacher_loss 0.7772 (0.8685) loss_zs_kd 0.0194 (0.0334) loss_oracle 0.4812 (0.4630) acc 81.2500 (75.9375) lr 1.4258e-03 eta 0:22:40
epoch [20/50] batch [80/288] time 0.154 (0.153) data 0.000 (0.003) loss 1.6398 (1.8485) teacher_loss 0.6986 (0.8689) loss_zs_kd 0.0303 (0.0327) loss_oracle 0.4156 (0.4658) acc 78.1250 (76.1719) lr 1.4258e-03 eta 0:22:34
epoch [20/50] batch [100/288] time 0.163 (0.154) data 0.000 (0.002) loss 1.8624 (1.8466) teacher_loss 0.9387 (0.8646) loss_zs_kd 0.0407 (0.0322) loss_oracle 0.4974 (0.4688) acc 71.8750 (76.3125) lr 1.4258e-03 eta 0:22:41
epoch [20/50] batch [120/288] time 0.150 (0.156) data 0.000 (0.002) loss 1.3786 (1.8702) teacher_loss 0.4724 (0.8873) loss_zs_kd 0.0154 (0.0329) loss_oracle 0.3738 (0.4697) acc 90.6250 (75.6510) lr 1.4258e-03 eta 0:22:56
epoch [20/50] batch [140/288] time 0.180 (0.158) data 0.000 (0.002) loss 1.6764 (1.8783) teacher_loss 0.7505 (0.8921) loss_zs_kd 0.0320 (0.0332) loss_oracle 0.4460 (0.4718) acc 78.1250 (75.6250) lr 1.4258e-03 eta 0:23:10
epoch [20/50] batch [160/288] time 0.160 (0.160) data 0.000 (0.002) loss 1.7852 (1.8715) teacher_loss 0.9092 (0.8882) loss_zs_kd 0.0299 (0.0326) loss_oracle 0.4645 (0.4727) acc 75.0000 (75.9180) lr 1.4258e-03 eta 0:23:26
epoch [20/50] batch [180/288] time 0.179 (0.162) data 0.000 (0.001) loss 1.6850 (1.8614) teacher_loss 0.6979 (0.8806) loss_zs_kd 0.0206 (0.0325) loss_oracle 0.4680 (0.4704) acc 81.2500 (76.0417) lr 1.4258e-03 eta 0:23:35
epoch [20/50] batch [200/288] time 0.161 (0.163) data 0.000 (0.001) loss 2.3717 (1.8623) teacher_loss 1.3233 (0.8834) loss_zs_kd 0.0901 (0.0323) loss_oracle 0.5275 (0.4704) acc 62.5000 (75.8594) lr 1.4258e-03 eta 0:23:39
epoch [20/50] batch [220/288] time 0.193 (0.163) data 0.001 (0.001) loss 1.7432 (1.8528) teacher_loss 0.7723 (0.8734) loss_zs_kd 0.0272 (0.0323) loss_oracle 0.4611 (0.4699) acc 78.1250 (76.0938) lr 1.4258e-03 eta 0:23:42
epoch [20/50] batch [240/288] time 0.173 (0.165) data 0.000 (0.001) loss 2.1707 (1.8537) teacher_loss 1.1027 (0.8732) loss_zs_kd 0.0310 (0.0321) loss_oracle 0.4718 (0.4690) acc 65.6250 (76.0156) lr 1.4258e-03 eta 0:23:50
epoch [20/50] batch [260/288] time 0.160 (0.165) data 0.000 (0.001) loss 2.0829 (1.8437) teacher_loss 1.1954 (0.8666) loss_zs_kd 0.0214 (0.0320) loss_oracle 0.4479 (0.4676) acc 65.6250 (76.2380) lr 1.4258e-03 eta 0:23:50
epoch [20/50] batch [280/288] time 0.091 (0.163) data 0.000 (0.001) loss 1.5434 (1.8439) teacher_loss 0.6705 (0.8654) loss_zs_kd 0.0227 (0.0320) loss_oracle 0.4584 (0.4690) acc 84.3750 (76.3393) lr 1.4258e-03 eta 0:23:30
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,442
* accuracy: 87.4%
* error: 12.6%
* macro_f1: 86.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,999
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 78.9%
******* Domain a best val acc:      87.6%, epoch: 18 *******
******* Domain a best val test acc: 82.8%, epoch: 18 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [21/50] batch [20/288] time 0.174 (0.188) data 0.000 (0.017) loss 1.4190 (1.7913) teacher_loss 0.3626 (0.8118) loss_zs_kd 0.0287 (0.0329) loss_oracle 0.4673 (0.4718) acc 93.7500 (77.6562) lr 1.3681e-03 eta 0:26:59
epoch [21/50] batch [40/288] time 0.087 (0.177) data 0.000 (0.009) loss 1.5877 (1.8312) teacher_loss 0.4903 (0.8310) loss_zs_kd 0.0326 (0.0301) loss_oracle 0.5620 (0.4742) acc 81.2500 (76.4844) lr 1.3681e-03 eta 0:25:19
epoch [21/50] batch [60/288] time 0.406 (0.171) data 0.000 (0.006) loss 1.7498 (1.8473) teacher_loss 0.6829 (0.8570) loss_zs_kd 0.0390 (0.0303) loss_oracle 0.4791 (0.4633) acc 81.2500 (76.1979) lr 1.3681e-03 eta 0:24:28
epoch [21/50] batch [80/288] time 0.093 (0.164) data 0.000 (0.004) loss 1.5562 (1.8638) teacher_loss 0.5014 (0.8772) loss_zs_kd 0.0192 (0.0304) loss_oracle 0.3622 (0.4607) acc 84.3750 (75.7031) lr 1.3681e-03 eta 0:23:21
epoch [21/50] batch [100/288] time 0.149 (0.158) data 0.000 (0.004) loss 1.8309 (1.8474) teacher_loss 0.8808 (0.8630) loss_zs_kd 0.0180 (0.0302) loss_oracle 0.4756 (0.4583) acc 75.0000 (76.1562) lr 1.3681e-03 eta 0:22:26
epoch [21/50] batch [120/288] time 0.148 (0.157) data 0.000 (0.003) loss 1.6962 (1.8536) teacher_loss 0.6746 (0.8693) loss_zs_kd 0.0260 (0.0301) loss_oracle 0.4711 (0.4572) acc 81.2500 (76.2240) lr 1.3681e-03 eta 0:22:16
epoch [21/50] batch [140/288] time 0.144 (0.156) data 0.000 (0.003) loss 1.9713 (1.8658) teacher_loss 0.7917 (0.8777) loss_zs_kd 0.0343 (0.0303) loss_oracle 0.5568 (0.4576) acc 78.1250 (76.0938) lr 1.3681e-03 eta 0:22:09
epoch [21/50] batch [160/288] time 0.150 (0.156) data 0.000 (0.002) loss 1.6901 (1.8844) teacher_loss 0.7841 (0.8910) loss_zs_kd 0.0277 (0.0313) loss_oracle 0.4630 (0.4603) acc 78.1250 (75.7031) lr 1.3681e-03 eta 0:21:59
epoch [21/50] batch [180/288] time 0.152 (0.155) data 0.000 (0.002) loss 2.2808 (1.8863) teacher_loss 1.1595 (0.8938) loss_zs_kd 0.0183 (0.0318) loss_oracle 0.4082 (0.4587) acc 71.8750 (75.5208) lr 1.3681e-03 eta 0:21:53
epoch [21/50] batch [200/288] time 0.129 (0.155) data 0.000 (0.002) loss 2.2598 (1.8798) teacher_loss 1.0979 (0.8901) loss_zs_kd 0.0329 (0.0318) loss_oracle 0.4358 (0.4566) acc 68.7500 (75.6250) lr 1.3681e-03 eta 0:21:46
epoch [21/50] batch [220/288] time 0.151 (0.155) data 0.000 (0.002) loss 1.6158 (1.8738) teacher_loss 0.7371 (0.8887) loss_zs_kd 0.0202 (0.0316) loss_oracle 0.3455 (0.4506) acc 81.2500 (75.7102) lr 1.3681e-03 eta 0:21:48
epoch [21/50] batch [240/288] time 0.151 (0.156) data 0.000 (0.002) loss 1.7244 (1.8781) teacher_loss 0.8464 (0.8985) loss_zs_kd 0.0472 (0.0313) loss_oracle 0.4011 (0.4465) acc 81.2500 (75.6120) lr 1.3681e-03 eta 0:21:53
epoch [21/50] batch [260/288] time 0.152 (0.157) data 0.000 (0.002) loss 1.8003 (1.8713) teacher_loss 0.8416 (0.8967) loss_zs_kd 0.0307 (0.0317) loss_oracle 0.4409 (0.4428) acc 75.0000 (75.5769) lr 1.3681e-03 eta 0:21:52
epoch [21/50] batch [280/288] time 0.156 (0.157) data 0.000 (0.001) loss 1.8241 (1.8700) teacher_loss 0.8623 (0.8959) loss_zs_kd 0.0311 (0.0317) loss_oracle 0.3883 (0.4412) acc 81.2500 (75.5357) lr 1.3681e-03 eta 0:21:52
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,445
* accuracy: 87.5%
* error: 12.5%
* macro_f1: 87.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,008
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 79.3%
******* Domain a best val acc:      87.6%, epoch: 18 *******
******* Domain a best val test acc: 82.8%, epoch: 18 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [22/50] batch [20/288] time 0.150 (0.169) data 0.000 (0.011) loss 2.3156 (1.8380) teacher_loss 1.2864 (0.8816) loss_zs_kd 0.0348 (0.0337) loss_oracle 0.4473 (0.4478) acc 65.6250 (75.4688) lr 1.3090e-03 eta 0:23:29
epoch [22/50] batch [40/288] time 0.110 (0.132) data 0.000 (0.006) loss 1.6111 (1.8033) teacher_loss 0.8872 (0.8772) loss_zs_kd 0.0083 (0.0321) loss_oracle 0.3564 (0.4458) acc 81.2500 (75.5469) lr 1.3090e-03 eta 0:18:20
epoch [22/50] batch [60/288] time 0.097 (0.140) data 0.000 (0.004) loss 1.5851 (1.8038) teacher_loss 0.6267 (0.8926) loss_zs_kd 0.0261 (0.0311) loss_oracle 0.4755 (0.4423) acc 78.1250 (75.6250) lr 1.3090e-03 eta 0:19:18
epoch [22/50] batch [80/288] time 0.099 (0.148) data 0.000 (0.003) loss 2.3858 (1.7922) teacher_loss 1.2971 (0.8755) loss_zs_kd 0.0351 (0.0326) loss_oracle 0.4619 (0.4415) acc 68.7500 (76.2891) lr 1.3090e-03 eta 0:20:20
epoch [22/50] batch [100/288] time 0.164 (0.144) data 0.000 (0.002) loss 1.3441 (1.7997) teacher_loss 0.5677 (0.8748) loss_zs_kd 0.0129 (0.0316) loss_oracle 0.3484 (0.4375) acc 84.3750 (76.2812) lr 1.3090e-03 eta 0:19:45
epoch [22/50] batch [120/288] time 0.219 (0.148) data 0.000 (0.002) loss 1.7769 (1.8129) teacher_loss 0.9481 (0.8824) loss_zs_kd 0.0383 (0.0328) loss_oracle 0.3769 (0.4378) acc 75.0000 (76.3542) lr 1.3090e-03 eta 0:20:16
epoch [22/50] batch [140/288] time 0.099 (0.147) data 0.000 (0.002) loss 1.9082 (1.8097) teacher_loss 0.8976 (0.8771) loss_zs_kd 0.0577 (0.0325) loss_oracle 0.4297 (0.4390) acc 75.0000 (76.4732) lr 1.3090e-03 eta 0:20:07
epoch [22/50] batch [160/288] time 0.107 (0.147) data 0.000 (0.002) loss 1.4872 (1.8229) teacher_loss 0.5190 (0.8856) loss_zs_kd 0.0182 (0.0325) loss_oracle 0.4766 (0.4385) acc 87.5000 (76.4453) lr 1.3090e-03 eta 0:20:02
epoch [22/50] batch [180/288] time 0.333 (0.149) data 0.000 (0.001) loss 1.8358 (1.8246) teacher_loss 0.9241 (0.8822) loss_zs_kd 0.0205 (0.0321) loss_oracle 0.4195 (0.4396) acc 81.2500 (76.5451) lr 1.3090e-03 eta 0:20:19
epoch [22/50] batch [200/288] time 0.138 (0.147) data 0.000 (0.001) loss 1.9160 (1.8307) teacher_loss 0.8932 (0.8848) loss_zs_kd 0.0334 (0.0322) loss_oracle 0.4136 (0.4395) acc 81.2500 (76.5156) lr 1.3090e-03 eta 0:19:55
epoch [22/50] batch [220/288] time 0.151 (0.147) data 0.000 (0.001) loss 1.2367 (1.8313) teacher_loss 0.4710 (0.8854) loss_zs_kd 0.0186 (0.0326) loss_oracle 0.3708 (0.4385) acc 87.5000 (76.2358) lr 1.3090e-03 eta 0:19:58
epoch [22/50] batch [240/288] time 0.152 (0.148) data 0.000 (0.001) loss 1.8839 (1.8324) teacher_loss 0.9177 (0.8883) loss_zs_kd 0.0283 (0.0324) loss_oracle 0.4346 (0.4371) acc 71.8750 (76.1458) lr 1.3090e-03 eta 0:20:00
epoch [22/50] batch [260/288] time 0.168 (0.150) data 0.000 (0.001) loss 1.8550 (1.8319) teacher_loss 0.8735 (0.8892) loss_zs_kd 0.0184 (0.0321) loss_oracle 0.4377 (0.4360) acc 78.1250 (76.1659) lr 1.3090e-03 eta 0:20:10
epoch [22/50] batch [280/288] time 0.153 (0.151) data 0.000 (0.001) loss 1.3611 (1.8373) teacher_loss 0.5202 (0.8906) loss_zs_kd 0.0277 (0.0322) loss_oracle 0.4006 (0.4343) acc 84.3750 (75.9821) lr 1.3090e-03 eta 0:20:15
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,437
* accuracy: 87.3%
* error: 12.7%
* macro_f1: 86.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,004
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.1%
******* Domain a best val acc:      87.6%, epoch: 18 *******
******* Domain a best val test acc: 82.8%, epoch: 18 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
epoch [23/50] batch [20/288] time 0.177 (0.183) data 0.000 (0.015) loss 1.9143 (1.8205) teacher_loss 1.0615 (0.8509) loss_zs_kd 0.0271 (0.0296) loss_oracle 0.3364 (0.4096) acc 71.8750 (78.2812) lr 1.2487e-03 eta 0:24:30
epoch [23/50] batch [40/288] time 0.174 (0.173) data 0.000 (0.007) loss 1.6372 (1.8033) teacher_loss 0.8424 (0.8497) loss_zs_kd 0.0319 (0.0322) loss_oracle 0.3738 (0.4055) acc 75.0000 (77.5781) lr 1.2487e-03 eta 0:23:05
epoch [23/50] batch [60/288] time 0.184 (0.170) data 0.000 (0.005) loss 2.2290 (1.8241) teacher_loss 1.1053 (0.8738) loss_zs_kd 0.0527 (0.0347) loss_oracle 0.4322 (0.4059) acc 62.5000 (76.9271) lr 1.2487e-03 eta 0:22:41
epoch [23/50] batch [80/288] time 0.180 (0.172) data 0.000 (0.004) loss 2.0137 (1.8151) teacher_loss 1.0734 (0.8612) loss_zs_kd 0.0376 (0.0341) loss_oracle 0.4109 (0.4086) acc 75.0000 (77.3828) lr 1.2487e-03 eta 0:22:53
epoch [23/50] batch [100/288] time 0.102 (0.167) data 0.000 (0.003) loss 1.7423 (1.8159) teacher_loss 0.7426 (0.8561) loss_zs_kd 0.0265 (0.0338) loss_oracle 0.4393 (0.4154) acc 81.2500 (77.5000) lr 1.2487e-03 eta 0:22:12
epoch [23/50] batch [120/288] time 0.152 (0.165) data 0.000 (0.003) loss 1.3305 (1.8230) teacher_loss 0.4607 (0.8617) loss_zs_kd 0.0337 (0.0343) loss_oracle 0.4178 (0.4192) acc 90.6250 (77.4219) lr 1.2487e-03 eta 0:21:51
epoch [23/50] batch [140/288] time 0.093 (0.167) data 0.000 (0.002) loss 1.5649 (1.8149) teacher_loss 0.9037 (0.8596) loss_zs_kd 0.0245 (0.0341) loss_oracle 0.4183 (0.4192) acc 81.2500 (77.5000) lr 1.2487e-03 eta 0:22:03
epoch [23/50] batch [160/288] time 0.369 (0.161) data 0.000 (0.002) loss 1.9604 (1.8260) teacher_loss 0.8701 (0.8733) loss_zs_kd 0.0254 (0.0341) loss_oracle 0.4471 (0.4184) acc 78.1250 (76.9141) lr 1.2487e-03 eta 0:21:08
epoch [23/50] batch [180/288] time 0.098 (0.159) data 0.000 (0.002) loss 1.7307 (1.8223) teacher_loss 0.7635 (0.8686) loss_zs_kd 0.0159 (0.0338) loss_oracle 0.4286 (0.4190) acc 84.3750 (77.1528) lr 1.2487e-03 eta 0:20:52
epoch [23/50] batch [200/288] time 0.092 (0.160) data 0.000 (0.002) loss 1.6266 (1.8254) teacher_loss 0.6771 (0.8689) loss_zs_kd 0.0272 (0.0336) loss_oracle 0.4001 (0.4200) acc 78.1250 (77.2812) lr 1.2487e-03 eta 0:20:54
epoch [23/50] batch [220/288] time 0.084 (0.160) data 0.000 (0.002) loss 2.2864 (1.8310) teacher_loss 1.1407 (0.8703) loss_zs_kd 0.0250 (0.0336) loss_oracle 0.4853 (0.4198) acc 71.8750 (77.0312) lr 1.2487e-03 eta 0:20:53
epoch [23/50] batch [240/288] time 0.086 (0.160) data 0.000 (0.001) loss 1.7133 (1.8427) teacher_loss 0.7854 (0.8798) loss_zs_kd 0.0215 (0.0333) loss_oracle 0.4851 (0.4223) acc 75.0000 (76.7969) lr 1.2487e-03 eta 0:20:53
epoch [23/50] batch [260/288] time 0.148 (0.158) data 0.000 (0.001) loss 1.5912 (1.8545) teacher_loss 0.6583 (0.8867) loss_zs_kd 0.0318 (0.0332) loss_oracle 0.4363 (0.4249) acc 84.3750 (76.4543) lr 1.2487e-03 eta 0:20:30
epoch [23/50] batch [280/288] time 0.152 (0.158) data 0.000 (0.001) loss 2.0133 (1.8511) teacher_loss 0.8671 (0.8812) loss_zs_kd 0.0478 (0.0332) loss_oracle 0.5042 (0.4267) acc 71.8750 (76.5067) lr 1.2487e-03 eta 0:20:28
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,438
* accuracy: 87.3%
* error: 12.7%
* macro_f1: 86.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,013
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.5%
******* Domain a best val acc:      87.6%, epoch: 18 *******
******* Domain a best val test acc: 82.8%, epoch: 18 *******
******* Domain a best test acc:     83.7%, epoch: 2 *******
