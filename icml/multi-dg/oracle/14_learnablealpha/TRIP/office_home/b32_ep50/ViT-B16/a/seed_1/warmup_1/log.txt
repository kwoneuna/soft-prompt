Loading trainer: TRIP
Loading dataset: SPG_OfficeHome
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------------------------------
Dataset    SPG_OfficeHome
Source     ['clipart', 'product', 'real_world']
Target     ['art']
# classes  65
# train_x  9,222
# val      3,939
# test     2,427
---------  ------------------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
alpha_logit                                        1
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,196
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/288] time 0.107 (0.147) data 0.000 (0.018) loss 1.1822 (1.3108) teacher_loss 1.1114 (1.2378) loss_zs_kd 0.0000 (0.0000) loss_oracle 0.0707 (0.0729) acc 68.7500 (68.4375) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:35:14
epoch [1/50] batch [40/288] time 0.112 (0.127) data 0.000 (0.009) loss 1.2976 (1.3301) teacher_loss 1.2158 (1.2591) loss_zs_kd 0.0002 (0.0000) loss_oracle 0.0818 (0.0710) acc 65.6250 (68.2812) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:30:21
epoch [1/50] batch [60/288] time 0.107 (0.119) data 0.001 (0.006) loss 1.4846 (1.3401) teacher_loss 1.4298 (1.2661) loss_zs_kd 0.0006 (0.0001) loss_oracle 0.0545 (0.0740) acc 59.3750 (67.6042) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:28:23
epoch [1/50] batch [80/288] time 0.108 (0.114) data 0.000 (0.005) loss 1.3266 (1.3478) teacher_loss 1.2566 (1.2702) loss_zs_kd 0.0008 (0.0002) loss_oracle 0.0696 (0.0776) acc 62.5000 (67.3828) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:27:07
epoch [1/50] batch [100/288] time 0.098 (0.111) data 0.000 (0.004) loss 1.1709 (1.3193) teacher_loss 1.0896 (1.2425) loss_zs_kd 0.0006 (0.0003) loss_oracle 0.0810 (0.0767) acc 62.5000 (68.0938) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:26:20
epoch [1/50] batch [120/288] time 0.093 (0.108) data 0.000 (0.003) loss 1.1185 (1.3140) teacher_loss 1.0337 (1.2363) loss_zs_kd 0.0007 (0.0004) loss_oracle 0.0844 (0.0775) acc 75.0000 (68.1510) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:25:44
epoch [1/50] batch [140/288] time 0.092 (0.106) data 0.000 (0.003) loss 1.5773 (1.3011) teacher_loss 1.4880 (1.2224) loss_zs_kd 0.0010 (0.0005) loss_oracle 0.0887 (0.0785) acc 56.2500 (68.2589) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:25:17
epoch [1/50] batch [160/288] time 0.096 (0.105) data 0.000 (0.002) loss 1.0896 (1.2981) teacher_loss 1.0577 (1.2200) loss_zs_kd 0.0018 (0.0006) loss_oracle 0.0310 (0.0779) acc 71.8750 (68.5938) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:24:58
epoch [1/50] batch [180/288] time 0.098 (0.104) data 0.000 (0.002) loss 1.0592 (1.3012) teacher_loss 0.9939 (1.2235) loss_zs_kd 0.0023 (0.0007) loss_oracle 0.0642 (0.0773) acc 71.8750 (68.3854) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:24:39
epoch [1/50] batch [200/288] time 0.099 (0.103) data 0.000 (0.002) loss 1.4315 (1.2978) teacher_loss 1.3373 (1.2203) loss_zs_kd 0.0019 (0.0008) loss_oracle 0.0933 (0.0771) acc 62.5000 (68.2812) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:24:23
epoch [1/50] batch [220/288] time 0.105 (0.102) data 0.000 (0.002) loss 1.4316 (1.2930) teacher_loss 1.4162 (1.2160) loss_zs_kd 0.0022 (0.0010) loss_oracle 0.0144 (0.0766) acc 65.6250 (68.5511) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:24:09
epoch [1/50] batch [240/288] time 0.096 (0.102) data 0.000 (0.002) loss 1.0545 (1.2902) teacher_loss 0.9854 (1.2135) loss_zs_kd 0.0010 (0.0011) loss_oracle 0.0686 (0.0761) acc 81.2500 (68.8281) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:23:58
epoch [1/50] batch [260/288] time 0.101 (0.102) data 0.000 (0.002) loss 1.4115 (1.2959) teacher_loss 1.3034 (1.2191) loss_zs_kd 0.0053 (0.0012) loss_oracle 0.1054 (0.0761) acc 71.8750 (68.6899) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:24:03
epoch [1/50] batch [280/288] time 0.089 (0.101) data 0.000 (0.001) loss 1.5663 (1.2957) teacher_loss 1.5186 (1.2190) loss_zs_kd 0.0052 (0.0014) loss_oracle 0.0451 (0.0760) acc 62.5000 (68.7500) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:23:52
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,264
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 81.9%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,963
* accuracy: 80.9%
* error: 19.1%
* macro_f1: 76.7%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      82.9%, epoch: 1 *******
******* Domain a best val test acc: 80.9%, epoch: 1 *******
******* Domain a best test acc:     80.9%, epoch: 1 *******
epoch [2/50] batch [20/288] time 0.097 (0.116) data 0.000 (0.019) loss 1.2363 (1.3865) teacher_loss 0.9678 (1.1794) loss_zs_kd 0.1071 (0.0996) loss_oracle 0.2149 (0.1572) acc 75.0000 (69.8438) alaph_mean 0.5001 (0.5000) alpha_val 0.5001 (0.5000) lr 2.0000e-03 eta 0:27:18
epoch [2/50] batch [40/288] time 0.101 (0.107) data 0.000 (0.010) loss 1.0455 (1.3852) teacher_loss 0.7081 (1.1093) loss_zs_kd 0.1187 (0.1211) loss_oracle 0.2781 (0.2153) acc 81.2500 (71.0156) alaph_mean 0.5003 (0.5001) alpha_val 0.5003 (0.5001) lr 2.0000e-03 eta 0:25:04
epoch [2/50] batch [60/288] time 0.097 (0.103) data 0.000 (0.007) loss 1.6554 (1.4086) teacher_loss 1.3275 (1.1095) loss_zs_kd 0.0938 (0.1231) loss_oracle 0.2810 (0.2376) acc 68.7500 (71.7188) alaph_mean 0.5006 (0.5002) alpha_val 0.5006 (0.5002) lr 2.0000e-03 eta 0:24:10
epoch [2/50] batch [80/288] time 0.093 (0.102) data 0.000 (0.005) loss 1.1240 (1.4195) teacher_loss 0.8144 (1.0951) loss_zs_kd 0.1114 (0.1207) loss_oracle 0.2538 (0.2641) acc 68.7500 (72.1875) alaph_mean 0.5007 (0.5003) alpha_val 0.5007 (0.5003) lr 2.0000e-03 eta 0:23:48
epoch [2/50] batch [100/288] time 0.091 (0.101) data 0.000 (0.004) loss 1.8034 (1.4271) teacher_loss 1.4516 (1.0952) loss_zs_kd 0.1031 (0.1181) loss_oracle 0.3003 (0.2729) acc 62.5000 (72.0938) alaph_mean 0.5008 (0.5004) alpha_val 0.5008 (0.5004) lr 2.0000e-03 eta 0:23:29
epoch [2/50] batch [120/288] time 0.104 (0.100) data 0.000 (0.003) loss 1.3734 (1.4250) teacher_loss 1.0385 (1.0912) loss_zs_kd 0.0851 (0.1188) loss_oracle 0.2923 (0.2744) acc 75.0000 (72.0312) alaph_mean 0.5013 (0.5005) alpha_val 0.5013 (0.5005) lr 2.0000e-03 eta 0:23:21
epoch [2/50] batch [140/288] time 0.100 (0.100) data 0.000 (0.003) loss 1.5728 (1.4385) teacher_loss 1.1397 (1.0948) loss_zs_kd 0.1266 (0.1163) loss_oracle 0.3698 (0.2855) acc 68.7500 (72.0536) alaph_mean 0.5017 (0.5007) alpha_val 0.5017 (0.5007) lr 2.0000e-03 eta 0:23:13
epoch [2/50] batch [160/288] time 0.097 (0.099) data 0.000 (0.003) loss 1.4666 (1.4436) teacher_loss 1.1254 (1.0966) loss_zs_kd 0.1856 (0.1150) loss_oracle 0.2484 (0.2895) acc 71.8750 (71.9141) alaph_mean 0.5024 (0.5008) alpha_val 0.5024 (0.5008) lr 2.0000e-03 eta 0:23:01
epoch [2/50] batch [180/288] time 0.097 (0.099) data 0.000 (0.002) loss 1.3156 (1.4381) teacher_loss 1.0077 (1.0908) loss_zs_kd 0.1225 (0.1152) loss_oracle 0.2466 (0.2898) acc 78.1250 (72.0660) alaph_mean 0.5030 (0.5011) alpha_val 0.5030 (0.5011) lr 2.0000e-03 eta 0:22:55
epoch [2/50] batch [200/288] time 0.098 (0.098) data 0.000 (0.002) loss 2.1442 (1.4482) teacher_loss 1.7425 (1.0934) loss_zs_kd 0.1394 (0.1155) loss_oracle 0.3320 (0.2971) acc 59.3750 (71.8125) alaph_mean 0.5037 (0.5013) alpha_val 0.5037 (0.5013) lr 2.0000e-03 eta 0:22:49
epoch [2/50] batch [220/288] time 0.101 (0.098) data 0.000 (0.002) loss 1.5703 (1.4408) teacher_loss 1.0948 (1.0818) loss_zs_kd 0.1278 (0.1151) loss_oracle 0.4116 (0.3015) acc 75.0000 (72.0455) alaph_mean 0.5043 (0.5015) alpha_val 0.5043 (0.5015) lr 2.0000e-03 eta 0:22:42
epoch [2/50] batch [240/288] time 0.095 (0.099) data 0.000 (0.002) loss 1.8601 (1.4498) teacher_loss 1.4409 (1.0776) loss_zs_kd 0.1041 (0.1151) loss_oracle 0.3672 (0.3146) acc 62.5000 (72.0312) alaph_mean 0.5048 (0.5018) alpha_val 0.5048 (0.5018) lr 2.0000e-03 eta 0:22:59
epoch [2/50] batch [260/288] time 0.095 (0.099) data 0.000 (0.002) loss 1.5347 (1.4493) teacher_loss 1.0611 (1.0735) loss_zs_kd 0.1614 (0.1150) loss_oracle 0.3929 (0.3183) acc 71.8750 (71.9712) alaph_mean 0.5055 (0.5020) alpha_val 0.5055 (0.5020) lr 2.0000e-03 eta 0:22:54
epoch [2/50] batch [280/288] time 0.086 (0.099) data 0.000 (0.002) loss 1.3076 (1.4510) teacher_loss 0.8415 (1.0685) loss_zs_kd 0.1328 (0.1146) loss_oracle 0.3997 (0.3252) acc 75.0000 (72.0647) alaph_mean 0.5060 (0.5023) alpha_val 0.5060 (0.5023) lr 2.0000e-03 eta 0:22:44
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,365
* accuracy: 85.4%
* error: 14.6%
* macro_f1: 84.6%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,019
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.6%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      85.4%, epoch: 2 *******
******* Domain a best val test acc: 83.2%, epoch: 2 *******
******* Domain a best test acc:     83.2%, epoch: 2 *******
epoch [3/50] batch [20/288] time 0.091 (0.111) data 0.000 (0.013) loss 1.7943 (1.4607) teacher_loss 1.3738 (1.0383) loss_zs_kd 0.1415 (0.1173) loss_oracle 0.3498 (0.3637) acc 62.5000 (73.1250) alaph_mean 0.5068 (0.5065) alpha_val 0.5068 (0.5065) lr 1.9980e-03 eta 0:25:33
epoch [3/50] batch [40/288] time 0.103 (0.105) data 0.000 (0.007) loss 1.1242 (1.3744) teacher_loss 0.7905 (0.9739) loss_zs_kd 0.0763 (0.1153) loss_oracle 0.2955 (0.3429) acc 81.2500 (74.5312) alaph_mean 0.5072 (0.5068) alpha_val 0.5072 (0.5068) lr 1.9980e-03 eta 0:24:02
epoch [3/50] batch [60/288] time 0.096 (0.101) data 0.000 (0.004) loss 1.3539 (1.4489) teacher_loss 0.9107 (1.0241) loss_zs_kd 0.0832 (0.1117) loss_oracle 0.4016 (0.3689) acc 75.0000 (73.2812) alaph_mean 0.5079 (0.5070) alpha_val 0.5079 (0.5070) lr 1.9980e-03 eta 0:23:05
epoch [3/50] batch [80/288] time 0.094 (0.099) data 0.000 (0.003) loss 1.3193 (1.4482) teacher_loss 0.9972 (1.0291) loss_zs_kd 0.0957 (0.1095) loss_oracle 0.2742 (0.3643) acc 75.0000 (73.0859) alaph_mean 0.5083 (0.5073) alpha_val 0.5083 (0.5073) lr 1.9980e-03 eta 0:22:45
epoch [3/50] batch [100/288] time 0.089 (0.098) data 0.000 (0.003) loss 1.4038 (1.4587) teacher_loss 0.9509 (1.0392) loss_zs_kd 0.1061 (0.1099) loss_oracle 0.3998 (0.3645) acc 75.0000 (72.8750) alaph_mean 0.5091 (0.5076) alpha_val 0.5091 (0.5076) lr 1.9980e-03 eta 0:22:25
epoch [3/50] batch [120/288] time 0.092 (0.097) data 0.000 (0.002) loss 1.0803 (1.4550) teacher_loss 0.6619 (1.0268) loss_zs_kd 0.1207 (0.1143) loss_oracle 0.3580 (0.3710) acc 78.1250 (73.1771) alaph_mean 0.5099 (0.5079) alpha_val 0.5099 (0.5079) lr 1.9980e-03 eta 0:22:14
epoch [3/50] batch [140/288] time 0.105 (0.097) data 0.000 (0.002) loss 1.3682 (1.4536) teacher_loss 0.9884 (1.0237) loss_zs_kd 0.0758 (0.1154) loss_oracle 0.3419 (0.3722) acc 75.0000 (73.1920) alaph_mean 0.5108 (0.5083) alpha_val 0.5108 (0.5083) lr 1.9980e-03 eta 0:22:11
epoch [3/50] batch [160/288] time 0.093 (0.097) data 0.000 (0.002) loss 1.8874 (1.4556) teacher_loss 1.4919 (1.0305) loss_zs_kd 0.1467 (0.1181) loss_oracle 0.3221 (0.3661) acc 59.3750 (73.0078) alaph_mean 0.5114 (0.5086) alpha_val 0.5114 (0.5086) lr 1.9980e-03 eta 0:22:03
epoch [3/50] batch [180/288] time 0.102 (0.097) data 0.000 (0.002) loss 1.5296 (1.4533) teacher_loss 1.0933 (1.0302) loss_zs_kd 0.1251 (0.1178) loss_oracle 0.3737 (0.3642) acc 71.8750 (73.0903) alaph_mean 0.5120 (0.5090) alpha_val 0.5120 (0.5090) lr 1.9980e-03 eta 0:22:00
epoch [3/50] batch [200/288] time 0.095 (0.097) data 0.000 (0.001) loss 1.4527 (1.4517) teacher_loss 0.9746 (1.0268) loss_zs_kd 0.0898 (0.1178) loss_oracle 0.4333 (0.3659) acc 78.1250 (73.2344) alaph_mean 0.5129 (0.5093) alpha_val 0.5129 (0.5093) lr 1.9980e-03 eta 0:21:58
epoch [3/50] batch [220/288] time 0.090 (0.098) data 0.000 (0.001) loss 1.3198 (1.4654) teacher_loss 0.8293 (1.0372) loss_zs_kd 0.1168 (0.1185) loss_oracle 0.4321 (0.3690) acc 75.0000 (73.1108) alaph_mean 0.5139 (0.5097) alpha_val 0.5139 (0.5097) lr 1.9980e-03 eta 0:22:15
epoch [3/50] batch [240/288] time 0.096 (0.098) data 0.000 (0.001) loss 1.6200 (1.4692) teacher_loss 1.3332 (1.0419) loss_zs_kd 0.0839 (0.1175) loss_oracle 0.2448 (0.3686) acc 62.5000 (72.9818) alaph_mean 0.5149 (0.5101) alpha_val 0.5149 (0.5101) lr 1.9980e-03 eta 0:22:09
epoch [3/50] batch [260/288] time 0.099 (0.098) data 0.000 (0.001) loss 1.1241 (1.4607) teacher_loss 0.6993 (1.0376) loss_zs_kd 0.1162 (0.1168) loss_oracle 0.3666 (0.3647) acc 78.1250 (72.9447) alaph_mean 0.5158 (0.5105) alpha_val 0.5158 (0.5105) lr 1.9980e-03 eta 0:22:03
epoch [3/50] batch [280/288] time 0.085 (0.097) data 0.000 (0.001) loss 1.6063 (1.4597) teacher_loss 1.0548 (1.0335) loss_zs_kd 0.2006 (0.1183) loss_oracle 0.4512 (0.3670) acc 75.0000 (73.1473) alaph_mean 0.5166 (0.5109) alpha_val 0.5166 (0.5109) lr 1.9980e-03 eta 0:21:54
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,384
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.1%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,019
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.8%
******* Domain a best val acc:      85.9%, epoch: 3 *******
******* Domain a best val test acc: 83.2%, epoch: 3 *******
******* Domain a best test acc:     83.2%, epoch: 2 *******
epoch [4/50] batch [20/288] time 0.109 (0.118) data 0.001 (0.013) loss 1.2670 (1.3986) teacher_loss 0.8295 (0.9561) loss_zs_kd 0.1050 (0.1152) loss_oracle 0.3850 (0.3849) acc 84.3750 (74.2188) alaph_mean 0.5181 (0.5176) alpha_val 0.5181 (0.5176) lr 1.9921e-03 eta 0:26:29
epoch [4/50] batch [40/288] time 0.127 (0.118) data 0.000 (0.007) loss 1.5178 (1.3986) teacher_loss 1.0605 (0.9780) loss_zs_kd 0.1637 (0.1171) loss_oracle 0.3754 (0.3621) acc 71.8750 (73.9844) alaph_mean 0.5188 (0.5180) alpha_val 0.5188 (0.5180) lr 1.9921e-03 eta 0:26:29
epoch [4/50] batch [60/288] time 0.115 (0.117) data 0.001 (0.005) loss 1.5620 (1.3825) teacher_loss 1.1414 (0.9728) loss_zs_kd 0.0917 (0.1148) loss_oracle 0.3748 (0.3523) acc 68.7500 (74.0625) alaph_mean 0.5195 (0.5184) alpha_val 0.5195 (0.5184) lr 1.9921e-03 eta 0:26:12
epoch [4/50] batch [80/288] time 0.104 (0.114) data 0.000 (0.004) loss 1.8473 (1.4117) teacher_loss 1.4084 (0.9933) loss_zs_kd 0.0964 (0.1125) loss_oracle 0.3908 (0.3622) acc 68.7500 (73.8672) alaph_mean 0.5199 (0.5187) alpha_val 0.5199 (0.5187) lr 1.9921e-03 eta 0:25:39
epoch [4/50] batch [100/288] time 0.111 (0.112) data 0.000 (0.003) loss 1.2605 (1.4280) teacher_loss 0.8887 (1.0108) loss_zs_kd 0.0984 (0.1104) loss_oracle 0.3226 (0.3620) acc 78.1250 (73.5312) alaph_mean 0.5205 (0.5190) alpha_val 0.5205 (0.5190) lr 1.9921e-03 eta 0:25:09
epoch [4/50] batch [120/288] time 0.106 (0.111) data 0.000 (0.002) loss 1.3244 (1.4341) teacher_loss 0.9659 (1.0206) loss_zs_kd 0.1047 (0.1091) loss_oracle 0.3062 (0.3589) acc 78.1250 (73.2552) alaph_mean 0.5212 (0.5193) alpha_val 0.5212 (0.5193) lr 1.9921e-03 eta 0:24:50
epoch [4/50] batch [140/288] time 0.103 (0.110) data 0.001 (0.002) loss 1.1889 (1.4353) teacher_loss 0.7935 (1.0262) loss_zs_kd 0.0983 (0.1091) loss_oracle 0.3463 (0.3546) acc 78.1250 (72.9018) alaph_mean 0.5221 (0.5197) alpha_val 0.5221 (0.5197) lr 1.9921e-03 eta 0:24:34
epoch [4/50] batch [160/288] time 0.101 (0.109) data 0.000 (0.002) loss 1.5916 (1.4282) teacher_loss 1.1557 (1.0186) loss_zs_kd 0.1142 (0.1106) loss_oracle 0.3787 (0.3544) acc 68.7500 (73.1445) alaph_mean 0.5230 (0.5200) alpha_val 0.5230 (0.5200) lr 1.9921e-03 eta 0:24:21
epoch [4/50] batch [180/288] time 0.161 (0.110) data 0.000 (0.002) loss 1.7395 (1.4256) teacher_loss 1.4047 (1.0163) loss_zs_kd 0.1348 (0.1110) loss_oracle 0.2674 (0.3537) acc 62.5000 (73.2118) alaph_mean 0.5237 (0.5204) alpha_val 0.5237 (0.5204) lr 1.9921e-03 eta 0:24:29
epoch [4/50] batch [200/288] time 0.102 (0.110) data 0.001 (0.002) loss 1.5173 (1.4265) teacher_loss 1.0920 (1.0133) loss_zs_kd 0.1047 (0.1132) loss_oracle 0.3729 (0.3566) acc 68.7500 (73.5000) alaph_mean 0.5247 (0.5208) alpha_val 0.5247 (0.5208) lr 1.9921e-03 eta 0:24:30
epoch [4/50] batch [220/288] time 0.106 (0.110) data 0.001 (0.001) loss 1.5470 (1.4310) teacher_loss 1.1323 (1.0159) loss_zs_kd 0.1141 (0.1142) loss_oracle 0.3577 (0.3580) acc 71.8750 (73.3381) alaph_mean 0.5258 (0.5212) alpha_val 0.5258 (0.5212) lr 1.9921e-03 eta 0:24:20
epoch [4/50] batch [240/288] time 0.099 (0.109) data 0.000 (0.001) loss 1.6373 (1.4333) teacher_loss 1.1751 (1.0138) loss_zs_kd 0.0958 (0.1135) loss_oracle 0.4143 (0.3627) acc 75.0000 (73.5677) alaph_mean 0.5268 (0.5216) alpha_val 0.5268 (0.5216) lr 1.9921e-03 eta 0:24:08
epoch [4/50] batch [260/288] time 0.099 (0.109) data 0.000 (0.001) loss 1.2346 (1.4405) teacher_loss 0.8307 (1.0184) loss_zs_kd 0.1125 (0.1134) loss_oracle 0.3477 (0.3653) acc 71.8750 (73.3534) alaph_mean 0.5274 (0.5220) alpha_val 0.5274 (0.5220) lr 1.9921e-03 eta 0:24:05
epoch [4/50] batch [280/288] time 0.108 (0.109) data 0.000 (0.001) loss 1.1532 (1.4355) teacher_loss 0.7598 (1.0133) loss_zs_kd 0.0815 (0.1126) loss_oracle 0.3527 (0.3659) acc 81.2500 (73.5714) alaph_mean 0.5282 (0.5224) alpha_val 0.5282 (0.5224) lr 1.9921e-03 eta 0:23:59
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,385
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.1%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,033
* accuracy: 83.8%
* error: 16.2%
* macro_f1: 80.2%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      85.9%, epoch: 4 *******
******* Domain a best val test acc: 83.8%, epoch: 4 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [5/50] batch [20/288] time 0.129 (0.141) data 0.000 (0.019) loss 1.8000 (1.5735) teacher_loss 1.4228 (1.1570) loss_zs_kd 0.1318 (0.1249) loss_oracle 0.3113 (0.3541) acc 68.7500 (69.8438) alaph_mean 0.5298 (0.5291) alpha_val 0.5298 (0.5291) lr 1.9823e-03 eta 0:31:08
epoch [5/50] batch [40/288] time 0.117 (0.132) data 0.001 (0.010) loss 0.9398 (1.4839) teacher_loss 0.4960 (1.0478) loss_zs_kd 0.1658 (0.1260) loss_oracle 0.3609 (0.3731) acc 90.6250 (72.8906) alaph_mean 0.5309 (0.5297) alpha_val 0.5309 (0.5297) lr 1.9823e-03 eta 0:29:05
epoch [5/50] batch [60/288] time 0.102 (0.125) data 0.000 (0.006) loss 1.5540 (1.4571) teacher_loss 1.1131 (1.0131) loss_zs_kd 0.0841 (0.1217) loss_oracle 0.3989 (0.3831) acc 65.6250 (73.7500) alaph_mean 0.5322 (0.5303) alpha_val 0.5322 (0.5303) lr 1.9823e-03 eta 0:27:35
epoch [5/50] batch [80/288] time 0.109 (0.121) data 0.000 (0.005) loss 1.7615 (1.4298) teacher_loss 1.4368 (0.9949) loss_zs_kd 0.0967 (0.1215) loss_oracle 0.2764 (0.3741) acc 71.8750 (74.1016) alaph_mean 0.5331 (0.5309) alpha_val 0.5331 (0.5309) lr 1.9823e-03 eta 0:26:35
epoch [5/50] batch [100/288] time 0.110 (0.118) data 0.000 (0.004) loss 1.0630 (1.4287) teacher_loss 0.6636 (1.0062) loss_zs_kd 0.1301 (0.1220) loss_oracle 0.3344 (0.3615) acc 84.3750 (73.8125) alaph_mean 0.5341 (0.5315) alpha_val 0.5341 (0.5315) lr 1.9823e-03 eta 0:25:50
epoch [5/50] batch [120/288] time 0.103 (0.116) data 0.000 (0.003) loss 1.3911 (1.4129) teacher_loss 0.9560 (0.9982) loss_zs_kd 0.1707 (0.1212) loss_oracle 0.3497 (0.3541) acc 78.1250 (74.0885) alaph_mean 0.5349 (0.5320) alpha_val 0.5349 (0.5320) lr 1.9823e-03 eta 0:25:25
epoch [5/50] batch [140/288] time 0.119 (0.117) data 0.000 (0.003) loss 1.4107 (1.4100) teacher_loss 1.0385 (1.0004) loss_zs_kd 0.1070 (0.1206) loss_oracle 0.3187 (0.3493) acc 65.6250 (73.9955) alaph_mean 0.5357 (0.5325) alpha_val 0.5357 (0.5325) lr 1.9823e-03 eta 0:25:38
epoch [5/50] batch [160/288] time 0.114 (0.116) data 0.000 (0.003) loss 1.6116 (1.4094) teacher_loss 1.2008 (1.0015) loss_zs_kd 0.1048 (0.1195) loss_oracle 0.3584 (0.3481) acc 68.7500 (73.9453) alaph_mean 0.5363 (0.5329) alpha_val 0.5363 (0.5329) lr 1.9823e-03 eta 0:25:18
epoch [5/50] batch [180/288] time 0.118 (0.117) data 0.001 (0.002) loss 1.3281 (1.4159) teacher_loss 0.8674 (1.0076) loss_zs_kd 0.1147 (0.1198) loss_oracle 0.4034 (0.3484) acc 75.0000 (73.7847) alaph_mean 0.5371 (0.5333) alpha_val 0.5371 (0.5333) lr 1.9823e-03 eta 0:25:24
epoch [5/50] batch [200/288] time 0.107 (0.117) data 0.000 (0.002) loss 1.2857 (1.4171) teacher_loss 0.9220 (1.0106) loss_zs_kd 0.1117 (0.1190) loss_oracle 0.3079 (0.3470) acc 78.1250 (73.9688) alaph_mean 0.5382 (0.5337) alpha_val 0.5382 (0.5337) lr 1.9823e-03 eta 0:25:30
epoch [5/50] batch [220/288] time 0.101 (0.118) data 0.000 (0.002) loss 1.5714 (1.4178) teacher_loss 1.2232 (1.0133) loss_zs_kd 0.1220 (0.1188) loss_oracle 0.2872 (0.3452) acc 75.0000 (73.7500) alaph_mean 0.5393 (0.5342) alpha_val 0.5393 (0.5342) lr 1.9823e-03 eta 0:25:32
epoch [5/50] batch [240/288] time 0.131 (0.117) data 0.001 (0.002) loss 1.1910 (1.4137) teacher_loss 0.8081 (1.0098) loss_zs_kd 0.1043 (0.1191) loss_oracle 0.3308 (0.3443) acc 81.2500 (73.8802) alaph_mean 0.5401 (0.5347) alpha_val 0.5401 (0.5347) lr 1.9823e-03 eta 0:25:26
epoch [5/50] batch [260/288] time 0.113 (0.118) data 0.000 (0.002) loss 1.5313 (1.4092) teacher_loss 1.1371 (1.0043) loss_zs_kd 0.0774 (0.1187) loss_oracle 0.3555 (0.3455) acc 65.6250 (74.0024) alaph_mean 0.5411 (0.5351) alpha_val 0.5411 (0.5351) lr 1.9823e-03 eta 0:25:26
epoch [5/50] batch [280/288] time 0.109 (0.118) data 0.000 (0.002) loss 1.2016 (1.4013) teacher_loss 0.7476 (0.9948) loss_zs_kd 0.1210 (0.1183) loss_oracle 0.3935 (0.3474) acc 84.3750 (74.2522) alaph_mean 0.5421 (0.5356) alpha_val 0.5421 (0.5356) lr 1.9823e-03 eta 0:25:24
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,380
* accuracy: 85.8%
* error: 14.2%
* macro_f1: 85.1%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,027
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.1%
******* Domain a best val acc:      85.9%, epoch: 4 *******
******* Domain a best val test acc: 83.8%, epoch: 4 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [6/50] batch [20/288] time 0.096 (0.123) data 0.000 (0.018) loss 1.4253 (1.2763) teacher_loss 1.0714 (0.9021) loss_zs_kd 0.0813 (0.1021) loss_oracle 0.3133 (0.3232) acc 68.7500 (74.3750) alaph_mean 0.5436 (0.5432) alpha_val 0.5436 (0.5432) lr 1.9686e-03 eta 0:26:36
epoch [6/50] batch [40/288] time 0.102 (0.115) data 0.000 (0.009) loss 1.3134 (1.3243) teacher_loss 0.9107 (0.9517) loss_zs_kd 0.1033 (0.1091) loss_oracle 0.3511 (0.3181) acc 75.0000 (73.9844) alaph_mean 0.5447 (0.5437) alpha_val 0.5447 (0.5437) lr 1.9686e-03 eta 0:24:51
epoch [6/50] batch [60/288] time 0.097 (0.110) data 0.000 (0.006) loss 1.4100 (1.3158) teacher_loss 0.9904 (0.9392) loss_zs_kd 0.1475 (0.1122) loss_oracle 0.3459 (0.3205) acc 71.8750 (74.3229) alaph_mean 0.5456 (0.5442) alpha_val 0.5456 (0.5442) lr 1.9686e-03 eta 0:23:44
epoch [6/50] batch [80/288] time 0.097 (0.112) data 0.000 (0.005) loss 1.3468 (1.3219) teacher_loss 0.9013 (0.9434) loss_zs_kd 0.1518 (0.1171) loss_oracle 0.3695 (0.3200) acc 75.0000 (74.1406) alaph_mean 0.5465 (0.5447) alpha_val 0.5465 (0.5447) lr 1.9686e-03 eta 0:24:07
epoch [6/50] batch [100/288] time 0.098 (0.110) data 0.000 (0.004) loss 1.5607 (1.3368) teacher_loss 1.1473 (0.9593) loss_zs_kd 0.1342 (0.1176) loss_oracle 0.3463 (0.3187) acc 68.7500 (74.3750) alaph_mean 0.5474 (0.5451) alpha_val 0.5474 (0.5451) lr 1.9686e-03 eta 0:23:36
epoch [6/50] batch [120/288] time 0.108 (0.108) data 0.000 (0.003) loss 1.5201 (1.3359) teacher_loss 1.1349 (0.9532) loss_zs_kd 0.1274 (0.1176) loss_oracle 0.3214 (0.3239) acc 71.8750 (74.6875) alaph_mean 0.5483 (0.5456) alpha_val 0.5483 (0.5456) lr 1.9686e-03 eta 0:23:09
epoch [6/50] batch [140/288] time 0.101 (0.107) data 0.000 (0.003) loss 1.0724 (1.3357) teacher_loss 0.7699 (0.9551) loss_zs_kd 0.0762 (0.1170) loss_oracle 0.2644 (0.3221) acc 81.2500 (74.4866) alaph_mean 0.5490 (0.5460) alpha_val 0.5490 (0.5460) lr 1.9686e-03 eta 0:22:53
epoch [6/50] batch [160/288] time 0.098 (0.107) data 0.000 (0.003) loss 1.0824 (1.3396) teacher_loss 0.6631 (0.9564) loss_zs_kd 0.1482 (0.1191) loss_oracle 0.3452 (0.3237) acc 78.1250 (74.3359) alaph_mean 0.5502 (0.5465) alpha_val 0.5502 (0.5465) lr 1.9686e-03 eta 0:22:43
epoch [6/50] batch [180/288] time 0.103 (0.107) data 0.000 (0.002) loss 1.2086 (1.3484) teacher_loss 0.7264 (0.9591) loss_zs_kd 0.1568 (0.1200) loss_oracle 0.4038 (0.3293) acc 84.3750 (74.3750) alaph_mean 0.5514 (0.5469) alpha_val 0.5514 (0.5469) lr 1.9686e-03 eta 0:22:42
epoch [6/50] batch [200/288] time 0.099 (0.107) data 0.000 (0.002) loss 1.3325 (1.3525) teacher_loss 0.9804 (0.9644) loss_zs_kd 0.0910 (0.1197) loss_oracle 0.3066 (0.3282) acc 68.7500 (74.3281) alaph_mean 0.5527 (0.5475) alpha_val 0.5527 (0.5475) lr 1.9686e-03 eta 0:22:39
epoch [6/50] batch [220/288] time 0.102 (0.107) data 0.000 (0.002) loss 1.5523 (1.3552) teacher_loss 1.1433 (0.9683) loss_zs_kd 0.1446 (0.1200) loss_oracle 0.3367 (0.3268) acc 75.0000 (74.2898) alaph_mean 0.5538 (0.5480) alpha_val 0.5538 (0.5480) lr 1.9686e-03 eta 0:22:37
epoch [6/50] batch [240/288] time 0.101 (0.106) data 0.000 (0.002) loss 1.3015 (1.3598) teacher_loss 0.9638 (0.9739) loss_zs_kd 0.0763 (0.1199) loss_oracle 0.2996 (0.3259) acc 75.0000 (74.1406) alaph_mean 0.5551 (0.5485) alpha_val 0.5551 (0.5485) lr 1.9686e-03 eta 0:22:32
epoch [6/50] batch [260/288] time 0.108 (0.106) data 0.000 (0.002) loss 1.5858 (1.3627) teacher_loss 1.2130 (0.9766) loss_zs_kd 0.1025 (0.1202) loss_oracle 0.3216 (0.3261) acc 68.7500 (74.0505) alaph_mean 0.5562 (0.5491) alpha_val 0.5562 (0.5491) lr 1.9686e-03 eta 0:22:27
epoch [6/50] batch [280/288] time 0.109 (0.106) data 0.001 (0.002) loss 1.5998 (1.3620) teacher_loss 1.2105 (0.9776) loss_zs_kd 0.1580 (0.1206) loss_oracle 0.3103 (0.3242) acc 71.8750 (74.1629) alaph_mean 0.5571 (0.5496) alpha_val 0.5571 (0.5496) lr 1.9686e-03 eta 0:22:23
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,389
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.4%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.9%
******* Domain a best val acc:      86.0%, epoch: 6 *******
******* Domain a best val test acc: 83.2%, epoch: 6 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [7/50] batch [20/288] time 0.094 (0.112) data 0.000 (0.012) loss 1.5790 (1.3762) teacher_loss 1.1652 (0.9994) loss_zs_kd 0.1121 (0.1173) loss_oracle 0.3577 (0.3182) acc 71.8750 (76.0938) alaph_mean 0.5584 (0.5579) alpha_val 0.5584 (0.5579) lr 1.9511e-03 eta 0:23:40
epoch [7/50] batch [40/288] time 0.106 (0.112) data 0.000 (0.006) loss 1.0836 (1.3265) teacher_loss 0.6972 (0.9477) loss_zs_kd 0.1133 (0.1189) loss_oracle 0.3298 (0.3193) acc 87.5000 (77.0312) alaph_mean 0.5595 (0.5585) alpha_val 0.5595 (0.5585) lr 1.9511e-03 eta 0:23:35
epoch [7/50] batch [60/288] time 0.093 (0.107) data 0.000 (0.004) loss 1.4853 (1.3065) teacher_loss 1.1022 (0.9334) loss_zs_kd 0.1294 (0.1194) loss_oracle 0.3184 (0.3134) acc 71.8750 (76.6146) alaph_mean 0.5606 (0.5590) alpha_val 0.5606 (0.5590) lr 1.9511e-03 eta 0:22:30
epoch [7/50] batch [80/288] time 0.096 (0.105) data 0.000 (0.003) loss 1.3311 (1.3104) teacher_loss 0.9079 (0.9349) loss_zs_kd 0.1400 (0.1225) loss_oracle 0.3532 (0.3143) acc 75.0000 (76.1328) alaph_mean 0.5614 (0.5595) alpha_val 0.5614 (0.5595) lr 1.9511e-03 eta 0:21:58
epoch [7/50] batch [100/288] time 0.096 (0.103) data 0.000 (0.003) loss 0.9541 (1.3104) teacher_loss 0.6383 (0.9335) loss_zs_kd 0.0949 (0.1245) loss_oracle 0.2684 (0.3146) acc 87.5000 (75.8125) alaph_mean 0.5626 (0.5600) alpha_val 0.5626 (0.5600) lr 1.9511e-03 eta 0:21:29
epoch [7/50] batch [120/288] time 0.099 (0.102) data 0.000 (0.002) loss 1.6108 (1.3132) teacher_loss 1.2621 (0.9339) loss_zs_kd 0.1306 (0.1250) loss_oracle 0.2834 (0.3168) acc 71.8750 (75.7031) alaph_mean 0.5640 (0.5606) alpha_val 0.5640 (0.5606) lr 1.9511e-03 eta 0:21:14
epoch [7/50] batch [140/288] time 0.101 (0.101) data 0.000 (0.002) loss 1.3851 (1.3214) teacher_loss 1.0298 (0.9430) loss_zs_kd 0.1180 (0.1258) loss_oracle 0.2963 (0.3155) acc 68.7500 (75.3348) alaph_mean 0.5652 (0.5611) alpha_val 0.5652 (0.5611) lr 1.9511e-03 eta 0:21:05
epoch [7/50] batch [160/288] time 0.096 (0.100) data 0.000 (0.002) loss 1.3611 (1.3395) teacher_loss 0.9829 (0.9603) loss_zs_kd 0.1292 (0.1263) loss_oracle 0.3136 (0.3161) acc 78.1250 (74.9609) alaph_mean 0.5665 (0.5617) alpha_val 0.5665 (0.5617) lr 1.9511e-03 eta 0:20:56
epoch [7/50] batch [180/288] time 0.095 (0.100) data 0.000 (0.002) loss 1.4188 (1.3461) teacher_loss 1.0155 (0.9654) loss_zs_kd 0.1345 (0.1262) loss_oracle 0.3360 (0.3176) acc 71.8750 (75.0174) alaph_mean 0.5673 (0.5623) alpha_val 0.5673 (0.5623) lr 1.9511e-03 eta 0:20:49
epoch [7/50] batch [200/288] time 0.096 (0.100) data 0.000 (0.001) loss 1.0471 (1.3408) teacher_loss 0.6649 (0.9590) loss_zs_kd 0.1326 (0.1257) loss_oracle 0.3159 (0.3190) acc 75.0000 (75.1406) alaph_mean 0.5685 (0.5629) alpha_val 0.5685 (0.5629) lr 1.9511e-03 eta 0:20:43
epoch [7/50] batch [220/288] time 0.097 (0.099) data 0.000 (0.001) loss 1.6055 (1.3386) teacher_loss 1.2328 (0.9611) loss_zs_kd 0.1248 (0.1244) loss_oracle 0.3103 (0.3153) acc 71.8750 (74.9716) alaph_mean 0.5697 (0.5634) alpha_val 0.5697 (0.5634) lr 1.9511e-03 eta 0:20:35
epoch [7/50] batch [240/288] time 0.106 (0.099) data 0.000 (0.001) loss 1.7823 (1.3451) teacher_loss 1.4631 (0.9681) loss_zs_kd 0.1130 (0.1240) loss_oracle 0.2627 (0.3150) acc 65.6250 (74.8568) alaph_mean 0.5705 (0.5640) alpha_val 0.5705 (0.5640) lr 1.9511e-03 eta 0:20:31
epoch [7/50] batch [260/288] time 0.092 (0.099) data 0.000 (0.001) loss 1.2944 (1.3473) teacher_loss 0.9571 (0.9713) loss_zs_kd 0.0976 (0.1233) loss_oracle 0.2885 (0.3143) acc 75.0000 (74.7596) alaph_mean 0.5716 (0.5645) alpha_val 0.5716 (0.5645) lr 1.9511e-03 eta 0:20:25
epoch [7/50] batch [280/288] time 0.085 (0.098) data 0.000 (0.001) loss 1.0352 (1.3417) teacher_loss 0.6887 (0.9672) loss_zs_kd 0.1031 (0.1226) loss_oracle 0.2949 (0.3132) acc 78.1250 (75.0112) alaph_mean 0.5725 (0.5651) alpha_val 0.5725 (0.5651) lr 1.9511e-03 eta 0:20:17
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,394
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.4%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.9%
******* Domain a best val acc:      86.2%, epoch: 7 *******
******* Domain a best val test acc: 83.4%, epoch: 7 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [8/50] batch [20/288] time 0.134 (0.118) data 0.001 (0.013) loss 1.3229 (1.3874) teacher_loss 0.9050 (0.9720) loss_zs_kd 0.1734 (0.1256) loss_oracle 0.3312 (0.3526) acc 84.3750 (74.8438) alaph_mean 0.5745 (0.5737) alpha_val 0.5745 (0.5737) lr 1.9298e-03 eta 0:24:16
epoch [8/50] batch [40/288] time 0.109 (0.114) data 0.001 (0.007) loss 1.2075 (1.3467) teacher_loss 0.7922 (0.9465) loss_zs_kd 0.1526 (0.1259) loss_oracle 0.3391 (0.3372) acc 84.3750 (74.9219) alaph_mean 0.5758 (0.5744) alpha_val 0.5758 (0.5744) lr 1.9298e-03 eta 0:23:28
epoch [8/50] batch [60/288] time 0.105 (0.112) data 0.000 (0.005) loss 1.8357 (1.3241) teacher_loss 1.4706 (0.9315) loss_zs_kd 0.0855 (0.1252) loss_oracle 0.3224 (0.3300) acc 62.5000 (75.1042) alaph_mean 0.5769 (0.5751) alpha_val 0.5769 (0.5751) lr 1.9298e-03 eta 0:22:56
epoch [8/50] batch [80/288] time 0.096 (0.109) data 0.000 (0.004) loss 1.4630 (1.3092) teacher_loss 1.0835 (0.9208) loss_zs_kd 0.0872 (0.1260) loss_oracle 0.3359 (0.3255) acc 68.7500 (75.5078) alaph_mean 0.5779 (0.5757) alpha_val 0.5779 (0.5757) lr 1.9298e-03 eta 0:22:23
epoch [8/50] batch [100/288] time 0.100 (0.107) data 0.000 (0.003) loss 1.2391 (1.3130) teacher_loss 0.7509 (0.9266) loss_zs_kd 0.1982 (0.1286) loss_oracle 0.3891 (0.3221) acc 87.5000 (75.5000) alaph_mean 0.5791 (0.5763) alpha_val 0.5791 (0.5763) lr 1.9298e-03 eta 0:21:59
epoch [8/50] batch [120/288] time 0.099 (0.106) data 0.000 (0.002) loss 1.2563 (1.3023) teacher_loss 0.8474 (0.9160) loss_zs_kd 0.1520 (0.1302) loss_oracle 0.3329 (0.3212) acc 78.1250 (75.9896) alaph_mean 0.5804 (0.5768) alpha_val 0.5804 (0.5768) lr 1.9298e-03 eta 0:21:44
epoch [8/50] batch [140/288] time 0.095 (0.105) data 0.000 (0.002) loss 1.2541 (1.3122) teacher_loss 0.8777 (0.9248) loss_zs_kd 0.1517 (0.1315) loss_oracle 0.3005 (0.3217) acc 71.8750 (75.8036) alaph_mean 0.5815 (0.5774) alpha_val 0.5815 (0.5774) lr 1.9298e-03 eta 0:21:26
epoch [8/50] batch [160/288] time 0.099 (0.104) data 0.000 (0.002) loss 1.4804 (1.3329) teacher_loss 1.1019 (0.9441) loss_zs_kd 0.1684 (0.1313) loss_oracle 0.2942 (0.3232) acc 68.7500 (75.3125) alaph_mean 0.5827 (0.5780) alpha_val 0.5827 (0.5780) lr 1.9298e-03 eta 0:21:13
epoch [8/50] batch [180/288] time 0.094 (0.103) data 0.000 (0.002) loss 1.7791 (1.3328) teacher_loss 1.3772 (0.9469) loss_zs_kd 0.1082 (0.1292) loss_oracle 0.3478 (0.3212) acc 68.7500 (75.2257) alaph_mean 0.5835 (0.5786) alpha_val 0.5835 (0.5786) lr 1.9298e-03 eta 0:21:01
epoch [8/50] batch [200/288] time 0.100 (0.103) data 0.000 (0.002) loss 1.5738 (1.3352) teacher_loss 1.2064 (0.9509) loss_zs_kd 0.1120 (0.1284) loss_oracle 0.3114 (0.3200) acc 68.7500 (75.1406) alaph_mean 0.5846 (0.5791) alpha_val 0.5846 (0.5791) lr 1.9298e-03 eta 0:20:52
epoch [8/50] batch [220/288] time 0.101 (0.102) data 0.000 (0.001) loss 1.0892 (1.3290) teacher_loss 0.7093 (0.9469) loss_zs_kd 0.1079 (0.1277) loss_oracle 0.3260 (0.3183) acc 84.3750 (75.2699) alaph_mean 0.5854 (0.5797) alpha_val 0.5854 (0.5797) lr 1.9298e-03 eta 0:20:42
epoch [8/50] batch [240/288] time 0.100 (0.102) data 0.000 (0.001) loss 1.2850 (1.3308) teacher_loss 0.9131 (0.9501) loss_zs_kd 0.1747 (0.1284) loss_oracle 0.2845 (0.3165) acc 75.0000 (75.1042) alaph_mean 0.5863 (0.5802) alpha_val 0.5863 (0.5802) lr 1.9298e-03 eta 0:20:37
epoch [8/50] batch [260/288] time 0.101 (0.102) data 0.000 (0.001) loss 1.2841 (1.3288) teacher_loss 0.8245 (0.9492) loss_zs_kd 0.1702 (0.1277) loss_oracle 0.3745 (0.3157) acc 81.2500 (75.1322) alaph_mean 0.5875 (0.5807) alpha_val 0.5875 (0.5807) lr 1.9298e-03 eta 0:20:32
epoch [8/50] batch [280/288] time 0.089 (0.101) data 0.000 (0.001) loss 1.8093 (1.3347) teacher_loss 1.3669 (0.9556) loss_zs_kd 0.1615 (0.1273) loss_oracle 0.3617 (0.3155) acc 62.5000 (74.9554) alaph_mean 0.5882 (0.5812) alpha_val 0.5882 (0.5812) lr 1.9298e-03 eta 0:20:23
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,389
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,015
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.6%
******* Domain a best val acc:      86.2%, epoch: 7 *******
******* Domain a best val test acc: 83.4%, epoch: 7 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [9/50] batch [20/288] time 0.100 (0.126) data 0.000 (0.013) loss 1.4055 (1.3506) teacher_loss 1.0660 (0.9783) loss_zs_kd 0.1180 (0.1282) loss_oracle 0.2805 (0.3082) acc 71.8750 (73.5938) alaph_mean 0.5902 (0.5894) alpha_val 0.5902 (0.5894) lr 1.9048e-03 eta 0:25:18
epoch [9/50] batch [40/288] time 0.102 (0.112) data 0.000 (0.007) loss 0.9875 (1.2930) teacher_loss 0.5713 (0.9186) loss_zs_kd 0.0828 (0.1260) loss_oracle 0.3748 (0.3114) acc 84.3750 (75.8594) alaph_mean 0.5913 (0.5901) alpha_val 0.5913 (0.5901) lr 1.9048e-03 eta 0:22:25
epoch [9/50] batch [60/288] time 0.100 (0.106) data 0.000 (0.005) loss 1.4515 (1.2953) teacher_loss 1.1174 (0.9182) loss_zs_kd 0.1344 (0.1285) loss_oracle 0.2669 (0.3129) acc 65.6250 (75.6250) alaph_mean 0.5923 (0.5907) alpha_val 0.5923 (0.5907) lr 1.9048e-03 eta 0:21:21
epoch [9/50] batch [80/288] time 0.094 (0.104) data 0.000 (0.003) loss 1.2702 (1.3289) teacher_loss 0.7894 (0.9481) loss_zs_kd 0.1385 (0.1331) loss_oracle 0.4115 (0.3143) acc 87.5000 (75.2344) alaph_mean 0.5931 (0.5912) alpha_val 0.5931 (0.5912) lr 1.9048e-03 eta 0:20:51
epoch [9/50] batch [100/288] time 0.096 (0.103) data 0.000 (0.003) loss 1.2610 (1.3322) teacher_loss 0.8245 (0.9510) loss_zs_kd 0.1227 (0.1307) loss_oracle 0.3751 (0.3158) acc 71.8750 (75.2812) alaph_mean 0.5939 (0.5917) alpha_val 0.5939 (0.5917) lr 1.9048e-03 eta 0:20:30
epoch [9/50] batch [120/288] time 0.093 (0.101) data 0.000 (0.002) loss 1.3491 (1.3453) teacher_loss 0.9608 (0.9649) loss_zs_kd 0.1363 (0.1296) loss_oracle 0.3201 (0.3156) acc 68.7500 (74.8177) alaph_mean 0.5949 (0.5921) alpha_val 0.5949 (0.5921) lr 1.9048e-03 eta 0:20:14
epoch [9/50] batch [140/288] time 0.093 (0.101) data 0.000 (0.002) loss 1.2649 (1.3518) teacher_loss 0.9561 (0.9730) loss_zs_kd 0.0702 (0.1290) loss_oracle 0.2737 (0.3143) acc 75.0000 (74.9554) alaph_mean 0.5962 (0.5926) alpha_val 0.5962 (0.5926) lr 1.9048e-03 eta 0:20:05
epoch [9/50] batch [160/288] time 0.092 (0.100) data 0.000 (0.002) loss 1.1468 (1.3433) teacher_loss 0.7546 (0.9643) loss_zs_kd 0.1327 (0.1307) loss_oracle 0.3259 (0.3137) acc 87.5000 (75.0781) alaph_mean 0.5974 (0.5931) alpha_val 0.5974 (0.5931) lr 1.9048e-03 eta 0:19:56
epoch [9/50] batch [180/288] time 0.093 (0.100) data 0.000 (0.002) loss 1.9508 (1.3428) teacher_loss 1.5470 (0.9641) loss_zs_kd 0.1619 (0.1302) loss_oracle 0.3228 (0.3135) acc 65.6250 (75.0521) alaph_mean 0.5986 (0.5937) alpha_val 0.5986 (0.5937) lr 1.9048e-03 eta 0:19:48
epoch [9/50] batch [200/288] time 0.097 (0.099) data 0.000 (0.002) loss 1.6201 (1.3524) teacher_loss 1.1596 (0.9711) loss_zs_kd 0.1477 (0.1315) loss_oracle 0.3867 (0.3155) acc 68.7500 (74.8750) alaph_mean 0.6000 (0.5942) alpha_val 0.6000 (0.5942) lr 1.9048e-03 eta 0:19:42
epoch [9/50] batch [220/288] time 0.098 (0.099) data 0.000 (0.001) loss 1.0128 (1.3523) teacher_loss 0.6249 (0.9701) loss_zs_kd 0.1517 (0.1314) loss_oracle 0.3120 (0.3165) acc 87.5000 (74.8722) alaph_mean 0.6011 (0.5948) alpha_val 0.6011 (0.5948) lr 1.9048e-03 eta 0:19:36
epoch [9/50] batch [240/288] time 0.097 (0.099) data 0.000 (0.001) loss 1.4038 (1.3490) teacher_loss 0.9517 (0.9666) loss_zs_kd 0.2127 (0.1317) loss_oracle 0.3457 (0.3166) acc 78.1250 (74.9740) alaph_mean 0.6019 (0.5954) alpha_val 0.6019 (0.5954) lr 1.9048e-03 eta 0:19:31
epoch [9/50] batch [260/288] time 0.100 (0.099) data 0.000 (0.001) loss 1.2444 (1.3529) teacher_loss 0.9190 (0.9701) loss_zs_kd 0.1190 (0.1322) loss_oracle 0.2658 (0.3167) acc 68.7500 (74.8918) alaph_mean 0.6031 (0.5959) alpha_val 0.6031 (0.5959) lr 1.9048e-03 eta 0:19:27
epoch [9/50] batch [280/288] time 0.087 (0.098) data 0.000 (0.001) loss 1.1793 (1.3463) teacher_loss 0.8621 (0.9637) loss_zs_kd 0.0944 (0.1325) loss_oracle 0.2701 (0.3164) acc 78.1250 (75.0335) alaph_mean 0.6041 (0.5965) alpha_val 0.6041 (0.5965) lr 1.9048e-03 eta 0:19:20
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,400
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,011
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.4%
******* Domain a best val acc:      86.3%, epoch: 9 *******
******* Domain a best val test acc: 82.9%, epoch: 9 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [10/50] batch [20/288] time 0.093 (0.119) data 0.000 (0.015) loss 1.1579 (1.3466) teacher_loss 0.7870 (0.9746) loss_zs_kd 0.0986 (0.1327) loss_oracle 0.3216 (0.3056) acc 78.1250 (76.5625) alaph_mean 0.6058 (0.6053) alpha_val 0.6058 (0.6053) lr 1.8763e-03 eta 0:23:25
epoch [10/50] batch [40/288] time 0.100 (0.109) data 0.000 (0.007) loss 1.5406 (1.3108) teacher_loss 1.1048 (0.9343) loss_zs_kd 0.1777 (0.1329) loss_oracle 0.3470 (0.3100) acc 68.7500 (77.1094) alaph_mean 0.6068 (0.6058) alpha_val 0.6068 (0.6058) lr 1.8763e-03 eta 0:21:19
epoch [10/50] batch [60/288] time 0.100 (0.105) data 0.000 (0.005) loss 1.2383 (1.2794) teacher_loss 0.8847 (0.9068) loss_zs_kd 0.1226 (0.1321) loss_oracle 0.2924 (0.3066) acc 81.2500 (77.1875) alaph_mean 0.6079 (0.6063) alpha_val 0.6079 (0.6063) lr 1.8763e-03 eta 0:20:32
epoch [10/50] batch [80/288] time 0.101 (0.102) data 0.000 (0.004) loss 1.4835 (1.3025) teacher_loss 1.0958 (0.9263) loss_zs_kd 0.0904 (0.1348) loss_oracle 0.3425 (0.3088) acc 81.2500 (76.6016) alaph_mean 0.6090 (0.6068) alpha_val 0.6090 (0.6068) lr 1.8763e-03 eta 0:19:57
epoch [10/50] batch [100/288] time 0.094 (0.101) data 0.000 (0.003) loss 1.6266 (1.2930) teacher_loss 1.2517 (0.9192) loss_zs_kd 0.1176 (0.1306) loss_oracle 0.3161 (0.3085) acc 71.8750 (76.8125) alaph_mean 0.6100 (0.6074) alpha_val 0.6100 (0.6074) lr 1.8763e-03 eta 0:19:37
epoch [10/50] batch [120/288] time 0.095 (0.100) data 0.000 (0.003) loss 1.2244 (1.2899) teacher_loss 0.8777 (0.9178) loss_zs_kd 0.1171 (0.1293) loss_oracle 0.2882 (0.3075) acc 81.2500 (77.0573) alaph_mean 0.6109 (0.6079) alpha_val 0.6109 (0.6079) lr 1.8763e-03 eta 0:19:25
epoch [10/50] batch [140/288] time 0.107 (0.099) data 0.000 (0.002) loss 1.5655 (1.3070) teacher_loss 1.2048 (0.9331) loss_zs_kd 0.1220 (0.1313) loss_oracle 0.2998 (0.3083) acc 71.8750 (76.6071) alaph_mean 0.6120 (0.6084) alpha_val 0.6120 (0.6084) lr 1.8763e-03 eta 0:19:20
epoch [10/50] batch [160/288] time 0.097 (0.099) data 0.000 (0.002) loss 1.0344 (1.3049) teacher_loss 0.6467 (0.9309) loss_zs_kd 0.1087 (0.1307) loss_oracle 0.3333 (0.3087) acc 90.6250 (76.4844) alaph_mean 0.6130 (0.6089) alpha_val 0.6130 (0.6089) lr 1.8763e-03 eta 0:19:13
epoch [10/50] batch [180/288] time 0.099 (0.099) data 0.000 (0.002) loss 1.3939 (1.3147) teacher_loss 1.0912 (0.9411) loss_zs_kd 0.1244 (0.1312) loss_oracle 0.2405 (0.3080) acc 78.1250 (76.1285) alaph_mean 0.6142 (0.6094) alpha_val 0.6142 (0.6094) lr 1.8763e-03 eta 0:19:06
epoch [10/50] batch [200/288] time 0.092 (0.098) data 0.000 (0.002) loss 1.5570 (1.3215) teacher_loss 1.2216 (0.9482) loss_zs_kd 0.1213 (0.1313) loss_oracle 0.2748 (0.3076) acc 68.7500 (75.9219) alaph_mean 0.6155 (0.6100) alpha_val 0.6155 (0.6100) lr 1.8763e-03 eta 0:19:01
epoch [10/50] batch [220/288] time 0.097 (0.098) data 0.000 (0.002) loss 1.4444 (1.3214) teacher_loss 1.0478 (0.9482) loss_zs_kd 0.1349 (0.1309) loss_oracle 0.3292 (0.3077) acc 71.8750 (76.0653) alaph_mean 0.6166 (0.6105) alpha_val 0.6166 (0.6105) lr 1.8763e-03 eta 0:18:55
epoch [10/50] batch [240/288] time 0.091 (0.098) data 0.000 (0.001) loss 1.2521 (1.3231) teacher_loss 0.8707 (0.9508) loss_zs_kd 0.1420 (0.1308) loss_oracle 0.3104 (0.3069) acc 71.8750 (75.8724) alaph_mean 0.6178 (0.6111) alpha_val 0.6178 (0.6111) lr 1.8763e-03 eta 0:18:50
epoch [10/50] batch [260/288] time 0.093 (0.097) data 0.000 (0.001) loss 1.4888 (1.3242) teacher_loss 1.0843 (0.9511) loss_zs_kd 0.1493 (0.1316) loss_oracle 0.3298 (0.3072) acc 71.8750 (75.8293) alaph_mean 0.6191 (0.6117) alpha_val 0.6191 (0.6117) lr 1.8763e-03 eta 0:18:44
epoch [10/50] batch [280/288] time 0.086 (0.097) data 0.000 (0.001) loss 1.8325 (1.3331) teacher_loss 1.4277 (0.9580) loss_zs_kd 0.1794 (0.1327) loss_oracle 0.3151 (0.3087) acc 65.6250 (75.6696) alaph_mean 0.6203 (0.6122) alpha_val 0.6203 (0.6122) lr 1.8763e-03 eta 0:18:37
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,404
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.7%
******* Domain a best val acc:      86.4%, epoch: 10 *******
******* Domain a best val test acc: 83.3%, epoch: 10 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [11/50] batch [20/288] time 0.102 (0.116) data 0.000 (0.014) loss 1.8640 (1.3644) teacher_loss 1.4791 (0.9578) loss_zs_kd 0.1331 (0.1387) loss_oracle 0.3184 (0.3372) acc 65.6250 (74.6875) alaph_mean 0.6214 (0.6211) alpha_val 0.6214 (0.6211) lr 1.8443e-03 eta 0:22:14
epoch [11/50] batch [40/288] time 0.099 (0.109) data 0.000 (0.007) loss 1.5466 (1.3647) teacher_loss 1.1132 (0.9637) loss_zs_kd 0.1478 (0.1381) loss_oracle 0.3595 (0.3320) acc 75.0000 (74.3750) alaph_mean 0.6225 (0.6215) alpha_val 0.6225 (0.6215) lr 1.8443e-03 eta 0:20:51
epoch [11/50] batch [60/288] time 0.099 (0.106) data 0.001 (0.005) loss 1.1166 (1.3557) teacher_loss 0.6942 (0.9589) loss_zs_kd 0.1628 (0.1368) loss_oracle 0.3411 (0.3285) acc 81.2500 (74.9479) alaph_mean 0.6236 (0.6220) alpha_val 0.6236 (0.6220) lr 1.8443e-03 eta 0:20:18
epoch [11/50] batch [80/288] time 0.105 (0.105) data 0.001 (0.004) loss 1.3152 (1.3275) teacher_loss 1.0212 (0.9379) loss_zs_kd 0.0878 (0.1351) loss_oracle 0.2501 (0.3221) acc 68.7500 (75.6641) alaph_mean 0.6244 (0.6225) alpha_val 0.6244 (0.6225) lr 1.8443e-03 eta 0:20:02
epoch [11/50] batch [100/288] time 0.106 (0.105) data 0.001 (0.003) loss 1.5253 (1.3155) teacher_loss 1.1551 (0.9256) loss_zs_kd 0.1564 (0.1350) loss_oracle 0.2920 (0.3224) acc 75.0000 (76.0000) alaph_mean 0.6252 (0.6230) alpha_val 0.6252 (0.6230) lr 1.8443e-03 eta 0:20:04
epoch [11/50] batch [120/288] time 0.099 (0.105) data 0.000 (0.003) loss 1.8152 (1.3174) teacher_loss 1.4452 (0.9301) loss_zs_kd 0.1257 (0.1348) loss_oracle 0.3071 (0.3200) acc 53.1250 (75.8073) alaph_mean 0.6265 (0.6235) alpha_val 0.6265 (0.6235) lr 1.8443e-03 eta 0:19:56
epoch [11/50] batch [140/288] time 0.095 (0.104) data 0.000 (0.002) loss 1.2106 (1.3177) teacher_loss 0.8050 (0.9310) loss_zs_kd 0.0941 (0.1345) loss_oracle 0.3585 (0.3194) acc 84.3750 (75.8929) alaph_mean 0.6277 (0.6240) alpha_val 0.6277 (0.6240) lr 1.8443e-03 eta 0:19:47
epoch [11/50] batch [160/288] time 0.114 (0.105) data 0.001 (0.002) loss 1.5482 (1.3264) teacher_loss 1.2122 (0.9428) loss_zs_kd 0.1250 (0.1337) loss_oracle 0.2735 (0.3168) acc 68.7500 (75.6055) alaph_mean 0.6288 (0.6245) alpha_val 0.6288 (0.6245) lr 1.8443e-03 eta 0:19:48
epoch [11/50] batch [180/288] time 0.101 (0.104) data 0.000 (0.002) loss 1.3636 (1.3192) teacher_loss 1.0019 (0.9388) loss_zs_kd 0.1512 (0.1336) loss_oracle 0.2861 (0.3136) acc 78.1250 (75.5382) alaph_mean 0.6300 (0.6251) alpha_val 0.6300 (0.6251) lr 1.8443e-03 eta 0:19:44
epoch [11/50] batch [200/288] time 0.102 (0.104) data 0.000 (0.002) loss 1.4231 (1.3126) teacher_loss 0.9307 (0.9335) loss_zs_kd 0.2283 (0.1342) loss_oracle 0.3782 (0.3120) acc 78.1250 (75.7344) alaph_mean 0.6315 (0.6257) alpha_val 0.6315 (0.6257) lr 1.8443e-03 eta 0:19:42
epoch [11/50] batch [220/288] time 0.100 (0.104) data 0.000 (0.002) loss 1.2901 (1.3141) teacher_loss 0.8631 (0.9340) loss_zs_kd 0.1502 (0.1363) loss_oracle 0.3520 (0.3120) acc 81.2500 (75.6534) alaph_mean 0.6326 (0.6262) alpha_val 0.6326 (0.6262) lr 1.8443e-03 eta 0:19:35
epoch [11/50] batch [240/288] time 0.103 (0.104) data 0.000 (0.001) loss 1.3145 (1.3119) teacher_loss 0.9388 (0.9319) loss_zs_kd 0.1189 (0.1367) loss_oracle 0.3162 (0.3117) acc 75.0000 (75.7422) alaph_mean 0.6338 (0.6268) alpha_val 0.6338 (0.6268) lr 1.8443e-03 eta 0:19:31
epoch [11/50] batch [260/288] time 0.098 (0.104) data 0.000 (0.001) loss 1.2171 (1.3136) teacher_loss 0.8273 (0.9334) loss_zs_kd 0.1166 (0.1368) loss_oracle 0.3314 (0.3118) acc 78.1250 (75.6250) alaph_mean 0.6352 (0.6274) alpha_val 0.6352 (0.6274) lr 1.8443e-03 eta 0:19:28
epoch [11/50] batch [280/288] time 0.114 (0.104) data 0.000 (0.001) loss 1.0423 (1.3054) teacher_loss 0.6735 (0.9252) loss_zs_kd 0.0988 (0.1365) loss_oracle 0.3194 (0.3120) acc 81.2500 (75.8036) alaph_mean 0.6364 (0.6280) alpha_val 0.6364 (0.6280) lr 1.8443e-03 eta 0:19:28
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,398
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.8%
******* Domain a best val acc:      86.4%, epoch: 10 *******
******* Domain a best val test acc: 83.3%, epoch: 10 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [12/50] batch [20/288] time 0.109 (0.117) data 0.000 (0.012) loss 1.2628 (1.3404) teacher_loss 0.9113 (0.9486) loss_zs_kd 0.1000 (0.1406) loss_oracle 0.3015 (0.3215) acc 78.1250 (75.0000) alaph_mean 0.6379 (0.6373) alpha_val 0.6379 (0.6373) lr 1.8090e-03 eta 0:21:53
epoch [12/50] batch [40/288] time 0.094 (0.107) data 0.000 (0.006) loss 1.3311 (1.3099) teacher_loss 1.0402 (0.9216) loss_zs_kd 0.0951 (0.1401) loss_oracle 0.2433 (0.3183) acc 78.1250 (76.0938) alaph_mean 0.6396 (0.6381) alpha_val 0.6396 (0.6381) lr 1.8090e-03 eta 0:19:57
epoch [12/50] batch [60/288] time 0.097 (0.103) data 0.000 (0.004) loss 1.5318 (1.3278) teacher_loss 1.1191 (0.9425) loss_zs_kd 0.1429 (0.1376) loss_oracle 0.3412 (0.3165) acc 75.0000 (75.3646) alaph_mean 0.6408 (0.6388) alpha_val 0.6408 (0.6388) lr 1.8090e-03 eta 0:19:12
epoch [12/50] batch [80/288] time 0.095 (0.101) data 0.000 (0.003) loss 2.2585 (1.3333) teacher_loss 1.8651 (0.9483) loss_zs_kd 0.1674 (0.1402) loss_oracle 0.3097 (0.3149) acc 62.5000 (75.7422) alaph_mean 0.6419 (0.6394) alpha_val 0.6419 (0.6394) lr 1.8090e-03 eta 0:18:51
epoch [12/50] batch [100/288] time 0.098 (0.100) data 0.000 (0.003) loss 1.3842 (1.3106) teacher_loss 0.9837 (0.9297) loss_zs_kd 0.1388 (0.1381) loss_oracle 0.3311 (0.3118) acc 59.3750 (75.9375) alaph_mean 0.6433 (0.6401) alpha_val 0.6433 (0.6401) lr 1.8090e-03 eta 0:18:37
epoch [12/50] batch [120/288] time 0.096 (0.100) data 0.000 (0.002) loss 1.4859 (1.3063) teacher_loss 1.1400 (0.9239) loss_zs_kd 0.1497 (0.1387) loss_oracle 0.2710 (0.3130) acc 65.6250 (76.1719) alaph_mean 0.6442 (0.6407) alpha_val 0.6442 (0.6407) lr 1.8090e-03 eta 0:18:28
epoch [12/50] batch [140/288] time 0.100 (0.099) data 0.000 (0.002) loss 0.7842 (1.3099) teacher_loss 0.4072 (0.9272) loss_zs_kd 0.1260 (0.1406) loss_oracle 0.3140 (0.3124) acc 93.7500 (76.0491) alaph_mean 0.6454 (0.6413) alpha_val 0.6454 (0.6413) lr 1.8090e-03 eta 0:18:18
epoch [12/50] batch [160/288] time 0.097 (0.098) data 0.000 (0.002) loss 1.4075 (1.3114) teacher_loss 0.9755 (0.9263) loss_zs_kd 0.2063 (0.1417) loss_oracle 0.3289 (0.3143) acc 81.2500 (76.0352) alaph_mean 0.6464 (0.6419) alpha_val 0.6464 (0.6419) lr 1.8090e-03 eta 0:18:10
epoch [12/50] batch [180/288] time 0.098 (0.098) data 0.000 (0.002) loss 1.2255 (1.3238) teacher_loss 0.8731 (0.9389) loss_zs_kd 0.1589 (0.1422) loss_oracle 0.2729 (0.3137) acc 78.1250 (75.6424) alaph_mean 0.6476 (0.6425) alpha_val 0.6476 (0.6425) lr 1.8090e-03 eta 0:18:05
epoch [12/50] batch [200/288] time 0.093 (0.098) data 0.000 (0.001) loss 1.3981 (1.3265) teacher_loss 1.0026 (0.9408) loss_zs_kd 0.1508 (0.1420) loss_oracle 0.3200 (0.3147) acc 75.0000 (75.7188) alaph_mean 0.6489 (0.6430) alpha_val 0.6489 (0.6430) lr 1.8090e-03 eta 0:18:04
epoch [12/50] batch [220/288] time 0.098 (0.098) data 0.000 (0.001) loss 1.0986 (1.3230) teacher_loss 0.7167 (0.9376) loss_zs_kd 0.1961 (0.1430) loss_oracle 0.2838 (0.3139) acc 81.2500 (75.8665) alaph_mean 0.6501 (0.6436) alpha_val 0.6501 (0.6436) lr 1.8090e-03 eta 0:17:59
epoch [12/50] batch [240/288] time 0.092 (0.098) data 0.000 (0.001) loss 1.6762 (1.3264) teacher_loss 1.1989 (0.9402) loss_zs_kd 0.1886 (0.1434) loss_oracle 0.3830 (0.3144) acc 62.5000 (75.8594) alaph_mean 0.6512 (0.6442) alpha_val 0.6512 (0.6442) lr 1.8090e-03 eta 0:17:54
epoch [12/50] batch [260/288] time 0.093 (0.098) data 0.000 (0.001) loss 1.1227 (1.3240) teacher_loss 0.7488 (0.9381) loss_zs_kd 0.1376 (0.1430) loss_oracle 0.3052 (0.3145) acc 81.2500 (75.9014) alaph_mean 0.6522 (0.6448) alpha_val 0.6522 (0.6448) lr 1.8090e-03 eta 0:17:51
epoch [12/50] batch [280/288] time 0.086 (0.097) data 0.000 (0.001) loss 1.2510 (1.3215) teacher_loss 0.8602 (0.9352) loss_zs_kd 0.1160 (0.1425) loss_oracle 0.3328 (0.3151) acc 75.0000 (75.9263) alaph_mean 0.6531 (0.6453) alpha_val 0.6531 (0.6453) lr 1.8090e-03 eta 0:17:45
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,398
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.9%
******* Domain a best val acc:      86.4%, epoch: 10 *******
******* Domain a best val test acc: 83.3%, epoch: 10 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [13/50] batch [20/288] time 0.099 (0.114) data 0.000 (0.013) loss 1.1112 (1.2660) teacher_loss 0.7301 (0.8954) loss_zs_kd 0.1308 (0.1377) loss_oracle 0.3157 (0.3017) acc 84.3750 (75.9375) alaph_mean 0.6545 (0.6540) alpha_val 0.6545 (0.6540) lr 1.7705e-03 eta 0:20:44
epoch [13/50] batch [40/288] time 0.106 (0.105) data 0.000 (0.007) loss 1.0948 (1.3521) teacher_loss 0.6731 (0.9651) loss_zs_kd 0.1742 (0.1406) loss_oracle 0.3346 (0.3167) acc 81.2500 (74.3750) alaph_mean 0.6555 (0.6546) alpha_val 0.6555 (0.6546) lr 1.7705e-03 eta 0:19:00
epoch [13/50] batch [60/288] time 0.098 (0.102) data 0.000 (0.004) loss 1.8800 (1.3979) teacher_loss 1.3982 (1.0143) loss_zs_kd 0.1903 (0.1430) loss_oracle 0.3866 (0.3121) acc 59.3750 (73.3333) alaph_mean 0.6567 (0.6551) alpha_val 0.6567 (0.6551) lr 1.7705e-03 eta 0:18:25
epoch [13/50] batch [80/288] time 0.091 (0.100) data 0.000 (0.003) loss 1.7138 (1.3732) teacher_loss 1.2806 (0.9853) loss_zs_kd 0.2120 (0.1475) loss_oracle 0.3273 (0.3141) acc 75.0000 (74.4141) alaph_mean 0.6581 (0.6557) alpha_val 0.6581 (0.6557) lr 1.7705e-03 eta 0:18:08
epoch [13/50] batch [100/288] time 0.094 (0.099) data 0.000 (0.003) loss 0.9466 (1.3399) teacher_loss 0.6371 (0.9550) loss_zs_kd 0.0725 (0.1456) loss_oracle 0.2732 (0.3121) acc 84.3750 (75.2812) alaph_mean 0.6592 (0.6563) alpha_val 0.6592 (0.6563) lr 1.7705e-03 eta 0:17:54
epoch [13/50] batch [120/288] time 0.096 (0.099) data 0.000 (0.002) loss 1.3583 (1.3305) teacher_loss 0.9848 (0.9466) loss_zs_kd 0.1539 (0.1458) loss_oracle 0.2965 (0.3109) acc 78.1250 (75.6771) alaph_mean 0.6601 (0.6568) alpha_val 0.6601 (0.6568) lr 1.7705e-03 eta 0:17:50
epoch [13/50] batch [140/288] time 0.104 (0.099) data 0.000 (0.002) loss 1.3047 (1.3363) teacher_loss 0.9354 (0.9564) loss_zs_kd 0.1411 (0.1439) loss_oracle 0.2986 (0.3080) acc 78.1250 (75.4464) alaph_mean 0.6613 (0.6574) alpha_val 0.6613 (0.6574) lr 1.7705e-03 eta 0:17:44
epoch [13/50] batch [160/288] time 0.103 (0.098) data 0.000 (0.002) loss 1.4222 (1.3226) teacher_loss 1.0578 (0.9436) loss_zs_kd 0.1202 (0.1419) loss_oracle 0.3042 (0.3081) acc 75.0000 (75.5859) alaph_mean 0.6623 (0.6580) alpha_val 0.6623 (0.6580) lr 1.7705e-03 eta 0:17:40
epoch [13/50] batch [180/288] time 0.101 (0.098) data 0.000 (0.002) loss 0.9881 (1.3328) teacher_loss 0.6006 (0.9509) loss_zs_kd 0.1034 (0.1432) loss_oracle 0.3359 (0.3103) acc 81.2500 (75.3993) alaph_mean 0.6634 (0.6585) alpha_val 0.6634 (0.6585) lr 1.7705e-03 eta 0:17:37
epoch [13/50] batch [200/288] time 0.091 (0.098) data 0.000 (0.001) loss 1.7190 (1.3315) teacher_loss 1.3951 (0.9485) loss_zs_kd 0.1157 (0.1449) loss_oracle 0.2660 (0.3105) acc 65.6250 (75.3906) alaph_mean 0.6649 (0.6591) alpha_val 0.6649 (0.6591) lr 1.7705e-03 eta 0:17:36
epoch [13/50] batch [220/288] time 0.094 (0.098) data 0.000 (0.001) loss 1.1427 (1.3264) teacher_loss 0.7489 (0.9421) loss_zs_kd 0.1778 (0.1459) loss_oracle 0.3049 (0.3114) acc 81.2500 (75.5540) alaph_mean 0.6659 (0.6596) alpha_val 0.6659 (0.6596) lr 1.7705e-03 eta 0:17:32
epoch [13/50] batch [240/288] time 0.104 (0.098) data 0.000 (0.001) loss 1.6195 (1.3242) teacher_loss 1.1635 (0.9387) loss_zs_kd 0.1648 (0.1452) loss_oracle 0.3736 (0.3129) acc 75.0000 (75.6510) alaph_mean 0.6668 (0.6602) alpha_val 0.6668 (0.6602) lr 1.7705e-03 eta 0:17:32
epoch [13/50] batch [260/288] time 0.102 (0.098) data 0.000 (0.001) loss 1.3884 (1.3143) teacher_loss 1.0017 (0.9270) loss_zs_kd 0.1453 (0.1446) loss_oracle 0.3140 (0.3150) acc 68.7500 (76.0337) alaph_mean 0.6676 (0.6607) alpha_val 0.6676 (0.6607) lr 1.7705e-03 eta 0:17:27
epoch [13/50] batch [280/288] time 0.091 (0.098) data 0.000 (0.001) loss 1.0000 (1.3156) teacher_loss 0.6590 (0.9283) loss_zs_kd 0.1101 (0.1445) loss_oracle 0.2859 (0.3150) acc 78.1250 (75.9598) alaph_mean 0.6688 (0.6613) alpha_val 0.6688 (0.6613) lr 1.7705e-03 eta 0:17:22
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,406
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,019
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.7%
******* Domain a best val acc:      86.5%, epoch: 13 *******
******* Domain a best val test acc: 83.2%, epoch: 13 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [14/50] batch [20/288] time 0.094 (0.111) data 0.000 (0.014) loss 1.3507 (1.3205) teacher_loss 0.9557 (0.9495) loss_zs_kd 0.1348 (0.1417) loss_oracle 0.3276 (0.3001) acc 75.0000 (75.9375) alaph_mean 0.6702 (0.6698) alpha_val 0.6702 (0.6698) lr 1.7290e-03 eta 0:19:40
epoch [14/50] batch [40/288] time 0.093 (0.103) data 0.000 (0.007) loss 1.0648 (1.2905) teacher_loss 0.6308 (0.9097) loss_zs_kd 0.1784 (0.1388) loss_oracle 0.3448 (0.3115) acc 81.2500 (77.1875) alaph_mean 0.6711 (0.6702) alpha_val 0.6711 (0.6702) lr 1.7290e-03 eta 0:18:16
epoch [14/50] batch [60/288] time 0.103 (0.102) data 0.000 (0.005) loss 1.1768 (1.2708) teacher_loss 0.8625 (0.8884) loss_zs_kd 0.1231 (0.1426) loss_oracle 0.2527 (0.3111) acc 75.0000 (77.3958) alaph_mean 0.6723 (0.6707) alpha_val 0.6723 (0.6707) lr 1.7290e-03 eta 0:17:57
epoch [14/50] batch [80/288] time 0.094 (0.101) data 0.000 (0.004) loss 1.4362 (1.2936) teacher_loss 1.0788 (0.9073) loss_zs_kd 0.1501 (0.1472) loss_oracle 0.2823 (0.3127) acc 75.0000 (76.6797) alaph_mean 0.6735 (0.6713) alpha_val 0.6735 (0.6713) lr 1.7290e-03 eta 0:17:45
epoch [14/50] batch [100/288] time 0.112 (0.099) data 0.000 (0.003) loss 1.5720 (1.2949) teacher_loss 1.1460 (0.9059) loss_zs_kd 0.1641 (0.1508) loss_oracle 0.3439 (0.3136) acc 71.8750 (76.6250) alaph_mean 0.6749 (0.6719) alpha_val 0.6749 (0.6719) lr 1.7290e-03 eta 0:17:30
epoch [14/50] batch [120/288] time 0.112 (0.100) data 0.000 (0.002) loss 1.1459 (1.3068) teacher_loss 0.7249 (0.9168) loss_zs_kd 0.1332 (0.1491) loss_oracle 0.3543 (0.3154) acc 84.3750 (76.4323) alaph_mean 0.6759 (0.6725) alpha_val 0.6759 (0.6725) lr 1.7290e-03 eta 0:17:30
epoch [14/50] batch [140/288] time 0.101 (0.100) data 0.000 (0.002) loss 1.1596 (1.3167) teacher_loss 0.7637 (0.9277) loss_zs_kd 0.1408 (0.1484) loss_oracle 0.3255 (0.3148) acc 81.2500 (76.1830) alaph_mean 0.6771 (0.6731) alpha_val 0.6771 (0.6731) lr 1.7290e-03 eta 0:17:32
epoch [14/50] batch [160/288] time 0.111 (0.101) data 0.000 (0.002) loss 1.0397 (1.3052) teacher_loss 0.6975 (0.9167) loss_zs_kd 0.1445 (0.1472) loss_oracle 0.2699 (0.3149) acc 78.1250 (76.4453) alaph_mean 0.6779 (0.6736) alpha_val 0.6779 (0.6736) lr 1.7290e-03 eta 0:17:43
epoch [14/50] batch [180/288] time 0.094 (0.101) data 0.000 (0.002) loss 1.0146 (1.2943) teacher_loss 0.6486 (0.9080) loss_zs_kd 0.1045 (0.1455) loss_oracle 0.3137 (0.3135) acc 87.5000 (76.5972) alaph_mean 0.6788 (0.6742) alpha_val 0.6788 (0.6742) lr 1.7290e-03 eta 0:17:34
epoch [14/50] batch [200/288] time 0.091 (0.100) data 0.000 (0.002) loss 1.4830 (1.2978) teacher_loss 1.0695 (0.9120) loss_zs_kd 0.1777 (0.1457) loss_oracle 0.3247 (0.3129) acc 68.7500 (76.4844) alaph_mean 0.6797 (0.6747) alpha_val 0.6797 (0.6747) lr 1.7290e-03 eta 0:17:28
epoch [14/50] batch [220/288] time 0.099 (0.100) data 0.000 (0.001) loss 0.7611 (1.2923) teacher_loss 0.3844 (0.9065) loss_zs_kd 0.1216 (0.1458) loss_oracle 0.3159 (0.3129) acc 87.5000 (76.6903) alaph_mean 0.6808 (0.6752) alpha_val 0.6808 (0.6752) lr 1.7290e-03 eta 0:17:20
epoch [14/50] batch [240/288] time 0.095 (0.099) data 0.000 (0.001) loss 1.2492 (1.2869) teacher_loss 0.8315 (0.9016) loss_zs_kd 0.1503 (0.1453) loss_oracle 0.3425 (0.3126) acc 81.2500 (76.7578) alaph_mean 0.6816 (0.6757) alpha_val 0.6816 (0.6757) lr 1.7290e-03 eta 0:17:13
epoch [14/50] batch [260/288] time 0.090 (0.099) data 0.000 (0.001) loss 1.6940 (1.2899) teacher_loss 1.2914 (0.9029) loss_zs_kd 0.1428 (0.1455) loss_oracle 0.3312 (0.3143) acc 68.7500 (76.7548) alaph_mean 0.6823 (0.6762) alpha_val 0.6823 (0.6762) lr 1.7290e-03 eta 0:17:08
epoch [14/50] batch [280/288] time 0.086 (0.100) data 0.000 (0.001) loss 0.9129 (1.2897) teacher_loss 0.4866 (0.9013) loss_zs_kd 0.1217 (0.1454) loss_oracle 0.3655 (0.3156) acc 87.5000 (76.7634) alaph_mean 0.6834 (0.6766) alpha_val 0.6834 (0.6766) lr 1.7290e-03 eta 0:17:14
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,402
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.9%
******* Domain a best val acc:      86.5%, epoch: 13 *******
******* Domain a best val test acc: 83.2%, epoch: 13 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [15/50] batch [20/288] time 0.102 (0.107) data 0.000 (0.014) loss 1.4167 (1.3093) teacher_loss 1.0312 (0.9117) loss_zs_kd 0.1148 (0.1477) loss_oracle 0.3281 (0.3238) acc 78.1250 (77.3438) alaph_mean 0.6845 (0.6841) alpha_val 0.6845 (0.6841) lr 1.6845e-03 eta 0:18:28
epoch [15/50] batch [40/288] time 0.090 (0.102) data 0.000 (0.007) loss 1.4279 (1.3650) teacher_loss 1.0547 (0.9686) loss_zs_kd 0.1267 (0.1446) loss_oracle 0.3098 (0.3240) acc 78.1250 (75.7031) alaph_mean 0.6855 (0.6845) alpha_val 0.6855 (0.6845) lr 1.6845e-03 eta 0:17:38
epoch [15/50] batch [60/288] time 0.101 (0.101) data 0.000 (0.005) loss 0.9590 (1.3180) teacher_loss 0.5544 (0.9185) loss_zs_kd 0.1324 (0.1478) loss_oracle 0.3384 (0.3256) acc 81.2500 (76.8229) alaph_mean 0.6864 (0.6851) alpha_val 0.6864 (0.6851) lr 1.6845e-03 eta 0:17:22
epoch [15/50] batch [80/288] time 0.095 (0.100) data 0.000 (0.004) loss 1.2808 (1.2772) teacher_loss 0.9228 (0.8806) loss_zs_kd 0.1456 (0.1481) loss_oracle 0.2851 (0.3226) acc 84.3750 (78.0469) alaph_mean 0.6874 (0.6855) alpha_val 0.6874 (0.6855) lr 1.6845e-03 eta 0:17:06
epoch [15/50] batch [100/288] time 0.099 (0.099) data 0.000 (0.003) loss 1.2595 (1.2697) teacher_loss 0.8581 (0.8688) loss_zs_kd 0.1313 (0.1509) loss_oracle 0.3358 (0.3254) acc 84.3750 (78.2188) alaph_mean 0.6885 (0.6860) alpha_val 0.6885 (0.6860) lr 1.6845e-03 eta 0:16:55
epoch [15/50] batch [120/288] time 0.106 (0.099) data 0.000 (0.003) loss 1.5961 (1.2713) teacher_loss 1.1635 (0.8707) loss_zs_kd 0.2040 (0.1497) loss_oracle 0.3306 (0.3258) acc 71.8750 (78.0990) alaph_mean 0.6892 (0.6865) alpha_val 0.6892 (0.6865) lr 1.6845e-03 eta 0:16:57
epoch [15/50] batch [140/288] time 0.105 (0.099) data 0.000 (0.002) loss 1.3741 (1.2834) teacher_loss 0.9170 (0.8818) loss_zs_kd 0.1341 (0.1501) loss_oracle 0.3901 (0.3266) acc 84.3750 (77.7455) alaph_mean 0.6900 (0.6869) alpha_val 0.6900 (0.6869) lr 1.6845e-03 eta 0:16:54
epoch [15/50] batch [160/288] time 0.097 (0.099) data 0.000 (0.002) loss 1.3968 (1.2893) teacher_loss 0.9991 (0.8876) loss_zs_kd 0.1201 (0.1506) loss_oracle 0.3377 (0.3264) acc 75.0000 (77.4805) alaph_mean 0.6911 (0.6874) alpha_val 0.6911 (0.6874) lr 1.6845e-03 eta 0:16:48
epoch [15/50] batch [180/288] time 0.095 (0.099) data 0.000 (0.002) loss 1.7623 (1.2975) teacher_loss 1.3564 (0.8952) loss_zs_kd 0.1695 (0.1499) loss_oracle 0.3212 (0.3274) acc 71.8750 (77.2743) alaph_mean 0.6923 (0.6879) alpha_val 0.6923 (0.6879) lr 1.6845e-03 eta 0:16:45
epoch [15/50] batch [200/288] time 0.096 (0.099) data 0.000 (0.002) loss 1.1374 (1.3065) teacher_loss 0.6587 (0.9022) loss_zs_kd 0.1704 (0.1502) loss_oracle 0.3935 (0.3292) acc 78.1250 (76.9688) alaph_mean 0.6937 (0.6884) alpha_val 0.6937 (0.6884) lr 1.6845e-03 eta 0:16:43
epoch [15/50] batch [220/288] time 0.099 (0.098) data 0.000 (0.001) loss 1.5081 (1.3059) teacher_loss 1.0379 (0.9028) loss_zs_kd 0.1373 (0.1500) loss_oracle 0.4015 (0.3281) acc 75.0000 (76.9744) alaph_mean 0.6949 (0.6889) alpha_val 0.6949 (0.6889) lr 1.6845e-03 eta 0:16:38
epoch [15/50] batch [240/288] time 0.096 (0.098) data 0.000 (0.001) loss 1.2037 (1.3007) teacher_loss 0.7248 (0.8978) loss_zs_kd 0.2206 (0.1505) loss_oracle 0.3687 (0.3276) acc 78.1250 (77.1224) alaph_mean 0.6958 (0.6895) alpha_val 0.6958 (0.6895) lr 1.6845e-03 eta 0:16:32
epoch [15/50] batch [260/288] time 0.097 (0.099) data 0.000 (0.001) loss 1.3183 (1.3031) teacher_loss 0.9281 (0.9007) loss_zs_kd 0.1225 (0.1501) loss_oracle 0.3289 (0.3274) acc 78.1250 (77.0192) alaph_mean 0.6967 (0.6900) alpha_val 0.6967 (0.6900) lr 1.6845e-03 eta 0:16:39
epoch [15/50] batch [280/288] time 0.087 (0.098) data 0.000 (0.001) loss 1.2603 (1.3009) teacher_loss 0.9020 (0.8997) loss_zs_kd 0.1184 (0.1489) loss_oracle 0.2992 (0.3267) acc 71.8750 (77.0201) alaph_mean 0.6974 (0.6905) alpha_val 0.6974 (0.6905) lr 1.6845e-03 eta 0:16:31
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,026
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 79.8%
******* Domain a best val acc:      86.6%, epoch: 15 *******
******* Domain a best val test acc: 83.5%, epoch: 15 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [16/50] batch [20/288] time 0.091 (0.110) data 0.000 (0.013) loss 1.5542 (1.3450) teacher_loss 1.1040 (0.9421) loss_zs_kd 0.1731 (0.1470) loss_oracle 0.3636 (0.3294) acc 71.8750 (76.4062) alaph_mean 0.6984 (0.6981) alpha_val 0.6984 (0.6981) lr 1.6374e-03 eta 0:18:27
epoch [16/50] batch [40/288] time 0.096 (0.103) data 0.000 (0.007) loss 1.5525 (1.3146) teacher_loss 1.1280 (0.9049) loss_zs_kd 0.1467 (0.1507) loss_oracle 0.3512 (0.3344) acc 71.8750 (77.2656) alaph_mean 0.6994 (0.6985) alpha_val 0.6994 (0.6985) lr 1.6374e-03 eta 0:17:09
epoch [16/50] batch [60/288] time 0.093 (0.100) data 0.000 (0.005) loss 1.4343 (1.2980) teacher_loss 1.0754 (0.8953) loss_zs_kd 0.1859 (0.1495) loss_oracle 0.2659 (0.3279) acc 81.2500 (77.6562) alaph_mean 0.7004 (0.6989) alpha_val 0.7004 (0.6989) lr 1.6374e-03 eta 0:16:41
epoch [16/50] batch [80/288] time 0.096 (0.099) data 0.000 (0.003) loss 1.6091 (1.2956) teacher_loss 1.2622 (0.8964) loss_zs_kd 0.1641 (0.1483) loss_oracle 0.2649 (0.3250) acc 68.7500 (77.5781) alaph_mean 0.7014 (0.6995) alpha_val 0.7014 (0.6995) lr 1.6374e-03 eta 0:16:29
epoch [16/50] batch [100/288] time 0.100 (0.098) data 0.000 (0.003) loss 1.4705 (1.3070) teacher_loss 0.9796 (0.9076) loss_zs_kd 0.1839 (0.1500) loss_oracle 0.3990 (0.3244) acc 71.8750 (76.9062) alaph_mean 0.7025 (0.7000) alpha_val 0.7025 (0.7000) lr 1.6374e-03 eta 0:16:23
epoch [16/50] batch [120/288] time 0.096 (0.098) data 0.000 (0.002) loss 1.1634 (1.3015) teacher_loss 0.7520 (0.9023) loss_zs_kd 0.1802 (0.1483) loss_oracle 0.3214 (0.3251) acc 81.2500 (76.8750) alaph_mean 0.7034 (0.7005) alpha_val 0.7034 (0.7005) lr 1.6374e-03 eta 0:16:14
epoch [16/50] batch [140/288] time 0.092 (0.097) data 0.000 (0.002) loss 1.3515 (1.2845) teacher_loss 0.9619 (0.8850) loss_zs_kd 0.1423 (0.1485) loss_oracle 0.3185 (0.3253) acc 78.1250 (77.2991) alaph_mean 0.7045 (0.7009) alpha_val 0.7045 (0.7009) lr 1.6374e-03 eta 0:16:08
epoch [16/50] batch [160/288] time 0.100 (0.097) data 0.000 (0.002) loss 1.3616 (1.2829) teacher_loss 0.9606 (0.8823) loss_zs_kd 0.1283 (0.1498) loss_oracle 0.3369 (0.3257) acc 78.1250 (77.4609) alaph_mean 0.7058 (0.7015) alpha_val 0.7058 (0.7015) lr 1.6374e-03 eta 0:16:04
epoch [16/50] batch [180/288] time 0.090 (0.097) data 0.000 (0.002) loss 1.0932 (1.2933) teacher_loss 0.7418 (0.8926) loss_zs_kd 0.1126 (0.1503) loss_oracle 0.2950 (0.3256) acc 81.2500 (77.4479) alaph_mean 0.7071 (0.7020) alpha_val 0.7071 (0.7020) lr 1.6374e-03 eta 0:15:59
epoch [16/50] batch [200/288] time 0.087 (0.097) data 0.000 (0.002) loss 1.1445 (1.2977) teacher_loss 0.7415 (0.8973) loss_zs_kd 0.1567 (0.1492) loss_oracle 0.3247 (0.3258) acc 75.0000 (77.1406) alaph_mean 0.7079 (0.7026) alpha_val 0.7079 (0.7026) lr 1.6374e-03 eta 0:15:56
epoch [16/50] batch [220/288] time 0.096 (0.097) data 0.000 (0.001) loss 1.0248 (1.2961) teacher_loss 0.6250 (0.8954) loss_zs_kd 0.1407 (0.1491) loss_oracle 0.3295 (0.3261) acc 87.5000 (77.0170) alaph_mean 0.7092 (0.7031) alpha_val 0.7092 (0.7031) lr 1.6374e-03 eta 0:15:54
epoch [16/50] batch [240/288] time 0.097 (0.097) data 0.000 (0.001) loss 1.1061 (1.2878) teacher_loss 0.7243 (0.8869) loss_zs_kd 0.1681 (0.1493) loss_oracle 0.2978 (0.3263) acc 81.2500 (77.3047) alaph_mean 0.7101 (0.7037) alpha_val 0.7101 (0.7037) lr 1.6374e-03 eta 0:15:52
epoch [16/50] batch [260/288] time 0.104 (0.097) data 0.000 (0.001) loss 1.1927 (1.2943) teacher_loss 0.8605 (0.8915) loss_zs_kd 0.1170 (0.1497) loss_oracle 0.2737 (0.3280) acc 78.1250 (77.1635) alaph_mean 0.7110 (0.7042) alpha_val 0.7110 (0.7042) lr 1.6374e-03 eta 0:15:56
epoch [16/50] batch [280/288] time 0.086 (0.097) data 0.000 (0.001) loss 1.8089 (1.3030) teacher_loss 1.3776 (0.8996) loss_zs_kd 0.2205 (0.1506) loss_oracle 0.3211 (0.3280) acc 65.6250 (77.0647) alaph_mean 0.7117 (0.7047) alpha_val 0.7117 (0.7047) lr 1.6374e-03 eta 0:15:52
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,401
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.1%
******* Domain a best val acc:      86.6%, epoch: 15 *******
******* Domain a best val test acc: 83.5%, epoch: 15 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [17/50] batch [20/288] time 0.091 (0.114) data 0.000 (0.014) loss 1.8086 (1.1709) teacher_loss 1.3470 (0.7677) loss_zs_kd 0.1279 (0.1397) loss_oracle 0.3976 (0.3334) acc 81.2500 (80.9375) alaph_mean 0.7133 (0.7128) alpha_val 0.7133 (0.7128) lr 1.5878e-03 eta 0:18:31
epoch [17/50] batch [40/288] time 0.095 (0.104) data 0.000 (0.007) loss 1.1851 (1.2536) teacher_loss 0.7854 (0.8437) loss_zs_kd 0.1749 (0.1440) loss_oracle 0.3123 (0.3380) acc 84.3750 (79.4531) alaph_mean 0.7141 (0.7133) alpha_val 0.7141 (0.7133) lr 1.5878e-03 eta 0:16:51
epoch [17/50] batch [60/288] time 0.101 (0.101) data 0.000 (0.005) loss 1.6379 (1.2664) teacher_loss 1.2698 (0.8643) loss_zs_kd 0.1422 (0.1456) loss_oracle 0.2970 (0.3293) acc 71.8750 (78.2812) alaph_mean 0.7150 (0.7137) alpha_val 0.7150 (0.7137) lr 1.5878e-03 eta 0:16:24
epoch [17/50] batch [80/288] time 0.094 (0.100) data 0.000 (0.004) loss 1.1466 (1.2629) teacher_loss 0.7447 (0.8608) loss_zs_kd 0.1534 (0.1480) loss_oracle 0.3252 (0.3281) acc 84.3750 (78.6328) alaph_mean 0.7160 (0.7142) alpha_val 0.7160 (0.7142) lr 1.5878e-03 eta 0:16:11
epoch [17/50] batch [100/288] time 0.109 (0.100) data 0.001 (0.003) loss 1.3986 (1.2627) teacher_loss 1.0213 (0.8609) loss_zs_kd 0.1503 (0.1486) loss_oracle 0.3021 (0.3275) acc 62.5000 (78.4688) alaph_mean 0.7172 (0.7147) alpha_val 0.7172 (0.7147) lr 1.5878e-03 eta 0:16:08
epoch [17/50] batch [120/288] time 0.097 (0.101) data 0.000 (0.003) loss 1.2748 (1.2687) teacher_loss 0.9323 (0.8709) loss_zs_kd 0.1129 (0.1481) loss_oracle 0.2860 (0.3237) acc 81.2500 (77.8125) alaph_mean 0.7181 (0.7152) alpha_val 0.7181 (0.7152) lr 1.5878e-03 eta 0:16:13
epoch [17/50] batch [140/288] time 0.120 (0.101) data 0.000 (0.002) loss 1.5498 (1.2930) teacher_loss 1.1144 (0.8928) loss_zs_kd 0.1714 (0.1501) loss_oracle 0.3497 (0.3251) acc 71.8750 (77.3214) alaph_mean 0.7193 (0.7157) alpha_val 0.7193 (0.7157) lr 1.5878e-03 eta 0:16:17
epoch [17/50] batch [160/288] time 0.097 (0.102) data 0.000 (0.002) loss 1.3502 (1.2939) teacher_loss 0.9498 (0.8937) loss_zs_kd 0.1715 (0.1512) loss_oracle 0.3146 (0.3246) acc 75.0000 (77.1094) alaph_mean 0.7202 (0.7162) alpha_val 0.7202 (0.7162) lr 1.5878e-03 eta 0:16:22
epoch [17/50] batch [180/288] time 0.109 (0.103) data 0.000 (0.002) loss 1.1823 (1.2828) teacher_loss 0.7707 (0.8838) loss_zs_kd 0.1296 (0.1506) loss_oracle 0.3468 (0.3237) acc 81.2500 (77.3611) alaph_mean 0.7210 (0.7167) alpha_val 0.7210 (0.7167) lr 1.5878e-03 eta 0:16:26
epoch [17/50] batch [200/288] time 0.099 (0.103) data 0.000 (0.002) loss 1.1354 (1.2815) teacher_loss 0.7183 (0.8815) loss_zs_kd 0.1326 (0.1513) loss_oracle 0.3508 (0.3244) acc 75.0000 (77.5469) alaph_mean 0.7219 (0.7172) alpha_val 0.7219 (0.7172) lr 1.5878e-03 eta 0:16:24
epoch [17/50] batch [220/288] time 0.171 (0.103) data 0.001 (0.002) loss 1.1367 (1.2810) teacher_loss 0.7387 (0.8810) loss_zs_kd 0.1047 (0.1521) loss_oracle 0.3456 (0.3239) acc 84.3750 (77.4858) alaph_mean 0.7227 (0.7176) alpha_val 0.7227 (0.7176) lr 1.5878e-03 eta 0:16:29
epoch [17/50] batch [240/288] time 0.097 (0.104) data 0.000 (0.001) loss 1.2472 (1.2785) teacher_loss 0.7551 (0.8786) loss_zs_kd 0.1500 (0.1525) loss_oracle 0.4171 (0.3237) acc 75.0000 (77.5130) alaph_mean 0.7236 (0.7181) alpha_val 0.7236 (0.7181) lr 1.5878e-03 eta 0:16:38
epoch [17/50] batch [260/288] time 0.105 (0.104) data 0.000 (0.001) loss 1.5947 (1.2823) teacher_loss 1.1819 (0.8808) loss_zs_kd 0.1527 (0.1533) loss_oracle 0.3365 (0.3248) acc 75.0000 (77.4519) alaph_mean 0.7246 (0.7185) alpha_val 0.7246 (0.7185) lr 1.5878e-03 eta 0:16:33
epoch [17/50] batch [280/288] time 0.089 (0.104) data 0.000 (0.001) loss 1.5613 (1.2813) teacher_loss 1.1818 (0.8796) loss_zs_kd 0.1937 (0.1544) loss_oracle 0.2826 (0.3245) acc 68.7500 (77.5446) alaph_mean 0.7257 (0.7190) alpha_val 0.7257 (0.7190) lr 1.5878e-03 eta 0:16:25
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,418
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.1%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 80.1%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [18/50] batch [20/288] time 0.099 (0.124) data 0.000 (0.014) loss 1.5763 (1.2734) teacher_loss 1.1718 (0.8680) loss_zs_kd 0.1610 (0.1657) loss_oracle 0.3240 (0.3226) acc 68.7500 (78.2812) alaph_mean 0.7271 (0.7267) alpha_val 0.7271 (0.7267) lr 1.5358e-03 eta 0:19:39
epoch [18/50] batch [40/288] time 0.110 (0.115) data 0.000 (0.007) loss 1.8368 (1.3173) teacher_loss 1.3891 (0.9002) loss_zs_kd 0.2191 (0.1657) loss_oracle 0.3382 (0.3343) acc 68.7500 (76.7188) alaph_mean 0.7281 (0.7272) alpha_val 0.7281 (0.7272) lr 1.5358e-03 eta 0:18:03
epoch [18/50] batch [60/288] time 0.099 (0.112) data 0.000 (0.005) loss 0.9617 (1.3181) teacher_loss 0.5061 (0.8986) loss_zs_kd 0.1919 (0.1623) loss_oracle 0.3596 (0.3384) acc 87.5000 (76.8750) alaph_mean 0.7292 (0.7277) alpha_val 0.7292 (0.7277) lr 1.5358e-03 eta 0:17:33
epoch [18/50] batch [80/288] time 0.099 (0.109) data 0.000 (0.004) loss 2.1964 (1.3444) teacher_loss 1.8044 (0.9229) loss_zs_kd 0.2160 (0.1627) loss_oracle 0.2840 (0.3401) acc 53.1250 (76.0547) alaph_mean 0.7299 (0.7282) alpha_val 0.7299 (0.7282) lr 1.5358e-03 eta 0:17:05
epoch [18/50] batch [100/288] time 0.102 (0.107) data 0.000 (0.003) loss 1.1734 (1.3322) teacher_loss 0.7998 (0.9131) loss_zs_kd 0.1261 (0.1609) loss_oracle 0.3105 (0.3387) acc 87.5000 (76.3750) alaph_mean 0.7309 (0.7286) alpha_val 0.7309 (0.7286) lr 1.5358e-03 eta 0:16:50
epoch [18/50] batch [120/288] time 0.102 (0.106) data 0.000 (0.003) loss 1.3451 (1.3194) teacher_loss 1.0070 (0.9011) loss_zs_kd 0.1590 (0.1595) loss_oracle 0.2585 (0.3386) acc 75.0000 (76.7708) alaph_mean 0.7317 (0.7291) alpha_val 0.7317 (0.7291) lr 1.5358e-03 eta 0:16:39
epoch [18/50] batch [140/288] time 0.111 (0.106) data 0.000 (0.002) loss 1.4697 (1.3270) teacher_loss 1.0634 (0.9071) loss_zs_kd 0.1597 (0.1615) loss_oracle 0.3264 (0.3391) acc 75.0000 (76.6295) alaph_mean 0.7324 (0.7295) alpha_val 0.7324 (0.7295) lr 1.5358e-03 eta 0:16:33
epoch [18/50] batch [160/288] time 0.110 (0.106) data 0.000 (0.002) loss 1.2016 (1.3205) teacher_loss 0.7617 (0.9009) loss_zs_kd 0.1878 (0.1624) loss_oracle 0.3460 (0.3385) acc 78.1250 (76.7969) alaph_mean 0.7330 (0.7299) alpha_val 0.7330 (0.7299) lr 1.5358e-03 eta 0:16:26
epoch [18/50] batch [180/288] time 0.102 (0.105) data 0.000 (0.002) loss 1.1795 (1.3119) teacher_loss 0.7369 (0.8921) loss_zs_kd 0.2059 (0.1635) loss_oracle 0.3396 (0.3380) acc 75.0000 (77.1181) alaph_mean 0.7337 (0.7303) alpha_val 0.7337 (0.7303) lr 1.5358e-03 eta 0:16:21
epoch [18/50] batch [200/288] time 0.103 (0.107) data 0.000 (0.002) loss 1.6160 (1.3004) teacher_loss 1.2398 (0.8828) loss_zs_kd 0.1510 (0.1635) loss_oracle 0.3007 (0.3359) acc 68.7500 (77.3125) alaph_mean 0.7345 (0.7307) alpha_val 0.7345 (0.7307) lr 1.5358e-03 eta 0:16:31
epoch [18/50] batch [220/288] time 0.107 (0.106) data 0.000 (0.002) loss 1.7302 (1.3022) teacher_loss 1.3923 (0.8857) loss_zs_kd 0.1396 (0.1641) loss_oracle 0.2681 (0.3345) acc 65.6250 (77.2017) alaph_mean 0.7355 (0.7311) alpha_val 0.7355 (0.7311) lr 1.5358e-03 eta 0:16:26
epoch [18/50] batch [240/288] time 0.103 (0.106) data 0.000 (0.001) loss 1.0756 (1.3034) teacher_loss 0.6653 (0.8865) loss_zs_kd 0.1615 (0.1646) loss_oracle 0.3296 (0.3346) acc 81.2500 (77.1484) alaph_mean 0.7366 (0.7315) alpha_val 0.7366 (0.7315) lr 1.5358e-03 eta 0:16:21
epoch [18/50] batch [260/288] time 0.099 (0.105) data 0.000 (0.001) loss 1.3459 (1.3036) teacher_loss 0.9815 (0.8867) loss_zs_kd 0.1032 (0.1639) loss_oracle 0.3129 (0.3349) acc 78.1250 (77.3077) alaph_mean 0.7376 (0.7319) alpha_val 0.7376 (0.7319) lr 1.5358e-03 eta 0:16:13
epoch [18/50] batch [280/288] time 0.085 (0.104) data 0.000 (0.001) loss 1.5513 (1.3070) teacher_loss 1.0989 (0.8906) loss_zs_kd 0.1995 (0.1641) loss_oracle 0.3527 (0.3344) acc 68.7500 (77.2210) alaph_mean 0.7387 (0.7324) alpha_val 0.7387 (0.7324) lr 1.5358e-03 eta 0:16:02
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.8%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [19/50] batch [20/288] time 0.097 (0.122) data 0.000 (0.016) loss 1.3405 (1.2526) teacher_loss 0.9461 (0.8452) loss_zs_kd 0.1257 (0.1577) loss_oracle 0.3315 (0.3286) acc 75.0000 (78.5938) alaph_mean 0.7398 (0.7395) alpha_val 0.7398 (0.7395) lr 1.4818e-03 eta 0:18:37
epoch [19/50] batch [40/288] time 0.092 (0.110) data 0.000 (0.008) loss 1.3277 (1.3086) teacher_loss 0.9375 (0.9077) loss_zs_kd 0.1515 (0.1554) loss_oracle 0.3144 (0.3232) acc 65.6250 (76.7188) alaph_mean 0.7406 (0.7398) alpha_val 0.7406 (0.7398) lr 1.4818e-03 eta 0:16:48
epoch [19/50] batch [60/288] time 0.093 (0.105) data 0.000 (0.006) loss 1.5643 (1.3482) teacher_loss 1.1557 (0.9447) loss_zs_kd 0.1842 (0.1586) loss_oracle 0.3165 (0.3241) acc 75.0000 (76.1979) alaph_mean 0.7414 (0.7402) alpha_val 0.7414 (0.7402) lr 1.4818e-03 eta 0:16:01
epoch [19/50] batch [80/288] time 0.092 (0.103) data 0.000 (0.004) loss 1.1771 (1.3080) teacher_loss 0.7922 (0.9031) loss_zs_kd 0.1542 (0.1596) loss_oracle 0.3079 (0.3251) acc 75.0000 (77.2266) alaph_mean 0.7420 (0.7406) alpha_val 0.7420 (0.7406) lr 1.4818e-03 eta 0:15:37
epoch [19/50] batch [100/288] time 0.094 (0.101) data 0.000 (0.003) loss 1.3400 (1.3030) teacher_loss 0.9526 (0.8980) loss_zs_kd 0.1316 (0.1587) loss_oracle 0.3216 (0.3256) acc 65.6250 (77.3125) alaph_mean 0.7428 (0.7410) alpha_val 0.7428 (0.7410) lr 1.4818e-03 eta 0:15:22
epoch [19/50] batch [120/288] time 0.093 (0.100) data 0.000 (0.003) loss 1.7613 (1.2994) teacher_loss 1.3922 (0.8952) loss_zs_kd 0.1643 (0.1581) loss_oracle 0.2870 (0.3251) acc 65.6250 (77.5521) alaph_mean 0.7436 (0.7413) alpha_val 0.7436 (0.7413) lr 1.4818e-03 eta 0:15:10
epoch [19/50] batch [140/288] time 0.095 (0.099) data 0.000 (0.003) loss 1.4913 (1.2969) teacher_loss 1.1790 (0.8929) loss_zs_kd 0.1368 (0.1582) loss_oracle 0.2440 (0.3249) acc 78.1250 (77.7232) alaph_mean 0.7443 (0.7417) alpha_val 0.7443 (0.7417) lr 1.4818e-03 eta 0:15:01
epoch [19/50] batch [160/288] time 0.102 (0.099) data 0.000 (0.002) loss 1.2145 (1.2963) teacher_loss 0.7801 (0.8936) loss_zs_kd 0.1713 (0.1568) loss_oracle 0.3488 (0.3243) acc 81.2500 (77.6758) alaph_mean 0.7451 (0.7421) alpha_val 0.7451 (0.7421) lr 1.4818e-03 eta 0:14:57
epoch [19/50] batch [180/288] time 0.096 (0.100) data 0.000 (0.002) loss 1.3469 (1.2964) teacher_loss 0.9732 (0.8918) loss_zs_kd 0.1597 (0.1579) loss_oracle 0.2939 (0.3256) acc 75.0000 (77.5521) alaph_mean 0.7459 (0.7425) alpha_val 0.7459 (0.7425) lr 1.4818e-03 eta 0:15:05
epoch [19/50] batch [200/288] time 0.096 (0.100) data 0.000 (0.002) loss 1.0532 (1.2899) teacher_loss 0.6441 (0.8843) loss_zs_kd 0.1922 (0.1588) loss_oracle 0.3130 (0.3262) acc 84.3750 (77.6250) alaph_mean 0.7469 (0.7429) alpha_val 0.7469 (0.7429) lr 1.4818e-03 eta 0:14:58
epoch [19/50] batch [220/288] time 0.107 (0.099) data 0.000 (0.002) loss 1.3745 (1.3011) teacher_loss 0.9018 (0.8924) loss_zs_kd 0.2078 (0.1615) loss_oracle 0.3688 (0.3279) acc 81.2500 (77.5284) alaph_mean 0.7482 (0.7433) alpha_val 0.7482 (0.7433) lr 1.4818e-03 eta 0:14:53
epoch [19/50] batch [240/288] time 0.098 (0.099) data 0.000 (0.002) loss 1.0957 (1.2980) teacher_loss 0.6331 (0.8894) loss_zs_kd 0.2228 (0.1622) loss_oracle 0.3512 (0.3275) acc 84.3750 (77.5391) alaph_mean 0.7493 (0.7438) alpha_val 0.7493 (0.7438) lr 1.4818e-03 eta 0:14:50
epoch [19/50] batch [260/288] time 0.098 (0.099) data 0.000 (0.001) loss 1.2746 (1.2935) teacher_loss 0.8633 (0.8849) loss_zs_kd 0.1733 (0.1630) loss_oracle 0.3246 (0.3271) acc 78.1250 (77.7404) alaph_mean 0.7503 (0.7442) alpha_val 0.7503 (0.7442) lr 1.4818e-03 eta 0:14:45
epoch [19/50] batch [280/288] time 0.088 (0.099) data 0.000 (0.001) loss 1.8349 (1.2954) teacher_loss 1.3583 (0.8855) loss_zs_kd 0.2173 (0.1638) loss_oracle 0.3679 (0.3280) acc 68.7500 (77.7455) alaph_mean 0.7511 (0.7447) alpha_val 0.7511 (0.7447) lr 1.4818e-03 eta 0:14:41
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,409
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,010
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.2%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [20/50] batch [20/288] time 0.100 (0.117) data 0.000 (0.014) loss 1.6777 (1.2792) teacher_loss 1.3170 (0.8544) loss_zs_kd 0.1670 (0.1643) loss_oracle 0.2771 (0.3427) acc 62.5000 (77.6562) alaph_mean 0.7525 (0.7520) alpha_val 0.7525 (0.7520) lr 1.4258e-03 eta 0:17:22
epoch [20/50] batch [40/288] time 0.100 (0.109) data 0.000 (0.007) loss 1.3037 (1.2756) teacher_loss 0.8507 (0.8490) loss_zs_kd 0.2352 (0.1724) loss_oracle 0.3354 (0.3404) acc 81.2500 (77.8125) alaph_mean 0.7532 (0.7525) alpha_val 0.7532 (0.7525) lr 1.4258e-03 eta 0:16:09
epoch [20/50] batch [60/288] time 0.100 (0.106) data 0.000 (0.005) loss 1.0819 (1.2647) teacher_loss 0.7352 (0.8414) loss_zs_kd 0.1067 (0.1705) loss_oracle 0.2934 (0.3381) acc 81.2500 (78.3333) alaph_mean 0.7541 (0.7529) alpha_val 0.7541 (0.7529) lr 1.4258e-03 eta 0:15:41
epoch [20/50] batch [80/288] time 0.098 (0.105) data 0.000 (0.004) loss 1.1054 (1.2704) teacher_loss 0.6506 (0.8472) loss_zs_kd 0.1560 (0.1690) loss_oracle 0.3768 (0.3387) acc 87.5000 (78.3203) alaph_mean 0.7548 (0.7533) alpha_val 0.7548 (0.7533) lr 1.4258e-03 eta 0:15:26
epoch [20/50] batch [100/288] time 0.101 (0.104) data 0.000 (0.003) loss 1.4729 (1.2650) teacher_loss 1.0357 (0.8480) loss_zs_kd 0.1619 (0.1665) loss_oracle 0.3563 (0.3338) acc 68.7500 (78.2812) alaph_mean 0.7555 (0.7537) alpha_val 0.7555 (0.7537) lr 1.4258e-03 eta 0:15:18
epoch [20/50] batch [120/288] time 0.099 (0.104) data 0.000 (0.002) loss 0.8464 (1.2834) teacher_loss 0.5221 (0.8659) loss_zs_kd 0.0909 (0.1656) loss_oracle 0.2789 (0.3347) acc 87.5000 (77.7344) alaph_mean 0.7561 (0.7540) alpha_val 0.7561 (0.7540) lr 1.4258e-03 eta 0:15:11
epoch [20/50] batch [140/288] time 0.099 (0.102) data 0.000 (0.002) loss 1.0964 (1.2905) teacher_loss 0.7543 (0.8698) loss_zs_kd 0.1700 (0.1673) loss_oracle 0.2571 (0.3371) acc 81.2500 (77.6116) alaph_mean 0.7571 (0.7544) alpha_val 0.7571 (0.7544) lr 1.4258e-03 eta 0:15:00
epoch [20/50] batch [160/288] time 0.111 (0.102) data 0.000 (0.002) loss 1.3488 (1.2879) teacher_loss 0.9761 (0.8675) loss_zs_kd 0.1191 (0.1659) loss_oracle 0.3131 (0.3375) acc 75.0000 (77.7930) alaph_mean 0.7578 (0.7548) alpha_val 0.7578 (0.7548) lr 1.4258e-03 eta 0:14:52
epoch [20/50] batch [180/288] time 0.096 (0.101) data 0.000 (0.002) loss 1.1089 (1.2838) teacher_loss 0.6789 (0.8616) loss_zs_kd 0.1400 (0.1653) loss_oracle 0.3601 (0.3396) acc 81.2500 (77.9688) alaph_mean 0.7585 (0.7551) alpha_val 0.7585 (0.7551) lr 1.4258e-03 eta 0:14:45
epoch [20/50] batch [200/288] time 0.095 (0.101) data 0.000 (0.002) loss 1.7932 (1.2890) teacher_loss 1.2955 (0.8650) loss_zs_kd 0.2797 (0.1653) loss_oracle 0.3578 (0.3414) acc 65.6250 (77.9062) alaph_mean 0.7591 (0.7555) alpha_val 0.7591 (0.7555) lr 1.4258e-03 eta 0:14:38
epoch [20/50] batch [220/288] time 0.093 (0.100) data 0.000 (0.001) loss 1.2178 (1.2793) teacher_loss 0.8079 (0.8541) loss_zs_kd 0.1449 (0.1646) loss_oracle 0.3375 (0.3429) acc 78.1250 (78.0256) alaph_mean 0.7599 (0.7559) alpha_val 0.7599 (0.7559) lr 1.4258e-03 eta 0:14:33
epoch [20/50] batch [240/288] time 0.091 (0.100) data 0.000 (0.001) loss 1.3675 (1.2777) teacher_loss 0.9925 (0.8526) loss_zs_kd 0.1493 (0.1655) loss_oracle 0.3003 (0.3424) acc 68.7500 (78.0469) alaph_mean 0.7605 (0.7562) alpha_val 0.7605 (0.7562) lr 1.4258e-03 eta 0:14:29
epoch [20/50] batch [260/288] time 0.097 (0.100) data 0.000 (0.001) loss 1.6073 (1.2739) teacher_loss 1.2350 (0.8491) loss_zs_kd 0.1283 (0.1649) loss_oracle 0.3082 (0.3424) acc 68.7500 (78.1490) alaph_mean 0.7611 (0.7566) alpha_val 0.7611 (0.7566) lr 1.4258e-03 eta 0:14:24
epoch [20/50] batch [280/288] time 0.087 (0.099) data 0.000 (0.001) loss 1.0569 (1.2705) teacher_loss 0.6845 (0.8470) loss_zs_kd 0.1227 (0.1636) loss_oracle 0.3110 (0.3416) acc 84.3750 (78.1808) alaph_mean 0.7618 (0.7569) alpha_val 0.7618 (0.7569) lr 1.4258e-03 eta 0:14:17
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,413
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 86.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,027
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.4%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [21/50] batch [20/288] time 0.094 (0.117) data 0.000 (0.015) loss 0.8115 (1.2233) teacher_loss 0.4181 (0.8093) loss_zs_kd 0.1508 (0.1706) loss_oracle 0.3181 (0.3287) acc 93.7500 (80.0000) alaph_mean 0.7628 (0.7625) alpha_val 0.7628 (0.7625) lr 1.3681e-03 eta 0:16:45
epoch [21/50] batch [40/288] time 0.095 (0.106) data 0.000 (0.008) loss 0.9815 (1.2245) teacher_loss 0.5472 (0.8053) loss_zs_kd 0.1582 (0.1652) loss_oracle 0.3552 (0.3366) acc 84.3750 (78.6719) alaph_mean 0.7637 (0.7628) alpha_val 0.7637 (0.7628) lr 1.3681e-03 eta 0:15:14
epoch [21/50] batch [60/288] time 0.097 (0.104) data 0.000 (0.005) loss 1.1280 (1.2403) teacher_loss 0.7586 (0.8233) loss_zs_kd 0.1647 (0.1639) loss_oracle 0.2871 (0.3350) acc 75.0000 (78.1771) alaph_mean 0.7645 (0.7633) alpha_val 0.7645 (0.7633) lr 1.3681e-03 eta 0:14:48
epoch [21/50] batch [80/288] time 0.096 (0.102) data 0.000 (0.004) loss 0.9126 (1.2688) teacher_loss 0.5490 (0.8499) loss_zs_kd 0.1341 (0.1635) loss_oracle 0.2965 (0.3371) acc 84.3750 (77.8125) alaph_mean 0.7652 (0.7637) alpha_val 0.7652 (0.7637) lr 1.3681e-03 eta 0:14:31
epoch [21/50] batch [100/288] time 0.097 (0.101) data 0.000 (0.003) loss 1.2351 (1.2568) teacher_loss 0.8789 (0.8381) loss_zs_kd 0.1253 (0.1627) loss_oracle 0.2936 (0.3373) acc 81.2500 (78.2812) alaph_mean 0.7661 (0.7641) alpha_val 0.7661 (0.7641) lr 1.3681e-03 eta 0:14:18
epoch [21/50] batch [120/288] time 0.100 (0.100) data 0.000 (0.003) loss 0.9931 (1.2650) teacher_loss 0.6210 (0.8472) loss_zs_kd 0.1319 (0.1632) loss_oracle 0.3061 (0.3362) acc 87.5000 (78.3073) alaph_mean 0.7669 (0.7645) alpha_val 0.7669 (0.7645) lr 1.3681e-03 eta 0:14:11
epoch [21/50] batch [140/288] time 0.098 (0.099) data 0.000 (0.002) loss 1.2970 (1.2777) teacher_loss 0.8454 (0.8614) loss_zs_kd 0.1866 (0.1621) loss_oracle 0.3582 (0.3352) acc 81.2500 (77.9688) alaph_mean 0.7676 (0.7649) alpha_val 0.7676 (0.7649) lr 1.3681e-03 eta 0:14:03
epoch [21/50] batch [160/288] time 0.102 (0.099) data 0.001 (0.002) loss 1.3451 (1.2943) teacher_loss 0.8158 (0.8753) loss_zs_kd 0.1631 (0.1648) loss_oracle 0.4477 (0.3366) acc 78.1250 (77.5586) alaph_mean 0.7684 (0.7653) alpha_val 0.7684 (0.7653) lr 1.3681e-03 eta 0:13:58
epoch [21/50] batch [180/288] time 0.097 (0.099) data 0.000 (0.002) loss 1.4456 (1.3005) teacher_loss 0.9901 (0.8793) loss_zs_kd 0.1461 (0.1654) loss_oracle 0.3824 (0.3386) acc 75.0000 (77.4306) alaph_mean 0.7689 (0.7656) alpha_val 0.7689 (0.7656) lr 1.3681e-03 eta 0:13:59
epoch [21/50] batch [200/288] time 0.099 (0.099) data 0.000 (0.002) loss 1.5829 (1.2969) teacher_loss 1.1466 (0.8733) loss_zs_kd 0.1522 (0.1651) loss_oracle 0.3602 (0.3411) acc 75.0000 (77.6406) alaph_mean 0.7695 (0.7660) alpha_val 0.7695 (0.7660) lr 1.3681e-03 eta 0:13:58
epoch [21/50] batch [220/288] time 0.097 (0.100) data 0.000 (0.002) loss 1.0950 (1.2952) teacher_loss 0.6640 (0.8705) loss_zs_kd 0.1274 (0.1641) loss_oracle 0.3673 (0.3427) acc 81.2500 (77.5710) alaph_mean 0.7701 (0.7663) alpha_val 0.7701 (0.7663) lr 1.3681e-03 eta 0:14:00
epoch [21/50] batch [240/288] time 0.101 (0.100) data 0.000 (0.001) loss 1.2414 (1.3047) teacher_loss 0.8091 (0.8803) loss_zs_kd 0.2042 (0.1638) loss_oracle 0.3301 (0.3425) acc 84.3750 (77.4479) alaph_mean 0.7707 (0.7667) alpha_val 0.7707 (0.7667) lr 1.3681e-03 eta 0:14:01
epoch [21/50] batch [260/288] time 0.103 (0.100) data 0.000 (0.001) loss 1.1630 (1.3007) teacher_loss 0.7074 (0.8760) loss_zs_kd 0.1655 (0.1641) loss_oracle 0.3729 (0.3426) acc 81.2500 (77.5601) alaph_mean 0.7713 (0.7670) alpha_val 0.7713 (0.7670) lr 1.3681e-03 eta 0:14:00
epoch [21/50] batch [280/288] time 0.105 (0.100) data 0.000 (0.001) loss 1.2544 (1.2976) teacher_loss 0.8699 (0.8731) loss_zs_kd 0.1222 (0.1643) loss_oracle 0.3234 (0.3423) acc 81.2500 (77.7344) alaph_mean 0.7722 (0.7674) alpha_val 0.7722 (0.7674) lr 1.3681e-03 eta 0:13:58
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,408
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.5%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [22/50] batch [20/288] time 0.094 (0.114) data 0.000 (0.012) loss 1.6275 (1.2805) teacher_loss 1.1030 (0.8298) loss_zs_kd 0.2090 (0.1866) loss_oracle 0.4200 (0.3574) acc 71.8750 (78.1250) alaph_mean 0.7732 (0.7729) alpha_val 0.7732 (0.7729) lr 1.3090e-03 eta 0:15:47
epoch [22/50] batch [40/288] time 0.100 (0.105) data 0.000 (0.006) loss 1.1454 (1.2597) teacher_loss 0.7264 (0.8168) loss_zs_kd 0.1180 (0.1815) loss_oracle 0.3600 (0.3521) acc 81.2500 (78.5156) alaph_mean 0.7740 (0.7733) alpha_val 0.7740 (0.7733) lr 1.3090e-03 eta 0:14:32
epoch [22/50] batch [60/288] time 0.094 (0.102) data 0.000 (0.004) loss 1.0018 (1.2884) teacher_loss 0.5515 (0.8383) loss_zs_kd 0.2182 (0.1823) loss_oracle 0.3412 (0.3589) acc 90.6250 (78.2812) alaph_mean 0.7747 (0.7736) alpha_val 0.7747 (0.7736) lr 1.3090e-03 eta 0:14:07
epoch [22/50] batch [80/288] time 0.095 (0.100) data 0.000 (0.003) loss 1.6559 (1.2812) teacher_loss 1.2452 (0.8325) loss_zs_kd 0.1682 (0.1817) loss_oracle 0.3266 (0.3578) acc 68.7500 (78.7109) alaph_mean 0.7754 (0.7740) alpha_val 0.7754 (0.7740) lr 1.3090e-03 eta 0:13:46
epoch [22/50] batch [100/288] time 0.085 (0.099) data 0.000 (0.003) loss 0.9436 (1.2824) teacher_loss 0.5614 (0.8379) loss_zs_kd 0.1119 (0.1750) loss_oracle 0.3262 (0.3570) acc 84.3750 (78.4688) alaph_mean 0.7759 (0.7743) alpha_val 0.7759 (0.7743) lr 1.3090e-03 eta 0:13:35
epoch [22/50] batch [120/288] time 0.095 (0.098) data 0.000 (0.002) loss 1.3322 (1.2927) teacher_loss 0.8428 (0.8460) loss_zs_kd 0.1980 (0.1756) loss_oracle 0.3904 (0.3589) acc 81.2500 (78.3854) alaph_mean 0.7765 (0.7746) alpha_val 0.7765 (0.7746) lr 1.3090e-03 eta 0:13:25
epoch [22/50] batch [140/288] time 0.088 (0.099) data 0.000 (0.002) loss 1.3037 (1.2823) teacher_loss 0.8664 (0.8408) loss_zs_kd 0.1836 (0.1720) loss_oracle 0.3455 (0.3555) acc 78.1250 (78.5714) alaph_mean 0.7774 (0.7750) alpha_val 0.7774 (0.7750) lr 1.3090e-03 eta 0:13:34
epoch [22/50] batch [160/288] time 0.107 (0.098) data 0.000 (0.002) loss 0.9052 (1.2919) teacher_loss 0.5377 (0.8522) loss_zs_kd 0.1050 (0.1715) loss_oracle 0.3149 (0.3540) acc 84.3750 (78.3594) alaph_mean 0.7781 (0.7753) alpha_val 0.7781 (0.7753) lr 1.3090e-03 eta 0:13:25
epoch [22/50] batch [180/288] time 0.085 (0.097) data 0.000 (0.002) loss 1.1969 (1.2873) teacher_loss 0.8382 (0.8496) loss_zs_kd 0.0952 (0.1701) loss_oracle 0.3111 (0.3527) acc 81.2500 (78.3681) alaph_mean 0.7789 (0.7757) alpha_val 0.7789 (0.7757) lr 1.3090e-03 eta 0:13:16
epoch [22/50] batch [200/288] time 0.096 (0.097) data 0.000 (0.001) loss 1.2143 (1.2899) teacher_loss 0.7965 (0.8537) loss_zs_kd 0.1499 (0.1700) loss_oracle 0.3429 (0.3512) acc 78.1250 (78.2344) alaph_mean 0.7796 (0.7760) alpha_val 0.7796 (0.7760) lr 1.3090e-03 eta 0:13:08
epoch [22/50] batch [220/288] time 0.085 (0.096) data 0.000 (0.001) loss 0.9155 (1.2900) teacher_loss 0.4316 (0.8530) loss_zs_kd 0.1873 (0.1713) loss_oracle 0.3902 (0.3513) acc 90.6250 (78.0824) alaph_mean 0.7803 (0.7764) alpha_val 0.7803 (0.7764) lr 1.3090e-03 eta 0:13:01
epoch [22/50] batch [240/288] time 0.095 (0.096) data 0.000 (0.001) loss 1.3306 (1.2928) teacher_loss 0.9285 (0.8562) loss_zs_kd 0.1700 (0.1716) loss_oracle 0.3171 (0.3507) acc 68.7500 (77.9688) alaph_mean 0.7809 (0.7767) alpha_val 0.7809 (0.7767) lr 1.3090e-03 eta 0:12:59
epoch [22/50] batch [260/288] time 0.100 (0.096) data 0.000 (0.001) loss 1.2168 (1.2949) teacher_loss 0.8063 (0.8587) loss_zs_kd 0.1571 (0.1707) loss_oracle 0.3320 (0.3508) acc 81.2500 (77.9087) alaph_mean 0.7814 (0.7771) alpha_val 0.7814 (0.7771) lr 1.3090e-03 eta 0:12:57
epoch [22/50] batch [280/288] time 0.090 (0.096) data 0.000 (0.001) loss 0.9011 (1.2968) teacher_loss 0.5052 (0.8609) loss_zs_kd 0.1440 (0.1705) loss_oracle 0.3239 (0.3507) acc 93.7500 (77.8571) alaph_mean 0.7821 (0.7774) alpha_val 0.7821 (0.7774) lr 1.3090e-03 eta 0:12:55
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.0%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [23/50] batch [20/288] time 0.104 (0.118) data 0.000 (0.013) loss 1.4749 (1.2441) teacher_loss 1.0363 (0.8200) loss_zs_kd 0.1616 (0.1580) loss_oracle 0.3578 (0.3452) acc 81.2500 (81.2500) alaph_mean 0.7829 (0.7826) alpha_val 0.7829 (0.7826) lr 1.2487e-03 eta 0:15:49
epoch [23/50] batch [40/288] time 0.100 (0.108) data 0.000 (0.007) loss 1.2473 (1.2553) teacher_loss 0.8371 (0.8203) loss_zs_kd 0.1385 (0.1628) loss_oracle 0.3410 (0.3535) acc 75.0000 (79.5312) alaph_mean 0.7834 (0.7829) alpha_val 0.7834 (0.7829) lr 1.2487e-03 eta 0:14:26
epoch [23/50] batch [60/288] time 0.093 (0.104) data 0.000 (0.004) loss 1.6315 (1.2848) teacher_loss 1.0741 (0.8440) loss_zs_kd 0.2444 (0.1691) loss_oracle 0.4352 (0.3563) acc 68.7500 (78.4375) alaph_mean 0.7841 (0.7832) alpha_val 0.7841 (0.7832) lr 1.2487e-03 eta 0:13:53
epoch [23/50] batch [80/288] time 0.094 (0.102) data 0.000 (0.003) loss 1.6402 (1.2823) teacher_loss 1.1833 (0.8381) loss_zs_kd 0.1968 (0.1732) loss_oracle 0.3585 (0.3575) acc 68.7500 (78.5938) alaph_mean 0.7848 (0.7835) alpha_val 0.7848 (0.7835) lr 1.2487e-03 eta 0:13:35
epoch [23/50] batch [100/288] time 0.101 (0.101) data 0.000 (0.003) loss 1.0751 (1.2779) teacher_loss 0.6237 (0.8332) loss_zs_kd 0.1862 (0.1745) loss_oracle 0.3583 (0.3574) acc 84.3750 (78.6250) alaph_mean 0.7856 (0.7839) alpha_val 0.7856 (0.7839) lr 1.2487e-03 eta 0:13:21
epoch [23/50] batch [120/288] time 0.094 (0.100) data 0.000 (0.002) loss 1.0148 (1.2804) teacher_loss 0.5452 (0.8392) loss_zs_kd 0.1917 (0.1744) loss_oracle 0.3738 (0.3540) acc 87.5000 (78.4635) alaph_mean 0.7864 (0.7842) alpha_val 0.7864 (0.7842) lr 1.2487e-03 eta 0:13:15
epoch [23/50] batch [140/288] time 0.094 (0.102) data 0.000 (0.002) loss 1.2662 (1.2755) teacher_loss 0.8565 (0.8374) loss_zs_kd 0.1500 (0.1738) loss_oracle 0.3347 (0.3512) acc 84.3750 (78.5938) alaph_mean 0.7870 (0.7846) alpha_val 0.7870 (0.7846) lr 1.2487e-03 eta 0:13:25
epoch [23/50] batch [160/288] time 0.098 (0.101) data 0.000 (0.002) loss 1.2410 (1.2830) teacher_loss 0.8437 (0.8452) loss_zs_kd 0.1476 (0.1736) loss_oracle 0.3235 (0.3510) acc 78.1250 (78.3594) alaph_mean 0.7875 (0.7849) alpha_val 0.7875 (0.7849) lr 1.2487e-03 eta 0:13:18
epoch [23/50] batch [180/288] time 0.099 (0.101) data 0.000 (0.002) loss 1.1442 (1.2770) teacher_loss 0.7480 (0.8416) loss_zs_kd 0.1012 (0.1727) loss_oracle 0.3456 (0.3491) acc 81.2500 (78.5764) alaph_mean 0.7879 (0.7852) alpha_val 0.7879 (0.7852) lr 1.2487e-03 eta 0:13:12
epoch [23/50] batch [200/288] time 0.094 (0.100) data 0.000 (0.001) loss 1.1235 (1.2774) teacher_loss 0.7039 (0.8416) loss_zs_kd 0.1589 (0.1729) loss_oracle 0.3401 (0.3494) acc 78.1250 (78.7969) alaph_mean 0.7886 (0.7855) alpha_val 0.7886 (0.7855) lr 1.2487e-03 eta 0:13:07
epoch [23/50] batch [220/288] time 0.092 (0.100) data 0.000 (0.001) loss 1.5228 (1.2793) teacher_loss 0.9862 (0.8448) loss_zs_kd 0.1843 (0.1728) loss_oracle 0.4444 (0.3481) acc 71.8750 (78.5511) alaph_mean 0.7893 (0.7858) alpha_val 0.7893 (0.7858) lr 1.2487e-03 eta 0:13:01
epoch [23/50] batch [240/288] time 0.094 (0.099) data 0.000 (0.001) loss 1.1564 (1.2857) teacher_loss 0.7112 (0.8528) loss_zs_kd 0.1559 (0.1713) loss_oracle 0.3673 (0.3472) acc 78.1250 (78.3464) alaph_mean 0.7899 (0.7862) alpha_val 0.7899 (0.7862) lr 1.2487e-03 eta 0:12:57
epoch [23/50] batch [260/288] time 0.095 (0.099) data 0.000 (0.001) loss 1.1164 (1.2942) teacher_loss 0.6906 (0.8602) loss_zs_kd 0.1540 (0.1717) loss_oracle 0.3488 (0.3481) acc 84.3750 (78.0889) alaph_mean 0.7907 (0.7865) alpha_val 0.7907 (0.7865) lr 1.2487e-03 eta 0:12:52
epoch [23/50] batch [280/288] time 0.090 (0.098) data 0.000 (0.001) loss 1.2026 (1.2877) teacher_loss 0.7960 (0.8539) loss_zs_kd 0.1955 (0.1713) loss_oracle 0.3088 (0.3481) acc 78.1250 (78.2478) alaph_mean 0.7913 (0.7868) alpha_val 0.7913 (0.7868) lr 1.2487e-03 eta 0:12:46
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,413
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,030
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.2%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [24/50] batch [20/288] time 0.094 (0.112) data 0.000 (0.015) loss 1.2967 (1.2587) teacher_loss 0.9190 (0.8264) loss_zs_kd 0.1416 (0.1618) loss_oracle 0.3069 (0.3514) acc 75.0000 (76.8750) alaph_mean 0.7918 (0.7916) alpha_val 0.7918 (0.7916) lr 1.1874e-03 eta 0:14:24
epoch [24/50] batch [40/288] time 0.090 (0.103) data 0.000 (0.007) loss 1.4428 (1.2941) teacher_loss 0.9356 (0.8648) loss_zs_kd 0.1937 (0.1646) loss_oracle 0.4103 (0.3470) acc 78.1250 (76.7188) alaph_mean 0.7923 (0.7918) alpha_val 0.7923 (0.7918) lr 1.1874e-03 eta 0:13:14
epoch [24/50] batch [60/288] time 0.098 (0.100) data 0.000 (0.005) loss 1.2058 (1.3116) teacher_loss 0.8380 (0.8787) loss_zs_kd 0.1550 (0.1694) loss_oracle 0.2903 (0.3482) acc 84.3750 (77.1354) alaph_mean 0.7928 (0.7921) alpha_val 0.7928 (0.7921) lr 1.1874e-03 eta 0:12:51
epoch [24/50] batch [80/288] time 0.093 (0.099) data 0.000 (0.004) loss 1.2242 (1.2866) teacher_loss 0.7234 (0.8555) loss_zs_kd 0.2198 (0.1673) loss_oracle 0.3909 (0.3475) acc 78.1250 (77.4219) alaph_mean 0.7933 (0.7923) alpha_val 0.7933 (0.7923) lr 1.1874e-03 eta 0:12:41
epoch [24/50] batch [100/288] time 0.107 (0.099) data 0.000 (0.003) loss 1.1651 (1.2841) teacher_loss 0.7053 (0.8531) loss_zs_kd 0.1841 (0.1663) loss_oracle 0.3678 (0.3480) acc 84.3750 (77.3125) alaph_mean 0.7939 (0.7926) alpha_val 0.7939 (0.7926) lr 1.1874e-03 eta 0:12:38
epoch [24/50] batch [120/288] time 0.171 (0.100) data 0.001 (0.003) loss 1.2871 (1.2777) teacher_loss 0.8523 (0.8422) loss_zs_kd 0.1523 (0.1673) loss_oracle 0.3587 (0.3518) acc 75.0000 (77.6562) alaph_mean 0.7944 (0.7928) alpha_val 0.7944 (0.7928) lr 1.1874e-03 eta 0:12:44
epoch [24/50] batch [140/288] time 0.106 (0.100) data 0.000 (0.002) loss 1.4240 (1.2780) teacher_loss 0.9008 (0.8380) loss_zs_kd 0.1542 (0.1695) loss_oracle 0.4461 (0.3553) acc 78.1250 (77.7902) alaph_mean 0.7950 (0.7931) alpha_val 0.7950 (0.7931) lr 1.1874e-03 eta 0:12:47
epoch [24/50] batch [160/288] time 0.093 (0.100) data 0.000 (0.002) loss 1.4638 (1.2867) teacher_loss 1.0782 (0.8430) loss_zs_kd 0.1527 (0.1729) loss_oracle 0.3093 (0.3572) acc 65.6250 (77.7539) alaph_mean 0.7957 (0.7934) alpha_val 0.7957 (0.7934) lr 1.1874e-03 eta 0:12:44
epoch [24/50] batch [180/288] time 0.104 (0.100) data 0.000 (0.002) loss 1.2057 (1.2944) teacher_loss 0.8000 (0.8521) loss_zs_kd 0.1320 (0.1730) loss_oracle 0.3397 (0.3559) acc 78.1250 (77.5521) alaph_mean 0.7963 (0.7937) alpha_val 0.7963 (0.7937) lr 1.1874e-03 eta 0:12:39
epoch [24/50] batch [200/288] time 0.096 (0.100) data 0.000 (0.002) loss 1.2597 (1.2959) teacher_loss 0.8616 (0.8545) loss_zs_kd 0.1693 (0.1733) loss_oracle 0.3135 (0.3548) acc 75.0000 (77.4844) alaph_mean 0.7968 (0.7940) alpha_val 0.7968 (0.7940) lr 1.1874e-03 eta 0:12:35
epoch [24/50] batch [220/288] time 0.093 (0.100) data 0.000 (0.002) loss 1.1916 (1.2966) teacher_loss 0.7128 (0.8549) loss_zs_kd 0.1893 (0.1727) loss_oracle 0.3842 (0.3553) acc 81.2500 (77.6420) alaph_mean 0.7974 (0.7942) alpha_val 0.7974 (0.7942) lr 1.1874e-03 eta 0:12:32
epoch [24/50] batch [240/288] time 0.101 (0.099) data 0.000 (0.001) loss 1.1068 (1.2958) teacher_loss 0.6907 (0.8544) loss_zs_kd 0.2047 (0.1722) loss_oracle 0.3137 (0.3553) acc 84.3750 (77.6562) alaph_mean 0.7979 (0.7945) alpha_val 0.7979 (0.7945) lr 1.1874e-03 eta 0:12:28
epoch [24/50] batch [260/288] time 0.094 (0.099) data 0.000 (0.001) loss 0.9662 (1.2969) teacher_loss 0.5432 (0.8565) loss_zs_kd 0.1528 (0.1713) loss_oracle 0.3466 (0.3547) acc 87.5000 (77.5721) alaph_mean 0.7983 (0.7948) alpha_val 0.7983 (0.7948) lr 1.1874e-03 eta 0:12:24
epoch [24/50] batch [280/288] time 0.092 (0.099) data 0.000 (0.001) loss 1.3177 (1.2996) teacher_loss 0.9303 (0.8602) loss_zs_kd 0.1553 (0.1717) loss_oracle 0.3097 (0.3535) acc 75.0000 (77.5670) alaph_mean 0.7990 (0.7951) alpha_val 0.7990 (0.7951) lr 1.1874e-03 eta 0:12:19
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.9%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [25/50] batch [20/288] time 0.093 (0.120) data 0.000 (0.014) loss 1.6170 (1.3347) teacher_loss 1.1413 (0.8820) loss_zs_kd 0.1581 (0.1705) loss_oracle 0.3966 (0.3675) acc 65.6250 (77.3438) alaph_mean 0.7997 (0.7995) alpha_val 0.7997 (0.7995) lr 1.1253e-03 eta 0:14:54
epoch [25/50] batch [40/288] time 0.105 (0.110) data 0.000 (0.007) loss 1.0499 (1.3190) teacher_loss 0.6281 (0.8689) loss_zs_kd 0.1508 (0.1718) loss_oracle 0.3464 (0.3642) acc 87.5000 (78.4375) alaph_mean 0.8002 (0.7997) alpha_val 0.8002 (0.7997) lr 1.1253e-03 eta 0:13:36
epoch [25/50] batch [60/288] time 0.097 (0.105) data 0.000 (0.005) loss 1.7383 (1.3307) teacher_loss 1.2048 (0.8831) loss_zs_kd 0.2220 (0.1732) loss_oracle 0.4224 (0.3610) acc 65.6250 (77.8125) alaph_mean 0.8008 (0.8000) alpha_val 0.8008 (0.8000) lr 1.1253e-03 eta 0:13:01
epoch [25/50] batch [80/288] time 0.093 (0.102) data 0.000 (0.004) loss 1.4384 (1.3261) teacher_loss 0.9526 (0.8773) loss_zs_kd 0.2063 (0.1731) loss_oracle 0.3826 (0.3623) acc 81.2500 (77.8125) alaph_mean 0.8016 (0.8003) alpha_val 0.8016 (0.8003) lr 1.1253e-03 eta 0:12:38
epoch [25/50] batch [100/288] time 0.099 (0.101) data 0.000 (0.003) loss 1.1927 (1.3236) teacher_loss 0.6977 (0.8724) loss_zs_kd 0.1994 (0.1767) loss_oracle 0.3953 (0.3629) acc 81.2500 (78.0312) alaph_mean 0.8023 (0.8006) alpha_val 0.8023 (0.8006) lr 1.1253e-03 eta 0:12:27
epoch [25/50] batch [120/288] time 0.105 (0.104) data 0.000 (0.002) loss 1.5442 (1.3087) teacher_loss 1.1229 (0.8614) loss_zs_kd 0.1699 (0.1740) loss_oracle 0.3363 (0.3603) acc 71.8750 (78.2552) alaph_mean 0.8028 (0.8010) alpha_val 0.8028 (0.8010) lr 1.1253e-03 eta 0:12:45
epoch [25/50] batch [140/288] time 0.105 (0.104) data 0.000 (0.002) loss 0.7778 (1.3086) teacher_loss 0.3327 (0.8611) loss_zs_kd 0.1283 (0.1746) loss_oracle 0.3809 (0.3602) acc 93.7500 (78.2589) alaph_mean 0.8034 (0.8013) alpha_val 0.8034 (0.8013) lr 1.1253e-03 eta 0:12:41
epoch [25/50] batch [160/288] time 0.105 (0.103) data 0.000 (0.002) loss 1.1091 (1.2969) teacher_loss 0.6241 (0.8485) loss_zs_kd 0.1508 (0.1748) loss_oracle 0.4096 (0.3610) acc 87.5000 (78.4570) alaph_mean 0.8039 (0.8016) alpha_val 0.8039 (0.8016) lr 1.1253e-03 eta 0:12:38
epoch [25/50] batch [180/288] time 0.099 (0.103) data 0.000 (0.002) loss 1.3694 (1.3025) teacher_loss 0.8804 (0.8519) loss_zs_kd 0.1848 (0.1762) loss_oracle 0.3965 (0.3625) acc 87.5000 (78.4896) alaph_mean 0.8045 (0.8019) alpha_val 0.8045 (0.8019) lr 1.1253e-03 eta 0:12:34
epoch [25/50] batch [200/288] time 0.099 (0.103) data 0.000 (0.002) loss 1.0579 (1.2997) teacher_loss 0.4874 (0.8466) loss_zs_kd 0.2189 (0.1777) loss_oracle 0.4610 (0.3643) acc 87.5000 (78.6094) alaph_mean 0.8049 (0.8021) alpha_val 0.8049 (0.8021) lr 1.1253e-03 eta 0:12:31
epoch [25/50] batch [220/288] time 0.107 (0.103) data 0.000 (0.001) loss 1.2565 (1.2950) teacher_loss 0.7352 (0.8407) loss_zs_kd 0.2267 (0.1787) loss_oracle 0.4079 (0.3650) acc 84.3750 (78.7784) alaph_mean 0.8056 (0.8024) alpha_val 0.8056 (0.8024) lr 1.1253e-03 eta 0:12:29
epoch [25/50] batch [240/288] time 0.113 (0.103) data 0.000 (0.001) loss 1.7898 (1.2974) teacher_loss 1.2810 (0.8435) loss_zs_kd 0.1896 (0.1787) loss_oracle 0.4140 (0.3646) acc 65.6250 (78.6979) alaph_mean 0.8061 (0.8027) alpha_val 0.8061 (0.8027) lr 1.1253e-03 eta 0:12:26
epoch [25/50] batch [260/288] time 0.103 (0.103) data 0.000 (0.001) loss 1.1635 (1.2962) teacher_loss 0.7546 (0.8421) loss_zs_kd 0.1756 (0.1795) loss_oracle 0.3210 (0.3644) acc 84.3750 (78.7380) alaph_mean 0.8066 (0.8030) alpha_val 0.8066 (0.8030) lr 1.1253e-03 eta 0:12:23
epoch [25/50] batch [280/288] time 0.110 (0.103) data 0.000 (0.001) loss 1.3522 (1.2947) teacher_loss 0.9579 (0.8418) loss_zs_kd 0.1306 (0.1788) loss_oracle 0.3290 (0.3635) acc 75.0000 (78.7388) alaph_mean 0.8074 (0.8033) alpha_val 0.8074 (0.8033) lr 1.1253e-03 eta 0:12:22
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.8%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [26/50] batch [20/288] time 0.101 (0.121) data 0.000 (0.014) loss 1.1051 (1.1752) teacher_loss 0.6627 (0.7437) loss_zs_kd 0.1351 (0.1698) loss_oracle 0.3749 (0.3465) acc 75.0000 (82.1875) alaph_mean 0.8080 (0.8078) alpha_val 0.8080 (0.8078) lr 1.0628e-03 eta 0:14:30
epoch [26/50] batch [40/288] time 0.100 (0.112) data 0.000 (0.007) loss 1.3461 (1.2280) teacher_loss 0.9911 (0.7935) loss_zs_kd 0.1299 (0.1704) loss_oracle 0.2900 (0.3494) acc 71.8750 (79.4531) alaph_mean 0.8085 (0.8081) alpha_val 0.8085 (0.8081) lr 1.0628e-03 eta 0:13:25
epoch [26/50] batch [60/288] time 0.106 (0.110) data 0.000 (0.005) loss 1.3523 (1.2429) teacher_loss 0.9084 (0.8076) loss_zs_kd 0.1745 (0.1735) loss_oracle 0.3566 (0.3486) acc 71.8750 (79.8438) alaph_mean 0.8090 (0.8083) alpha_val 0.8090 (0.8083) lr 1.0628e-03 eta 0:13:01
epoch [26/50] batch [80/288] time 0.145 (0.112) data 0.001 (0.004) loss 1.6452 (1.2528) teacher_loss 1.1913 (0.8159) loss_zs_kd 0.1668 (0.1751) loss_oracle 0.3705 (0.3494) acc 75.0000 (79.5703) alaph_mean 0.8095 (0.8085) alpha_val 0.8095 (0.8085) lr 1.0628e-03 eta 0:13:14
epoch [26/50] batch [100/288] time 0.105 (0.110) data 0.000 (0.003) loss 1.0036 (1.2659) teacher_loss 0.5943 (0.8283) loss_zs_kd 0.1457 (0.1742) loss_oracle 0.3365 (0.3506) acc 90.6250 (79.2500) alaph_mean 0.8099 (0.8088) alpha_val 0.8099 (0.8088) lr 1.0628e-03 eta 0:13:03
epoch [26/50] batch [120/288] time 0.121 (0.110) data 0.000 (0.003) loss 1.3193 (1.2686) teacher_loss 0.8466 (0.8313) loss_zs_kd 0.2161 (0.1720) loss_oracle 0.3646 (0.3512) acc 71.8750 (79.0365) alaph_mean 0.8104 (0.8090) alpha_val 0.8104 (0.8090) lr 1.0628e-03 eta 0:12:55
epoch [26/50] batch [140/288] time 0.101 (0.109) data 0.000 (0.002) loss 1.3751 (1.2619) teacher_loss 0.9300 (0.8244) loss_zs_kd 0.1785 (0.1712) loss_oracle 0.3558 (0.3519) acc 81.2500 (79.2857) alaph_mean 0.8109 (0.8093) alpha_val 0.8109 (0.8093) lr 1.0628e-03 eta 0:12:51
epoch [26/50] batch [160/288] time 0.121 (0.109) data 0.000 (0.002) loss 1.4819 (1.2641) teacher_loss 0.9696 (0.8219) loss_zs_kd 0.2089 (0.1724) loss_oracle 0.4079 (0.3560) acc 78.1250 (79.4922) alaph_mean 0.8113 (0.8095) alpha_val 0.8113 (0.8095) lr 1.0628e-03 eta 0:12:48
epoch [26/50] batch [180/288] time 0.100 (0.109) data 0.000 (0.002) loss 1.1143 (1.2639) teacher_loss 0.7377 (0.8203) loss_zs_kd 0.1500 (0.1738) loss_oracle 0.3016 (0.3566) acc 81.2500 (79.3924) alaph_mean 0.8116 (0.8097) alpha_val 0.8116 (0.8097) lr 1.0628e-03 eta 0:12:45
epoch [26/50] batch [200/288] time 0.111 (0.108) data 0.000 (0.002) loss 1.1858 (1.2684) teacher_loss 0.7600 (0.8236) loss_zs_kd 0.1743 (0.1755) loss_oracle 0.3387 (0.3570) acc 87.5000 (79.2656) alaph_mean 0.8121 (0.8099) alpha_val 0.8121 (0.8099) lr 1.0628e-03 eta 0:12:39
epoch [26/50] batch [220/288] time 0.099 (0.108) data 0.000 (0.002) loss 0.6882 (1.2700) teacher_loss 0.2952 (0.8254) loss_zs_kd 0.1220 (0.1764) loss_oracle 0.3320 (0.3564) acc 93.7500 (79.1477) alaph_mean 0.8126 (0.8101) alpha_val 0.8126 (0.8101) lr 1.0628e-03 eta 0:12:33
epoch [26/50] batch [240/288] time 0.087 (0.107) data 0.000 (0.001) loss 1.3380 (1.2750) teacher_loss 0.8366 (0.8282) loss_zs_kd 0.2140 (0.1774) loss_oracle 0.3944 (0.3581) acc 81.2500 (78.9714) alaph_mean 0.8131 (0.8104) alpha_val 0.8131 (0.8104) lr 1.0628e-03 eta 0:12:26
epoch [26/50] batch [260/288] time 0.102 (0.107) data 0.000 (0.001) loss 1.2611 (1.2837) teacher_loss 0.7966 (0.8342) loss_zs_kd 0.2022 (0.1799) loss_oracle 0.3634 (0.3596) acc 78.1250 (78.7861) alaph_mean 0.8137 (0.8106) alpha_val 0.8137 (0.8106) lr 1.0628e-03 eta 0:12:20
epoch [26/50] batch [280/288] time 0.105 (0.107) data 0.000 (0.001) loss 1.3638 (1.2885) teacher_loss 0.9414 (0.8370) loss_zs_kd 0.2161 (0.1813) loss_oracle 0.3144 (0.3609) acc 65.6250 (78.7500) alaph_mean 0.8143 (0.8109) alpha_val 0.8143 (0.8109) lr 1.0628e-03 eta 0:12:17
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,404
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,030
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.4%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [27/50] batch [20/288] time 0.105 (0.117) data 0.000 (0.011) loss 1.0498 (1.2646) teacher_loss 0.5661 (0.7870) loss_zs_kd 0.1701 (0.1929) loss_oracle 0.3986 (0.3812) acc 84.3750 (79.8438) alaph_mean 0.8150 (0.8148) alpha_val 0.8150 (0.8148) lr 1.0000e-03 eta 0:13:28
epoch [27/50] batch [40/288] time 0.104 (0.110) data 0.000 (0.006) loss 0.7677 (1.2964) teacher_loss 0.3834 (0.8370) loss_zs_kd 0.1411 (0.1843) loss_oracle 0.3137 (0.3673) acc 93.7500 (79.0625) alaph_mean 0.8154 (0.8150) alpha_val 0.8154 (0.8150) lr 1.0000e-03 eta 0:12:37
epoch [27/50] batch [60/288] time 0.100 (0.113) data 0.000 (0.004) loss 1.6904 (1.2794) teacher_loss 1.1723 (0.8186) loss_zs_kd 0.2493 (0.1846) loss_oracle 0.3935 (0.3685) acc 65.6250 (79.4792) alaph_mean 0.8158 (0.8152) alpha_val 0.8158 (0.8152) lr 1.0000e-03 eta 0:12:54
epoch [27/50] batch [80/288] time 0.098 (0.109) data 0.000 (0.003) loss 1.0666 (1.2727) teacher_loss 0.6418 (0.8105) loss_zs_kd 0.2151 (0.1831) loss_oracle 0.3173 (0.3706) acc 90.6250 (79.2578) alaph_mean 0.8161 (0.8154) alpha_val 0.8161 (0.8154) lr 1.0000e-03 eta 0:12:27
epoch [27/50] batch [100/288] time 0.098 (0.107) data 0.000 (0.002) loss 1.1986 (1.2827) teacher_loss 0.6631 (0.8180) loss_zs_kd 0.2341 (0.1855) loss_oracle 0.4185 (0.3719) acc 84.3750 (79.4062) alaph_mean 0.8166 (0.8156) alpha_val 0.8166 (0.8156) lr 1.0000e-03 eta 0:12:07
epoch [27/50] batch [120/288] time 0.104 (0.106) data 0.000 (0.002) loss 1.1426 (1.2655) teacher_loss 0.6658 (0.8007) loss_zs_kd 0.1661 (0.1847) loss_oracle 0.3937 (0.3724) acc 84.3750 (80.0521) alaph_mean 0.8170 (0.8158) alpha_val 0.8170 (0.8158) lr 1.0000e-03 eta 0:12:00
epoch [27/50] batch [140/288] time 0.100 (0.106) data 0.000 (0.002) loss 1.3072 (1.2765) teacher_loss 0.8165 (0.8086) loss_zs_kd 0.2528 (0.1875) loss_oracle 0.3643 (0.3741) acc 81.2500 (79.8214) alaph_mean 0.8175 (0.8160) alpha_val 0.8175 (0.8160) lr 1.0000e-03 eta 0:12:00
epoch [27/50] batch [160/288] time 0.108 (0.106) data 0.000 (0.002) loss 1.2835 (1.2754) teacher_loss 0.7701 (0.8057) loss_zs_kd 0.2286 (0.1893) loss_oracle 0.3991 (0.3750) acc 81.2500 (79.9414) alaph_mean 0.8181 (0.8162) alpha_val 0.8181 (0.8162) lr 1.0000e-03 eta 0:11:54
epoch [27/50] batch [180/288] time 0.103 (0.106) data 0.000 (0.001) loss 1.0611 (1.2812) teacher_loss 0.5802 (0.8106) loss_zs_kd 0.1938 (0.1892) loss_oracle 0.3840 (0.3761) acc 84.3750 (79.7569) alaph_mean 0.8187 (0.8165) alpha_val 0.8187 (0.8165) lr 1.0000e-03 eta 0:11:51
epoch [27/50] batch [200/288] time 0.093 (0.105) data 0.000 (0.001) loss 1.2321 (1.2757) teacher_loss 0.8061 (0.8052) loss_zs_kd 0.1327 (0.1874) loss_oracle 0.3596 (0.3768) acc 78.1250 (79.8750) alaph_mean 0.8192 (0.8167) alpha_val 0.8192 (0.8167) lr 1.0000e-03 eta 0:11:47
epoch [27/50] batch [220/288] time 0.094 (0.104) data 0.000 (0.001) loss 1.3026 (1.2742) teacher_loss 0.7232 (0.8054) loss_zs_kd 0.3056 (0.1865) loss_oracle 0.4266 (0.3755) acc 81.2500 (79.7869) alaph_mean 0.8196 (0.8170) alpha_val 0.8196 (0.8170) lr 1.0000e-03 eta 0:11:39
epoch [27/50] batch [240/288] time 0.094 (0.104) data 0.000 (0.001) loss 0.9502 (1.2849) teacher_loss 0.5344 (0.8171) loss_zs_kd 0.1301 (0.1865) loss_oracle 0.3508 (0.3745) acc 87.5000 (79.3359) alaph_mean 0.8201 (0.8172) alpha_val 0.8201 (0.8172) lr 1.0000e-03 eta 0:11:33
epoch [27/50] batch [260/288] time 0.103 (0.103) data 0.000 (0.001) loss 1.0575 (1.2880) teacher_loss 0.6428 (0.8192) loss_zs_kd 0.1708 (0.1871) loss_oracle 0.3292 (0.3752) acc 81.2500 (79.2909) alaph_mean 0.8205 (0.8174) alpha_val 0.8205 (0.8174) lr 1.0000e-03 eta 0:11:26
epoch [27/50] batch [280/288] time 0.086 (0.102) data 0.000 (0.001) loss 1.6355 (1.2904) teacher_loss 1.0948 (0.8218) loss_zs_kd 0.2483 (0.1874) loss_oracle 0.4166 (0.3749) acc 68.7500 (79.1741) alaph_mean 0.8210 (0.8177) alpha_val 0.8210 (0.8177) lr 1.0000e-03 eta 0:11:19
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,404
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,011
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.4%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [28/50] batch [20/288] time 0.105 (0.116) data 0.000 (0.014) loss 1.0315 (1.2856) teacher_loss 0.5427 (0.8040) loss_zs_kd 0.1587 (0.2015) loss_oracle 0.4095 (0.3809) acc 84.3750 (80.3125) alaph_mean 0.8217 (0.8215) alpha_val 0.8217 (0.8215) lr 9.3721e-04 eta 0:12:46
epoch [28/50] batch [40/288] time 0.097 (0.115) data 0.000 (0.007) loss 0.9522 (1.3060) teacher_loss 0.4276 (0.8320) loss_zs_kd 0.1803 (0.1869) loss_oracle 0.4345 (0.3805) acc 90.6250 (79.1406) alaph_mean 0.8222 (0.8217) alpha_val 0.8222 (0.8217) lr 9.3721e-04 eta 0:12:37
epoch [28/50] batch [60/288] time 0.104 (0.111) data 0.001 (0.005) loss 1.1385 (1.3140) teacher_loss 0.6485 (0.8380) loss_zs_kd 0.2219 (0.1885) loss_oracle 0.3791 (0.3817) acc 81.2500 (78.5938) alaph_mean 0.8226 (0.8219) alpha_val 0.8226 (0.8219) lr 9.3721e-04 eta 0:12:06
epoch [28/50] batch [80/288] time 0.111 (0.109) data 0.000 (0.004) loss 1.1835 (1.3262) teacher_loss 0.7076 (0.8495) loss_zs_kd 0.1819 (0.1898) loss_oracle 0.3850 (0.3818) acc 78.1250 (78.2031) alaph_mean 0.8230 (0.8222) alpha_val 0.8230 (0.8222) lr 9.3721e-04 eta 0:11:51
epoch [28/50] batch [100/288] time 0.093 (0.107) data 0.000 (0.003) loss 1.1669 (1.3152) teacher_loss 0.7764 (0.8401) loss_zs_kd 0.1236 (0.1902) loss_oracle 0.3287 (0.3801) acc 81.2500 (78.5938) alaph_mean 0.8235 (0.8224) alpha_val 0.8235 (0.8224) lr 9.3721e-04 eta 0:11:38
epoch [28/50] batch [120/288] time 0.107 (0.106) data 0.000 (0.003) loss 1.1424 (1.2986) teacher_loss 0.7430 (0.8287) loss_zs_kd 0.1473 (0.1868) loss_oracle 0.3257 (0.3765) acc 81.2500 (79.0104) alaph_mean 0.8239 (0.8226) alpha_val 0.8239 (0.8226) lr 9.3721e-04 eta 0:11:26
epoch [28/50] batch [140/288] time 0.102 (0.104) data 0.000 (0.002) loss 0.8094 (1.2911) teacher_loss 0.3582 (0.8236) loss_zs_kd 0.1455 (0.1869) loss_oracle 0.3785 (0.3740) acc 93.7500 (79.1964) alaph_mean 0.8243 (0.8228) alpha_val 0.8243 (0.8228) lr 9.3721e-04 eta 0:11:16
epoch [28/50] batch [160/288] time 0.097 (0.104) data 0.000 (0.002) loss 1.5170 (1.2811) teacher_loss 1.0501 (0.8150) loss_zs_kd 0.1770 (0.1842) loss_oracle 0.3783 (0.3740) acc 65.6250 (79.3750) alaph_mean 0.8247 (0.8230) alpha_val 0.8247 (0.8230) lr 9.3721e-04 eta 0:11:09
epoch [28/50] batch [180/288] time 0.096 (0.103) data 0.000 (0.002) loss 1.1592 (1.2772) teacher_loss 0.6635 (0.8106) loss_zs_kd 0.2335 (0.1841) loss_oracle 0.3790 (0.3745) acc 87.5000 (79.4792) alaph_mean 0.8252 (0.8232) alpha_val 0.8252 (0.8232) lr 9.3721e-04 eta 0:11:02
epoch [28/50] batch [200/288] time 0.098 (0.102) data 0.000 (0.002) loss 1.6252 (1.2851) teacher_loss 1.1630 (0.8183) loss_zs_kd 0.2151 (0.1852) loss_oracle 0.3547 (0.3742) acc 71.8750 (79.3438) alaph_mean 0.8256 (0.8234) alpha_val 0.8256 (0.8234) lr 9.3721e-04 eta 0:10:56
epoch [28/50] batch [220/288] time 0.104 (0.102) data 0.001 (0.001) loss 1.3552 (1.2858) teacher_loss 0.8792 (0.8186) loss_zs_kd 0.1601 (0.1852) loss_oracle 0.3959 (0.3746) acc 84.3750 (79.3750) alaph_mean 0.8260 (0.8237) alpha_val 0.8260 (0.8237) lr 9.3721e-04 eta 0:10:50
epoch [28/50] batch [240/288] time 0.097 (0.101) data 0.000 (0.001) loss 0.7326 (1.2848) teacher_loss 0.3358 (0.8192) loss_zs_kd 0.1458 (0.1848) loss_oracle 0.3239 (0.3732) acc 100.0000 (79.3490) alaph_mean 0.8263 (0.8239) alpha_val 0.8263 (0.8239) lr 9.3721e-04 eta 0:10:45
epoch [28/50] batch [260/288] time 0.102 (0.101) data 0.000 (0.001) loss 1.0358 (1.2882) teacher_loss 0.6140 (0.8222) loss_zs_kd 0.1618 (0.1847) loss_oracle 0.3409 (0.3736) acc 84.3750 (79.3870) alaph_mean 0.8267 (0.8241) alpha_val 0.8267 (0.8241) lr 9.3721e-04 eta 0:10:40
epoch [28/50] batch [280/288] time 0.089 (0.100) data 0.000 (0.001) loss 1.1426 (1.2869) teacher_loss 0.7069 (0.8214) loss_zs_kd 0.1909 (0.1850) loss_oracle 0.3402 (0.3729) acc 84.3750 (79.3304) alaph_mean 0.8271 (0.8243) alpha_val 0.8271 (0.8243) lr 9.3721e-04 eta 0:10:35
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,402
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 80.0%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [29/50] batch [20/288] time 0.100 (0.131) data 0.000 (0.014) loss 1.0227 (1.1847) teacher_loss 0.5456 (0.7151) loss_zs_kd 0.2242 (0.1867) loss_oracle 0.3650 (0.3763) acc 84.3750 (82.3438) alaph_mean 0.8275 (0.8274) alpha_val 0.8275 (0.8274) lr 8.7467e-04 eta 0:13:50
epoch [29/50] batch [40/288] time 0.112 (0.119) data 0.000 (0.007) loss 1.2042 (1.2797) teacher_loss 0.7700 (0.8109) loss_zs_kd 0.1505 (0.1819) loss_oracle 0.3589 (0.3779) acc 78.1250 (79.5312) alaph_mean 0.8278 (0.8275) alpha_val 0.8278 (0.8275) lr 8.7467e-04 eta 0:12:28
epoch [29/50] batch [60/288] time 0.097 (0.112) data 0.000 (0.005) loss 2.0896 (1.3133) teacher_loss 1.5791 (0.8494) loss_zs_kd 0.1871 (0.1775) loss_oracle 0.4170 (0.3752) acc 59.3750 (78.4375) alaph_mean 0.8281 (0.8276) alpha_val 0.8281 (0.8276) lr 8.7467e-04 eta 0:11:43
epoch [29/50] batch [80/288] time 0.094 (0.108) data 0.000 (0.004) loss 1.0541 (1.3144) teacher_loss 0.5695 (0.8441) loss_zs_kd 0.2198 (0.1839) loss_oracle 0.3747 (0.3784) acc 87.5000 (78.5156) alaph_mean 0.8285 (0.8278) alpha_val 0.8285 (0.8278) lr 8.7467e-04 eta 0:11:16
epoch [29/50] batch [100/288] time 0.098 (0.106) data 0.000 (0.003) loss 1.7860 (1.3295) teacher_loss 1.2420 (0.8571) loss_zs_kd 0.2070 (0.1861) loss_oracle 0.4405 (0.3793) acc 71.8750 (78.3125) alaph_mean 0.8289 (0.8280) alpha_val 0.8289 (0.8280) lr 8.7467e-04 eta 0:11:00
epoch [29/50] batch [120/288] time 0.097 (0.104) data 0.000 (0.003) loss 1.2445 (1.3180) teacher_loss 0.7917 (0.8444) loss_zs_kd 0.1818 (0.1856) loss_oracle 0.3619 (0.3808) acc 84.3750 (78.7500) alaph_mean 0.8292 (0.8282) alpha_val 0.8292 (0.8282) lr 8.7467e-04 eta 0:10:46
epoch [29/50] batch [140/288] time 0.096 (0.103) data 0.000 (0.002) loss 1.2987 (1.3076) teacher_loss 0.9035 (0.8333) loss_zs_kd 0.1298 (0.1840) loss_oracle 0.3303 (0.3823) acc 65.6250 (78.7500) alaph_mean 0.8295 (0.8283) alpha_val 0.8295 (0.8283) lr 8.7467e-04 eta 0:10:37
epoch [29/50] batch [160/288] time 0.093 (0.102) data 0.000 (0.002) loss 1.2288 (1.3064) teacher_loss 0.6769 (0.8311) loss_zs_kd 0.2234 (0.1850) loss_oracle 0.4402 (0.3828) acc 84.3750 (78.9062) alaph_mean 0.8298 (0.8285) alpha_val 0.8298 (0.8285) lr 8.7467e-04 eta 0:10:29
epoch [29/50] batch [180/288] time 0.094 (0.101) data 0.000 (0.002) loss 1.5143 (1.3133) teacher_loss 1.0241 (0.8366) loss_zs_kd 0.2104 (0.1861) loss_oracle 0.3850 (0.3837) acc 68.7500 (78.6806) alaph_mean 0.8302 (0.8287) alpha_val 0.8302 (0.8287) lr 8.7467e-04 eta 0:10:23
epoch [29/50] batch [200/288] time 0.093 (0.101) data 0.000 (0.002) loss 1.4185 (1.3076) teacher_loss 0.8905 (0.8308) loss_zs_kd 0.1886 (0.1865) loss_oracle 0.4338 (0.3836) acc 81.2500 (78.9375) alaph_mean 0.8306 (0.8288) alpha_val 0.8306 (0.8288) lr 8.7467e-04 eta 0:10:18
epoch [29/50] batch [220/288] time 0.099 (0.101) data 0.001 (0.001) loss 1.3627 (1.3109) teacher_loss 0.8477 (0.8343) loss_zs_kd 0.1872 (0.1867) loss_oracle 0.4214 (0.3833) acc 81.2500 (78.9915) alaph_mean 0.8309 (0.8290) alpha_val 0.8309 (0.8290) lr 8.7467e-04 eta 0:10:14
epoch [29/50] batch [240/288] time 0.099 (0.100) data 0.000 (0.001) loss 1.2226 (1.3128) teacher_loss 0.7323 (0.8347) loss_zs_kd 0.1485 (0.1872) loss_oracle 0.4161 (0.3845) acc 84.3750 (79.0104) alaph_mean 0.8313 (0.8292) alpha_val 0.8313 (0.8292) lr 8.7467e-04 eta 0:10:10
epoch [29/50] batch [260/288] time 0.095 (0.100) data 0.000 (0.001) loss 1.1687 (1.3132) teacher_loss 0.6213 (0.8342) loss_zs_kd 0.2433 (0.1880) loss_oracle 0.4258 (0.3850) acc 75.0000 (79.0144) alaph_mean 0.8317 (0.8294) alpha_val 0.8317 (0.8294) lr 8.7467e-04 eta 0:10:06
epoch [29/50] batch [280/288] time 0.090 (0.099) data 0.000 (0.001) loss 1.7861 (1.3145) teacher_loss 1.2737 (0.8351) loss_zs_kd 0.2358 (0.1890) loss_oracle 0.3944 (0.3849) acc 65.6250 (79.0179) alaph_mean 0.8320 (0.8296) alpha_val 0.8320 (0.8296) lr 8.7467e-04 eta 0:10:01
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,398
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,029
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.2%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [30/50] batch [20/288] time 0.113 (0.134) data 0.000 (0.018) loss 1.1900 (1.2729) teacher_loss 0.7046 (0.7874) loss_zs_kd 0.1687 (0.1876) loss_oracle 0.4011 (0.3917) acc 78.1250 (79.0625) alaph_mean 0.8325 (0.8323) alpha_val 0.8325 (0.8323) lr 8.1262e-04 eta 0:13:29
epoch [30/50] batch [40/288] time 0.106 (0.120) data 0.000 (0.009) loss 1.2930 (1.2207) teacher_loss 0.7894 (0.7390) loss_zs_kd 0.1594 (0.1885) loss_oracle 0.4240 (0.3874) acc 75.0000 (80.2344) alaph_mean 0.8327 (0.8324) alpha_val 0.8327 (0.8324) lr 8.1262e-04 eta 0:12:00
epoch [30/50] batch [60/288] time 0.103 (0.113) data 0.001 (0.006) loss 1.5805 (1.2544) teacher_loss 1.0548 (0.7742) loss_zs_kd 0.2359 (0.1925) loss_oracle 0.4077 (0.3839) acc 71.8750 (79.6875) alaph_mean 0.8330 (0.8326) alpha_val 0.8330 (0.8326) lr 8.1262e-04 eta 0:11:18
epoch [30/50] batch [80/288] time 0.109 (0.110) data 0.000 (0.005) loss 1.1684 (1.2737) teacher_loss 0.7971 (0.7968) loss_zs_kd 0.1659 (0.1934) loss_oracle 0.2884 (0.3802) acc 81.2500 (79.6484) alaph_mean 0.8334 (0.8327) alpha_val 0.8334 (0.8327) lr 8.1262e-04 eta 0:10:59
epoch [30/50] batch [100/288] time 0.095 (0.108) data 0.000 (0.004) loss 0.9812 (1.2907) teacher_loss 0.5482 (0.8165) loss_zs_kd 0.1438 (0.1910) loss_oracle 0.3611 (0.3787) acc 90.6250 (79.2500) alaph_mean 0.8337 (0.8329) alpha_val 0.8337 (0.8329) lr 8.1262e-04 eta 0:10:43
epoch [30/50] batch [120/288] time 0.095 (0.107) data 0.000 (0.003) loss 1.2453 (1.2968) teacher_loss 0.8098 (0.8251) loss_zs_kd 0.1635 (0.1905) loss_oracle 0.3538 (0.3764) acc 81.2500 (79.0885) alaph_mean 0.8340 (0.8331) alpha_val 0.8340 (0.8331) lr 8.1262e-04 eta 0:10:34
epoch [30/50] batch [140/288] time 0.106 (0.106) data 0.000 (0.003) loss 1.4598 (1.3117) teacher_loss 0.9192 (0.8364) loss_zs_kd 0.1395 (0.1916) loss_oracle 0.4709 (0.3795) acc 71.8750 (78.9955) alaph_mean 0.8344 (0.8332) alpha_val 0.8344 (0.8332) lr 8.1262e-04 eta 0:10:26
epoch [30/50] batch [160/288] time 0.093 (0.105) data 0.000 (0.003) loss 1.0392 (1.3071) teacher_loss 0.5450 (0.8311) loss_zs_kd 0.1834 (0.1912) loss_oracle 0.4025 (0.3805) acc 84.3750 (79.0234) alaph_mean 0.8347 (0.8334) alpha_val 0.8347 (0.8334) lr 8.1262e-04 eta 0:10:19
epoch [30/50] batch [180/288] time 0.106 (0.105) data 0.000 (0.002) loss 1.4834 (1.3156) teacher_loss 1.0257 (0.8389) loss_zs_kd 0.2005 (0.1911) loss_oracle 0.3575 (0.3812) acc 71.8750 (78.8368) alaph_mean 0.8351 (0.8336) alpha_val 0.8351 (0.8336) lr 8.1262e-04 eta 0:10:14
epoch [30/50] batch [200/288] time 0.098 (0.104) data 0.000 (0.002) loss 1.4879 (1.3110) teacher_loss 1.0350 (0.8356) loss_zs_kd 0.1887 (0.1909) loss_oracle 0.3585 (0.3800) acc 75.0000 (78.8438) alaph_mean 0.8355 (0.8337) alpha_val 0.8355 (0.8337) lr 8.1262e-04 eta 0:10:09
epoch [30/50] batch [220/288] time 0.096 (0.104) data 0.000 (0.002) loss 1.4174 (1.3125) teacher_loss 0.9746 (0.8374) loss_zs_kd 0.2035 (0.1912) loss_oracle 0.3410 (0.3795) acc 78.1250 (78.7642) alaph_mean 0.8358 (0.8339) alpha_val 0.8358 (0.8339) lr 8.1262e-04 eta 0:10:04
epoch [30/50] batch [240/288] time 0.096 (0.103) data 0.000 (0.002) loss 1.0153 (1.3161) teacher_loss 0.5985 (0.8405) loss_zs_kd 0.1648 (0.1903) loss_oracle 0.3344 (0.3805) acc 84.3750 (78.6589) alaph_mean 0.8361 (0.8341) alpha_val 0.8361 (0.8341) lr 8.1262e-04 eta 0:10:01
epoch [30/50] batch [260/288] time 0.096 (0.103) data 0.000 (0.002) loss 1.0702 (1.3235) teacher_loss 0.6693 (0.8471) loss_zs_kd 0.1687 (0.1906) loss_oracle 0.3166 (0.3812) acc 84.3750 (78.4736) alaph_mean 0.8365 (0.8343) alpha_val 0.8365 (0.8343) lr 8.1262e-04 eta 0:09:57
epoch [30/50] batch [280/288] time 0.087 (0.103) data 0.000 (0.002) loss 0.9893 (1.3152) teacher_loss 0.5882 (0.8381) loss_zs_kd 0.1554 (0.1900) loss_oracle 0.3234 (0.3821) acc 87.5000 (78.7723) alaph_mean 0.8368 (0.8344) alpha_val 0.8368 (0.8344) lr 8.1262e-04 eta 0:09:51
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,413
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 86.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,031
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 80.3%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.8%, epoch: 4 *******
epoch [31/50] batch [20/288] time 0.104 (0.127) data 0.000 (0.017) loss 0.7539 (1.2659) teacher_loss 0.3203 (0.7697) loss_zs_kd 0.1296 (0.2024) loss_oracle 0.3688 (0.3949) acc 90.6250 (81.0938) alaph_mean 0.8373 (0.8371) alpha_val 0.8373 (0.8371) lr 7.5131e-04 eta 0:12:08
epoch [31/50] batch [40/288] time 0.105 (0.118) data 0.000 (0.009) loss 1.0208 (1.3052) teacher_loss 0.5682 (0.8183) loss_zs_kd 0.1673 (0.1940) loss_oracle 0.3690 (0.3899) acc 87.5000 (79.6094) alaph_mean 0.8376 (0.8373) alpha_val 0.8376 (0.8373) lr 7.5131e-04 eta 0:11:16
epoch [31/50] batch [60/288] time 0.111 (0.116) data 0.000 (0.006) loss 1.2695 (1.3122) teacher_loss 0.7370 (0.8235) loss_zs_kd 0.2235 (0.1922) loss_oracle 0.4208 (0.3926) acc 90.6250 (79.7396) alaph_mean 0.8379 (0.8375) alpha_val 0.8379 (0.8375) lr 7.5131e-04 eta 0:11:00
epoch [31/50] batch [80/288] time 0.120 (0.115) data 0.001 (0.004) loss 1.1583 (1.2966) teacher_loss 0.6830 (0.8042) loss_zs_kd 0.1762 (0.1956) loss_oracle 0.3872 (0.3947) acc 81.2500 (79.6875) alaph_mean 0.8382 (0.8376) alpha_val 0.8382 (0.8376) lr 7.5131e-04 eta 0:10:51
epoch [31/50] batch [100/288] time 0.102 (0.113) data 0.000 (0.004) loss 1.4180 (1.2860) teacher_loss 0.9225 (0.7937) loss_zs_kd 0.2408 (0.1947) loss_oracle 0.3751 (0.3950) acc 78.1250 (80.1562) alaph_mean 0.8386 (0.8378) alpha_val 0.8386 (0.8378) lr 7.5131e-04 eta 0:10:39
epoch [31/50] batch [120/288] time 0.094 (0.111) data 0.000 (0.003) loss 0.8902 (1.2945) teacher_loss 0.4240 (0.8013) loss_zs_kd 0.1685 (0.1965) loss_oracle 0.3819 (0.3949) acc 93.7500 (80.0260) alaph_mean 0.8389 (0.8379) alpha_val 0.8389 (0.8379) lr 7.5131e-04 eta 0:10:25
epoch [31/50] batch [140/288] time 0.096 (0.110) data 0.000 (0.003) loss 1.0557 (1.2932) teacher_loss 0.4803 (0.8020) loss_zs_kd 0.2310 (0.1948) loss_oracle 0.4599 (0.3938) acc 87.5000 (80.3125) alaph_mean 0.8391 (0.8381) alpha_val 0.8391 (0.8381) lr 7.5131e-04 eta 0:10:18
epoch [31/50] batch [160/288] time 0.096 (0.109) data 0.000 (0.002) loss 1.0925 (1.2962) teacher_loss 0.6712 (0.8063) loss_zs_kd 0.1911 (0.1944) loss_oracle 0.3258 (0.3926) acc 84.3750 (80.1172) alaph_mean 0.8394 (0.8382) alpha_val 0.8394 (0.8382) lr 7.5131e-04 eta 0:10:10
epoch [31/50] batch [180/288] time 0.102 (0.109) data 0.000 (0.002) loss 1.5488 (1.2986) teacher_loss 1.1420 (0.8102) loss_zs_kd 0.2061 (0.1932) loss_oracle 0.3038 (0.3919) acc 71.8750 (79.8438) alaph_mean 0.8396 (0.8384) alpha_val 0.8396 (0.8384) lr 7.5131e-04 eta 0:10:06
epoch [31/50] batch [200/288] time 0.111 (0.108) data 0.000 (0.002) loss 1.3025 (1.2984) teacher_loss 0.7221 (0.8088) loss_zs_kd 0.1953 (0.1936) loss_oracle 0.4828 (0.3928) acc 87.5000 (79.8906) alaph_mean 0.8398 (0.8385) alpha_val 0.8398 (0.8385) lr 7.5131e-04 eta 0:10:00
epoch [31/50] batch [220/288] time 0.104 (0.107) data 0.000 (0.002) loss 1.3651 (1.3042) teacher_loss 0.8337 (0.8131) loss_zs_kd 0.2170 (0.1939) loss_oracle 0.4229 (0.3941) acc 78.1250 (79.9858) alaph_mean 0.8401 (0.8386) alpha_val 0.8401 (0.8386) lr 7.5131e-04 eta 0:09:55
epoch [31/50] batch [240/288] time 0.101 (0.107) data 0.000 (0.002) loss 1.3664 (1.3046) teacher_loss 0.8717 (0.8130) loss_zs_kd 0.1721 (0.1939) loss_oracle 0.4086 (0.3947) acc 78.1250 (79.8307) alaph_mean 0.8404 (0.8388) alpha_val 0.8404 (0.8388) lr 7.5131e-04 eta 0:09:50
epoch [31/50] batch [260/288] time 0.100 (0.107) data 0.000 (0.002) loss 0.9413 (1.3066) teacher_loss 0.5103 (0.8164) loss_zs_kd 0.1297 (0.1926) loss_oracle 0.3661 (0.3939) acc 93.7500 (79.7356) alaph_mean 0.8407 (0.8389) alpha_val 0.8407 (0.8389) lr 7.5131e-04 eta 0:09:47
epoch [31/50] batch [280/288] time 0.142 (0.107) data 0.000 (0.002) loss 1.1641 (1.2976) teacher_loss 0.7370 (0.8082) loss_zs_kd 0.1342 (0.1914) loss_oracle 0.3600 (0.3937) acc 87.5000 (80.0781) alaph_mean 0.8409 (0.8390) alpha_val 0.8409 (0.8390) lr 7.5131e-04 eta 0:09:46
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,409
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,036
* accuracy: 83.9%
* error: 16.1%
* macro_f1: 80.4%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [32/50] batch [20/288] time 0.111 (0.119) data 0.001 (0.013) loss 1.2923 (1.3043) teacher_loss 0.9172 (0.8209) loss_zs_kd 0.1323 (0.1865) loss_oracle 0.3090 (0.3902) acc 78.1250 (80.1562) alaph_mean 0.8413 (0.8411) alpha_val 0.8413 (0.8411) lr 6.9098e-04 eta 0:10:47
epoch [32/50] batch [40/288] time 0.101 (0.111) data 0.000 (0.007) loss 0.8073 (1.3132) teacher_loss 0.4230 (0.8267) loss_zs_kd 0.1382 (0.1943) loss_oracle 0.3152 (0.3894) acc 90.6250 (79.2969) alaph_mean 0.8416 (0.8413) alpha_val 0.8416 (0.8413) lr 6.9098e-04 eta 0:10:04
epoch [32/50] batch [60/288] time 0.092 (0.106) data 0.000 (0.005) loss 1.1560 (1.3313) teacher_loss 0.6483 (0.8431) loss_zs_kd 0.2357 (0.1978) loss_oracle 0.3898 (0.3893) acc 81.2500 (78.6979) alaph_mean 0.8419 (0.8414) alpha_val 0.8419 (0.8414) lr 6.9098e-04 eta 0:09:35
epoch [32/50] batch [80/288] time 0.094 (0.104) data 0.000 (0.003) loss 1.6413 (1.3062) teacher_loss 1.1867 (0.8216) loss_zs_kd 0.1841 (0.1967) loss_oracle 0.3625 (0.3862) acc 68.7500 (79.4531) alaph_mean 0.8422 (0.8416) alpha_val 0.8422 (0.8416) lr 6.9098e-04 eta 0:09:22
epoch [32/50] batch [100/288] time 0.101 (0.103) data 0.000 (0.003) loss 0.8776 (1.3066) teacher_loss 0.4038 (0.8228) loss_zs_kd 0.1560 (0.1923) loss_oracle 0.3957 (0.3876) acc 90.6250 (79.4375) alaph_mean 0.8424 (0.8417) alpha_val 0.8424 (0.8417) lr 6.9098e-04 eta 0:09:12
epoch [32/50] batch [120/288] time 0.102 (0.102) data 0.000 (0.002) loss 0.9512 (1.3031) teacher_loss 0.5205 (0.8181) loss_zs_kd 0.1535 (0.1934) loss_oracle 0.3539 (0.3883) acc 84.3750 (79.4531) alaph_mean 0.8427 (0.8419) alpha_val 0.8427 (0.8419) lr 6.9098e-04 eta 0:09:05
epoch [32/50] batch [140/288] time 0.104 (0.102) data 0.000 (0.002) loss 1.3568 (1.3022) teacher_loss 0.8066 (0.8188) loss_zs_kd 0.2177 (0.1919) loss_oracle 0.4413 (0.3875) acc 84.3750 (79.3973) alaph_mean 0.8430 (0.8420) alpha_val 0.8430 (0.8420) lr 6.9098e-04 eta 0:09:01
epoch [32/50] batch [160/288] time 0.099 (0.101) data 0.000 (0.002) loss 1.0625 (1.2994) teacher_loss 0.5796 (0.8167) loss_zs_kd 0.2029 (0.1908) loss_oracle 0.3815 (0.3873) acc 87.5000 (79.4531) alaph_mean 0.8433 (0.8422) alpha_val 0.8433 (0.8422) lr 6.9098e-04 eta 0:08:55
epoch [32/50] batch [180/288] time 0.102 (0.101) data 0.000 (0.002) loss 1.2887 (1.3095) teacher_loss 0.7630 (0.8257) loss_zs_kd 0.2239 (0.1918) loss_oracle 0.4138 (0.3879) acc 81.2500 (79.1840) alaph_mean 0.8435 (0.8423) alpha_val 0.8435 (0.8423) lr 6.9098e-04 eta 0:08:53
epoch [32/50] batch [200/288] time 0.097 (0.101) data 0.001 (0.002) loss 1.2520 (1.3061) teacher_loss 0.7812 (0.8227) loss_zs_kd 0.2239 (0.1921) loss_oracle 0.3588 (0.3874) acc 81.2500 (79.3125) alaph_mean 0.8437 (0.8424) alpha_val 0.8437 (0.8424) lr 6.9098e-04 eta 0:08:49
epoch [32/50] batch [220/288] time 0.094 (0.100) data 0.000 (0.001) loss 1.1058 (1.3091) teacher_loss 0.6558 (0.8242) loss_zs_kd 0.1358 (0.1918) loss_oracle 0.3821 (0.3890) acc 84.3750 (79.2756) alaph_mean 0.8439 (0.8425) alpha_val 0.8439 (0.8425) lr 6.9098e-04 eta 0:08:46
epoch [32/50] batch [240/288] time 0.095 (0.100) data 0.000 (0.001) loss 1.3044 (1.3022) teacher_loss 0.8216 (0.8163) loss_zs_kd 0.1551 (0.1913) loss_oracle 0.4052 (0.3902) acc 84.3750 (79.5312) alaph_mean 0.8441 (0.8427) alpha_val 0.8441 (0.8427) lr 6.9098e-04 eta 0:08:43
epoch [32/50] batch [260/288] time 0.109 (0.101) data 0.000 (0.001) loss 1.3396 (1.3085) teacher_loss 0.8680 (0.8222) loss_zs_kd 0.1524 (0.1915) loss_oracle 0.3955 (0.3906) acc 84.3750 (79.3990) alaph_mean 0.8443 (0.8428) alpha_val 0.8443 (0.8428) lr 6.9098e-04 eta 0:08:46
epoch [32/50] batch [280/288] time 0.110 (0.101) data 0.000 (0.001) loss 1.0840 (1.3072) teacher_loss 0.5319 (0.8208) loss_zs_kd 0.1641 (0.1915) loss_oracle 0.4701 (0.3907) acc 81.2500 (79.4085) alaph_mean 0.8445 (0.8429) alpha_val 0.8445 (0.8429) lr 6.9098e-04 eta 0:08:46
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,408
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,026
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.1%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [33/50] batch [20/288] time 0.094 (0.121) data 0.000 (0.018) loss 1.4174 (1.3649) teacher_loss 0.9467 (0.8692) loss_zs_kd 0.1752 (0.2005) loss_oracle 0.3831 (0.3955) acc 81.2500 (79.2188) alaph_mean 0.8449 (0.8448) alpha_val 0.8449 (0.8448) lr 6.3188e-04 eta 0:10:22
epoch [33/50] batch [40/288] time 0.101 (0.110) data 0.000 (0.009) loss 1.3983 (1.3711) teacher_loss 0.9890 (0.8738) loss_zs_kd 0.1740 (0.1995) loss_oracle 0.3223 (0.3976) acc 81.2500 (77.8906) alaph_mean 0.8452 (0.8449) alpha_val 0.8452 (0.8449) lr 6.3188e-04 eta 0:09:25
epoch [33/50] batch [60/288] time 0.101 (0.107) data 0.000 (0.006) loss 1.2801 (1.3455) teacher_loss 0.7777 (0.8486) loss_zs_kd 0.2522 (0.1989) loss_oracle 0.3763 (0.3975) acc 84.3750 (78.6458) alaph_mean 0.8455 (0.8451) alpha_val 0.8455 (0.8451) lr 6.3188e-04 eta 0:09:07
epoch [33/50] batch [80/288] time 0.101 (0.105) data 0.000 (0.005) loss 1.0950 (1.3425) teacher_loss 0.6369 (0.8497) loss_zs_kd 0.1411 (0.1963) loss_oracle 0.3876 (0.3946) acc 84.3750 (78.7891) alaph_mean 0.8458 (0.8452) alpha_val 0.8458 (0.8452) lr 6.3188e-04 eta 0:08:55
epoch [33/50] batch [100/288] time 0.103 (0.103) data 0.000 (0.004) loss 1.2192 (1.3221) teacher_loss 0.7604 (0.8284) loss_zs_kd 0.1708 (0.1972) loss_oracle 0.3734 (0.3951) acc 81.2500 (79.2500) alaph_mean 0.8460 (0.8454) alpha_val 0.8460 (0.8454) lr 6.3188e-04 eta 0:08:43
epoch [33/50] batch [120/288] time 0.113 (0.102) data 0.000 (0.003) loss 1.8808 (1.3218) teacher_loss 1.2865 (0.8280) loss_zs_kd 0.2156 (0.1980) loss_oracle 0.4866 (0.3949) acc 65.6250 (79.2969) alaph_mean 0.8463 (0.8455) alpha_val 0.8463 (0.8455) lr 6.3188e-04 eta 0:08:36
epoch [33/50] batch [140/288] time 0.106 (0.102) data 0.000 (0.003) loss 1.5971 (1.3209) teacher_loss 0.9252 (0.8262) loss_zs_kd 0.2531 (0.1986) loss_oracle 0.5454 (0.3954) acc 78.1250 (79.3750) alaph_mean 0.8465 (0.8456) alpha_val 0.8465 (0.8456) lr 6.3188e-04 eta 0:08:33
epoch [33/50] batch [160/288] time 0.100 (0.102) data 0.000 (0.003) loss 1.4480 (1.3150) teacher_loss 0.9538 (0.8226) loss_zs_kd 0.2265 (0.1983) loss_oracle 0.3809 (0.3932) acc 84.3750 (79.5703) alaph_mean 0.8468 (0.8458) alpha_val 0.8468 (0.8458) lr 6.3188e-04 eta 0:08:30
epoch [33/50] batch [180/288] time 0.099 (0.102) data 0.000 (0.002) loss 1.5438 (1.3154) teacher_loss 1.0575 (0.8251) loss_zs_kd 0.2144 (0.1970) loss_oracle 0.3791 (0.3918) acc 78.1250 (79.6528) alaph_mean 0.8470 (0.8459) alpha_val 0.8470 (0.8459) lr 6.3188e-04 eta 0:08:28
epoch [33/50] batch [200/288] time 0.092 (0.101) data 0.000 (0.002) loss 1.3634 (1.3107) teacher_loss 0.8629 (0.8212) loss_zs_kd 0.1842 (0.1966) loss_oracle 0.4083 (0.3912) acc 81.2500 (79.8594) alaph_mean 0.8473 (0.8460) alpha_val 0.8473 (0.8460) lr 6.3188e-04 eta 0:08:23
epoch [33/50] batch [220/288] time 0.100 (0.101) data 0.000 (0.002) loss 1.3954 (1.3065) teacher_loss 0.9745 (0.8179) loss_zs_kd 0.0997 (0.1964) loss_oracle 0.3711 (0.3905) acc 75.0000 (79.7869) alaph_mean 0.8475 (0.8461) alpha_val 0.8475 (0.8461) lr 6.3188e-04 eta 0:08:19
epoch [33/50] batch [240/288] time 0.100 (0.102) data 0.000 (0.002) loss 1.0786 (1.2965) teacher_loss 0.5637 (0.8075) loss_zs_kd 0.1970 (0.1964) loss_oracle 0.4164 (0.3908) acc 84.3750 (79.9609) alaph_mean 0.8477 (0.8463) alpha_val 0.8477 (0.8463) lr 6.3188e-04 eta 0:08:22
epoch [33/50] batch [260/288] time 0.096 (0.101) data 0.000 (0.002) loss 1.2502 (1.3029) teacher_loss 0.7974 (0.8127) loss_zs_kd 0.1456 (0.1964) loss_oracle 0.3800 (0.3919) acc 81.2500 (79.8197) alaph_mean 0.8480 (0.8464) alpha_val 0.8480 (0.8464) lr 6.3188e-04 eta 0:08:19
epoch [33/50] batch [280/288] time 0.087 (0.101) data 0.000 (0.002) loss 1.2957 (1.3058) teacher_loss 0.8480 (0.8161) loss_zs_kd 0.1569 (0.1965) loss_oracle 0.3693 (0.3914) acc 71.8750 (79.7210) alaph_mean 0.8482 (0.8465) alpha_val 0.8482 (0.8465) lr 6.3188e-04 eta 0:08:14
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,398
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,026
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.0%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [34/50] batch [20/288] time 0.099 (0.121) data 0.000 (0.015) loss 1.5129 (1.2138) teacher_loss 1.0642 (0.7354) loss_zs_kd 0.2000 (0.1901) loss_oracle 0.3487 (0.3834) acc 75.0000 (81.7188) alaph_mean 0.8485 (0.8484) alpha_val 0.8485 (0.8484) lr 5.7422e-04 eta 0:09:49
epoch [34/50] batch [40/288] time 0.103 (0.112) data 0.000 (0.008) loss 1.1411 (1.2675) teacher_loss 0.6454 (0.7796) loss_zs_kd 0.1617 (0.1902) loss_oracle 0.4149 (0.3928) acc 87.5000 (80.4688) alaph_mean 0.8487 (0.8485) alpha_val 0.8487 (0.8485) lr 5.7422e-04 eta 0:09:02
epoch [34/50] batch [60/288] time 0.099 (0.107) data 0.000 (0.005) loss 1.0989 (1.2818) teacher_loss 0.6848 (0.7942) loss_zs_kd 0.1406 (0.1925) loss_oracle 0.3439 (0.3913) acc 78.1250 (80.1042) alaph_mean 0.8489 (0.8486) alpha_val 0.8489 (0.8486) lr 5.7422e-04 eta 0:08:39
epoch [34/50] batch [80/288] time 0.103 (0.105) data 0.000 (0.004) loss 1.2899 (1.2886) teacher_loss 0.7536 (0.7985) loss_zs_kd 0.2477 (0.1941) loss_oracle 0.4125 (0.3931) acc 81.2500 (80.2734) alaph_mean 0.8491 (0.8487) alpha_val 0.8491 (0.8487) lr 5.7422e-04 eta 0:08:28
epoch [34/50] batch [100/288] time 0.098 (0.105) data 0.000 (0.003) loss 1.4452 (1.2757) teacher_loss 0.9124 (0.7854) loss_zs_kd 0.2715 (0.1947) loss_oracle 0.3971 (0.3929) acc 78.1250 (80.8125) alaph_mean 0.8493 (0.8488) alpha_val 0.8493 (0.8488) lr 5.7422e-04 eta 0:08:22
epoch [34/50] batch [120/288] time 0.100 (0.104) data 0.000 (0.003) loss 1.2978 (1.2872) teacher_loss 0.8325 (0.7972) loss_zs_kd 0.1730 (0.1943) loss_oracle 0.3788 (0.3929) acc 84.3750 (80.4427) alaph_mean 0.8495 (0.8489) alpha_val 0.8495 (0.8489) lr 5.7422e-04 eta 0:08:17
epoch [34/50] batch [140/288] time 0.095 (0.104) data 0.000 (0.002) loss 1.3777 (1.2824) teacher_loss 0.8615 (0.7930) loss_zs_kd 0.2062 (0.1956) loss_oracle 0.4132 (0.3916) acc 71.8750 (80.4241) alaph_mean 0.8497 (0.8490) alpha_val 0.8497 (0.8490) lr 5.7422e-04 eta 0:08:14
epoch [34/50] batch [160/288] time 0.105 (0.104) data 0.000 (0.002) loss 0.9451 (1.2787) teacher_loss 0.4201 (0.7873) loss_zs_kd 0.1602 (0.1967) loss_oracle 0.4448 (0.3930) acc 87.5000 (80.6445) alaph_mean 0.8499 (0.8491) alpha_val 0.8499 (0.8491) lr 5.7422e-04 eta 0:08:11
epoch [34/50] batch [180/288] time 0.102 (0.104) data 0.000 (0.002) loss 1.2219 (1.2870) teacher_loss 0.7219 (0.7921) loss_zs_kd 0.1976 (0.1987) loss_oracle 0.4012 (0.3955) acc 78.1250 (80.2951) alaph_mean 0.8501 (0.8492) alpha_val 0.8501 (0.8492) lr 5.7422e-04 eta 0:08:09
epoch [34/50] batch [200/288] time 0.096 (0.104) data 0.000 (0.002) loss 1.8027 (1.2993) teacher_loss 1.3285 (0.8062) loss_zs_kd 0.2395 (0.1988) loss_oracle 0.3545 (0.3937) acc 68.7500 (79.9844) alaph_mean 0.8503 (0.8493) alpha_val 0.8503 (0.8493) lr 5.7422e-04 eta 0:08:06
epoch [34/50] batch [220/288] time 0.099 (0.105) data 0.000 (0.002) loss 1.4134 (1.3002) teacher_loss 0.9156 (0.8073) loss_zs_kd 0.1691 (0.1994) loss_oracle 0.4133 (0.3933) acc 75.0000 (79.9716) alaph_mean 0.8505 (0.8494) alpha_val 0.8505 (0.8494) lr 5.7422e-04 eta 0:08:10
epoch [34/50] batch [240/288] time 0.103 (0.105) data 0.000 (0.002) loss 1.3038 (1.3057) teacher_loss 0.8305 (0.8118) loss_zs_kd 0.2046 (0.1996) loss_oracle 0.3709 (0.3941) acc 75.0000 (79.8568) alaph_mean 0.8507 (0.8495) alpha_val 0.8507 (0.8495) lr 5.7422e-04 eta 0:08:07
epoch [34/50] batch [260/288] time 0.113 (0.105) data 0.000 (0.001) loss 1.1650 (1.3030) teacher_loss 0.6937 (0.8096) loss_zs_kd 0.2030 (0.1995) loss_oracle 0.3698 (0.3937) acc 87.5000 (79.9880) alaph_mean 0.8509 (0.8496) alpha_val 0.8509 (0.8496) lr 5.7422e-04 eta 0:08:05
epoch [34/50] batch [280/288] time 0.108 (0.105) data 0.001 (0.001) loss 1.4882 (1.2980) teacher_loss 0.9470 (0.8050) loss_zs_kd 0.2608 (0.1984) loss_oracle 0.4109 (0.3939) acc 75.0000 (80.0335) alaph_mean 0.8511 (0.8497) alpha_val 0.8511 (0.8497) lr 5.7422e-04 eta 0:08:03
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,399
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,013
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.3%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [35/50] batch [20/288] time 0.107 (0.113) data 0.000 (0.012) loss 1.6252 (1.3833) teacher_loss 1.0814 (0.8558) loss_zs_kd 0.2262 (0.2116) loss_oracle 0.4307 (0.4217) acc 78.1250 (78.7500) alaph_mean 0.8513 (0.8513) alpha_val 0.8513 (0.8513) lr 5.1825e-04 eta 0:08:38
epoch [35/50] batch [40/288] time 0.094 (0.105) data 0.000 (0.006) loss 1.5103 (1.4091) teacher_loss 0.9828 (0.8867) loss_zs_kd 0.1628 (0.2119) loss_oracle 0.4461 (0.4164) acc 75.0000 (77.5781) alaph_mean 0.8515 (0.8514) alpha_val 0.8515 (0.8514) lr 5.1825e-04 eta 0:07:58
epoch [35/50] batch [60/288] time 0.107 (0.102) data 0.000 (0.004) loss 1.8432 (1.3799) teacher_loss 1.3928 (0.8687) loss_zs_kd 0.1865 (0.2046) loss_oracle 0.3572 (0.4089) acc 71.8750 (78.2812) alaph_mean 0.8517 (0.8514) alpha_val 0.8517 (0.8514) lr 5.1825e-04 eta 0:07:45
epoch [35/50] batch [80/288] time 0.093 (0.101) data 0.000 (0.003) loss 1.3541 (1.3781) teacher_loss 0.7894 (0.8688) loss_zs_kd 0.2417 (0.2029) loss_oracle 0.4439 (0.4079) acc 78.1250 (78.6719) alaph_mean 0.8519 (0.8515) alpha_val 0.8519 (0.8515) lr 5.1825e-04 eta 0:07:38
epoch [35/50] batch [100/288] time 0.107 (0.100) data 0.000 (0.002) loss 1.4496 (1.3734) teacher_loss 0.9233 (0.8639) loss_zs_kd 0.1918 (0.2033) loss_oracle 0.4304 (0.4078) acc 81.2500 (78.8438) alaph_mean 0.8520 (0.8516) alpha_val 0.8520 (0.8516) lr 5.1825e-04 eta 0:07:32
epoch [35/50] batch [120/288] time 0.103 (0.100) data 0.000 (0.002) loss 1.2468 (1.3569) teacher_loss 0.6917 (0.8501) loss_zs_kd 0.2758 (0.2018) loss_oracle 0.4171 (0.4059) acc 87.5000 (79.3229) alaph_mean 0.8522 (0.8517) alpha_val 0.8522 (0.8517) lr 5.1825e-04 eta 0:07:29
epoch [35/50] batch [140/288] time 0.102 (0.100) data 0.000 (0.002) loss 0.8999 (1.3490) teacher_loss 0.4623 (0.8461) loss_zs_kd 0.0969 (0.2005) loss_oracle 0.3892 (0.4026) acc 87.5000 (79.3527) alaph_mean 0.8524 (0.8518) alpha_val 0.8524 (0.8518) lr 5.1825e-04 eta 0:07:26
epoch [35/50] batch [160/288] time 0.092 (0.099) data 0.000 (0.002) loss 1.2265 (1.3420) teacher_loss 0.7485 (0.8385) loss_zs_kd 0.1793 (0.2003) loss_oracle 0.3883 (0.4033) acc 78.1250 (79.6094) alaph_mean 0.8526 (0.8519) alpha_val 0.8526 (0.8519) lr 5.1825e-04 eta 0:07:22
epoch [35/50] batch [180/288] time 0.102 (0.099) data 0.000 (0.001) loss 1.0601 (1.3239) teacher_loss 0.6038 (0.8233) loss_zs_kd 0.1279 (0.1977) loss_oracle 0.3924 (0.4018) acc 81.2500 (79.8958) alaph_mean 0.8528 (0.8520) alpha_val 0.8528 (0.8520) lr 5.1825e-04 eta 0:07:19
epoch [35/50] batch [200/288] time 0.096 (0.101) data 0.000 (0.001) loss 0.9305 (1.3191) teacher_loss 0.4681 (0.8178) loss_zs_kd 0.1831 (0.1976) loss_oracle 0.3709 (0.4025) acc 84.3750 (80.0000) alaph_mean 0.8530 (0.8521) alpha_val 0.8530 (0.8521) lr 5.1825e-04 eta 0:07:23
epoch [35/50] batch [220/288] time 0.096 (0.100) data 0.000 (0.001) loss 1.0138 (1.3154) teacher_loss 0.5473 (0.8134) loss_zs_kd 0.1264 (0.1978) loss_oracle 0.4033 (0.4031) acc 84.3750 (80.1136) alaph_mean 0.8531 (0.8521) alpha_val 0.8531 (0.8521) lr 5.1825e-04 eta 0:07:19
epoch [35/50] batch [240/288] time 0.096 (0.100) data 0.000 (0.001) loss 0.8890 (1.3097) teacher_loss 0.4188 (0.8084) loss_zs_kd 0.2097 (0.1979) loss_oracle 0.3654 (0.4023) acc 87.5000 (80.1172) alaph_mean 0.8533 (0.8522) alpha_val 0.8533 (0.8522) lr 5.1825e-04 eta 0:07:16
epoch [35/50] batch [260/288] time 0.108 (0.100) data 0.001 (0.001) loss 1.0803 (1.2989) teacher_loss 0.5724 (0.7981) loss_zs_kd 0.1745 (0.1977) loss_oracle 0.4207 (0.4019) acc 90.6250 (80.3365) alaph_mean 0.8535 (0.8523) alpha_val 0.8535 (0.8523) lr 5.1825e-04 eta 0:07:14
epoch [35/50] batch [280/288] time 0.093 (0.099) data 0.000 (0.001) loss 1.2398 (1.3020) teacher_loss 0.7182 (0.8004) loss_zs_kd 0.1890 (0.1988) loss_oracle 0.4272 (0.4022) acc 81.2500 (80.1897) alaph_mean 0.8537 (0.8524) alpha_val 0.8537 (0.8524) lr 5.1825e-04 eta 0:07:10
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,393
* accuracy: 86.1%
* error: 13.9%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.5%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [36/50] batch [20/288] time 0.104 (0.117) data 0.000 (0.013) loss 1.5258 (1.2482) teacher_loss 1.0815 (0.7671) loss_zs_kd 0.1879 (0.1936) loss_oracle 0.3504 (0.3843) acc 71.8750 (81.8750) alaph_mean 0.8540 (0.8539) alpha_val 0.8540 (0.8539) lr 4.6417e-04 eta 0:08:20
epoch [36/50] batch [40/288] time 0.097 (0.107) data 0.000 (0.007) loss 1.3059 (1.3092) teacher_loss 0.8212 (0.8124) loss_zs_kd 0.1819 (0.2033) loss_oracle 0.3938 (0.3952) acc 75.0000 (80.0781) alaph_mean 0.8542 (0.8540) alpha_val 0.8542 (0.8540) lr 4.6417e-04 eta 0:07:40
epoch [36/50] batch [60/288] time 0.099 (0.105) data 0.001 (0.005) loss 1.1710 (1.3073) teacher_loss 0.6461 (0.8116) loss_zs_kd 0.1993 (0.2024) loss_oracle 0.4253 (0.3945) acc 84.3750 (80.0521) alaph_mean 0.8543 (0.8541) alpha_val 0.8543 (0.8541) lr 4.6417e-04 eta 0:07:25
epoch [36/50] batch [80/288] time 0.107 (0.104) data 0.000 (0.003) loss 1.2804 (1.2904) teacher_loss 0.7826 (0.7938) loss_zs_kd 0.1839 (0.1994) loss_oracle 0.4059 (0.3969) acc 75.0000 (80.5859) alaph_mean 0.8545 (0.8542) alpha_val 0.8545 (0.8542) lr 4.6417e-04 eta 0:07:19
epoch [36/50] batch [100/288] time 0.097 (0.103) data 0.000 (0.003) loss 1.4956 (1.2650) teacher_loss 1.0099 (0.7676) loss_zs_kd 0.1739 (0.1998) loss_oracle 0.3987 (0.3975) acc 81.2500 (81.2812) alaph_mean 0.8546 (0.8542) alpha_val 0.8546 (0.8542) lr 4.6417e-04 eta 0:07:12
epoch [36/50] batch [120/288] time 0.099 (0.102) data 0.000 (0.002) loss 1.0146 (1.2736) teacher_loss 0.5498 (0.7792) loss_zs_kd 0.1730 (0.1982) loss_oracle 0.3783 (0.3953) acc 90.6250 (81.0677) alaph_mean 0.8547 (0.8543) alpha_val 0.8547 (0.8543) lr 4.6417e-04 eta 0:07:06
epoch [36/50] batch [140/288] time 0.101 (0.101) data 0.001 (0.002) loss 1.2542 (1.2775) teacher_loss 0.7944 (0.7839) loss_zs_kd 0.1819 (0.1980) loss_oracle 0.3688 (0.3946) acc 78.1250 (81.0045) alaph_mean 0.8549 (0.8544) alpha_val 0.8549 (0.8544) lr 4.6417e-04 eta 0:07:02
epoch [36/50] batch [160/288] time 0.099 (0.101) data 0.000 (0.002) loss 1.1757 (1.2742) teacher_loss 0.7600 (0.7793) loss_zs_kd 0.1464 (0.1988) loss_oracle 0.3425 (0.3955) acc 78.1250 (81.0938) alaph_mean 0.8551 (0.8545) alpha_val 0.8551 (0.8545) lr 4.6417e-04 eta 0:06:58
epoch [36/50] batch [180/288] time 0.094 (0.102) data 0.000 (0.002) loss 1.6130 (1.2818) teacher_loss 1.1545 (0.7859) loss_zs_kd 0.1719 (0.1982) loss_oracle 0.3725 (0.3967) acc 75.0000 (80.9375) alaph_mean 0.8552 (0.8545) alpha_val 0.8552 (0.8545) lr 4.6417e-04 eta 0:07:03
epoch [36/50] batch [200/288] time 0.099 (0.102) data 0.000 (0.001) loss 1.4241 (1.2895) teacher_loss 0.9255 (0.7933) loss_zs_kd 0.1852 (0.1989) loss_oracle 0.4060 (0.3967) acc 71.8750 (80.7188) alaph_mean 0.8554 (0.8546) alpha_val 0.8554 (0.8546) lr 4.6417e-04 eta 0:07:00
epoch [36/50] batch [220/288] time 0.102 (0.102) data 0.000 (0.001) loss 1.3766 (1.2883) teacher_loss 0.8427 (0.7907) loss_zs_kd 0.2056 (0.1984) loss_oracle 0.4312 (0.3984) acc 78.1250 (80.6250) alaph_mean 0.8556 (0.8547) alpha_val 0.8556 (0.8547) lr 4.6417e-04 eta 0:06:58
epoch [36/50] batch [240/288] time 0.111 (0.102) data 0.000 (0.001) loss 1.1259 (1.2850) teacher_loss 0.6217 (0.7864) loss_zs_kd 0.1915 (0.1983) loss_oracle 0.4085 (0.3994) acc 81.2500 (80.5859) alaph_mean 0.8557 (0.8548) alpha_val 0.8557 (0.8548) lr 4.6417e-04 eta 0:06:56
epoch [36/50] batch [260/288] time 0.099 (0.102) data 0.000 (0.001) loss 1.0144 (1.2813) teacher_loss 0.5274 (0.7832) loss_zs_kd 0.1975 (0.1982) loss_oracle 0.3882 (0.3991) acc 90.6250 (80.5409) alaph_mean 0.8559 (0.8549) alpha_val 0.8559 (0.8549) lr 4.6417e-04 eta 0:06:55
epoch [36/50] batch [280/288] time 0.106 (0.102) data 0.000 (0.001) loss 1.2524 (1.2858) teacher_loss 0.7275 (0.7856) loss_zs_kd 0.1960 (0.1999) loss_oracle 0.4270 (0.4003) acc 84.3750 (80.5246) alaph_mean 0.8560 (0.8549) alpha_val 0.8560 (0.8549) lr 4.6417e-04 eta 0:06:53
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,401
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,016
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.4%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [37/50] batch [20/288] time 0.101 (0.119) data 0.000 (0.013) loss 1.3036 (1.3019) teacher_loss 0.8856 (0.7948) loss_zs_kd 0.1743 (0.2162) loss_oracle 0.3308 (0.3989) acc 78.1250 (81.4062) alaph_mean 0.8563 (0.8562) alpha_val 0.8563 (0.8562) lr 4.1221e-04 eta 0:07:56
epoch [37/50] batch [40/288] time 0.085 (0.108) data 0.000 (0.007) loss 1.3193 (1.3260) teacher_loss 0.8078 (0.8265) loss_zs_kd 0.2085 (0.2064) loss_oracle 0.4071 (0.3963) acc 84.3750 (80.1562) alaph_mean 0.8564 (0.8563) alpha_val 0.8564 (0.8563) lr 4.1221e-04 eta 0:07:09
epoch [37/50] batch [60/288] time 0.100 (0.104) data 0.000 (0.004) loss 1.2360 (1.3155) teacher_loss 0.6708 (0.8169) loss_zs_kd 0.2086 (0.2013) loss_oracle 0.4608 (0.3980) acc 81.2500 (80.4688) alaph_mean 0.8565 (0.8563) alpha_val 0.8565 (0.8563) lr 4.1221e-04 eta 0:06:51
epoch [37/50] batch [80/288] time 0.089 (0.102) data 0.000 (0.003) loss 1.0070 (1.3086) teacher_loss 0.5042 (0.8089) loss_zs_kd 0.1442 (0.2008) loss_oracle 0.4307 (0.3993) acc 90.6250 (80.5469) alaph_mean 0.8566 (0.8564) alpha_val 0.8566 (0.8564) lr 4.1221e-04 eta 0:06:42
epoch [37/50] batch [100/288] time 0.104 (0.101) data 0.000 (0.003) loss 1.2090 (1.2915) teacher_loss 0.6606 (0.7889) loss_zs_kd 0.2113 (0.2028) loss_oracle 0.4428 (0.4012) acc 87.5000 (81.0312) alaph_mean 0.8568 (0.8565) alpha_val 0.8568 (0.8565) lr 4.1221e-04 eta 0:06:36
epoch [37/50] batch [120/288] time 0.087 (0.100) data 0.000 (0.002) loss 1.5094 (1.3083) teacher_loss 1.0382 (0.8039) loss_zs_kd 0.1772 (0.2052) loss_oracle 0.3826 (0.4018) acc 71.8750 (80.4167) alaph_mean 0.8570 (0.8565) alpha_val 0.8570 (0.8565) lr 4.1221e-04 eta 0:06:29
epoch [37/50] batch [140/288] time 0.098 (0.099) data 0.000 (0.002) loss 1.3264 (1.3094) teacher_loss 0.8539 (0.8031) loss_zs_kd 0.1684 (0.2066) loss_oracle 0.3883 (0.4030) acc 75.0000 (80.2009) alaph_mean 0.8571 (0.8566) alpha_val 0.8571 (0.8566) lr 4.1221e-04 eta 0:06:23
epoch [37/50] batch [160/288] time 0.097 (0.100) data 0.000 (0.002) loss 1.1887 (1.3051) teacher_loss 0.6856 (0.7973) loss_zs_kd 0.1735 (0.2070) loss_oracle 0.4164 (0.4043) acc 87.5000 (80.3906) alaph_mean 0.8572 (0.8567) alpha_val 0.8572 (0.8567) lr 4.1221e-04 eta 0:06:26
epoch [37/50] batch [180/288] time 0.098 (0.100) data 0.000 (0.002) loss 1.1994 (1.2932) teacher_loss 0.6948 (0.7865) loss_zs_kd 0.2013 (0.2060) loss_oracle 0.4040 (0.4037) acc 78.1250 (80.6076) alaph_mean 0.8574 (0.8567) alpha_val 0.8574 (0.8567) lr 4.1221e-04 eta 0:06:23
epoch [37/50] batch [200/288] time 0.091 (0.099) data 0.000 (0.001) loss 1.1715 (1.2939) teacher_loss 0.6749 (0.7886) loss_zs_kd 0.1803 (0.2060) loss_oracle 0.4065 (0.4023) acc 81.2500 (80.5000) alaph_mean 0.8575 (0.8568) alpha_val 0.8575 (0.8568) lr 4.1221e-04 eta 0:06:20
epoch [37/50] batch [220/288] time 0.097 (0.099) data 0.000 (0.001) loss 1.5786 (1.2967) teacher_loss 0.9994 (0.7927) loss_zs_kd 0.3055 (0.2053) loss_oracle 0.4266 (0.4014) acc 78.1250 (80.3835) alaph_mean 0.8577 (0.8569) alpha_val 0.8577 (0.8569) lr 4.1221e-04 eta 0:06:17
epoch [37/50] batch [240/288] time 0.096 (0.099) data 0.000 (0.001) loss 1.1313 (1.2956) teacher_loss 0.6167 (0.7919) loss_zs_kd 0.1868 (0.2049) loss_oracle 0.4212 (0.4012) acc 87.5000 (80.3125) alaph_mean 0.8578 (0.8570) alpha_val 0.8578 (0.8570) lr 4.1221e-04 eta 0:06:14
epoch [37/50] batch [260/288] time 0.096 (0.099) data 0.000 (0.001) loss 1.5157 (1.3006) teacher_loss 1.0038 (0.7958) loss_zs_kd 0.2335 (0.2054) loss_oracle 0.3952 (0.4022) acc 81.2500 (80.2043) alaph_mean 0.8580 (0.8570) alpha_val 0.8580 (0.8570) lr 4.1221e-04 eta 0:06:12
epoch [37/50] batch [280/288] time 0.089 (0.098) data 0.000 (0.001) loss 1.0681 (1.2966) teacher_loss 0.5928 (0.7916) loss_zs_kd 0.2143 (0.2055) loss_oracle 0.3682 (0.4022) acc 87.5000 (80.3125) alaph_mean 0.8581 (0.8571) alpha_val 0.8581 (0.8571) lr 4.1221e-04 eta 0:06:08
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.7%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [38/50] batch [20/288] time 0.094 (0.115) data 0.000 (0.013) loss 1.3910 (1.3516) teacher_loss 0.9188 (0.8470) loss_zs_kd 0.1531 (0.2070) loss_oracle 0.3956 (0.4010) acc 75.0000 (78.1250) alaph_mean 0.8583 (0.8582) alpha_val 0.8583 (0.8582) lr 3.6258e-04 eta 0:07:08
epoch [38/50] batch [40/288] time 0.099 (0.106) data 0.000 (0.007) loss 0.8163 (1.2755) teacher_loss 0.2668 (0.7751) loss_zs_kd 0.2476 (0.2039) loss_oracle 0.4256 (0.3984) acc 90.6250 (80.8594) alaph_mean 0.8584 (0.8583) alpha_val 0.8584 (0.8583) lr 3.6258e-04 eta 0:06:32
epoch [38/50] batch [60/288] time 0.095 (0.103) data 0.000 (0.004) loss 1.0690 (1.2717) teacher_loss 0.6211 (0.7707) loss_zs_kd 0.1808 (0.2049) loss_oracle 0.3575 (0.3985) acc 84.3750 (80.7812) alaph_mean 0.8586 (0.8584) alpha_val 0.8586 (0.8584) lr 3.6258e-04 eta 0:06:20
epoch [38/50] batch [80/288] time 0.097 (0.101) data 0.000 (0.003) loss 1.3488 (1.2809) teacher_loss 0.8914 (0.7791) loss_zs_kd 0.1593 (0.2053) loss_oracle 0.3777 (0.3992) acc 78.1250 (80.8594) alaph_mean 0.8587 (0.8584) alpha_val 0.8587 (0.8584) lr 3.6258e-04 eta 0:06:10
epoch [38/50] batch [100/288] time 0.095 (0.100) data 0.000 (0.003) loss 1.3249 (1.2973) teacher_loss 0.8889 (0.7955) loss_zs_kd 0.1732 (0.2042) loss_oracle 0.3495 (0.3997) acc 84.3750 (80.3750) alaph_mean 0.8588 (0.8585) alpha_val 0.8588 (0.8585) lr 3.6258e-04 eta 0:06:04
epoch [38/50] batch [120/288] time 0.094 (0.100) data 0.000 (0.002) loss 1.7415 (1.3078) teacher_loss 1.1589 (0.8011) loss_zs_kd 0.2316 (0.2044) loss_oracle 0.4668 (0.4045) acc 75.0000 (80.1302) alaph_mean 0.8590 (0.8586) alpha_val 0.8590 (0.8586) lr 3.6258e-04 eta 0:06:01
epoch [38/50] batch [140/288] time 0.110 (0.102) data 0.000 (0.002) loss 1.2020 (1.3180) teacher_loss 0.6727 (0.8074) loss_zs_kd 0.2317 (0.2067) loss_oracle 0.4135 (0.4072) acc 87.5000 (79.8438) alaph_mean 0.8591 (0.8586) alpha_val 0.8591 (0.8586) lr 3.6258e-04 eta 0:06:06
epoch [38/50] batch [160/288] time 0.092 (0.102) data 0.000 (0.002) loss 0.9530 (1.3300) teacher_loss 0.5220 (0.8159) loss_zs_kd 0.1437 (0.2091) loss_oracle 0.3592 (0.4096) acc 84.3750 (79.5898) alaph_mean 0.8593 (0.8587) alpha_val 0.8593 (0.8587) lr 3.6258e-04 eta 0:06:04
epoch [38/50] batch [180/288] time 0.098 (0.101) data 0.000 (0.002) loss 1.4577 (1.3393) teacher_loss 0.9354 (0.8214) loss_zs_kd 0.2428 (0.2131) loss_oracle 0.4009 (0.4114) acc 81.2500 (79.4792) alaph_mean 0.8594 (0.8588) alpha_val 0.8594 (0.8588) lr 3.6258e-04 eta 0:06:00
epoch [38/50] batch [200/288] time 0.097 (0.101) data 0.000 (0.001) loss 1.0677 (1.3278) teacher_loss 0.5835 (0.8115) loss_zs_kd 0.1424 (0.2113) loss_oracle 0.4130 (0.4107) acc 78.1250 (79.6562) alaph_mean 0.8595 (0.8588) alpha_val 0.8595 (0.8588) lr 3.6258e-04 eta 0:05:56
epoch [38/50] batch [220/288] time 0.095 (0.101) data 0.000 (0.001) loss 0.9913 (1.3238) teacher_loss 0.4959 (0.8080) loss_zs_kd 0.2015 (0.2106) loss_oracle 0.3946 (0.4104) acc 84.3750 (79.7869) alaph_mean 0.8596 (0.8589) alpha_val 0.8596 (0.8589) lr 3.6258e-04 eta 0:05:54
epoch [38/50] batch [240/288] time 0.099 (0.100) data 0.000 (0.001) loss 0.9041 (1.3241) teacher_loss 0.4190 (0.8076) loss_zs_kd 0.1978 (0.2110) loss_oracle 0.3862 (0.4110) acc 93.7500 (79.9870) alaph_mean 0.8598 (0.8590) alpha_val 0.8598 (0.8590) lr 3.6258e-04 eta 0:05:51
epoch [38/50] batch [260/288] time 0.100 (0.100) data 0.000 (0.001) loss 1.3030 (1.3225) teacher_loss 0.8661 (0.8069) loss_zs_kd 0.1528 (0.2105) loss_oracle 0.3605 (0.4104) acc 81.2500 (80.0240) alaph_mean 0.8599 (0.8590) alpha_val 0.8599 (0.8590) lr 3.6258e-04 eta 0:05:48
epoch [38/50] batch [280/288] time 0.089 (0.100) data 0.000 (0.001) loss 0.9788 (1.3175) teacher_loss 0.5726 (0.8032) loss_zs_kd 0.1528 (0.2095) loss_oracle 0.3298 (0.4096) acc 87.5000 (80.0893) alaph_mean 0.8600 (0.8591) alpha_val 0.8600 (0.8591) lr 3.6258e-04 eta 0:05:45
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.5%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [39/50] batch [20/288] time 0.101 (0.122) data 0.000 (0.015) loss 1.2187 (1.2551) teacher_loss 0.7126 (0.7488) loss_zs_kd 0.1643 (0.1958) loss_oracle 0.4240 (0.4084) acc 81.2500 (79.0625) alaph_mean 0.8601 (0.8601) alpha_val 0.8601 (0.8601) lr 3.1545e-04 eta 0:07:00
epoch [39/50] batch [40/288] time 0.096 (0.112) data 0.000 (0.008) loss 1.5717 (1.3021) teacher_loss 1.0076 (0.7853) loss_zs_kd 0.2117 (0.2038) loss_oracle 0.4582 (0.4149) acc 68.7500 (79.5312) alaph_mean 0.8602 (0.8601) alpha_val 0.8602 (0.8601) lr 3.1545e-04 eta 0:06:23
epoch [39/50] batch [60/288] time 0.109 (0.109) data 0.000 (0.005) loss 1.1194 (1.2995) teacher_loss 0.6570 (0.7847) loss_zs_kd 0.1355 (0.2022) loss_oracle 0.3947 (0.4137) acc 81.2500 (79.0104) alaph_mean 0.8603 (0.8602) alpha_val 0.8603 (0.8602) lr 3.1545e-04 eta 0:06:08
epoch [39/50] batch [80/288] time 0.098 (0.107) data 0.000 (0.004) loss 1.6239 (1.3008) teacher_loss 1.0633 (0.7879) loss_zs_kd 0.2054 (0.2027) loss_oracle 0.4578 (0.4115) acc 71.8750 (79.4531) alaph_mean 0.8604 (0.8602) alpha_val 0.8604 (0.8602) lr 3.1545e-04 eta 0:06:01
epoch [39/50] batch [100/288] time 0.097 (0.106) data 0.000 (0.003) loss 1.3344 (1.3062) teacher_loss 0.8279 (0.7944) loss_zs_kd 0.2070 (0.2023) loss_oracle 0.4029 (0.4106) acc 81.2500 (79.6250) alaph_mean 0.8606 (0.8603) alpha_val 0.8606 (0.8603) lr 3.1545e-04 eta 0:05:54
epoch [39/50] batch [120/288] time 0.142 (0.106) data 0.000 (0.003) loss 0.9497 (1.3101) teacher_loss 0.5324 (0.8011) loss_zs_kd 0.1638 (0.2025) loss_oracle 0.3355 (0.4078) acc 87.5000 (79.4792) alaph_mean 0.8607 (0.8603) alpha_val 0.8607 (0.8603) lr 3.1545e-04 eta 0:05:52
epoch [39/50] batch [140/288] time 0.114 (0.105) data 0.000 (0.002) loss 1.3568 (1.3108) teacher_loss 0.8344 (0.8005) loss_zs_kd 0.2281 (0.2052) loss_oracle 0.4083 (0.4077) acc 75.0000 (79.6652) alaph_mean 0.8608 (0.8604) alpha_val 0.8608 (0.8604) lr 3.1545e-04 eta 0:05:49
epoch [39/50] batch [160/288] time 0.102 (0.105) data 0.000 (0.002) loss 1.4852 (1.3073) teacher_loss 0.9408 (0.7957) loss_zs_kd 0.2183 (0.2054) loss_oracle 0.4353 (0.4089) acc 81.2500 (79.9414) alaph_mean 0.8609 (0.8604) alpha_val 0.8609 (0.8604) lr 3.1545e-04 eta 0:05:46
epoch [39/50] batch [180/288] time 0.100 (0.105) data 0.000 (0.002) loss 1.1258 (1.3039) teacher_loss 0.6513 (0.7923) loss_zs_kd 0.1699 (0.2038) loss_oracle 0.3896 (0.4097) acc 78.1250 (79.9479) alaph_mean 0.8610 (0.8605) alpha_val 0.8610 (0.8605) lr 3.1545e-04 eta 0:05:43
epoch [39/50] batch [200/288] time 0.097 (0.104) data 0.001 (0.002) loss 1.0791 (1.3139) teacher_loss 0.5675 (0.8017) loss_zs_kd 0.2249 (0.2043) loss_oracle 0.3992 (0.4101) acc 84.3750 (79.6562) alaph_mean 0.8611 (0.8605) alpha_val 0.8611 (0.8605) lr 3.1545e-04 eta 0:05:40
epoch [39/50] batch [220/288] time 0.104 (0.104) data 0.000 (0.002) loss 1.5060 (1.3115) teacher_loss 0.9473 (0.8006) loss_zs_kd 0.2594 (0.2042) loss_oracle 0.4290 (0.4088) acc 75.0000 (79.7869) alaph_mean 0.8612 (0.8606) alpha_val 0.8612 (0.8606) lr 3.1545e-04 eta 0:05:37
epoch [39/50] batch [240/288] time 0.099 (0.104) data 0.000 (0.002) loss 1.0680 (1.3150) teacher_loss 0.5451 (0.8026) loss_zs_kd 0.2395 (0.2063) loss_oracle 0.4031 (0.4093) acc 78.1250 (79.7005) alaph_mean 0.8613 (0.8607) alpha_val 0.8613 (0.8607) lr 3.1545e-04 eta 0:05:35
epoch [39/50] batch [260/288] time 0.096 (0.104) data 0.000 (0.001) loss 0.9640 (1.3131) teacher_loss 0.4975 (0.8008) loss_zs_kd 0.1295 (0.2060) loss_oracle 0.4017 (0.4093) acc 84.3750 (79.6394) alaph_mean 0.8614 (0.8607) alpha_val 0.8614 (0.8607) lr 3.1545e-04 eta 0:05:32
epoch [39/50] batch [280/288] time 0.095 (0.104) data 0.000 (0.001) loss 1.1775 (1.3107) teacher_loss 0.7338 (0.7985) loss_zs_kd 0.1486 (0.2060) loss_oracle 0.3694 (0.4092) acc 78.1250 (79.5424) alaph_mean 0.8615 (0.8608) alpha_val 0.8615 (0.8608) lr 3.1545e-04 eta 0:05:29
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,401
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,027
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.0%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [40/50] batch [20/288] time 0.095 (0.112) data 0.000 (0.014) loss 1.1519 (1.2868) teacher_loss 0.7173 (0.7695) loss_zs_kd 0.2079 (0.2070) loss_oracle 0.3306 (0.4138) acc 78.1250 (80.0000) alaph_mean 0.8616 (0.8616) alpha_val 0.8616 (0.8616) lr 2.7103e-04 eta 0:05:53
epoch [40/50] batch [40/288] time 0.105 (0.107) data 0.000 (0.007) loss 1.5855 (1.3108) teacher_loss 1.0154 (0.7878) loss_zs_kd 0.2805 (0.2146) loss_oracle 0.4298 (0.4158) acc 78.1250 (80.4688) alaph_mean 0.8617 (0.8616) alpha_val 0.8617 (0.8616) lr 2.7103e-04 eta 0:05:33
epoch [40/50] batch [60/288] time 0.097 (0.104) data 0.000 (0.005) loss 1.1786 (1.2954) teacher_loss 0.6653 (0.7704) loss_zs_kd 0.2241 (0.2126) loss_oracle 0.4013 (0.4187) acc 84.3750 (80.6250) alaph_mean 0.8618 (0.8617) alpha_val 0.8618 (0.8617) lr 2.7103e-04 eta 0:05:23
epoch [40/50] batch [80/288] time 0.092 (0.102) data 0.000 (0.004) loss 1.5651 (1.3062) teacher_loss 1.0004 (0.7820) loss_zs_kd 0.2244 (0.2143) loss_oracle 0.4525 (0.4171) acc 75.0000 (80.6250) alaph_mean 0.8619 (0.8617) alpha_val 0.8619 (0.8617) lr 2.7103e-04 eta 0:05:15
epoch [40/50] batch [100/288] time 0.103 (0.105) data 0.000 (0.003) loss 1.2942 (1.3157) teacher_loss 0.8034 (0.7912) loss_zs_kd 0.1921 (0.2129) loss_oracle 0.3947 (0.4181) acc 75.0000 (80.1875) alaph_mean 0.8620 (0.8618) alpha_val 0.8620 (0.8618) lr 2.7103e-04 eta 0:05:20
epoch [40/50] batch [120/288] time 0.096 (0.104) data 0.000 (0.003) loss 1.3754 (1.3159) teacher_loss 0.8544 (0.7914) loss_zs_kd 0.2347 (0.2129) loss_oracle 0.4037 (0.4180) acc 81.2500 (80.2083) alaph_mean 0.8621 (0.8618) alpha_val 0.8621 (0.8618) lr 2.7103e-04 eta 0:05:15
epoch [40/50] batch [140/288] time 0.095 (0.103) data 0.000 (0.002) loss 1.3678 (1.3161) teacher_loss 0.9456 (0.7933) loss_zs_kd 0.1926 (0.2116) loss_oracle 0.3259 (0.4169) acc 68.7500 (80.1786) alaph_mean 0.8622 (0.8619) alpha_val 0.8622 (0.8619) lr 2.7103e-04 eta 0:05:11
epoch [40/50] batch [160/288] time 0.103 (0.103) data 0.000 (0.002) loss 1.6707 (1.3171) teacher_loss 1.1138 (0.7947) loss_zs_kd 0.2078 (0.2122) loss_oracle 0.4530 (0.4162) acc 78.1250 (80.3125) alaph_mean 0.8623 (0.8619) alpha_val 0.8623 (0.8619) lr 2.7103e-04 eta 0:05:08
epoch [40/50] batch [180/288] time 0.094 (0.102) data 0.000 (0.002) loss 1.0841 (1.3264) teacher_loss 0.5602 (0.8054) loss_zs_kd 0.1798 (0.2120) loss_oracle 0.4340 (0.4151) acc 87.5000 (80.1215) alaph_mean 0.8624 (0.8620) alpha_val 0.8624 (0.8620) lr 2.7103e-04 eta 0:05:03
epoch [40/50] batch [200/288] time 0.096 (0.101) data 0.000 (0.002) loss 1.2586 (1.3183) teacher_loss 0.6800 (0.7992) loss_zs_kd 0.2544 (0.2106) loss_oracle 0.4514 (0.4138) acc 78.1250 (80.1719) alaph_mean 0.8625 (0.8620) alpha_val 0.8625 (0.8620) lr 2.7103e-04 eta 0:04:59
epoch [40/50] batch [220/288] time 0.106 (0.101) data 0.000 (0.001) loss 1.1765 (1.3209) teacher_loss 0.7150 (0.8017) loss_zs_kd 0.1234 (0.2110) loss_oracle 0.3998 (0.4137) acc 84.3750 (80.1420) alaph_mean 0.8626 (0.8620) alpha_val 0.8626 (0.8620) lr 2.7103e-04 eta 0:04:56
epoch [40/50] batch [240/288] time 0.102 (0.101) data 0.000 (0.001) loss 1.0016 (1.3119) teacher_loss 0.5052 (0.7942) loss_zs_kd 0.2142 (0.2098) loss_oracle 0.3893 (0.4128) acc 84.3750 (80.3646) alaph_mean 0.8626 (0.8621) alpha_val 0.8626 (0.8621) lr 2.7103e-04 eta 0:04:54
epoch [40/50] batch [260/288] time 0.102 (0.101) data 0.000 (0.001) loss 1.4973 (1.3074) teacher_loss 0.9704 (0.7891) loss_zs_kd 0.2455 (0.2104) loss_oracle 0.4042 (0.4131) acc 75.0000 (80.3486) alaph_mean 0.8627 (0.8621) alpha_val 0.8627 (0.8621) lr 2.7103e-04 eta 0:04:52
epoch [40/50] batch [280/288] time 0.094 (0.101) data 0.000 (0.001) loss 1.4639 (1.3049) teacher_loss 1.0057 (0.7879) loss_zs_kd 0.2414 (0.2094) loss_oracle 0.3375 (0.4123) acc 75.0000 (80.3795) alaph_mean 0.8628 (0.8622) alpha_val 0.8628 (0.8622) lr 2.7103e-04 eta 0:04:50
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,027
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.1%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [41/50] batch [20/288] time 0.092 (0.122) data 0.000 (0.014) loss 1.1685 (1.3166) teacher_loss 0.6269 (0.7859) loss_zs_kd 0.2146 (0.2212) loss_oracle 0.4343 (0.4200) acc 81.2500 (79.6875) alaph_mean 0.8630 (0.8629) alpha_val 0.8630 (0.8629) lr 2.2949e-04 eta 0:05:48
epoch [41/50] batch [40/288] time 0.093 (0.108) data 0.000 (0.007) loss 1.2425 (1.3098) teacher_loss 0.7122 (0.7904) loss_zs_kd 0.2126 (0.2084) loss_oracle 0.4240 (0.4152) acc 81.2500 (79.7656) alaph_mean 0.8630 (0.8630) alpha_val 0.8630 (0.8630) lr 2.2949e-04 eta 0:05:06
epoch [41/50] batch [60/288] time 0.097 (0.104) data 0.000 (0.005) loss 1.5615 (1.3124) teacher_loss 0.9960 (0.7973) loss_zs_kd 0.2103 (0.2025) loss_oracle 0.4604 (0.4139) acc 78.1250 (79.5312) alaph_mean 0.8631 (0.8630) alpha_val 0.8631 (0.8630) lr 2.2949e-04 eta 0:04:53
epoch [41/50] batch [80/288] time 0.136 (0.104) data 0.000 (0.004) loss 0.9303 (1.3095) teacher_loss 0.4202 (0.7969) loss_zs_kd 0.2000 (0.2007) loss_oracle 0.4101 (0.4122) acc 93.7500 (80.0000) alaph_mean 0.8632 (0.8630) alpha_val 0.8632 (0.8630) lr 2.2949e-04 eta 0:04:51
epoch [41/50] batch [100/288] time 0.094 (0.104) data 0.000 (0.003) loss 1.7306 (1.3164) teacher_loss 1.2660 (0.8004) loss_zs_kd 0.2060 (0.2060) loss_oracle 0.3615 (0.4130) acc 71.8750 (79.8438) alaph_mean 0.8633 (0.8631) alpha_val 0.8633 (0.8631) lr 2.2949e-04 eta 0:04:48
epoch [41/50] batch [120/288] time 0.094 (0.103) data 0.000 (0.003) loss 1.6375 (1.3137) teacher_loss 1.1087 (0.7976) loss_zs_kd 0.2489 (0.2060) loss_oracle 0.4044 (0.4131) acc 71.8750 (80.1823) alaph_mean 0.8634 (0.8631) alpha_val 0.8634 (0.8631) lr 2.2949e-04 eta 0:04:42
epoch [41/50] batch [140/288] time 0.095 (0.102) data 0.000 (0.002) loss 1.4835 (1.3075) teacher_loss 1.0348 (0.7924) loss_zs_kd 0.1678 (0.2062) loss_oracle 0.3647 (0.4119) acc 71.8750 (80.4018) alaph_mean 0.8634 (0.8632) alpha_val 0.8634 (0.8632) lr 2.2949e-04 eta 0:04:38
epoch [41/50] batch [160/288] time 0.098 (0.101) data 0.000 (0.002) loss 0.9936 (1.2977) teacher_loss 0.5872 (0.7835) loss_zs_kd 0.1666 (0.2048) loss_oracle 0.3231 (0.4119) acc 90.6250 (80.5859) alaph_mean 0.8635 (0.8632) alpha_val 0.8635 (0.8632) lr 2.2949e-04 eta 0:04:34
epoch [41/50] batch [180/288] time 0.095 (0.101) data 0.000 (0.002) loss 1.9799 (1.2990) teacher_loss 1.3787 (0.7848) loss_zs_kd 0.3083 (0.2057) loss_oracle 0.4470 (0.4114) acc 65.6250 (80.5208) alaph_mean 0.8636 (0.8632) alpha_val 0.8636 (0.8632) lr 2.2949e-04 eta 0:04:31
epoch [41/50] batch [200/288] time 0.096 (0.100) data 0.000 (0.002) loss 1.2681 (1.3013) teacher_loss 0.7617 (0.7856) loss_zs_kd 0.2210 (0.2068) loss_oracle 0.3959 (0.4123) acc 71.8750 (80.4531) alaph_mean 0.8637 (0.8633) alpha_val 0.8637 (0.8633) lr 2.2949e-04 eta 0:04:28
epoch [41/50] batch [220/288] time 0.101 (0.100) data 0.000 (0.002) loss 1.3636 (1.3021) teacher_loss 0.7970 (0.7851) loss_zs_kd 0.2659 (0.2075) loss_oracle 0.4336 (0.4132) acc 84.3750 (80.3977) alaph_mean 0.8638 (0.8633) alpha_val 0.8638 (0.8633) lr 2.2949e-04 eta 0:04:26
epoch [41/50] batch [240/288] time 0.101 (0.100) data 0.000 (0.001) loss 1.3252 (1.3016) teacher_loss 0.7284 (0.7852) loss_zs_kd 0.2537 (0.2066) loss_oracle 0.4699 (0.4131) acc 87.5000 (80.4036) alaph_mean 0.8638 (0.8634) alpha_val 0.8638 (0.8634) lr 2.2949e-04 eta 0:04:25
epoch [41/50] batch [260/288] time 0.099 (0.100) data 0.000 (0.001) loss 1.9463 (1.3040) teacher_loss 1.3882 (0.7879) loss_zs_kd 0.2438 (0.2071) loss_oracle 0.4362 (0.4126) acc 62.5000 (80.2764) alaph_mean 0.8639 (0.8634) alpha_val 0.8639 (0.8634) lr 2.2949e-04 eta 0:04:23
epoch [41/50] batch [280/288] time 0.105 (0.101) data 0.000 (0.001) loss 1.0328 (1.3037) teacher_loss 0.6129 (0.7863) loss_zs_kd 0.1810 (0.2069) loss_oracle 0.3294 (0.4139) acc 81.2500 (80.3125) alaph_mean 0.8640 (0.8634) alpha_val 0.8640 (0.8634) lr 2.2949e-04 eta 0:04:21
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,026
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.0%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [42/50] batch [20/288] time 0.097 (0.128) data 0.000 (0.017) loss 1.0793 (1.2868) teacher_loss 0.6212 (0.7662) loss_zs_kd 0.1684 (0.2011) loss_oracle 0.3739 (0.4201) acc 81.2500 (80.7812) alaph_mean 0.8641 (0.8641) alpha_val 0.8641 (0.8641) lr 1.9098e-04 eta 0:05:28
epoch [42/50] batch [40/288] time 0.112 (0.118) data 0.000 (0.008) loss 1.3551 (1.3074) teacher_loss 0.8365 (0.7878) loss_zs_kd 0.1840 (0.2035) loss_oracle 0.4267 (0.4179) acc 78.1250 (80.3125) alaph_mean 0.8641 (0.8641) alpha_val 0.8641 (0.8641) lr 1.9098e-04 eta 0:05:00
epoch [42/50] batch [60/288] time 0.103 (0.118) data 0.001 (0.006) loss 1.5902 (1.3297) teacher_loss 1.0869 (0.8058) loss_zs_kd 0.2076 (0.2085) loss_oracle 0.3995 (0.4197) acc 78.1250 (80.1042) alaph_mean 0.8642 (0.8641) alpha_val 0.8642 (0.8641) lr 1.9098e-04 eta 0:04:58
epoch [42/50] batch [80/288] time 0.101 (0.115) data 0.000 (0.004) loss 1.2283 (1.3156) teacher_loss 0.6829 (0.7940) loss_zs_kd 0.2133 (0.2091) loss_oracle 0.4387 (0.4171) acc 81.2500 (80.5078) alaph_mean 0.8643 (0.8641) alpha_val 0.8643 (0.8641) lr 1.9098e-04 eta 0:04:47
epoch [42/50] batch [100/288] time 0.103 (0.113) data 0.000 (0.004) loss 1.2604 (1.3272) teacher_loss 0.7169 (0.8035) loss_zs_kd 0.2428 (0.2111) loss_oracle 0.4221 (0.4182) acc 84.3750 (80.4062) alaph_mean 0.8643 (0.8642) alpha_val 0.8643 (0.8642) lr 1.9098e-04 eta 0:04:41
epoch [42/50] batch [120/288] time 0.097 (0.112) data 0.000 (0.003) loss 1.1448 (1.2986) teacher_loss 0.6120 (0.7772) loss_zs_kd 0.2315 (0.2071) loss_oracle 0.4171 (0.4179) acc 78.1250 (80.7812) alaph_mean 0.8644 (0.8642) alpha_val 0.8644 (0.8642) lr 1.9098e-04 eta 0:04:36
epoch [42/50] batch [140/288] time 0.098 (0.111) data 0.000 (0.003) loss 1.4259 (1.2960) teacher_loss 0.9081 (0.7739) loss_zs_kd 0.2319 (0.2085) loss_oracle 0.4019 (0.4178) acc 75.0000 (80.7143) alaph_mean 0.8645 (0.8642) alpha_val 0.8645 (0.8642) lr 1.9098e-04 eta 0:04:30
epoch [42/50] batch [160/288] time 0.101 (0.109) data 0.001 (0.002) loss 1.5418 (1.2984) teacher_loss 0.9995 (0.7763) loss_zs_kd 0.2518 (0.2098) loss_oracle 0.4164 (0.4172) acc 75.0000 (80.5859) alaph_mean 0.8645 (0.8643) alpha_val 0.8645 (0.8643) lr 1.9098e-04 eta 0:04:26
epoch [42/50] batch [180/288] time 0.092 (0.109) data 0.000 (0.002) loss 1.4009 (1.2953) teacher_loss 0.8265 (0.7738) loss_zs_kd 0.2894 (0.2095) loss_oracle 0.4298 (0.4168) acc 81.2500 (80.7118) alaph_mean 0.8646 (0.8643) alpha_val 0.8646 (0.8643) lr 1.9098e-04 eta 0:04:21
epoch [42/50] batch [200/288] time 0.103 (0.108) data 0.000 (0.002) loss 1.2673 (1.2971) teacher_loss 0.8088 (0.7769) loss_zs_kd 0.1845 (0.2090) loss_oracle 0.3663 (0.4158) acc 78.1250 (80.5938) alaph_mean 0.8647 (0.8643) alpha_val 0.8647 (0.8643) lr 1.9098e-04 eta 0:04:17
epoch [42/50] batch [220/288] time 0.105 (0.107) data 0.000 (0.002) loss 1.2746 (1.3008) teacher_loss 0.8209 (0.7798) loss_zs_kd 0.1701 (0.2094) loss_oracle 0.3687 (0.4163) acc 78.1250 (80.4545) alaph_mean 0.8647 (0.8644) alpha_val 0.8647 (0.8644) lr 1.9098e-04 eta 0:04:13
epoch [42/50] batch [240/288] time 0.105 (0.106) data 0.000 (0.002) loss 1.3882 (1.3005) teacher_loss 0.8833 (0.7804) loss_zs_kd 0.2111 (0.2087) loss_oracle 0.3993 (0.4158) acc 78.1250 (80.4557) alaph_mean 0.8648 (0.8644) alpha_val 0.8648 (0.8644) lr 1.9098e-04 eta 0:04:08
epoch [42/50] batch [260/288] time 0.101 (0.105) data 0.000 (0.002) loss 1.4369 (1.3051) teacher_loss 0.9771 (0.7849) loss_zs_kd 0.2383 (0.2093) loss_oracle 0.3406 (0.4156) acc 78.1250 (80.2885) alaph_mean 0.8649 (0.8644) alpha_val 0.8649 (0.8644) lr 1.9098e-04 eta 0:04:05
epoch [42/50] batch [280/288] time 0.089 (0.104) data 0.000 (0.001) loss 1.1887 (1.2988) teacher_loss 0.7248 (0.7807) loss_zs_kd 0.1896 (0.2086) loss_oracle 0.3691 (0.4138) acc 84.3750 (80.4688) alaph_mean 0.8649 (0.8645) alpha_val 0.8649 (0.8645) lr 1.9098e-04 eta 0:04:01
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.7%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [43/50] batch [20/288] time 0.098 (0.111) data 0.000 (0.012) loss 1.5015 (1.3542) teacher_loss 1.0013 (0.8367) loss_zs_kd 0.1872 (0.2102) loss_oracle 0.4066 (0.4124) acc 75.0000 (79.5312) alaph_mean 0.8650 (0.8650) alpha_val 0.8650 (0.8650) lr 1.5567e-04 eta 0:04:12
epoch [43/50] batch [40/288] time 0.103 (0.113) data 0.000 (0.006) loss 1.3205 (1.3341) teacher_loss 0.7478 (0.8116) loss_zs_kd 0.2510 (0.2092) loss_oracle 0.4472 (0.4178) acc 81.2500 (79.3750) alaph_mean 0.8651 (0.8650) alpha_val 0.8651 (0.8650) lr 1.5567e-04 eta 0:04:16
epoch [43/50] batch [60/288] time 0.099 (0.110) data 0.000 (0.004) loss 1.9725 (1.3570) teacher_loss 1.3795 (0.8294) loss_zs_kd 0.2497 (0.2143) loss_oracle 0.4682 (0.4205) acc 68.7500 (79.0104) alaph_mean 0.8651 (0.8650) alpha_val 0.8651 (0.8650) lr 1.5567e-04 eta 0:04:06
epoch [43/50] batch [80/288] time 0.113 (0.108) data 0.000 (0.003) loss 0.8872 (1.3097) teacher_loss 0.4928 (0.7883) loss_zs_kd 0.0996 (0.2071) loss_oracle 0.3446 (0.4179) acc 84.3750 (79.8828) alaph_mean 0.8652 (0.8651) alpha_val 0.8652 (0.8651) lr 1.5567e-04 eta 0:04:01
epoch [43/50] batch [100/288] time 0.099 (0.107) data 0.000 (0.003) loss 1.6474 (1.3118) teacher_loss 1.0180 (0.7926) loss_zs_kd 0.2517 (0.2060) loss_oracle 0.5035 (0.4161) acc 75.0000 (79.8125) alaph_mean 0.8652 (0.8651) alpha_val 0.8652 (0.8651) lr 1.5567e-04 eta 0:03:56
epoch [43/50] batch [120/288] time 0.115 (0.107) data 0.000 (0.002) loss 1.1478 (1.3140) teacher_loss 0.6458 (0.7925) loss_zs_kd 0.1786 (0.2081) loss_oracle 0.4127 (0.4174) acc 84.3750 (79.7917) alaph_mean 0.8653 (0.8651) alpha_val 0.8653 (0.8651) lr 1.5567e-04 eta 0:03:52
epoch [43/50] batch [140/288] time 0.100 (0.106) data 0.000 (0.002) loss 1.1633 (1.3134) teacher_loss 0.5944 (0.7910) loss_zs_kd 0.2300 (0.2089) loss_oracle 0.4539 (0.4180) acc 87.5000 (79.9107) alaph_mean 0.8653 (0.8652) alpha_val 0.8653 (0.8652) lr 1.5567e-04 eta 0:03:48
epoch [43/50] batch [160/288] time 0.093 (0.104) data 0.000 (0.002) loss 1.1466 (1.3091) teacher_loss 0.6531 (0.7848) loss_zs_kd 0.1740 (0.2103) loss_oracle 0.4066 (0.4191) acc 81.2500 (80.0000) alaph_mean 0.8654 (0.8652) alpha_val 0.8654 (0.8652) lr 1.5567e-04 eta 0:03:44
epoch [43/50] batch [180/288] time 0.099 (0.104) data 0.000 (0.002) loss 1.0386 (1.2985) teacher_loss 0.5139 (0.7742) loss_zs_kd 0.1762 (0.2095) loss_oracle 0.4366 (0.4196) acc 90.6250 (80.1910) alaph_mean 0.8654 (0.8652) alpha_val 0.8654 (0.8652) lr 1.5567e-04 eta 0:03:39
epoch [43/50] batch [200/288] time 0.097 (0.103) data 0.000 (0.001) loss 1.2717 (1.3015) teacher_loss 0.7105 (0.7775) loss_zs_kd 0.2206 (0.2096) loss_oracle 0.4509 (0.4192) acc 84.3750 (80.2500) alaph_mean 0.8655 (0.8652) alpha_val 0.8655 (0.8652) lr 1.5567e-04 eta 0:03:36
epoch [43/50] batch [220/288] time 0.102 (0.102) data 0.000 (0.001) loss 1.2767 (1.3028) teacher_loss 0.7077 (0.7798) loss_zs_kd 0.2065 (0.2089) loss_oracle 0.4657 (0.4185) acc 84.3750 (80.3125) alaph_mean 0.8655 (0.8653) alpha_val 0.8655 (0.8653) lr 1.5567e-04 eta 0:03:33
epoch [43/50] batch [240/288] time 0.097 (0.102) data 0.000 (0.001) loss 1.3809 (1.2993) teacher_loss 0.8555 (0.7743) loss_zs_kd 0.2881 (0.2095) loss_oracle 0.3813 (0.4203) acc 71.8750 (80.3776) alaph_mean 0.8656 (0.8653) alpha_val 0.8656 (0.8653) lr 1.5567e-04 eta 0:03:30
epoch [43/50] batch [260/288] time 0.099 (0.101) data 0.000 (0.001) loss 1.1115 (1.3006) teacher_loss 0.5637 (0.7746) loss_zs_kd 0.2167 (0.2102) loss_oracle 0.4394 (0.4209) acc 84.3750 (80.3486) alaph_mean 0.8656 (0.8653) alpha_val 0.8656 (0.8653) lr 1.5567e-04 eta 0:03:27
epoch [43/50] batch [280/288] time 0.086 (0.101) data 0.000 (0.001) loss 1.3284 (1.3004) teacher_loss 0.7814 (0.7750) loss_zs_kd 0.2310 (0.2105) loss_oracle 0.4316 (0.4202) acc 87.5000 (80.2902) alaph_mean 0.8657 (0.8653) alpha_val 0.8657 (0.8653) lr 1.5567e-04 eta 0:03:24
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,402
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.7%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [44/50] batch [20/288] time 0.095 (0.135) data 0.000 (0.013) loss 1.4601 (1.3163) teacher_loss 0.9149 (0.7809) loss_zs_kd 0.2132 (0.2155) loss_oracle 0.4386 (0.4276) acc 68.7500 (79.6875) alaph_mean 0.8657 (0.8657) alpha_val 0.8657 (0.8657) lr 1.2369e-04 eta 0:04:30
epoch [44/50] batch [40/288] time 0.095 (0.116) data 0.000 (0.007) loss 1.0558 (1.3387) teacher_loss 0.5812 (0.8096) loss_zs_kd 0.2083 (0.2134) loss_oracle 0.3705 (0.4224) acc 84.3750 (79.3750) alaph_mean 0.8658 (0.8657) alpha_val 0.8658 (0.8657) lr 1.2369e-04 eta 0:03:48
epoch [44/50] batch [60/288] time 0.097 (0.110) data 0.000 (0.005) loss 1.0936 (1.3217) teacher_loss 0.6110 (0.7900) loss_zs_kd 0.1568 (0.2163) loss_oracle 0.4043 (0.4236) acc 87.5000 (80.1562) alaph_mean 0.8658 (0.8657) alpha_val 0.8658 (0.8657) lr 1.2369e-04 eta 0:03:34
epoch [44/50] batch [80/288] time 0.103 (0.107) data 0.000 (0.004) loss 1.4824 (1.3215) teacher_loss 0.9302 (0.7894) loss_zs_kd 0.2507 (0.2155) loss_oracle 0.4269 (0.4243) acc 81.2500 (80.1562) alaph_mean 0.8658 (0.8658) alpha_val 0.8658 (0.8658) lr 1.2369e-04 eta 0:03:28
epoch [44/50] batch [100/288] time 0.093 (0.106) data 0.000 (0.003) loss 1.3426 (1.3158) teacher_loss 0.8399 (0.7858) loss_zs_kd 0.1971 (0.2143) loss_oracle 0.4042 (0.4228) acc 78.1250 (80.2812) alaph_mean 0.8659 (0.8658) alpha_val 0.8659 (0.8658) lr 1.2369e-04 eta 0:03:23
epoch [44/50] batch [120/288] time 0.096 (0.106) data 0.000 (0.002) loss 1.3995 (1.3063) teacher_loss 0.8716 (0.7763) loss_zs_kd 0.2800 (0.2135) loss_oracle 0.3879 (0.4232) acc 78.1250 (80.6771) alaph_mean 0.8659 (0.8658) alpha_val 0.8659 (0.8658) lr 1.2369e-04 eta 0:03:20
epoch [44/50] batch [140/288] time 0.102 (0.105) data 0.000 (0.002) loss 1.3293 (1.3091) teacher_loss 0.8094 (0.7789) loss_zs_kd 0.2417 (0.2135) loss_oracle 0.3990 (0.4235) acc 81.2500 (80.7589) alaph_mean 0.8660 (0.8658) alpha_val 0.8660 (0.8658) lr 1.2369e-04 eta 0:03:16
epoch [44/50] batch [160/288] time 0.096 (0.105) data 0.000 (0.002) loss 1.4777 (1.3135) teacher_loss 0.8943 (0.7842) loss_zs_kd 0.2323 (0.2144) loss_oracle 0.4673 (0.4221) acc 71.8750 (80.4492) alaph_mean 0.8660 (0.8658) alpha_val 0.8660 (0.8658) lr 1.2369e-04 eta 0:03:14
epoch [44/50] batch [180/288] time 0.111 (0.104) data 0.000 (0.002) loss 1.0674 (1.3167) teacher_loss 0.6231 (0.7893) loss_zs_kd 0.1706 (0.2152) loss_oracle 0.3591 (0.4199) acc 81.2500 (80.5208) alaph_mean 0.8661 (0.8659) alpha_val 0.8661 (0.8659) lr 1.2369e-04 eta 0:03:11
epoch [44/50] batch [200/288] time 0.112 (0.104) data 0.000 (0.002) loss 1.2069 (1.3189) teacher_loss 0.6585 (0.7901) loss_zs_kd 0.2265 (0.2166) loss_oracle 0.4351 (0.4205) acc 84.3750 (80.6406) alaph_mean 0.8661 (0.8659) alpha_val 0.8661 (0.8659) lr 1.2369e-04 eta 0:03:09
epoch [44/50] batch [220/288] time 0.103 (0.104) data 0.000 (0.001) loss 1.2890 (1.3199) teacher_loss 0.7902 (0.7920) loss_zs_kd 0.2126 (0.2160) loss_oracle 0.3925 (0.4199) acc 81.2500 (80.5966) alaph_mean 0.8662 (0.8659) alpha_val 0.8662 (0.8659) lr 1.2369e-04 eta 0:03:06
epoch [44/50] batch [240/288] time 0.098 (0.104) data 0.000 (0.001) loss 1.3189 (1.3339) teacher_loss 0.8324 (0.8055) loss_zs_kd 0.2025 (0.2159) loss_oracle 0.3853 (0.4203) acc 78.1250 (80.1953) alaph_mean 0.8662 (0.8659) alpha_val 0.8662 (0.8659) lr 1.2369e-04 eta 0:03:04
epoch [44/50] batch [260/288] time 0.097 (0.104) data 0.000 (0.001) loss 1.6859 (1.3295) teacher_loss 1.2536 (0.8008) loss_zs_kd 0.1755 (0.2151) loss_oracle 0.3445 (0.4210) acc 68.7500 (80.3966) alaph_mean 0.8663 (0.8660) alpha_val 0.8663 (0.8660) lr 1.2369e-04 eta 0:03:02
epoch [44/50] batch [280/288] time 0.113 (0.104) data 0.000 (0.001) loss 1.6278 (1.3295) teacher_loss 1.1254 (0.8009) loss_zs_kd 0.2083 (0.2154) loss_oracle 0.3982 (0.4209) acc 71.8750 (80.2344) alaph_mean 0.8663 (0.8660) alpha_val 0.8663 (0.8660) lr 1.2369e-04 eta 0:03:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,399
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.7%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [45/50] batch [20/288] time 0.105 (0.115) data 0.000 (0.014) loss 1.6446 (1.3331) teacher_loss 1.1447 (0.7988) loss_zs_kd 0.1865 (0.2160) loss_oracle 0.4067 (0.4263) acc 68.7500 (79.3750) alaph_mean 0.8664 (0.8663) alpha_val 0.8664 (0.8663) lr 9.5173e-05 eta 0:03:15
epoch [45/50] batch [40/288] time 0.097 (0.105) data 0.000 (0.007) loss 1.1370 (1.2708) teacher_loss 0.6346 (0.7481) loss_zs_kd 0.1678 (0.2133) loss_oracle 0.4184 (0.4160) acc 84.3750 (80.1562) alaph_mean 0.8664 (0.8664) alpha_val 0.8664 (0.8664) lr 9.5173e-05 eta 0:02:56
epoch [45/50] batch [60/288] time 0.106 (0.103) data 0.001 (0.005) loss 0.8564 (1.2669) teacher_loss 0.3850 (0.7475) loss_zs_kd 0.1894 (0.2073) loss_oracle 0.3767 (0.4157) acc 90.6250 (80.2604) alaph_mean 0.8664 (0.8664) alpha_val 0.8664 (0.8664) lr 9.5173e-05 eta 0:02:51
epoch [45/50] batch [80/288] time 0.102 (0.101) data 0.000 (0.004) loss 0.9623 (1.2768) teacher_loss 0.4669 (0.7538) loss_zs_kd 0.1600 (0.2084) loss_oracle 0.4154 (0.4187) acc 87.5000 (80.5078) alaph_mean 0.8664 (0.8664) alpha_val 0.8664 (0.8664) lr 9.5173e-05 eta 0:02:47
epoch [45/50] batch [100/288] time 0.094 (0.101) data 0.000 (0.003) loss 1.7559 (1.2749) teacher_loss 1.2440 (0.7511) loss_zs_kd 0.2336 (0.2091) loss_oracle 0.3951 (0.4192) acc 68.7500 (80.6875) alaph_mean 0.8665 (0.8664) alpha_val 0.8665 (0.8664) lr 9.5173e-05 eta 0:02:44
epoch [45/50] batch [120/288] time 0.102 (0.100) data 0.000 (0.003) loss 1.2343 (1.2834) teacher_loss 0.6432 (0.7605) loss_zs_kd 0.2008 (0.2113) loss_oracle 0.4907 (0.4172) acc 84.3750 (80.5990) alaph_mean 0.8665 (0.8664) alpha_val 0.8665 (0.8664) lr 9.5173e-05 eta 0:02:41
epoch [45/50] batch [140/288] time 0.094 (0.100) data 0.000 (0.002) loss 0.7462 (1.2877) teacher_loss 0.2432 (0.7636) loss_zs_kd 0.1690 (0.2105) loss_oracle 0.4185 (0.4189) acc 96.8750 (80.6696) alaph_mean 0.8665 (0.8664) alpha_val 0.8665 (0.8664) lr 9.5173e-05 eta 0:02:38
epoch [45/50] batch [160/288] time 0.094 (0.099) data 0.000 (0.002) loss 1.3332 (1.2958) teacher_loss 0.6396 (0.7688) loss_zs_kd 0.2865 (0.2132) loss_oracle 0.5503 (0.4205) acc 87.5000 (80.5469) alaph_mean 0.8666 (0.8664) alpha_val 0.8666 (0.8664) lr 9.5173e-05 eta 0:02:35
epoch [45/50] batch [180/288] time 0.100 (0.099) data 0.000 (0.002) loss 1.3834 (1.2987) teacher_loss 0.7819 (0.7692) loss_zs_kd 0.2772 (0.2142) loss_oracle 0.4629 (0.4224) acc 81.2500 (80.3646) alaph_mean 0.8666 (0.8665) alpha_val 0.8666 (0.8665) lr 9.5173e-05 eta 0:02:33
epoch [45/50] batch [200/288] time 0.106 (0.099) data 0.000 (0.002) loss 1.0303 (1.3035) teacher_loss 0.4670 (0.7732) loss_zs_kd 0.2031 (0.2142) loss_oracle 0.4618 (0.4231) acc 87.5000 (80.3125) alaph_mean 0.8666 (0.8665) alpha_val 0.8666 (0.8665) lr 9.5173e-05 eta 0:02:31
epoch [45/50] batch [220/288] time 0.092 (0.099) data 0.000 (0.002) loss 1.0950 (1.2972) teacher_loss 0.5333 (0.7673) loss_zs_kd 0.2647 (0.2142) loss_oracle 0.4293 (0.4229) acc 90.6250 (80.5256) alaph_mean 0.8666 (0.8665) alpha_val 0.8666 (0.8665) lr 9.5173e-05 eta 0:02:29
epoch [45/50] batch [240/288] time 0.095 (0.099) data 0.000 (0.001) loss 1.1811 (1.2885) teacher_loss 0.6816 (0.7590) loss_zs_kd 0.1813 (0.2133) loss_oracle 0.4089 (0.4228) acc 84.3750 (80.8724) alaph_mean 0.8667 (0.8665) alpha_val 0.8667 (0.8665) lr 9.5173e-05 eta 0:02:26
epoch [45/50] batch [260/288] time 0.101 (0.099) data 0.000 (0.001) loss 1.5350 (1.2884) teacher_loss 0.8926 (0.7574) loss_zs_kd 0.2522 (0.2139) loss_oracle 0.5163 (0.4240) acc 75.0000 (80.8894) alaph_mean 0.8667 (0.8665) alpha_val 0.8667 (0.8665) lr 9.5173e-05 eta 0:02:24
epoch [45/50] batch [280/288] time 0.085 (0.098) data 0.000 (0.001) loss 1.3639 (1.2977) teacher_loss 0.8494 (0.7660) loss_zs_kd 0.2209 (0.2145) loss_oracle 0.4041 (0.4244) acc 81.2500 (80.6473) alaph_mean 0.8667 (0.8665) alpha_val 0.8667 (0.8665) lr 9.5173e-05 eta 0:02:22
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.6%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [46/50] batch [20/288] time 0.110 (0.116) data 0.001 (0.014) loss 1.1204 (1.2960) teacher_loss 0.6269 (0.7740) loss_zs_kd 0.1842 (0.2167) loss_oracle 0.4014 (0.4136) acc 81.2500 (81.0938) alaph_mean 0.8668 (0.8668) alpha_val 0.8668 (0.8668) lr 7.0224e-05 eta 0:02:45
epoch [46/50] batch [40/288] time 0.093 (0.107) data 0.000 (0.007) loss 1.4317 (1.3283) teacher_loss 0.9173 (0.7999) loss_zs_kd 0.2308 (0.2198) loss_oracle 0.3990 (0.4185) acc 75.0000 (80.7031) alaph_mean 0.8668 (0.8668) alpha_val 0.8668 (0.8668) lr 7.0224e-05 eta 0:02:29
epoch [46/50] batch [60/288] time 0.105 (0.104) data 0.001 (0.005) loss 1.2374 (1.3360) teacher_loss 0.7360 (0.7985) loss_zs_kd 0.1838 (0.2240) loss_oracle 0.4095 (0.4255) acc 78.1250 (80.1562) alaph_mean 0.8668 (0.8668) alpha_val 0.8668 (0.8668) lr 7.0224e-05 eta 0:02:23
epoch [46/50] batch [80/288] time 0.096 (0.103) data 0.000 (0.004) loss 1.1375 (1.3133) teacher_loss 0.6825 (0.7798) loss_zs_kd 0.1637 (0.2205) loss_oracle 0.3731 (0.4232) acc 87.5000 (80.5469) alaph_mean 0.8669 (0.8668) alpha_val 0.8669 (0.8668) lr 7.0224e-05 eta 0:02:19
epoch [46/50] batch [100/288] time 0.093 (0.101) data 0.000 (0.003) loss 1.1851 (1.3047) teacher_loss 0.5807 (0.7731) loss_zs_kd 0.2563 (0.2185) loss_oracle 0.4762 (0.4224) acc 84.3750 (80.9375) alaph_mean 0.8669 (0.8668) alpha_val 0.8669 (0.8668) lr 7.0224e-05 eta 0:02:15
epoch [46/50] batch [120/288] time 0.101 (0.101) data 0.000 (0.002) loss 0.8497 (1.3045) teacher_loss 0.3904 (0.7755) loss_zs_kd 0.1947 (0.2187) loss_oracle 0.3620 (0.4197) acc 93.7500 (80.7552) alaph_mean 0.8669 (0.8668) alpha_val 0.8669 (0.8668) lr 7.0224e-05 eta 0:02:13
epoch [46/50] batch [140/288] time 0.098 (0.101) data 0.000 (0.002) loss 1.2632 (1.3025) teacher_loss 0.7330 (0.7727) loss_zs_kd 0.2324 (0.2184) loss_oracle 0.4140 (0.4206) acc 78.1250 (80.7812) alaph_mean 0.8669 (0.8668) alpha_val 0.8669 (0.8668) lr 7.0224e-05 eta 0:02:11
epoch [46/50] batch [160/288] time 0.098 (0.101) data 0.000 (0.002) loss 1.3410 (1.2967) teacher_loss 0.6442 (0.7662) loss_zs_kd 0.2848 (0.2178) loss_oracle 0.5544 (0.4216) acc 84.3750 (80.9570) alaph_mean 0.8669 (0.8668) alpha_val 0.8669 (0.8668) lr 7.0224e-05 eta 0:02:09
epoch [46/50] batch [180/288] time 0.095 (0.101) data 0.000 (0.002) loss 1.4216 (1.3017) teacher_loss 0.8399 (0.7713) loss_zs_kd 0.2467 (0.2180) loss_oracle 0.4584 (0.4215) acc 84.3750 (80.7986) alaph_mean 0.8670 (0.8669) alpha_val 0.8670 (0.8669) lr 7.0224e-05 eta 0:02:07
epoch [46/50] batch [200/288] time 0.102 (0.101) data 0.000 (0.002) loss 1.4775 (1.3111) teacher_loss 1.0440 (0.7818) loss_zs_kd 0.1422 (0.2177) loss_oracle 0.3624 (0.4204) acc 75.0000 (80.5156) alaph_mean 0.8670 (0.8669) alpha_val 0.8670 (0.8669) lr 7.0224e-05 eta 0:02:05
epoch [46/50] batch [220/288] time 0.093 (0.101) data 0.000 (0.001) loss 1.2584 (1.3121) teacher_loss 0.6889 (0.7833) loss_zs_kd 0.1969 (0.2168) loss_oracle 0.4711 (0.4204) acc 84.3750 (80.4972) alaph_mean 0.8670 (0.8669) alpha_val 0.8670 (0.8669) lr 7.0224e-05 eta 0:02:03
epoch [46/50] batch [240/288] time 0.097 (0.100) data 0.000 (0.001) loss 1.2303 (1.3136) teacher_loss 0.7372 (0.7834) loss_zs_kd 0.2228 (0.2173) loss_oracle 0.3817 (0.4215) acc 84.3750 (80.4818) alaph_mean 0.8670 (0.8669) alpha_val 0.8670 (0.8669) lr 7.0224e-05 eta 0:02:00
epoch [46/50] batch [260/288] time 0.099 (0.100) data 0.000 (0.001) loss 1.1838 (1.3114) teacher_loss 0.6930 (0.7801) loss_zs_kd 0.1525 (0.2172) loss_oracle 0.4145 (0.4227) acc 87.5000 (80.5529) alaph_mean 0.8671 (0.8669) alpha_val 0.8671 (0.8669) lr 7.0224e-05 eta 0:01:58
epoch [46/50] batch [280/288] time 0.085 (0.100) data 0.000 (0.001) loss 1.4562 (1.3151) teacher_loss 0.9076 (0.7835) loss_zs_kd 0.2149 (0.2174) loss_oracle 0.4412 (0.4229) acc 78.1250 (80.5022) alaph_mean 0.8671 (0.8669) alpha_val 0.8671 (0.8669) lr 7.0224e-05 eta 0:01:55
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,404
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.9%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [47/50] batch [20/288] time 0.110 (0.118) data 0.000 (0.014) loss 1.5365 (1.3280) teacher_loss 1.0175 (0.7946) loss_zs_kd 0.2052 (0.2224) loss_oracle 0.4164 (0.4222) acc 71.8750 (81.0938) alaph_mean 0.8671 (0.8671) alpha_val 0.8671 (0.8671) lr 4.8943e-05 eta 0:02:13
epoch [47/50] batch [40/288] time 0.085 (0.106) data 0.000 (0.007) loss 1.5751 (1.3210) teacher_loss 1.0897 (0.7967) loss_zs_kd 0.1883 (0.2128) loss_oracle 0.3913 (0.4179) acc 75.0000 (80.4688) alaph_mean 0.8671 (0.8671) alpha_val 0.8671 (0.8671) lr 4.8943e-05 eta 0:01:57
epoch [47/50] batch [60/288] time 0.103 (0.101) data 0.000 (0.005) loss 1.9178 (1.3320) teacher_loss 1.3638 (0.8101) loss_zs_kd 0.2134 (0.2099) loss_oracle 0.4473 (0.4170) acc 68.7500 (80.1562) alaph_mean 0.8672 (0.8671) alpha_val 0.8672 (0.8671) lr 4.8943e-05 eta 0:01:49
epoch [47/50] batch [80/288] time 0.088 (0.100) data 0.000 (0.004) loss 1.3094 (1.3289) teacher_loss 0.8025 (0.8040) loss_zs_kd 0.1843 (0.2119) loss_oracle 0.4148 (0.4190) acc 84.3750 (80.1953) alaph_mean 0.8672 (0.8671) alpha_val 0.8672 (0.8671) lr 4.8943e-05 eta 0:01:46
epoch [47/50] batch [100/288] time 0.084 (0.098) data 0.000 (0.003) loss 0.9681 (1.3190) teacher_loss 0.5725 (0.7899) loss_zs_kd 0.1432 (0.2156) loss_oracle 0.3239 (0.4214) acc 84.3750 (80.5000) alaph_mean 0.8672 (0.8671) alpha_val 0.8672 (0.8671) lr 4.8943e-05 eta 0:01:42
epoch [47/50] batch [120/288] time 0.093 (0.097) data 0.000 (0.002) loss 1.6366 (1.3066) teacher_loss 1.1773 (0.7786) loss_zs_kd 0.1809 (0.2154) loss_oracle 0.3688 (0.4203) acc 75.0000 (80.7031) alaph_mean 0.8672 (0.8671) alpha_val 0.8672 (0.8671) lr 4.8943e-05 eta 0:01:40
epoch [47/50] batch [140/288] time 0.102 (0.098) data 0.000 (0.002) loss 1.1507 (1.3083) teacher_loss 0.6052 (0.7781) loss_zs_kd 0.2589 (0.2180) loss_oracle 0.4161 (0.4213) acc 87.5000 (80.6696) alaph_mean 0.8672 (0.8672) alpha_val 0.8672 (0.8672) lr 4.8943e-05 eta 0:01:38
epoch [47/50] batch [160/288] time 0.097 (0.098) data 0.000 (0.002) loss 1.3034 (1.3107) teacher_loss 0.6251 (0.7815) loss_zs_kd 0.3120 (0.2180) loss_oracle 0.5222 (0.4202) acc 90.6250 (80.4883) alaph_mean 0.8672 (0.8672) alpha_val 0.8672 (0.8672) lr 4.8943e-05 eta 0:01:37
epoch [47/50] batch [180/288] time 0.100 (0.098) data 0.000 (0.002) loss 1.2094 (1.3131) teacher_loss 0.6379 (0.7828) loss_zs_kd 0.2363 (0.2181) loss_oracle 0.4533 (0.4213) acc 84.3750 (80.3125) alaph_mean 0.8672 (0.8672) alpha_val 0.8672 (0.8672) lr 4.8943e-05 eta 0:01:35
epoch [47/50] batch [200/288] time 0.113 (0.099) data 0.000 (0.002) loss 0.7568 (1.3121) teacher_loss 0.2921 (0.7829) loss_zs_kd 0.1434 (0.2170) loss_oracle 0.3930 (0.4207) acc 90.6250 (80.3750) alaph_mean 0.8673 (0.8672) alpha_val 0.8673 (0.8672) lr 4.8943e-05 eta 0:01:33
epoch [47/50] batch [220/288] time 0.102 (0.099) data 0.000 (0.001) loss 1.1281 (1.3155) teacher_loss 0.5356 (0.7861) loss_zs_kd 0.2151 (0.2176) loss_oracle 0.4849 (0.4205) acc 87.5000 (80.2983) alaph_mean 0.8673 (0.8672) alpha_val 0.8673 (0.8672) lr 4.8943e-05 eta 0:01:32
epoch [47/50] batch [240/288] time 0.109 (0.099) data 0.000 (0.001) loss 1.0206 (1.3156) teacher_loss 0.5518 (0.7868) loss_zs_kd 0.1899 (0.2176) loss_oracle 0.3738 (0.4200) acc 90.6250 (80.3255) alaph_mean 0.8673 (0.8672) alpha_val 0.8673 (0.8672) lr 4.8943e-05 eta 0:01:30
epoch [47/50] batch [260/288] time 0.111 (0.099) data 0.000 (0.001) loss 1.8624 (1.3171) teacher_loss 1.2261 (0.7874) loss_zs_kd 0.2886 (0.2174) loss_oracle 0.4920 (0.4210) acc 71.8750 (80.3125) alaph_mean 0.8673 (0.8672) alpha_val 0.8673 (0.8672) lr 4.8943e-05 eta 0:01:28
epoch [47/50] batch [280/288] time 0.107 (0.100) data 0.000 (0.001) loss 0.9500 (1.3201) teacher_loss 0.4334 (0.7897) loss_zs_kd 0.1863 (0.2180) loss_oracle 0.4234 (0.4214) acc 93.7500 (80.3237) alaph_mean 0.8673 (0.8672) alpha_val 0.8673 (0.8672) lr 4.8943e-05 eta 0:01:26
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,406
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.7%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [48/50] batch [20/288] time 0.095 (0.109) data 0.000 (0.014) loss 1.3260 (1.3800) teacher_loss 0.7357 (0.8352) loss_zs_kd 0.2221 (0.2251) loss_oracle 0.4793 (0.4323) acc 78.1250 (79.2188) alaph_mean 0.8674 (0.8673) alpha_val 0.8674 (0.8673) lr 3.1417e-05 eta 0:01:32
epoch [48/50] batch [40/288] time 0.084 (0.100) data 0.000 (0.007) loss 1.6247 (1.3480) teacher_loss 1.0608 (0.8136) loss_zs_kd 0.2782 (0.2196) loss_oracle 0.4248 (0.4246) acc 78.1250 (79.9219) alaph_mean 0.8674 (0.8674) alpha_val 0.8674 (0.8674) lr 3.1417e-05 eta 0:01:22
epoch [48/50] batch [60/288] time 0.101 (0.098) data 0.000 (0.005) loss 1.4788 (1.3374) teacher_loss 0.9295 (0.7943) loss_zs_kd 0.1931 (0.2252) loss_oracle 0.4527 (0.4306) acc 71.8750 (80.2604) alaph_mean 0.8674 (0.8674) alpha_val 0.8674 (0.8674) lr 3.1417e-05 eta 0:01:19
epoch [48/50] batch [80/288] time 0.095 (0.098) data 0.000 (0.004) loss 1.4126 (1.3327) teacher_loss 0.8692 (0.7920) loss_zs_kd 0.2116 (0.2213) loss_oracle 0.4376 (0.4300) acc 81.2500 (80.0781) alaph_mean 0.8674 (0.8674) alpha_val 0.8674 (0.8674) lr 3.1417e-05 eta 0:01:17
epoch [48/50] batch [100/288] time 0.094 (0.098) data 0.000 (0.003) loss 1.2674 (1.3175) teacher_loss 0.7737 (0.7840) loss_zs_kd 0.1638 (0.2177) loss_oracle 0.4118 (0.4247) acc 75.0000 (80.2812) alaph_mean 0.8674 (0.8674) alpha_val 0.8674 (0.8674) lr 3.1417e-05 eta 0:01:14
epoch [48/50] batch [120/288] time 0.103 (0.099) data 0.000 (0.002) loss 1.4989 (1.3201) teacher_loss 0.9309 (0.7904) loss_zs_kd 0.3021 (0.2171) loss_oracle 0.4169 (0.4212) acc 71.8750 (80.1823) alaph_mean 0.8674 (0.8674) alpha_val 0.8674 (0.8674) lr 3.1417e-05 eta 0:01:13
epoch [48/50] batch [140/288] time 0.109 (0.099) data 0.000 (0.002) loss 1.2920 (1.3199) teacher_loss 0.7804 (0.7900) loss_zs_kd 0.2382 (0.2176) loss_oracle 0.3925 (0.4211) acc 78.1250 (80.0670) alaph_mean 0.8674 (0.8674) alpha_val 0.8674 (0.8674) lr 3.1417e-05 eta 0:01:11
epoch [48/50] batch [160/288] time 0.098 (0.100) data 0.000 (0.002) loss 1.2634 (1.3273) teacher_loss 0.7499 (0.7965) loss_zs_kd 0.1972 (0.2188) loss_oracle 0.4149 (0.4214) acc 78.1250 (79.9023) alaph_mean 0.8674 (0.8674) alpha_val 0.8674 (0.8674) lr 3.1417e-05 eta 0:01:10
epoch [48/50] batch [180/288] time 0.100 (0.100) data 0.000 (0.002) loss 1.1425 (1.3186) teacher_loss 0.6166 (0.7858) loss_zs_kd 0.2256 (0.2198) loss_oracle 0.4130 (0.4229) acc 87.5000 (80.1736) alaph_mean 0.8674 (0.8674) alpha_val 0.8674 (0.8674) lr 3.1417e-05 eta 0:01:08
epoch [48/50] batch [200/288] time 0.110 (0.100) data 0.000 (0.002) loss 1.1443 (1.3196) teacher_loss 0.6109 (0.7860) loss_zs_kd 0.2465 (0.2202) loss_oracle 0.4102 (0.4235) acc 90.6250 (80.2188) alaph_mean 0.8675 (0.8674) alpha_val 0.8675 (0.8674) lr 3.1417e-05 eta 0:01:06
epoch [48/50] batch [220/288] time 0.103 (0.100) data 0.000 (0.001) loss 1.2185 (1.3183) teacher_loss 0.5916 (0.7833) loss_zs_kd 0.2495 (0.2204) loss_oracle 0.5022 (0.4248) acc 84.3750 (80.3267) alaph_mean 0.8675 (0.8674) alpha_val 0.8675 (0.8674) lr 3.1417e-05 eta 0:01:04
epoch [48/50] batch [240/288] time 0.101 (0.101) data 0.001 (0.001) loss 1.3556 (1.3196) teacher_loss 0.8160 (0.7836) loss_zs_kd 0.2040 (0.2207) loss_oracle 0.4376 (0.4256) acc 81.2500 (80.3385) alaph_mean 0.8675 (0.8674) alpha_val 0.8675 (0.8674) lr 3.1417e-05 eta 0:01:02
epoch [48/50] batch [260/288] time 0.095 (0.101) data 0.000 (0.001) loss 1.3732 (1.3170) teacher_loss 0.7886 (0.7819) loss_zs_kd 0.2448 (0.2201) loss_oracle 0.4622 (0.4250) acc 81.2500 (80.4447) alaph_mean 0.8675 (0.8674) alpha_val 0.8675 (0.8674) lr 3.1417e-05 eta 0:01:00
epoch [48/50] batch [280/288] time 0.107 (0.102) data 0.000 (0.001) loss 1.2476 (1.3148) teacher_loss 0.6968 (0.7792) loss_zs_kd 0.2220 (0.2201) loss_oracle 0.4398 (0.4256) acc 90.6250 (80.5804) alaph_mean 0.8675 (0.8674) alpha_val 0.8675 (0.8674) lr 3.1417e-05 eta 0:00:59
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,402
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.6%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [49/50] batch [20/288] time 0.093 (0.115) data 0.000 (0.014) loss 1.4002 (1.2529) teacher_loss 0.8508 (0.7157) loss_zs_kd 0.1955 (0.2182) loss_oracle 0.4516 (0.4281) acc 75.0000 (81.4062) alaph_mean 0.8675 (0.8675) alpha_val 0.8675 (0.8675) lr 1.7713e-05 eta 0:01:04
epoch [49/50] batch [40/288] time 0.095 (0.105) data 0.000 (0.007) loss 1.7157 (1.2752) teacher_loss 1.2127 (0.7379) loss_zs_kd 0.2114 (0.2130) loss_oracle 0.3973 (0.4308) acc 65.6250 (81.4844) alaph_mean 0.8675 (0.8675) alpha_val 0.8675 (0.8675) lr 1.7713e-05 eta 0:00:56
epoch [49/50] batch [60/288] time 0.094 (0.102) data 0.000 (0.005) loss 0.9243 (1.2779) teacher_loss 0.4273 (0.7437) loss_zs_kd 0.1718 (0.2154) loss_oracle 0.4111 (0.4265) acc 87.5000 (81.1458) alaph_mean 0.8675 (0.8675) alpha_val 0.8675 (0.8675) lr 1.7713e-05 eta 0:00:52
epoch [49/50] batch [80/288] time 0.100 (0.100) data 0.000 (0.004) loss 1.4441 (1.2713) teacher_loss 0.9115 (0.7391) loss_zs_kd 0.2217 (0.2149) loss_oracle 0.4217 (0.4248) acc 81.2500 (82.0703) alaph_mean 0.8675 (0.8675) alpha_val 0.8675 (0.8675) lr 1.7713e-05 eta 0:00:49
epoch [49/50] batch [100/288] time 0.103 (0.099) data 0.000 (0.003) loss 0.9258 (1.2872) teacher_loss 0.4427 (0.7524) loss_zs_kd 0.2085 (0.2182) loss_oracle 0.3789 (0.4257) acc 93.7500 (81.6562) alaph_mean 0.8675 (0.8675) alpha_val 0.8675 (0.8675) lr 1.7713e-05 eta 0:00:47
epoch [49/50] batch [120/288] time 0.093 (0.099) data 0.000 (0.003) loss 1.3655 (1.2915) teacher_loss 0.9097 (0.7588) loss_zs_kd 0.2267 (0.2160) loss_oracle 0.3425 (0.4247) acc 84.3750 (81.3542) alaph_mean 0.8675 (0.8675) alpha_val 0.8675 (0.8675) lr 1.7713e-05 eta 0:00:45
epoch [49/50] batch [140/288] time 0.098 (0.099) data 0.000 (0.002) loss 1.6038 (1.2923) teacher_loss 1.0559 (0.7593) loss_zs_kd 0.2693 (0.2172) loss_oracle 0.4133 (0.4243) acc 75.0000 (81.3170) alaph_mean 0.8675 (0.8675) alpha_val 0.8675 (0.8675) lr 1.7713e-05 eta 0:00:43
epoch [49/50] batch [160/288] time 0.098 (0.099) data 0.000 (0.002) loss 1.1727 (1.2909) teacher_loss 0.6645 (0.7565) loss_zs_kd 0.1993 (0.2161) loss_oracle 0.4086 (0.4263) acc 87.5000 (81.5234) alaph_mean 0.8675 (0.8675) alpha_val 0.8675 (0.8675) lr 1.7713e-05 eta 0:00:41
epoch [49/50] batch [180/288] time 0.095 (0.099) data 0.000 (0.002) loss 1.5696 (1.2863) teacher_loss 0.9186 (0.7524) loss_zs_kd 0.3278 (0.2156) loss_oracle 0.4871 (0.4261) acc 78.1250 (81.7361) alaph_mean 0.8675 (0.8675) alpha_val 0.8675 (0.8675) lr 1.7713e-05 eta 0:00:39
epoch [49/50] batch [200/288] time 0.092 (0.099) data 0.000 (0.002) loss 1.9276 (1.3007) teacher_loss 1.3932 (0.7664) loss_zs_kd 0.2072 (0.2161) loss_oracle 0.4308 (0.4262) acc 75.0000 (81.3906) alaph_mean 0.8676 (0.8675) alpha_val 0.8676 (0.8675) lr 1.7713e-05 eta 0:00:37
epoch [49/50] batch [220/288] time 0.093 (0.098) data 0.000 (0.002) loss 1.5020 (1.3082) teacher_loss 0.9656 (0.7731) loss_zs_kd 0.2336 (0.2172) loss_oracle 0.4196 (0.4266) acc 71.8750 (81.2074) alaph_mean 0.8676 (0.8675) alpha_val 0.8676 (0.8675) lr 1.7713e-05 eta 0:00:34
epoch [49/50] batch [240/288] time 0.099 (0.098) data 0.000 (0.001) loss 0.9911 (1.3046) teacher_loss 0.5477 (0.7701) loss_zs_kd 0.1760 (0.2173) loss_oracle 0.3554 (0.4258) acc 84.3750 (81.2370) alaph_mean 0.8676 (0.8675) alpha_val 0.8676 (0.8675) lr 1.7713e-05 eta 0:00:32
epoch [49/50] batch [260/288] time 0.096 (0.099) data 0.000 (0.001) loss 1.4674 (1.2972) teacher_loss 0.8965 (0.7649) loss_zs_kd 0.2599 (0.2163) loss_oracle 0.4409 (0.4242) acc 81.2500 (81.4784) alaph_mean 0.8676 (0.8675) alpha_val 0.8676 (0.8675) lr 1.7713e-05 eta 0:00:31
epoch [49/50] batch [280/288] time 0.085 (0.099) data 0.000 (0.001) loss 0.9301 (1.2920) teacher_loss 0.4221 (0.7591) loss_zs_kd 0.1721 (0.2156) loss_oracle 0.4220 (0.4252) acc 87.5000 (81.4732) alaph_mean 0.8676 (0.8675) alpha_val 0.8676 (0.8675) lr 1.7713e-05 eta 0:00:29
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,404
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.6%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
epoch [50/50] batch [20/288] time 0.097 (0.115) data 0.000 (0.013) loss 1.4714 (1.2234) teacher_loss 0.9867 (0.6958) loss_zs_kd 0.1887 (0.2138) loss_oracle 0.3904 (0.4207) acc 78.1250 (82.3438) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:30
epoch [50/50] batch [40/288] time 0.092 (0.105) data 0.000 (0.007) loss 1.4140 (1.2875) teacher_loss 0.8777 (0.7584) loss_zs_kd 0.2245 (0.2181) loss_oracle 0.4240 (0.4201) acc 84.3750 (81.5625) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:25
epoch [50/50] batch [60/288] time 0.111 (0.102) data 0.000 (0.005) loss 1.2344 (1.2791) teacher_loss 0.6776 (0.7563) loss_zs_kd 0.1908 (0.2109) loss_oracle 0.4615 (0.4173) acc 84.3750 (81.6667) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:23
epoch [50/50] batch [80/288] time 0.096 (0.102) data 0.000 (0.004) loss 1.2112 (1.2919) teacher_loss 0.7212 (0.7605) loss_zs_kd 0.2053 (0.2149) loss_oracle 0.3873 (0.4239) acc 87.5000 (81.2109) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:21
epoch [50/50] batch [100/288] time 0.111 (0.103) data 0.000 (0.003) loss 1.5239 (1.3104) teacher_loss 0.8623 (0.7763) loss_zs_kd 0.2543 (0.2174) loss_oracle 0.5344 (0.4254) acc 75.0000 (80.6562) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:19
epoch [50/50] batch [120/288] time 0.096 (0.103) data 0.000 (0.002) loss 1.1466 (1.3109) teacher_loss 0.6411 (0.7778) loss_zs_kd 0.1945 (0.2159) loss_oracle 0.4083 (0.4252) acc 81.2500 (80.5990) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:17
epoch [50/50] batch [140/288] time 0.111 (0.102) data 0.000 (0.002) loss 1.4448 (1.3064) teacher_loss 0.8177 (0.7717) loss_zs_kd 0.2739 (0.2193) loss_oracle 0.4901 (0.4250) acc 84.3750 (80.9598) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:15
epoch [50/50] batch [160/288] time 0.095 (0.102) data 0.000 (0.002) loss 1.1373 (1.2957) teacher_loss 0.7164 (0.7624) loss_zs_kd 0.1557 (0.2176) loss_oracle 0.3431 (0.4246) acc 78.1250 (81.0938) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [180/288] time 0.102 (0.102) data 0.000 (0.002) loss 1.2416 (1.3095) teacher_loss 0.7589 (0.7746) loss_zs_kd 0.2360 (0.2187) loss_oracle 0.3647 (0.4255) acc 84.3750 (80.9549) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:11
epoch [50/50] batch [200/288] time 0.097 (0.102) data 0.000 (0.002) loss 1.5081 (1.3149) teacher_loss 1.0146 (0.7796) loss_zs_kd 0.1731 (0.2195) loss_oracle 0.4069 (0.4256) acc 65.6250 (80.8281) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [220/288] time 0.105 (0.102) data 0.000 (0.001) loss 1.1502 (1.3129) teacher_loss 0.5476 (0.7771) loss_zs_kd 0.2014 (0.2195) loss_oracle 0.5019 (0.4261) acc 78.1250 (80.7386) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [240/288] time 0.103 (0.103) data 0.000 (0.001) loss 1.1334 (1.3208) teacher_loss 0.6735 (0.7850) loss_zs_kd 0.1680 (0.2187) loss_oracle 0.3759 (0.4264) acc 84.3750 (80.5729) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [260/288] time 0.097 (0.103) data 0.000 (0.001) loss 1.0187 (1.3258) teacher_loss 0.5064 (0.7898) loss_zs_kd 0.2116 (0.2194) loss_oracle 0.4065 (0.4263) acc 84.3750 (80.4567) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [280/288] time 0.107 (0.103) data 0.000 (0.001) loss 1.5670 (1.3212) teacher_loss 1.0974 (0.7853) loss_zs_kd 0.2470 (0.2190) loss_oracle 0.3461 (0.4264) acc 65.6250 (80.5134) alaph_mean 0.8676 (0.8676) alpha_val 0.8676 (0.8676) lr 7.8853e-06 eta 0:00:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,405
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.7%
******* Domain a best val acc:      86.8%, epoch: 17 *******
******* Domain a best val test acc: 83.1%, epoch: 17 *******
******* Domain a best test acc:     83.9%, epoch: 31 *******
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model.pth.tar-50
Finish the whole training
Elapsed: 0:29:25
[Info] Hyperparameters saved to: icml/multi-dg/oracle/14_learnablealpha/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/hyperparameters.json
