Loading trainer: TRIP
Loading dataset: SPG_VLCS
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -----------------------------
Dataset    SPG_VLCS
Source     ['caltech', 'labelme', 'sun']
Target     ['pascal']
# classes  5
# train_x  5,147
# val      2,206
# test     3,376
---------  -----------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
alpha_logit                                        1
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,196
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/160] time 0.074 (0.123) data 0.000 (0.021) loss 1.0121 (0.8798) teacher_loss 0.8037 (0.6802) loss_zs_kd 0.0002 (0.0001) loss_oracle 0.2082 (0.1995) acc 71.8750 (75.4688) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:16:19
epoch [1/50] batch [40/160] time 0.085 (0.102) data 0.000 (0.010) loss 0.8894 (0.8701) teacher_loss 0.6803 (0.6688) loss_zs_kd 0.0012 (0.0004) loss_oracle 0.2085 (0.2011) acc 75.0000 (75.4688) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:13:31
epoch [1/50] batch [60/160] time 0.071 (0.095) data 0.000 (0.007) loss 0.7379 (0.8976) teacher_loss 0.5374 (0.6968) loss_zs_kd 0.0024 (0.0009) loss_oracle 0.1993 (0.2004) acc 78.1250 (75.0000) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:12:34
epoch [1/50] batch [80/160] time 0.078 (0.092) data 0.000 (0.005) loss 1.1215 (0.9086) teacher_loss 0.9208 (0.7074) loss_zs_kd 0.0030 (0.0014) loss_oracle 0.1991 (0.2005) acc 75.0000 (74.2188) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:12:06
epoch [1/50] batch [100/160] time 0.083 (0.091) data 0.000 (0.004) loss 0.8161 (0.9060) teacher_loss 0.6270 (0.7043) loss_zs_kd 0.0041 (0.0019) loss_oracle 0.1870 (0.2007) acc 81.2500 (74.3750) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:11:55
epoch [1/50] batch [120/160] time 0.086 (0.089) data 0.000 (0.004) loss 0.9126 (0.8964) teacher_loss 0.7156 (0.6946) loss_zs_kd 0.0061 (0.0025) loss_oracle 0.1939 (0.2006) acc 71.8750 (74.6875) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:11:41
epoch [1/50] batch [140/160] time 0.096 (0.089) data 0.000 (0.003) loss 0.7233 (0.8944) teacher_loss 0.5094 (0.6912) loss_zs_kd 0.0066 (0.0031) loss_oracle 0.2106 (0.2016) acc 87.5000 (74.7098) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 1.0000e-05 eta 0:11:38
epoch [1/50] batch [160/160] time 0.075 (0.090) data 0.000 (0.003) loss 1.1771 (0.8985) teacher_loss 0.9798 (0.6949) loss_zs_kd 0.0104 (0.0037) loss_oracle 0.1921 (0.2017) acc 65.6250 (74.5898) alaph_mean 0.5000 (0.5000) alpha_val 0.5000 (0.5000) lr 2.0000e-03 eta 0:11:41
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,729
* accuracy: 78.4%
* error: 21.6%
* macro_f1: 80.2%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,924
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 87.3%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      78.4%, epoch: 1 *******
******* Domain p best val test acc: 86.6%, epoch: 1 *******
******* Domain p best test acc:     86.6%, epoch: 1 *******
epoch [2/50] batch [20/160] time 0.070 (0.099) data 0.000 (0.019) loss 0.9803 (0.9138) teacher_loss 0.6673 (0.6202) loss_zs_kd 0.0800 (0.0622) loss_oracle 0.2729 (0.2624) acc 75.0000 (78.1250) alaph_mean 0.5002 (0.5001) alpha_val 0.5002 (0.5001) lr 2.0000e-03 eta 0:12:53
epoch [2/50] batch [40/160] time 0.061 (0.088) data 0.000 (0.009) loss 1.0800 (0.9239) teacher_loss 0.7280 (0.5892) loss_zs_kd 0.0525 (0.0832) loss_oracle 0.3258 (0.2931) acc 75.0000 (79.3750) alaph_mean 0.5008 (0.5003) alpha_val 0.5008 (0.5003) lr 2.0000e-03 eta 0:11:26
epoch [2/50] batch [60/160] time 0.079 (0.087) data 0.000 (0.006) loss 0.8592 (0.9405) teacher_loss 0.4475 (0.5872) loss_zs_kd 0.1435 (0.0895) loss_oracle 0.3400 (0.3085) acc 81.2500 (80.3125) alaph_mean 0.5013 (0.5005) alpha_val 0.5013 (0.5005) lr 2.0000e-03 eta 0:11:13
epoch [2/50] batch [80/160] time 0.081 (0.085) data 0.000 (0.005) loss 0.7812 (0.9444) teacher_loss 0.4234 (0.5736) loss_zs_kd 0.1257 (0.0982) loss_oracle 0.2950 (0.3216) acc 87.5000 (80.6641) alaph_mean 0.5019 (0.5008) alpha_val 0.5019 (0.5008) lr 2.0000e-03 eta 0:10:59
epoch [2/50] batch [100/160] time 0.096 (0.085) data 0.000 (0.004) loss 0.7372 (0.9391) teacher_loss 0.3162 (0.5664) loss_zs_kd 0.1661 (0.1026) loss_oracle 0.3379 (0.3214) acc 96.8750 (81.1875) alaph_mean 0.5027 (0.5011) alpha_val 0.5027 (0.5011) lr 2.0000e-03 eta 0:10:58
epoch [2/50] batch [120/160] time 0.089 (0.085) data 0.000 (0.003) loss 0.8530 (0.9565) teacher_loss 0.5524 (0.5665) loss_zs_kd 0.0691 (0.1051) loss_oracle 0.2660 (0.3374) acc 87.5000 (80.8333) alaph_mean 0.5028 (0.5014) alpha_val 0.5028 (0.5014) lr 2.0000e-03 eta 0:10:53
epoch [2/50] batch [140/160] time 0.088 (0.085) data 0.000 (0.003) loss 0.7882 (0.9546) teacher_loss 0.2809 (0.5509) loss_zs_kd 0.0769 (0.1031) loss_oracle 0.4689 (0.3521) acc 87.5000 (80.9821) alaph_mean 0.5030 (0.5016) alpha_val 0.5030 (0.5016) lr 2.0000e-03 eta 0:10:54
epoch [2/50] batch [160/160] time 0.074 (0.084) data 0.000 (0.003) loss 0.9348 (0.9543) teacher_loss 0.5071 (0.5408) loss_zs_kd 0.1248 (0.1029) loss_oracle 0.3653 (0.3620) acc 81.2500 (81.3086) alaph_mean 0.5034 (0.5018) alpha_val 0.5034 (0.5018) lr 1.9980e-03 eta 0:10:46
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,819
* accuracy: 82.5%
* error: 17.5%
* macro_f1: 84.0%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,971
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 88.7%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      82.5%, epoch: 2 *******
******* Domain p best val test acc: 88.0%, epoch: 2 *******
******* Domain p best test acc:     88.0%, epoch: 2 *******
epoch [3/50] batch [20/160] time 0.072 (0.095) data 0.000 (0.013) loss 0.8731 (0.9446) teacher_loss 0.4100 (0.4920) loss_zs_kd 0.1099 (0.0982) loss_oracle 0.4081 (0.4036) acc 84.3750 (82.8125) alaph_mean 0.5040 (0.5037) alpha_val 0.5040 (0.5037) lr 1.9980e-03 eta 0:12:06
epoch [3/50] batch [40/160] time 0.090 (0.088) data 0.000 (0.007) loss 0.8065 (0.9646) teacher_loss 0.3637 (0.5041) loss_zs_kd 0.0787 (0.1012) loss_oracle 0.4035 (0.4099) acc 84.3750 (83.0469) alaph_mean 0.5045 (0.5040) alpha_val 0.5045 (0.5040) lr 1.9980e-03 eta 0:11:15
epoch [3/50] batch [60/160] time 0.088 (0.087) data 0.000 (0.005) loss 1.1243 (0.9982) teacher_loss 0.6300 (0.5200) loss_zs_kd 0.0797 (0.0997) loss_oracle 0.4545 (0.4283) acc 78.1250 (81.9271) alaph_mean 0.5051 (0.5043) alpha_val 0.5051 (0.5043) lr 1.9980e-03 eta 0:11:01
epoch [3/50] batch [80/160] time 0.063 (0.090) data 0.000 (0.004) loss 1.0585 (1.0065) teacher_loss 0.5735 (0.5198) loss_zs_kd 0.1389 (0.0982) loss_oracle 0.4156 (0.4376) acc 81.2500 (81.5625) alaph_mean 0.5058 (0.5046) alpha_val 0.5058 (0.5046) lr 1.9980e-03 eta 0:11:21
epoch [3/50] batch [100/160] time 0.078 (0.087) data 0.000 (0.003) loss 0.9341 (0.9899) teacher_loss 0.4414 (0.5019) loss_zs_kd 0.0556 (0.0979) loss_oracle 0.4649 (0.4390) acc 84.3750 (82.4062) alaph_mean 0.5069 (0.5050) alpha_val 0.5069 (0.5050) lr 1.9980e-03 eta 0:11:01
epoch [3/50] batch [120/160] time 0.100 (0.087) data 0.000 (0.002) loss 0.9000 (0.9937) teacher_loss 0.3374 (0.4961) loss_zs_kd 0.1293 (0.0984) loss_oracle 0.4980 (0.4484) acc 93.7500 (82.5781) alaph_mean 0.5076 (0.5053) alpha_val 0.5076 (0.5053) lr 1.9980e-03 eta 0:11:00
epoch [3/50] batch [140/160] time 0.098 (0.088) data 0.000 (0.002) loss 0.8847 (0.9978) teacher_loss 0.3526 (0.4951) loss_zs_kd 0.0877 (0.0969) loss_oracle 0.4883 (0.4543) acc 81.2500 (82.2098) alaph_mean 0.5085 (0.5057) alpha_val 0.5085 (0.5057) lr 1.9980e-03 eta 0:11:02
epoch [3/50] batch [160/160] time 0.076 (0.087) data 0.000 (0.002) loss 1.6210 (1.0047) teacher_loss 1.1213 (0.4997) loss_zs_kd 0.1152 (0.0964) loss_oracle 0.4420 (0.4568) acc 65.6250 (82.1094) alaph_mean 0.5095 (0.5062) alpha_val 0.5095 (0.5062) lr 1.9921e-03 eta 0:10:50
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,823
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 84.6%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,973
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 89.1%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      82.6%, epoch: 3 *******
******* Domain p best val test acc: 88.1%, epoch: 3 *******
******* Domain p best test acc:     88.1%, epoch: 3 *******
epoch [4/50] batch [20/160] time 0.076 (0.105) data 0.000 (0.019) loss 1.0667 (1.0690) teacher_loss 0.4276 (0.5137) loss_zs_kd 0.0718 (0.0968) loss_oracle 0.6031 (0.5070) acc 81.2500 (81.8750) alaph_mean 0.5102 (0.5098) alpha_val 0.5102 (0.5098) lr 1.9921e-03 eta 0:13:06
epoch [4/50] batch [40/160] time 0.086 (0.094) data 0.001 (0.009) loss 0.8397 (0.9905) teacher_loss 0.2697 (0.4511) loss_zs_kd 0.1273 (0.0955) loss_oracle 0.5063 (0.4916) acc 96.8750 (83.7500) alaph_mean 0.5106 (0.5101) alpha_val 0.5106 (0.5101) lr 1.9921e-03 eta 0:11:45
epoch [4/50] batch [60/160] time 0.086 (0.093) data 0.001 (0.006) loss 0.9185 (0.9752) teacher_loss 0.4575 (0.4657) loss_zs_kd 0.1089 (0.0986) loss_oracle 0.4065 (0.4603) acc 81.2500 (83.3854) alaph_mean 0.5114 (0.5104) alpha_val 0.5114 (0.5104) lr 1.9921e-03 eta 0:11:33
epoch [4/50] batch [80/160] time 0.080 (0.090) data 0.000 (0.005) loss 0.9748 (0.9976) teacher_loss 0.4911 (0.4769) loss_zs_kd 0.0796 (0.1006) loss_oracle 0.4439 (0.4704) acc 71.8750 (82.6953) alaph_mean 0.5122 (0.5108) alpha_val 0.5122 (0.5108) lr 1.9921e-03 eta 0:11:11
epoch [4/50] batch [100/160] time 0.083 (0.088) data 0.000 (0.004) loss 0.8794 (1.0055) teacher_loss 0.4293 (0.4904) loss_zs_kd 0.0840 (0.1013) loss_oracle 0.4081 (0.4645) acc 87.5000 (82.4375) alaph_mean 0.5131 (0.5111) alpha_val 0.5131 (0.5111) lr 1.9921e-03 eta 0:10:54
epoch [4/50] batch [120/160] time 0.078 (0.087) data 0.000 (0.003) loss 1.2581 (1.0002) teacher_loss 0.6948 (0.4942) loss_zs_kd 0.0801 (0.0983) loss_oracle 0.5233 (0.4569) acc 81.2500 (82.4219) alaph_mean 0.5140 (0.5115) alpha_val 0.5140 (0.5115) lr 1.9921e-03 eta 0:10:41
epoch [4/50] batch [140/160] time 0.067 (0.085) data 0.000 (0.003) loss 0.9145 (1.0013) teacher_loss 0.5059 (0.4963) loss_zs_kd 0.0607 (0.0978) loss_oracle 0.3782 (0.4562) acc 81.2500 (82.2991) alaph_mean 0.5147 (0.5119) alpha_val 0.5147 (0.5119) lr 1.9921e-03 eta 0:10:28
epoch [4/50] batch [160/160] time 0.067 (0.083) data 0.000 (0.003) loss 0.8070 (0.9960) teacher_loss 0.2718 (0.4912) loss_zs_kd 0.0844 (0.0990) loss_oracle 0.4931 (0.4553) acc 93.7500 (82.5977) alaph_mean 0.5155 (0.5123) alpha_val 0.5155 (0.5123) lr 1.9823e-03 eta 0:10:12
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,828
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 84.5%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,983
* accuracy: 88.4%
* error: 11.6%
* macro_f1: 89.3%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      82.9%, epoch: 4 *******
******* Domain p best val test acc: 88.4%, epoch: 4 *******
******* Domain p best test acc:     88.4%, epoch: 4 *******
epoch [5/50] batch [20/160] time 0.080 (0.099) data 0.000 (0.020) loss 0.7766 (0.9996) teacher_loss 0.2930 (0.5021) loss_zs_kd 0.0831 (0.1006) loss_oracle 0.4421 (0.4471) acc 90.6250 (81.8750) alaph_mean 0.5162 (0.5159) alpha_val 0.5162 (0.5159) lr 1.9823e-03 eta 0:12:07
epoch [5/50] batch [40/160] time 0.081 (0.090) data 0.000 (0.010) loss 0.7812 (0.9815) teacher_loss 0.3247 (0.4912) loss_zs_kd 0.0832 (0.0983) loss_oracle 0.4149 (0.4412) acc 90.6250 (82.6562) alaph_mean 0.5169 (0.5162) alpha_val 0.5169 (0.5162) lr 1.9823e-03 eta 0:10:55
epoch [5/50] batch [60/160] time 0.076 (0.087) data 0.001 (0.007) loss 1.3237 (0.9875) teacher_loss 0.6872 (0.4848) loss_zs_kd 0.1122 (0.1027) loss_oracle 0.5805 (0.4514) acc 75.0000 (82.7083) alaph_mean 0.5180 (0.5166) alpha_val 0.5180 (0.5166) lr 1.9823e-03 eta 0:10:32
epoch [5/50] batch [80/160] time 0.083 (0.085) data 0.000 (0.005) loss 1.0539 (1.0041) teacher_loss 0.5286 (0.4933) loss_zs_kd 0.0769 (0.1003) loss_oracle 0.4868 (0.4606) acc 84.3750 (83.0859) alaph_mean 0.5187 (0.5171) alpha_val 0.5187 (0.5171) lr 1.9823e-03 eta 0:10:20
epoch [5/50] batch [100/160] time 0.078 (0.084) data 0.000 (0.004) loss 0.9038 (0.9832) teacher_loss 0.4487 (0.4807) loss_zs_kd 0.0863 (0.0969) loss_oracle 0.4119 (0.4540) acc 81.2500 (83.0000) alaph_mean 0.5197 (0.5175) alpha_val 0.5197 (0.5175) lr 1.9823e-03 eta 0:10:12
epoch [5/50] batch [120/160] time 0.087 (0.084) data 0.000 (0.004) loss 1.0262 (0.9791) teacher_loss 0.5469 (0.4874) loss_zs_kd 0.1017 (0.0987) loss_oracle 0.4285 (0.4423) acc 75.0000 (82.8385) alaph_mean 0.5209 (0.5180) alpha_val 0.5209 (0.5180) lr 1.9823e-03 eta 0:10:08
epoch [5/50] batch [140/160] time 0.094 (0.084) data 0.000 (0.003) loss 1.3701 (0.9888) teacher_loss 0.7828 (0.4856) loss_zs_kd 0.1119 (0.0992) loss_oracle 0.5314 (0.4537) acc 75.0000 (82.7679) alaph_mean 0.5217 (0.5184) alpha_val 0.5217 (0.5184) lr 1.9823e-03 eta 0:10:06
epoch [5/50] batch [160/160] time 0.063 (0.082) data 0.000 (0.003) loss 1.2397 (0.9885) teacher_loss 0.6915 (0.4813) loss_zs_kd 0.1123 (0.1007) loss_oracle 0.4921 (0.4569) acc 71.8750 (82.7148) alaph_mean 0.5223 (0.5189) alpha_val 0.5223 (0.5189) lr 1.9686e-03 eta 0:09:49
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,841
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 84.8%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,980
* accuracy: 88.3%
* error: 11.7%
* macro_f1: 89.1%
******* Domain p best val acc:      83.5%, epoch: 5 *******
******* Domain p best val test acc: 88.3%, epoch: 5 *******
******* Domain p best test acc:     88.4%, epoch: 4 *******
epoch [6/50] batch [20/160] time 0.083 (0.102) data 0.000 (0.018) loss 0.9330 (0.9733) teacher_loss 0.3738 (0.4671) loss_zs_kd 0.1050 (0.1027) loss_oracle 0.5067 (0.4548) acc 84.3750 (83.1250) alaph_mean 0.5235 (0.5229) alpha_val 0.5235 (0.5229) lr 1.9686e-03 eta 0:12:12
epoch [6/50] batch [40/160] time 0.084 (0.092) data 0.000 (0.009) loss 0.8098 (0.9597) teacher_loss 0.3790 (0.4502) loss_zs_kd 0.0911 (0.1056) loss_oracle 0.3852 (0.4567) acc 84.3750 (83.5156) alaph_mean 0.5243 (0.5234) alpha_val 0.5243 (0.5234) lr 1.9686e-03 eta 0:11:00
epoch [6/50] batch [60/160] time 0.087 (0.089) data 0.001 (0.006) loss 0.8308 (0.9499) teacher_loss 0.3356 (0.4536) loss_zs_kd 0.1205 (0.1028) loss_oracle 0.4349 (0.4449) acc 87.5000 (84.1146) alaph_mean 0.5253 (0.5239) alpha_val 0.5253 (0.5239) lr 1.9686e-03 eta 0:10:35
epoch [6/50] batch [80/160] time 0.090 (0.088) data 0.000 (0.005) loss 0.8730 (0.9633) teacher_loss 0.4700 (0.4663) loss_zs_kd 0.1016 (0.1029) loss_oracle 0.3522 (0.4455) acc 78.1250 (83.5938) alaph_mean 0.5265 (0.5244) alpha_val 0.5265 (0.5244) lr 1.9686e-03 eta 0:10:27
epoch [6/50] batch [100/160] time 0.074 (0.086) data 0.000 (0.004) loss 1.0020 (0.9596) teacher_loss 0.5272 (0.4643) loss_zs_kd 0.0764 (0.1019) loss_oracle 0.4366 (0.4443) acc 81.2500 (83.6250) alaph_mean 0.5275 (0.5249) alpha_val 0.5275 (0.5249) lr 1.9686e-03 eta 0:10:13
epoch [6/50] batch [120/160] time 0.078 (0.085) data 0.001 (0.003) loss 1.0878 (0.9627) teacher_loss 0.5075 (0.4633) loss_zs_kd 0.1345 (0.1040) loss_oracle 0.5131 (0.4473) acc 81.2500 (83.8021) alaph_mean 0.5286 (0.5255) alpha_val 0.5286 (0.5255) lr 1.9686e-03 eta 0:10:01
epoch [6/50] batch [140/160] time 0.081 (0.085) data 0.000 (0.003) loss 0.9447 (0.9585) teacher_loss 0.4294 (0.4558) loss_zs_kd 0.0638 (0.1020) loss_oracle 0.4834 (0.4517) acc 75.0000 (84.0402) alaph_mean 0.5291 (0.5259) alpha_val 0.5291 (0.5259) lr 1.9686e-03 eta 0:09:57
epoch [6/50] batch [160/160] time 0.076 (0.084) data 0.001 (0.003) loss 1.1826 (0.9721) teacher_loss 0.6156 (0.4635) loss_zs_kd 0.1138 (0.1024) loss_oracle 0.5101 (0.4574) acc 81.2500 (83.6133) alaph_mean 0.5293 (0.5264) alpha_val 0.5293 (0.5264) lr 1.9511e-03 eta 0:09:49
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,832
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 84.6%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,964
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 89.0%
******* Domain p best val acc:      83.5%, epoch: 5 *******
******* Domain p best val test acc: 88.3%, epoch: 5 *******
******* Domain p best test acc:     88.4%, epoch: 4 *******
epoch [7/50] batch [20/160] time 0.087 (0.103) data 0.000 (0.015) loss 1.0512 (0.8976) teacher_loss 0.5392 (0.4176) loss_zs_kd 0.0971 (0.0978) loss_oracle 0.4635 (0.4311) acc 71.8750 (85.6250) alaph_mean 0.5299 (0.5296) alpha_val 0.5299 (0.5296) lr 1.9511e-03 eta 0:12:03
epoch [7/50] batch [40/160] time 0.083 (0.094) data 0.000 (0.008) loss 1.0976 (0.9527) teacher_loss 0.5351 (0.4508) loss_zs_kd 0.1069 (0.1029) loss_oracle 0.5090 (0.4504) acc 84.3750 (84.3750) alaph_mean 0.5306 (0.5299) alpha_val 0.5306 (0.5299) lr 1.9511e-03 eta 0:10:55
epoch [7/50] batch [60/160] time 0.081 (0.091) data 0.001 (0.005) loss 1.0783 (0.9499) teacher_loss 0.5125 (0.4509) loss_zs_kd 0.1004 (0.1031) loss_oracle 0.5156 (0.4475) acc 84.3750 (84.8958) alaph_mean 0.5313 (0.5303) alpha_val 0.5313 (0.5303) lr 1.9511e-03 eta 0:10:32
epoch [7/50] batch [80/160] time 0.079 (0.089) data 0.000 (0.004) loss 1.1373 (0.9684) teacher_loss 0.5421 (0.4562) loss_zs_kd 0.1297 (0.1048) loss_oracle 0.5303 (0.4597) acc 87.5000 (84.3359) alaph_mean 0.5319 (0.5306) alpha_val 0.5319 (0.5306) lr 1.9511e-03 eta 0:10:21
epoch [7/50] batch [100/160] time 0.095 (0.089) data 0.000 (0.003) loss 0.9051 (0.9794) teacher_loss 0.3225 (0.4635) loss_zs_kd 0.0970 (0.1041) loss_oracle 0.5342 (0.4639) acc 90.6250 (83.7500) alaph_mean 0.5327 (0.5309) alpha_val 0.5327 (0.5309) lr 1.9511e-03 eta 0:10:17
epoch [7/50] batch [120/160] time 0.087 (0.088) data 0.000 (0.003) loss 0.8542 (0.9813) teacher_loss 0.3274 (0.4707) loss_zs_kd 0.0535 (0.1041) loss_oracle 0.5000 (0.4585) acc 90.6250 (83.3594) alaph_mean 0.5343 (0.5314) alpha_val 0.5343 (0.5314) lr 1.9511e-03 eta 0:10:10
epoch [7/50] batch [140/160] time 0.083 (0.088) data 0.000 (0.002) loss 0.9422 (0.9728) teacher_loss 0.4537 (0.4599) loss_zs_kd 0.0849 (0.1025) loss_oracle 0.4461 (0.4617) acc 84.3750 (83.7054) alaph_mean 0.5352 (0.5319) alpha_val 0.5352 (0.5319) lr 1.9511e-03 eta 0:10:03
epoch [7/50] batch [160/160] time 0.070 (0.086) data 0.000 (0.002) loss 1.1805 (0.9766) teacher_loss 0.6587 (0.4636) loss_zs_kd 0.1377 (0.1019) loss_oracle 0.4530 (0.4620) acc 75.0000 (83.7305) alaph_mean 0.5360 (0.5323) alpha_val 0.5360 (0.5323) lr 1.9298e-03 eta 0:09:49
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,823
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 84.4%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,980
* accuracy: 88.3%
* error: 11.7%
* macro_f1: 89.2%
******* Domain p best val acc:      83.5%, epoch: 5 *******
******* Domain p best val test acc: 88.3%, epoch: 5 *******
******* Domain p best test acc:     88.4%, epoch: 4 *******
epoch [8/50] batch [20/160] time 0.087 (0.104) data 0.000 (0.016) loss 1.0435 (0.9621) teacher_loss 0.5096 (0.4644) loss_zs_kd 0.1100 (0.1029) loss_oracle 0.4789 (0.4463) acc 90.6250 (84.5312) alaph_mean 0.5367 (0.5364) alpha_val 0.5367 (0.5364) lr 1.9298e-03 eta 0:11:51
epoch [8/50] batch [40/160] time 0.092 (0.095) data 0.000 (0.008) loss 1.1391 (0.9819) teacher_loss 0.5342 (0.4676) loss_zs_kd 0.1123 (0.1033) loss_oracle 0.5488 (0.4626) acc 71.8750 (83.3594) alaph_mean 0.5374 (0.5367) alpha_val 0.5374 (0.5367) lr 1.9298e-03 eta 0:10:46
epoch [8/50] batch [60/160] time 0.091 (0.091) data 0.001 (0.005) loss 0.9549 (0.9764) teacher_loss 0.4735 (0.4509) loss_zs_kd 0.1366 (0.1060) loss_oracle 0.4131 (0.4726) acc 75.0000 (83.8542) alaph_mean 0.5381 (0.5371) alpha_val 0.5381 (0.5371) lr 1.9298e-03 eta 0:10:18
epoch [8/50] batch [80/160] time 0.086 (0.090) data 0.001 (0.004) loss 0.7976 (0.9647) teacher_loss 0.2512 (0.4451) loss_zs_kd 0.1107 (0.1074) loss_oracle 0.4911 (0.4660) acc 93.7500 (84.1016) alaph_mean 0.5396 (0.5375) alpha_val 0.5396 (0.5375) lr 1.9298e-03 eta 0:10:10
epoch [8/50] batch [100/160] time 0.084 (0.089) data 0.000 (0.003) loss 0.9734 (0.9682) teacher_loss 0.4433 (0.4483) loss_zs_kd 0.0884 (0.1058) loss_oracle 0.4859 (0.4670) acc 87.5000 (84.0625) alaph_mean 0.5407 (0.5381) alpha_val 0.5407 (0.5381) lr 1.9298e-03 eta 0:10:01
epoch [8/50] batch [120/160] time 0.157 (0.090) data 0.001 (0.003) loss 0.9541 (0.9667) teacher_loss 0.3678 (0.4483) loss_zs_kd 0.1397 (0.1051) loss_oracle 0.5165 (0.4659) acc 93.7500 (84.2969) alaph_mean 0.5414 (0.5386) alpha_val 0.5414 (0.5386) lr 1.9298e-03 eta 0:10:08
epoch [8/50] batch [140/160] time 0.085 (0.090) data 0.000 (0.003) loss 0.9201 (0.9664) teacher_loss 0.3875 (0.4475) loss_zs_kd 0.1210 (0.1052) loss_oracle 0.4721 (0.4663) acc 84.3750 (84.2857) alaph_mean 0.5421 (0.5390) alpha_val 0.5421 (0.5390) lr 1.9298e-03 eta 0:10:07
epoch [8/50] batch [160/160] time 0.073 (0.089) data 0.000 (0.002) loss 0.9987 (0.9655) teacher_loss 0.4106 (0.4481) loss_zs_kd 0.0751 (0.1047) loss_oracle 0.5506 (0.4651) acc 81.2500 (84.2383) alaph_mean 0.5431 (0.5395) alpha_val 0.5431 (0.5395) lr 1.9048e-03 eta 0:09:55
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,829
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 84.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,983
* accuracy: 88.4%
* error: 11.6%
* macro_f1: 89.3%
******* Domain p best val acc:      83.5%, epoch: 5 *******
******* Domain p best val test acc: 88.3%, epoch: 5 *******
******* Domain p best test acc:     88.4%, epoch: 4 *******
epoch [9/50] batch [20/160] time 0.091 (0.090) data 0.000 (0.011) loss 0.7514 (0.9199) teacher_loss 0.3375 (0.4270) loss_zs_kd 0.0911 (0.0988) loss_oracle 0.3684 (0.4435) acc 84.3750 (83.9062) alaph_mean 0.5439 (0.5435) alpha_val 0.5439 (0.5435) lr 1.9048e-03 eta 0:10:06
epoch [9/50] batch [40/160] time 0.080 (0.086) data 0.000 (0.006) loss 0.9509 (0.9311) teacher_loss 0.4241 (0.4308) loss_zs_kd 0.1061 (0.1011) loss_oracle 0.4738 (0.4498) acc 84.3750 (84.8438) alaph_mean 0.5450 (0.5440) alpha_val 0.5450 (0.5440) lr 1.9048e-03 eta 0:09:33
epoch [9/50] batch [60/160] time 0.072 (0.085) data 0.000 (0.004) loss 0.9239 (0.9537) teacher_loss 0.4225 (0.4522) loss_zs_kd 0.1017 (0.1050) loss_oracle 0.4505 (0.4490) acc 87.5000 (84.0625) alaph_mean 0.5460 (0.5445) alpha_val 0.5460 (0.5445) lr 1.9048e-03 eta 0:09:23
epoch [9/50] batch [80/160] time 0.086 (0.084) data 0.000 (0.003) loss 0.9222 (0.9516) teacher_loss 0.3669 (0.4465) loss_zs_kd 0.1022 (0.1068) loss_oracle 0.5043 (0.4517) acc 90.6250 (84.5312) alaph_mean 0.5469 (0.5450) alpha_val 0.5469 (0.5450) lr 1.9048e-03 eta 0:09:19
epoch [9/50] batch [100/160] time 0.077 (0.084) data 0.000 (0.002) loss 0.9298 (0.9471) teacher_loss 0.4370 (0.4398) loss_zs_kd 0.1003 (0.1072) loss_oracle 0.4426 (0.4536) acc 93.7500 (84.7500) alaph_mean 0.5477 (0.5455) alpha_val 0.5477 (0.5455) lr 1.9048e-03 eta 0:09:19
epoch [9/50] batch [120/160] time 0.093 (0.085) data 0.001 (0.002) loss 1.1726 (0.9546) teacher_loss 0.6135 (0.4473) loss_zs_kd 0.0958 (0.1057) loss_oracle 0.5112 (0.4545) acc 71.8750 (84.6094) alaph_mean 0.5485 (0.5459) alpha_val 0.5485 (0.5459) lr 1.9048e-03 eta 0:09:18
epoch [9/50] batch [140/160] time 0.085 (0.085) data 0.000 (0.002) loss 0.9114 (0.9553) teacher_loss 0.3667 (0.4439) loss_zs_kd 0.1803 (0.1058) loss_oracle 0.4546 (0.4585) acc 93.7500 (84.8661) alaph_mean 0.5495 (0.5463) alpha_val 0.5495 (0.5463) lr 1.9048e-03 eta 0:09:18
epoch [9/50] batch [160/160] time 0.073 (0.084) data 0.000 (0.002) loss 0.8481 (0.9504) teacher_loss 0.3993 (0.4422) loss_zs_kd 0.0953 (0.1063) loss_oracle 0.4011 (0.4550) acc 84.3750 (84.8828) alaph_mean 0.5504 (0.5468) alpha_val 0.5504 (0.5468) lr 1.8763e-03 eta 0:09:10
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,827
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 84.3%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,978
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 89.1%
******* Domain p best val acc:      83.5%, epoch: 5 *******
******* Domain p best val test acc: 88.3%, epoch: 5 *******
******* Domain p best test acc:     88.4%, epoch: 4 *******
epoch [10/50] batch [20/160] time 0.085 (0.101) data 0.000 (0.016) loss 0.8236 (0.9349) teacher_loss 0.3090 (0.4588) loss_zs_kd 0.0826 (0.0985) loss_oracle 0.4733 (0.4268) acc 93.7500 (83.4375) alaph_mean 0.5516 (0.5510) alpha_val 0.5516 (0.5510) lr 1.8763e-03 eta 0:11:01
epoch [10/50] batch [40/160] time 0.080 (0.093) data 0.001 (0.008) loss 0.9795 (0.9281) teacher_loss 0.4404 (0.4382) loss_zs_kd 0.1103 (0.1030) loss_oracle 0.4840 (0.4384) acc 84.3750 (84.6094) alaph_mean 0.5523 (0.5515) alpha_val 0.5523 (0.5515) lr 1.8763e-03 eta 0:10:03
epoch [10/50] batch [60/160] time 0.092 (0.091) data 0.001 (0.005) loss 0.8501 (0.9453) teacher_loss 0.3318 (0.4462) loss_zs_kd 0.1195 (0.1062) loss_oracle 0.4586 (0.4459) acc 93.7500 (84.3229) alaph_mean 0.5532 (0.5519) alpha_val 0.5532 (0.5519) lr 1.8763e-03 eta 0:09:50
epoch [10/50] batch [80/160] time 0.086 (0.095) data 0.000 (0.004) loss 1.0915 (0.9474) teacher_loss 0.6101 (0.4469) loss_zs_kd 0.0954 (0.1068) loss_oracle 0.4337 (0.4471) acc 81.2500 (84.4531) alaph_mean 0.5546 (0.5524) alpha_val 0.5546 (0.5524) lr 1.8763e-03 eta 0:10:15
epoch [10/50] batch [100/160] time 0.091 (0.094) data 0.001 (0.003) loss 0.9364 (0.9516) teacher_loss 0.4729 (0.4437) loss_zs_kd 0.1518 (0.1070) loss_oracle 0.3876 (0.4544) acc 84.3750 (84.3438) alaph_mean 0.5554 (0.5529) alpha_val 0.5554 (0.5529) lr 1.8763e-03 eta 0:10:05
epoch [10/50] batch [120/160] time 0.090 (0.093) data 0.001 (0.003) loss 1.1805 (0.9556) teacher_loss 0.6894 (0.4462) loss_zs_kd 0.1596 (0.1076) loss_oracle 0.4112 (0.4556) acc 65.6250 (84.2448) alaph_mean 0.5563 (0.5534) alpha_val 0.5563 (0.5534) lr 1.8763e-03 eta 0:09:57
epoch [10/50] batch [140/160] time 0.100 (0.092) data 0.001 (0.003) loss 0.9138 (0.9544) teacher_loss 0.3755 (0.4407) loss_zs_kd 0.1024 (0.1082) loss_oracle 0.4871 (0.4596) acc 93.7500 (84.4643) alaph_mean 0.5574 (0.5539) alpha_val 0.5574 (0.5539) lr 1.8763e-03 eta 0:09:48
epoch [10/50] batch [160/160] time 0.074 (0.090) data 0.000 (0.002) loss 1.1216 (0.9708) teacher_loss 0.6212 (0.4539) loss_zs_kd 0.1048 (0.1091) loss_oracle 0.4480 (0.4624) acc 75.0000 (83.9258) alaph_mean 0.5586 (0.5544) alpha_val 0.5586 (0.5544) lr 1.8443e-03 eta 0:09:36
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,841
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 84.6%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,938
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 88.4%
******* Domain p best val acc:      83.5%, epoch: 5 *******
******* Domain p best val test acc: 88.3%, epoch: 5 *******
******* Domain p best test acc:     88.4%, epoch: 4 *******
epoch [11/50] batch [20/160] time 0.079 (0.099) data 0.000 (0.015) loss 0.8181 (0.9296) teacher_loss 0.3344 (0.4568) loss_zs_kd 0.0876 (0.0764) loss_oracle 0.4398 (0.4346) acc 87.5000 (84.2188) alaph_mean 0.5596 (0.5592) alpha_val 0.5596 (0.5592) lr 1.8443e-03 eta 0:10:33
epoch [11/50] batch [40/160] time 0.078 (0.090) data 0.000 (0.008) loss 0.8156 (0.9496) teacher_loss 0.3328 (0.4650) loss_zs_kd 0.0814 (0.0828) loss_oracle 0.4420 (0.4432) acc 90.6250 (83.7500) alaph_mean 0.5604 (0.5596) alpha_val 0.5604 (0.5596) lr 1.8443e-03 eta 0:09:32
epoch [11/50] batch [60/160] time 0.081 (0.088) data 0.000 (0.005) loss 0.8716 (0.9577) teacher_loss 0.3856 (0.4671) loss_zs_kd 0.0934 (0.0866) loss_oracle 0.4393 (0.4473) acc 84.3750 (84.0104) alaph_mean 0.5610 (0.5600) alpha_val 0.5610 (0.5600) lr 1.8443e-03 eta 0:09:17
epoch [11/50] batch [80/160] time 0.081 (0.087) data 0.000 (0.004) loss 1.0053 (0.9609) teacher_loss 0.4715 (0.4668) loss_zs_kd 0.1035 (0.0912) loss_oracle 0.4820 (0.4485) acc 84.3750 (84.2188) alaph_mean 0.5620 (0.5604) alpha_val 0.5620 (0.5604) lr 1.8443e-03 eta 0:09:10
epoch [11/50] batch [100/160] time 0.082 (0.087) data 0.001 (0.003) loss 0.8584 (0.9446) teacher_loss 0.3780 (0.4535) loss_zs_kd 0.1172 (0.0928) loss_oracle 0.4218 (0.4447) acc 90.6250 (84.5312) alaph_mean 0.5630 (0.5608) alpha_val 0.5630 (0.5608) lr 1.8443e-03 eta 0:09:09
epoch [11/50] batch [120/160] time 0.091 (0.087) data 0.001 (0.003) loss 0.9693 (0.9407) teacher_loss 0.4622 (0.4504) loss_zs_kd 0.0979 (0.0937) loss_oracle 0.4582 (0.4434) acc 87.5000 (84.5312) alaph_mean 0.5637 (0.5612) alpha_val 0.5637 (0.5612) lr 1.8443e-03 eta 0:09:05
epoch [11/50] batch [140/160] time 0.088 (0.087) data 0.000 (0.002) loss 0.8673 (0.9404) teacher_loss 0.4131 (0.4502) loss_zs_kd 0.1028 (0.0936) loss_oracle 0.4027 (0.4434) acc 87.5000 (84.6205) alaph_mean 0.5646 (0.5616) alpha_val 0.5646 (0.5616) lr 1.8443e-03 eta 0:09:03
epoch [11/50] batch [160/160] time 0.077 (0.086) data 0.001 (0.002) loss 0.7825 (0.9374) teacher_loss 0.3292 (0.4463) loss_zs_kd 0.1135 (0.0952) loss_oracle 0.3965 (0.4435) acc 90.6250 (84.8633) alaph_mean 0.5655 (0.5621) alpha_val 0.5655 (0.5621) lr 1.8090e-03 eta 0:08:54
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,841
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 84.8%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,954
* accuracy: 87.5%
* error: 12.5%
* macro_f1: 88.6%
******* Domain p best val acc:      83.5%, epoch: 5 *******
******* Domain p best val test acc: 88.3%, epoch: 5 *******
******* Domain p best test acc:     88.4%, epoch: 4 *******
epoch [12/50] batch [20/160] time 0.087 (0.116) data 0.000 (0.015) loss 0.9898 (0.9315) teacher_loss 0.3581 (0.4136) loss_zs_kd 0.1209 (0.1108) loss_oracle 0.5713 (0.4626) acc 90.6250 (85.4688) alaph_mean 0.5664 (0.5660) alpha_val 0.5664 (0.5660) lr 1.8090e-03 eta 0:12:01
epoch [12/50] batch [40/160] time 0.089 (0.097) data 0.002 (0.008) loss 0.9769 (0.9271) teacher_loss 0.4489 (0.3899) loss_zs_kd 0.0999 (0.1130) loss_oracle 0.4782 (0.4807) acc 87.5000 (86.7969) alaph_mean 0.5667 (0.5663) alpha_val 0.5667 (0.5663) lr 1.8090e-03 eta 0:10:02
epoch [12/50] batch [60/160] time 0.080 (0.091) data 0.001 (0.005) loss 0.9366 (0.9584) teacher_loss 0.4518 (0.4182) loss_zs_kd 0.1041 (0.1145) loss_oracle 0.4328 (0.4830) acc 81.2500 (85.5729) alaph_mean 0.5675 (0.5666) alpha_val 0.5675 (0.5666) lr 1.8090e-03 eta 0:09:24
epoch [12/50] batch [80/160] time 0.075 (0.088) data 0.000 (0.004) loss 0.8171 (0.9606) teacher_loss 0.3232 (0.4280) loss_zs_kd 0.1113 (0.1110) loss_oracle 0.4383 (0.4771) acc 84.3750 (84.9219) alaph_mean 0.5681 (0.5669) alpha_val 0.5681 (0.5669) lr 1.8090e-03 eta 0:09:03
epoch [12/50] batch [100/160] time 0.072 (0.085) data 0.000 (0.003) loss 0.8372 (0.9570) teacher_loss 0.3287 (0.4324) loss_zs_kd 0.0933 (0.1110) loss_oracle 0.4618 (0.4691) acc 84.3750 (85.0000) alaph_mean 0.5688 (0.5672) alpha_val 0.5688 (0.5672) lr 1.8090e-03 eta 0:08:40
epoch [12/50] batch [120/160] time 0.069 (0.083) data 0.000 (0.003) loss 0.8830 (0.9513) teacher_loss 0.4127 (0.4368) loss_zs_kd 0.0583 (0.1071) loss_oracle 0.4412 (0.4609) acc 87.5000 (84.6354) alaph_mean 0.5697 (0.5676) alpha_val 0.5697 (0.5676) lr 1.8090e-03 eta 0:08:27
epoch [12/50] batch [140/160] time 0.081 (0.081) data 0.000 (0.002) loss 0.7691 (0.9491) teacher_loss 0.3105 (0.4382) loss_zs_kd 0.0854 (0.1047) loss_oracle 0.4159 (0.4585) acc 90.6250 (84.6652) alaph_mean 0.5703 (0.5679) alpha_val 0.5703 (0.5679) lr 1.8090e-03 eta 0:08:16
epoch [12/50] batch [160/160] time 0.078 (0.080) data 0.000 (0.002) loss 0.9513 (0.9491) teacher_loss 0.4985 (0.4378) loss_zs_kd 0.0868 (0.1049) loss_oracle 0.4094 (0.4588) acc 81.2500 (84.6484) alaph_mean 0.5710 (0.5682) alpha_val 0.5710 (0.5682) lr 1.7705e-03 eta 0:08:07
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,846
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 85.1%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,988
* accuracy: 88.5%
* error: 11.5%
* macro_f1: 89.5%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      83.7%, epoch: 12 *******
******* Domain p best val test acc: 88.5%, epoch: 12 *******
******* Domain p best test acc:     88.5%, epoch: 12 *******
epoch [13/50] batch [20/160] time 0.080 (0.090) data 0.000 (0.013) loss 0.6999 (0.9777) teacher_loss 0.3059 (0.4593) loss_zs_kd 0.0683 (0.1103) loss_oracle 0.3598 (0.4633) acc 87.5000 (84.3750) alaph_mean 0.5718 (0.5714) alpha_val 0.5718 (0.5714) lr 1.7705e-03 eta 0:09:04
epoch [13/50] batch [40/160] time 0.078 (0.084) data 0.000 (0.007) loss 0.8806 (0.9579) teacher_loss 0.3747 (0.4511) loss_zs_kd 0.0728 (0.1058) loss_oracle 0.4695 (0.4539) acc 93.7500 (84.3750) alaph_mean 0.5725 (0.5718) alpha_val 0.5725 (0.5718) lr 1.7705e-03 eta 0:08:27
epoch [13/50] batch [60/160] time 0.090 (0.084) data 0.001 (0.005) loss 1.0538 (0.9476) teacher_loss 0.6270 (0.4474) loss_zs_kd 0.0678 (0.1043) loss_oracle 0.3929 (0.4481) acc 78.1250 (84.7396) alaph_mean 0.5732 (0.5722) alpha_val 0.5732 (0.5722) lr 1.7705e-03 eta 0:08:24
epoch [13/50] batch [80/160] time 0.086 (0.085) data 0.001 (0.004) loss 0.7605 (0.9409) teacher_loss 0.3294 (0.4457) loss_zs_kd 0.1176 (0.1030) loss_oracle 0.3724 (0.4437) acc 84.3750 (84.5312) alaph_mean 0.5739 (0.5725) alpha_val 0.5739 (0.5725) lr 1.7705e-03 eta 0:08:31
epoch [13/50] batch [100/160] time 0.080 (0.086) data 0.000 (0.003) loss 0.7458 (0.9381) teacher_loss 0.2966 (0.4462) loss_zs_kd 0.0690 (0.1039) loss_oracle 0.4148 (0.4399) acc 90.6250 (84.4062) alaph_mean 0.5747 (0.5729) alpha_val 0.5747 (0.5729) lr 1.7705e-03 eta 0:08:34
epoch [13/50] batch [120/160] time 0.081 (0.086) data 0.000 (0.002) loss 0.8421 (0.9404) teacher_loss 0.3719 (0.4445) loss_zs_kd 0.1040 (0.1038) loss_oracle 0.4182 (0.4441) acc 84.3750 (84.4271) alaph_mean 0.5754 (0.5733) alpha_val 0.5754 (0.5733) lr 1.7705e-03 eta 0:08:33
epoch [13/50] batch [140/160] time 0.080 (0.086) data 0.000 (0.002) loss 0.9082 (0.9369) teacher_loss 0.3640 (0.4383) loss_zs_kd 0.0962 (0.1050) loss_oracle 0.4962 (0.4461) acc 81.2500 (84.7768) alaph_mean 0.5758 (0.5736) alpha_val 0.5758 (0.5736) lr 1.7705e-03 eta 0:08:28
epoch [13/50] batch [160/160] time 0.073 (0.085) data 0.000 (0.002) loss 1.0355 (0.9386) teacher_loss 0.5654 (0.4402) loss_zs_kd 0.1011 (0.1041) loss_oracle 0.4196 (0.4464) acc 78.1250 (84.6680) alaph_mean 0.5763 (0.5739) alpha_val 0.5763 (0.5739) lr 1.7290e-03 eta 0:08:21
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,848
* accuracy: 83.8%
* error: 16.2%
* macro_f1: 84.8%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,974
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 89.2%
******* Domain p best val acc:      83.8%, epoch: 13 *******
******* Domain p best val test acc: 88.1%, epoch: 13 *******
******* Domain p best test acc:     88.5%, epoch: 12 *******
epoch [14/50] batch [20/160] time 0.070 (0.095) data 0.000 (0.019) loss 0.8897 (0.9560) teacher_loss 0.2998 (0.4326) loss_zs_kd 0.1143 (0.1057) loss_oracle 0.5327 (0.4705) acc 90.6250 (84.2188) alaph_mean 0.5769 (0.5766) alpha_val 0.5769 (0.5766) lr 1.7290e-03 eta 0:09:18
epoch [14/50] batch [40/160] time 0.079 (0.085) data 0.000 (0.010) loss 1.0481 (0.9497) teacher_loss 0.5675 (0.4398) loss_zs_kd 0.1280 (0.1053) loss_oracle 0.4166 (0.4573) acc 78.1250 (84.0625) alaph_mean 0.5769 (0.5768) alpha_val 0.5769 (0.5768) lr 1.7290e-03 eta 0:08:22
epoch [14/50] batch [60/160] time 0.070 (0.083) data 0.001 (0.006) loss 0.7834 (0.9406) teacher_loss 0.3327 (0.4294) loss_zs_kd 0.0872 (0.1081) loss_oracle 0.4071 (0.4571) acc 87.5000 (84.4271) alaph_mean 0.5770 (0.5768) alpha_val 0.5770 (0.5768) lr 1.7290e-03 eta 0:08:06
epoch [14/50] batch [80/160] time 0.079 (0.082) data 0.000 (0.005) loss 1.1054 (0.9492) teacher_loss 0.6957 (0.4504) loss_zs_kd 0.1038 (0.1091) loss_oracle 0.3578 (0.4443) acc 71.8750 (83.7891) alaph_mean 0.5777 (0.5769) alpha_val 0.5777 (0.5769) lr 1.7290e-03 eta 0:07:56
epoch [14/50] batch [100/160] time 0.075 (0.082) data 0.000 (0.004) loss 0.9122 (0.9372) teacher_loss 0.4572 (0.4420) loss_zs_kd 0.0921 (0.1091) loss_oracle 0.4089 (0.4406) acc 81.2500 (84.1250) alaph_mean 0.5782 (0.5771) alpha_val 0.5782 (0.5771) lr 1.7290e-03 eta 0:07:54
epoch [14/50] batch [120/160] time 0.079 (0.082) data 0.000 (0.003) loss 0.8589 (0.9272) teacher_loss 0.3974 (0.4415) loss_zs_kd 0.1124 (0.1088) loss_oracle 0.4053 (0.4313) acc 84.3750 (84.3750) alaph_mean 0.5790 (0.5774) alpha_val 0.5790 (0.5774) lr 1.7290e-03 eta 0:07:54
epoch [14/50] batch [140/160] time 0.087 (0.082) data 0.000 (0.003) loss 0.8969 (0.9153) teacher_loss 0.4599 (0.4339) loss_zs_kd 0.0989 (0.1093) loss_oracle 0.3875 (0.4267) acc 81.2500 (84.8661) alaph_mean 0.5794 (0.5777) alpha_val 0.5794 (0.5777) lr 1.7290e-03 eta 0:07:54
epoch [14/50] batch [160/160] time 0.073 (0.082) data 0.000 (0.003) loss 0.7222 (0.9101) teacher_loss 0.2163 (0.4295) loss_zs_kd 0.0859 (0.1102) loss_oracle 0.4629 (0.4255) acc 93.7500 (85.0195) alaph_mean 0.5800 (0.5779) alpha_val 0.5800 (0.5779) lr 1.6845e-03 eta 0:07:51
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,843
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 85.0%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,987
* accuracy: 88.5%
* error: 11.5%
* macro_f1: 89.4%
******* Domain p best val acc:      83.8%, epoch: 13 *******
******* Domain p best val test acc: 88.1%, epoch: 13 *******
******* Domain p best test acc:     88.5%, epoch: 12 *******
epoch [15/50] batch [20/160] time 0.100 (0.104) data 0.000 (0.014) loss 0.8026 (0.8836) teacher_loss 0.2754 (0.4002) loss_zs_kd 0.1444 (0.1135) loss_oracle 0.4550 (0.4266) acc 96.8750 (86.2500) alaph_mean 0.5804 (0.5802) alpha_val 0.5804 (0.5802) lr 1.6845e-03 eta 0:09:55
epoch [15/50] batch [40/160] time 0.084 (0.097) data 0.000 (0.007) loss 0.9016 (0.9085) teacher_loss 0.4062 (0.4237) loss_zs_kd 0.1541 (0.1141) loss_oracle 0.4183 (0.4278) acc 87.5000 (85.6250) alaph_mean 0.5812 (0.5805) alpha_val 0.5812 (0.5805) lr 1.6845e-03 eta 0:09:16
epoch [15/50] batch [60/160] time 0.083 (0.094) data 0.000 (0.005) loss 0.9893 (0.9155) teacher_loss 0.4566 (0.4153) loss_zs_kd 0.1368 (0.1134) loss_oracle 0.4642 (0.4435) acc 90.6250 (85.2083) alaph_mean 0.5821 (0.5809) alpha_val 0.5821 (0.5809) lr 1.6845e-03 eta 0:08:54
epoch [15/50] batch [80/160] time 0.082 (0.091) data 0.000 (0.004) loss 0.9459 (0.9249) teacher_loss 0.4403 (0.4201) loss_zs_kd 0.1075 (0.1133) loss_oracle 0.4519 (0.4482) acc 78.1250 (84.9609) alaph_mean 0.5828 (0.5813) alpha_val 0.5828 (0.5813) lr 1.6845e-03 eta 0:08:35
epoch [15/50] batch [100/160] time 0.082 (0.090) data 0.000 (0.003) loss 0.8313 (0.9229) teacher_loss 0.3399 (0.4186) loss_zs_kd 0.1318 (0.1100) loss_oracle 0.4255 (0.4493) acc 87.5000 (85.3438) alaph_mean 0.5837 (0.5817) alpha_val 0.5837 (0.5817) lr 1.6845e-03 eta 0:08:29
epoch [15/50] batch [120/160] time 0.084 (0.089) data 0.000 (0.003) loss 0.9117 (0.9217) teacher_loss 0.3267 (0.4185) loss_zs_kd 0.1258 (0.1109) loss_oracle 0.5221 (0.4477) acc 84.3750 (85.3125) alaph_mean 0.5843 (0.5821) alpha_val 0.5843 (0.5821) lr 1.6845e-03 eta 0:08:20
epoch [15/50] batch [140/160] time 0.094 (0.091) data 0.001 (0.002) loss 0.9227 (0.9199) teacher_loss 0.4257 (0.4185) loss_zs_kd 0.1314 (0.1108) loss_oracle 0.4314 (0.4459) acc 84.3750 (85.4241) alaph_mean 0.5849 (0.5824) alpha_val 0.5849 (0.5824) lr 1.6845e-03 eta 0:08:32
epoch [15/50] batch [160/160] time 0.077 (0.090) data 0.000 (0.002) loss 0.9992 (0.9289) teacher_loss 0.5212 (0.4284) loss_zs_kd 0.1164 (0.1114) loss_oracle 0.4198 (0.4449) acc 84.3750 (85.1758) alaph_mean 0.5854 (0.5828) alpha_val 0.5854 (0.5828) lr 1.6374e-03 eta 0:08:22
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,845
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 84.8%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,993
* accuracy: 88.7%
* error: 11.3%
* macro_f1: 89.5%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      83.8%, epoch: 13 *******
******* Domain p best val test acc: 88.1%, epoch: 13 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [16/50] batch [20/160] time 0.082 (0.102) data 0.000 (0.018) loss 0.9260 (0.9336) teacher_loss 0.3587 (0.4048) loss_zs_kd 0.0819 (0.1039) loss_oracle 0.5263 (0.4768) acc 90.6250 (85.6250) alaph_mean 0.5859 (0.5857) alpha_val 0.5859 (0.5857) lr 1.6374e-03 eta 0:09:27
epoch [16/50] batch [40/160] time 0.079 (0.093) data 0.000 (0.009) loss 0.9331 (0.9309) teacher_loss 0.2930 (0.4040) loss_zs_kd 0.0623 (0.1055) loss_oracle 0.6089 (0.4741) acc 90.6250 (86.7188) alaph_mean 0.5867 (0.5860) alpha_val 0.5867 (0.5860) lr 1.6374e-03 eta 0:08:36
epoch [16/50] batch [60/160] time 0.083 (0.090) data 0.001 (0.006) loss 0.7908 (0.9236) teacher_loss 0.2808 (0.3985) loss_zs_kd 0.0757 (0.1105) loss_oracle 0.4722 (0.4699) acc 90.6250 (86.5625) alaph_mean 0.5876 (0.5864) alpha_val 0.5876 (0.5864) lr 1.6374e-03 eta 0:08:21
epoch [16/50] batch [80/160] time 0.083 (0.089) data 0.000 (0.005) loss 0.9193 (0.9294) teacher_loss 0.4639 (0.4048) loss_zs_kd 0.1127 (0.1091) loss_oracle 0.3991 (0.4701) acc 87.5000 (85.8984) alaph_mean 0.5883 (0.5868) alpha_val 0.5883 (0.5868) lr 1.6374e-03 eta 0:08:12
epoch [16/50] batch [100/160] time 0.083 (0.088) data 0.000 (0.004) loss 0.8963 (0.9373) teacher_loss 0.3684 (0.4149) loss_zs_kd 0.1410 (0.1118) loss_oracle 0.4574 (0.4665) acc 78.1250 (85.6250) alaph_mean 0.5892 (0.5872) alpha_val 0.5892 (0.5872) lr 1.6374e-03 eta 0:08:06
epoch [16/50] batch [120/160] time 0.081 (0.088) data 0.000 (0.003) loss 0.8528 (0.9382) teacher_loss 0.3424 (0.4211) loss_zs_kd 0.1047 (0.1104) loss_oracle 0.4580 (0.4618) acc 90.6250 (85.0260) alaph_mean 0.5898 (0.5876) alpha_val 0.5898 (0.5876) lr 1.6374e-03 eta 0:08:02
epoch [16/50] batch [140/160] time 0.083 (0.088) data 0.000 (0.003) loss 1.3073 (0.9345) teacher_loss 0.8733 (0.4220) loss_zs_kd 0.1127 (0.1103) loss_oracle 0.3776 (0.4573) acc 68.7500 (85.1786) alaph_mean 0.5904 (0.5880) alpha_val 0.5904 (0.5880) lr 1.6374e-03 eta 0:07:59
epoch [16/50] batch [160/160] time 0.080 (0.087) data 0.000 (0.003) loss 0.7414 (0.9331) teacher_loss 0.2886 (0.4229) loss_zs_kd 0.0872 (0.1108) loss_oracle 0.4092 (0.4548) acc 90.6250 (85.4688) alaph_mean 0.5909 (0.5883) alpha_val 0.5909 (0.5883) lr 1.5878e-03 eta 0:07:51
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,853
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 85.3%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,961
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 88.9%
******* Domain p best val acc:      84.0%, epoch: 16 *******
******* Domain p best val test acc: 87.7%, epoch: 16 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [17/50] batch [20/160] time 0.081 (0.102) data 0.000 (0.019) loss 0.9465 (0.8960) teacher_loss 0.4475 (0.3987) loss_zs_kd 0.0959 (0.0986) loss_oracle 0.4511 (0.4479) acc 87.5000 (86.8750) alaph_mean 0.5915 (0.5912) alpha_val 0.5915 (0.5912) lr 1.5878e-03 eta 0:09:13
epoch [17/50] batch [40/160] time 0.083 (0.092) data 0.000 (0.009) loss 0.7698 (0.9053) teacher_loss 0.2558 (0.3969) loss_zs_kd 0.0948 (0.1030) loss_oracle 0.4665 (0.4569) acc 96.8750 (86.3281) alaph_mean 0.5922 (0.5916) alpha_val 0.5922 (0.5916) lr 1.5878e-03 eta 0:08:15
epoch [17/50] batch [60/160] time 0.082 (0.096) data 0.000 (0.006) loss 0.8636 (0.9147) teacher_loss 0.3339 (0.4085) loss_zs_kd 0.1297 (0.1022) loss_oracle 0.4648 (0.4551) acc 93.7500 (85.4167) alaph_mean 0.5930 (0.5919) alpha_val 0.5930 (0.5919) lr 1.5878e-03 eta 0:08:38
epoch [17/50] batch [80/160] time 0.087 (0.093) data 0.000 (0.005) loss 1.1781 (0.9308) teacher_loss 0.6750 (0.4203) loss_zs_kd 0.1041 (0.1059) loss_oracle 0.4511 (0.4576) acc 75.0000 (85.1562) alaph_mean 0.5940 (0.5923) alpha_val 0.5940 (0.5923) lr 1.5878e-03 eta 0:08:20
epoch [17/50] batch [100/160] time 0.083 (0.091) data 0.000 (0.004) loss 1.0111 (0.9272) teacher_loss 0.4894 (0.4193) loss_zs_kd 0.1182 (0.1083) loss_oracle 0.4626 (0.4538) acc 81.2500 (85.4062) alaph_mean 0.5948 (0.5927) alpha_val 0.5948 (0.5927) lr 1.5878e-03 eta 0:08:06
epoch [17/50] batch [120/160] time 0.081 (0.090) data 0.000 (0.003) loss 1.1570 (0.9260) teacher_loss 0.5864 (0.4191) loss_zs_kd 0.1335 (0.1095) loss_oracle 0.5038 (0.4522) acc 81.2500 (85.6510) alaph_mean 0.5954 (0.5931) alpha_val 0.5954 (0.5931) lr 1.5878e-03 eta 0:08:00
epoch [17/50] batch [140/160] time 0.088 (0.089) data 0.000 (0.003) loss 0.9082 (0.9337) teacher_loss 0.3876 (0.4250) loss_zs_kd 0.1196 (0.1112) loss_oracle 0.4608 (0.4531) acc 81.2500 (85.5580) alaph_mean 0.5965 (0.5936) alpha_val 0.5965 (0.5936) lr 1.5878e-03 eta 0:07:52
epoch [17/50] batch [160/160] time 0.081 (0.088) data 0.000 (0.003) loss 1.0150 (0.9312) teacher_loss 0.5551 (0.4225) loss_zs_kd 0.0890 (0.1121) loss_oracle 0.4154 (0.4526) acc 78.1250 (85.8594) alaph_mean 0.5970 (0.5939) alpha_val 0.5970 (0.5939) lr 1.5358e-03 eta 0:07:43
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,847
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 85.0%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,979
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 89.3%
******* Domain p best val acc:      84.0%, epoch: 16 *******
******* Domain p best val test acc: 87.7%, epoch: 16 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [18/50] batch [20/160] time 0.077 (0.098) data 0.000 (0.015) loss 0.9904 (0.9870) teacher_loss 0.5007 (0.4622) loss_zs_kd 0.1192 (0.1211) loss_oracle 0.4301 (0.4643) acc 81.2500 (84.3750) alaph_mean 0.5977 (0.5974) alpha_val 0.5977 (0.5974) lr 1.5358e-03 eta 0:08:34
epoch [18/50] batch [40/160] time 0.088 (0.090) data 0.000 (0.008) loss 1.0071 (0.9428) teacher_loss 0.4651 (0.4113) loss_zs_kd 0.1478 (0.1265) loss_oracle 0.4681 (0.4682) acc 87.5000 (86.1719) alaph_mean 0.5984 (0.5978) alpha_val 0.5984 (0.5978) lr 1.5358e-03 eta 0:07:53
epoch [18/50] batch [60/160] time 0.081 (0.088) data 0.000 (0.005) loss 1.0671 (0.9216) teacher_loss 0.5834 (0.3991) loss_zs_kd 0.1152 (0.1219) loss_oracle 0.4262 (0.4615) acc 81.2500 (86.8750) alaph_mean 0.5993 (0.5981) alpha_val 0.5993 (0.5981) lr 1.5358e-03 eta 0:07:40
epoch [18/50] batch [80/160] time 0.091 (0.088) data 0.000 (0.004) loss 0.7989 (0.9198) teacher_loss 0.3638 (0.4029) loss_zs_kd 0.1613 (0.1198) loss_oracle 0.3545 (0.4570) acc 90.6250 (86.6016) alaph_mean 0.6002 (0.5986) alpha_val 0.6002 (0.5986) lr 1.5358e-03 eta 0:07:35
epoch [18/50] batch [100/160] time 0.081 (0.087) data 0.000 (0.003) loss 1.2571 (0.9277) teacher_loss 0.7261 (0.4106) loss_zs_kd 0.0911 (0.1174) loss_oracle 0.4854 (0.4584) acc 81.2500 (86.1250) alaph_mean 0.6013 (0.5990) alpha_val 0.6013 (0.5990) lr 1.5358e-03 eta 0:07:32
epoch [18/50] batch [120/160] time 0.087 (0.087) data 0.001 (0.003) loss 0.9055 (0.9361) teacher_loss 0.4289 (0.4209) loss_zs_kd 0.1281 (0.1134) loss_oracle 0.4126 (0.4585) acc 84.3750 (85.4948) alaph_mean 0.6025 (0.5995) alpha_val 0.6025 (0.5995) lr 1.5358e-03 eta 0:07:30
epoch [18/50] batch [140/160] time 0.080 (0.087) data 0.000 (0.002) loss 1.1749 (0.9302) teacher_loss 0.6137 (0.4157) loss_zs_kd 0.1008 (0.1118) loss_oracle 0.5108 (0.4586) acc 75.0000 (85.8036) alaph_mean 0.6030 (0.6000) alpha_val 0.6030 (0.6000) lr 1.5358e-03 eta 0:07:27
epoch [18/50] batch [160/160] time 0.071 (0.086) data 0.000 (0.002) loss 1.2825 (0.9296) teacher_loss 0.8172 (0.4172) loss_zs_kd 0.1265 (0.1118) loss_oracle 0.4021 (0.4565) acc 78.1250 (85.8398) alaph_mean 0.6037 (0.6004) alpha_val 0.6037 (0.6004) lr 1.4818e-03 eta 0:07:18
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,860
* accuracy: 84.3%
* error: 15.7%
* macro_f1: 85.3%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,949
* accuracy: 87.4%
* error: 12.6%
* macro_f1: 88.7%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [19/50] batch [20/160] time 0.078 (0.102) data 0.000 (0.018) loss 0.9558 (0.9340) teacher_loss 0.4092 (0.4099) loss_zs_kd 0.1180 (0.1058) loss_oracle 0.4876 (0.4711) acc 84.3750 (85.6250) alaph_mean 0.6047 (0.6042) alpha_val 0.6047 (0.6042) lr 1.4818e-03 eta 0:08:40
epoch [19/50] batch [40/160] time 0.090 (0.092) data 0.000 (0.009) loss 0.9896 (0.9417) teacher_loss 0.4739 (0.4204) loss_zs_kd 0.1370 (0.1107) loss_oracle 0.4472 (0.4660) acc 81.2500 (85.3125) alaph_mean 0.6055 (0.6047) alpha_val 0.6055 (0.6047) lr 1.4818e-03 eta 0:07:47
epoch [19/50] batch [60/160] time 0.073 (0.088) data 0.001 (0.006) loss 0.8566 (0.9393) teacher_loss 0.2987 (0.4273) loss_zs_kd 0.0988 (0.1137) loss_oracle 0.5085 (0.4552) acc 90.6250 (85.1042) alaph_mean 0.6062 (0.6051) alpha_val 0.6062 (0.6051) lr 1.4818e-03 eta 0:07:26
epoch [19/50] batch [80/160] time 0.080 (0.087) data 0.000 (0.005) loss 0.9884 (0.9264) teacher_loss 0.5393 (0.4209) loss_zs_kd 0.1070 (0.1153) loss_oracle 0.3956 (0.4478) acc 87.5000 (85.2344) alaph_mean 0.6070 (0.6055) alpha_val 0.6070 (0.6055) lr 1.4818e-03 eta 0:07:16
epoch [19/50] batch [100/160] time 0.086 (0.086) data 0.001 (0.004) loss 0.9891 (0.9368) teacher_loss 0.4064 (0.4335) loss_zs_kd 0.1492 (0.1140) loss_oracle 0.5082 (0.4462) acc 87.5000 (84.8125) alaph_mean 0.6074 (0.6058) alpha_val 0.6074 (0.6058) lr 1.4818e-03 eta 0:07:13
epoch [19/50] batch [120/160] time 0.080 (0.086) data 0.000 (0.003) loss 1.1442 (0.9293) teacher_loss 0.5601 (0.4288) loss_zs_kd 0.2072 (0.1151) loss_oracle 0.4805 (0.4430) acc 81.2500 (85.0781) alaph_mean 0.6081 (0.6061) alpha_val 0.6081 (0.6061) lr 1.4818e-03 eta 0:07:07
epoch [19/50] batch [140/160] time 0.070 (0.085) data 0.000 (0.003) loss 0.7977 (0.9238) teacher_loss 0.2890 (0.4270) loss_zs_kd 0.0966 (0.1132) loss_oracle 0.4605 (0.4402) acc 87.5000 (85.3125) alaph_mean 0.6090 (0.6065) alpha_val 0.6090 (0.6065) lr 1.4818e-03 eta 0:07:02
epoch [19/50] batch [160/160] time 0.074 (0.084) data 0.000 (0.003) loss 0.9548 (0.9207) teacher_loss 0.4274 (0.4225) loss_zs_kd 0.1348 (0.1144) loss_oracle 0.4600 (0.4410) acc 84.3750 (85.4492) alaph_mean 0.6096 (0.6068) alpha_val 0.6096 (0.6068) lr 1.4258e-03 eta 0:06:55
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,858
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,962
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 88.9%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [20/50] batch [20/160] time 0.075 (0.102) data 0.000 (0.018) loss 0.9394 (0.9108) teacher_loss 0.4451 (0.4215) loss_zs_kd 0.1209 (0.1182) loss_oracle 0.4339 (0.4302) acc 81.2500 (84.5312) alaph_mean 0.6100 (0.6098) alpha_val 0.6100 (0.6098) lr 1.4258e-03 eta 0:08:21
epoch [20/50] batch [40/160] time 0.068 (0.090) data 0.000 (0.009) loss 1.0635 (0.9020) teacher_loss 0.5573 (0.4144) loss_zs_kd 0.0976 (0.1157) loss_oracle 0.4573 (0.4297) acc 75.0000 (85.0000) alaph_mean 0.6104 (0.6100) alpha_val 0.6104 (0.6100) lr 1.4258e-03 eta 0:07:21
epoch [20/50] batch [60/160] time 0.090 (0.086) data 0.001 (0.006) loss 0.7913 (0.8953) teacher_loss 0.2953 (0.4080) loss_zs_kd 0.1252 (0.1153) loss_oracle 0.4335 (0.4296) acc 93.7500 (85.8333) alaph_mean 0.6108 (0.6102) alpha_val 0.6108 (0.6102) lr 1.4258e-03 eta 0:07:01
epoch [20/50] batch [80/160] time 0.064 (0.083) data 0.000 (0.005) loss 0.9475 (0.9050) teacher_loss 0.5111 (0.4181) loss_zs_kd 0.1061 (0.1172) loss_oracle 0.3833 (0.4283) acc 87.5000 (85.7422) alaph_mean 0.6113 (0.6104) alpha_val 0.6113 (0.6104) lr 1.4258e-03 eta 0:06:45
epoch [20/50] batch [100/160] time 0.079 (0.081) data 0.000 (0.004) loss 0.8894 (0.9016) teacher_loss 0.3482 (0.4155) loss_zs_kd 0.1245 (0.1161) loss_oracle 0.4790 (0.4280) acc 84.3750 (85.5938) alaph_mean 0.6120 (0.6107) alpha_val 0.6120 (0.6107) lr 1.4258e-03 eta 0:06:34
epoch [20/50] batch [120/160] time 0.081 (0.080) data 0.000 (0.003) loss 0.8638 (0.9081) teacher_loss 0.3706 (0.4183) loss_zs_kd 0.1127 (0.1176) loss_oracle 0.4368 (0.4310) acc 93.7500 (85.6771) alaph_mean 0.6128 (0.6110) alpha_val 0.6128 (0.6110) lr 1.4258e-03 eta 0:06:26
epoch [20/50] batch [140/160] time 0.081 (0.080) data 0.000 (0.003) loss 0.6903 (0.8993) teacher_loss 0.2635 (0.4082) loss_zs_kd 0.0895 (0.1180) loss_oracle 0.3821 (0.4321) acc 90.6250 (85.9598) alaph_mean 0.6133 (0.6113) alpha_val 0.6133 (0.6113) lr 1.4258e-03 eta 0:06:26
epoch [20/50] batch [160/160] time 0.075 (0.080) data 0.000 (0.003) loss 0.8400 (0.8992) teacher_loss 0.4131 (0.4081) loss_zs_kd 0.0790 (0.1186) loss_oracle 0.3874 (0.4318) acc 84.3750 (86.0352) alaph_mean 0.6142 (0.6116) alpha_val 0.6142 (0.6116) lr 1.3681e-03 eta 0:06:22
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,853
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 85.1%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,953
* accuracy: 87.5%
* error: 12.5%
* macro_f1: 88.7%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [21/50] batch [20/160] time 0.078 (0.100) data 0.000 (0.017) loss 0.9363 (0.9007) teacher_loss 0.3962 (0.4028) loss_zs_kd 0.1274 (0.1123) loss_oracle 0.4763 (0.4418) acc 78.1250 (86.8750) alaph_mean 0.6154 (0.6148) alpha_val 0.6154 (0.6148) lr 1.3681e-03 eta 0:07:56
epoch [21/50] batch [40/160] time 0.063 (0.085) data 0.000 (0.009) loss 0.7057 (0.9093) teacher_loss 0.2196 (0.4199) loss_zs_kd 0.0853 (0.1060) loss_oracle 0.4434 (0.4364) acc 93.7500 (85.8594) alaph_mean 0.6164 (0.6154) alpha_val 0.6164 (0.6154) lr 1.3681e-03 eta 0:06:44
epoch [21/50] batch [60/160] time 0.076 (0.081) data 0.000 (0.006) loss 0.8537 (0.9222) teacher_loss 0.3647 (0.4273) loss_zs_kd 0.1227 (0.1052) loss_oracle 0.4276 (0.4423) acc 90.6250 (85.5208) alaph_mean 0.6175 (0.6159) alpha_val 0.6175 (0.6159) lr 1.3681e-03 eta 0:06:24
epoch [21/50] batch [80/160] time 0.060 (0.079) data 0.000 (0.005) loss 0.9466 (0.9253) teacher_loss 0.4634 (0.4274) loss_zs_kd 0.0843 (0.1045) loss_oracle 0.4410 (0.4456) acc 84.3750 (85.6250) alaph_mean 0.6183 (0.6164) alpha_val 0.6183 (0.6164) lr 1.3681e-03 eta 0:06:11
epoch [21/50] batch [100/160] time 0.067 (0.075) data 0.000 (0.004) loss 0.9205 (0.9209) teacher_loss 0.4419 (0.4273) loss_zs_kd 0.1092 (0.1055) loss_oracle 0.4240 (0.4408) acc 87.5000 (85.6250) alaph_mean 0.6192 (0.6169) alpha_val 0.6192 (0.6169) lr 1.3681e-03 eta 0:05:53
epoch [21/50] batch [120/160] time 0.058 (0.074) data 0.000 (0.003) loss 0.8801 (0.9200) teacher_loss 0.3862 (0.4275) loss_zs_kd 0.1016 (0.1056) loss_oracle 0.4431 (0.4397) acc 81.2500 (85.5208) alaph_mean 0.6205 (0.6174) alpha_val 0.6205 (0.6174) lr 1.3681e-03 eta 0:05:45
epoch [21/50] batch [140/160] time 0.058 (0.072) data 0.000 (0.003) loss 1.0607 (0.9249) teacher_loss 0.5598 (0.4314) loss_zs_kd 0.1634 (0.1071) loss_oracle 0.4192 (0.4401) acc 81.2500 (85.1339) alaph_mean 0.6215 (0.6179) alpha_val 0.6215 (0.6179) lr 1.3681e-03 eta 0:05:36
epoch [21/50] batch [160/160] time 0.069 (0.071) data 0.000 (0.002) loss 0.9212 (0.9282) teacher_loss 0.4561 (0.4340) loss_zs_kd 0.1233 (0.1084) loss_oracle 0.4034 (0.4401) acc 78.1250 (85.0195) alaph_mean 0.6223 (0.6184) alpha_val 0.6223 (0.6184) lr 1.3090e-03 eta 0:05:29
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,854
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 85.0%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,972
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 89.2%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [22/50] batch [20/160] time 0.100 (0.106) data 0.000 (0.017) loss 0.9263 (0.9212) teacher_loss 0.3688 (0.4352) loss_zs_kd 0.1357 (0.1095) loss_oracle 0.4897 (0.4312) acc 84.3750 (85.1562) alaph_mean 0.6227 (0.6225) alpha_val 0.6227 (0.6225) lr 1.3090e-03 eta 0:08:10
epoch [22/50] batch [40/160] time 0.080 (0.095) data 0.000 (0.009) loss 0.8388 (0.9261) teacher_loss 0.3357 (0.4356) loss_zs_kd 0.0922 (0.1095) loss_oracle 0.4570 (0.4358) acc 87.5000 (85.8594) alaph_mean 0.6231 (0.6227) alpha_val 0.6231 (0.6227) lr 1.3090e-03 eta 0:07:17
epoch [22/50] batch [60/160] time 0.096 (0.093) data 0.000 (0.006) loss 0.6406 (0.9258) teacher_loss 0.2443 (0.4391) loss_zs_kd 0.0671 (0.1136) loss_oracle 0.3627 (0.4298) acc 90.6250 (85.1042) alaph_mean 0.6238 (0.6229) alpha_val 0.6238 (0.6229) lr 1.3090e-03 eta 0:07:04
epoch [22/50] batch [80/160] time 0.081 (0.091) data 0.000 (0.004) loss 0.7964 (0.9192) teacher_loss 0.2949 (0.4339) loss_zs_kd 0.1800 (0.1161) loss_oracle 0.4115 (0.4273) acc 90.6250 (85.3516) alaph_mean 0.6244 (0.6232) alpha_val 0.6244 (0.6232) lr 1.3090e-03 eta 0:06:54
epoch [22/50] batch [100/160] time 0.109 (0.090) data 0.001 (0.004) loss 1.0179 (0.9166) teacher_loss 0.5728 (0.4285) loss_zs_kd 0.0983 (0.1152) loss_oracle 0.3960 (0.4305) acc 81.2500 (85.1562) alaph_mean 0.6247 (0.6235) alpha_val 0.6247 (0.6235) lr 1.3090e-03 eta 0:06:47
epoch [22/50] batch [120/160] time 0.091 (0.089) data 0.000 (0.003) loss 0.9693 (0.9150) teacher_loss 0.4644 (0.4262) loss_zs_kd 0.1050 (0.1146) loss_oracle 0.4524 (0.4315) acc 87.5000 (85.4167) alaph_mean 0.6250 (0.6237) alpha_val 0.6250 (0.6237) lr 1.3090e-03 eta 0:06:43
epoch [22/50] batch [140/160] time 0.085 (0.089) data 0.000 (0.003) loss 0.7113 (0.9104) teacher_loss 0.3003 (0.4231) loss_zs_kd 0.1313 (0.1128) loss_oracle 0.3453 (0.4308) acc 87.5000 (85.4018) alaph_mean 0.6255 (0.6240) alpha_val 0.6255 (0.6240) lr 1.3090e-03 eta 0:06:40
epoch [22/50] batch [160/160] time 0.068 (0.090) data 0.000 (0.002) loss 0.9762 (0.9090) teacher_loss 0.4943 (0.4214) loss_zs_kd 0.1137 (0.1146) loss_oracle 0.4251 (0.4303) acc 81.2500 (85.5859) alaph_mean 0.6261 (0.6242) alpha_val 0.6261 (0.6242) lr 1.2487e-03 eta 0:06:42
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,851
* accuracy: 83.9%
* error: 16.1%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,973
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 89.2%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [23/50] batch [20/160] time 0.080 (0.103) data 0.000 (0.017) loss 0.9305 (0.8798) teacher_loss 0.4065 (0.4064) loss_zs_kd 0.1720 (0.1082) loss_oracle 0.4381 (0.4192) acc 87.5000 (86.7188) alaph_mean 0.6266 (0.6263) alpha_val 0.6266 (0.6263) lr 1.2487e-03 eta 0:07:40
epoch [23/50] batch [40/160] time 0.084 (0.093) data 0.000 (0.008) loss 0.8996 (0.8957) teacher_loss 0.4008 (0.4070) loss_zs_kd 0.1535 (0.1140) loss_oracle 0.4221 (0.4317) acc 90.6250 (86.7188) alaph_mean 0.6271 (0.6266) alpha_val 0.6271 (0.6266) lr 1.2487e-03 eta 0:06:54
epoch [23/50] batch [60/160] time 0.078 (0.091) data 0.001 (0.006) loss 1.1045 (0.9196) teacher_loss 0.6231 (0.4267) loss_zs_kd 0.0778 (0.1166) loss_oracle 0.4425 (0.4346) acc 75.0000 (85.9896) alaph_mean 0.6276 (0.6269) alpha_val 0.6276 (0.6269) lr 1.2487e-03 eta 0:06:40
epoch [23/50] batch [80/160] time 0.084 (0.090) data 0.000 (0.004) loss 0.9307 (0.8990) teacher_loss 0.4487 (0.4129) loss_zs_kd 0.1252 (0.1122) loss_oracle 0.4193 (0.4300) acc 87.5000 (86.8359) alaph_mean 0.6281 (0.6271) alpha_val 0.6281 (0.6271) lr 1.2487e-03 eta 0:06:34
epoch [23/50] batch [100/160] time 0.092 (0.089) data 0.000 (0.004) loss 0.8809 (0.9061) teacher_loss 0.2930 (0.4179) loss_zs_kd 0.1351 (0.1143) loss_oracle 0.5203 (0.4310) acc 93.7500 (86.5312) alaph_mean 0.6287 (0.6274) alpha_val 0.6287 (0.6274) lr 1.2487e-03 eta 0:06:27
epoch [23/50] batch [120/160] time 0.095 (0.088) data 0.001 (0.003) loss 0.7753 (0.9086) teacher_loss 0.2606 (0.4174) loss_zs_kd 0.0868 (0.1184) loss_oracle 0.4713 (0.4320) acc 93.7500 (86.3542) alaph_mean 0.6293 (0.6277) alpha_val 0.6293 (0.6277) lr 1.2487e-03 eta 0:06:25
epoch [23/50] batch [140/160] time 0.082 (0.088) data 0.000 (0.003) loss 0.9431 (0.9082) teacher_loss 0.3908 (0.4156) loss_zs_kd 0.1325 (0.1192) loss_oracle 0.4860 (0.4329) acc 90.6250 (86.3170) alaph_mean 0.6300 (0.6280) alpha_val 0.6300 (0.6280) lr 1.2487e-03 eta 0:06:22
epoch [23/50] batch [160/160] time 0.079 (0.087) data 0.000 (0.002) loss 0.8984 (0.9054) teacher_loss 0.3706 (0.4145) loss_zs_kd 0.1196 (0.1198) loss_oracle 0.4679 (0.4310) acc 87.5000 (86.3672) alaph_mean 0.6309 (0.6283) alpha_val 0.6309 (0.6283) lr 1.1874e-03 eta 0:06:15
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,854
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,964
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 89.0%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [24/50] batch [20/160] time 0.083 (0.093) data 0.000 (0.013) loss 0.9929 (0.9074) teacher_loss 0.5248 (0.4346) loss_zs_kd 0.0861 (0.1049) loss_oracle 0.4250 (0.4203) acc 81.2500 (85.9375) alaph_mean 0.6314 (0.6312) alpha_val 0.6314 (0.6312) lr 1.1874e-03 eta 0:06:38
epoch [24/50] batch [40/160] time 0.082 (0.088) data 0.000 (0.007) loss 0.9516 (0.8976) teacher_loss 0.4213 (0.4246) loss_zs_kd 0.1220 (0.1086) loss_oracle 0.4694 (0.4186) acc 84.3750 (85.7031) alaph_mean 0.6319 (0.6314) alpha_val 0.6319 (0.6314) lr 1.1874e-03 eta 0:06:15
epoch [24/50] batch [60/160] time 0.087 (0.087) data 0.001 (0.005) loss 1.0056 (0.8903) teacher_loss 0.5501 (0.4143) loss_zs_kd 0.1013 (0.1084) loss_oracle 0.4049 (0.4218) acc 87.5000 (86.1979) alaph_mean 0.6326 (0.6317) alpha_val 0.6326 (0.6317) lr 1.1874e-03 eta 0:06:11
epoch [24/50] batch [80/160] time 0.083 (0.086) data 0.000 (0.004) loss 1.0056 (0.8926) teacher_loss 0.5108 (0.4137) loss_zs_kd 0.1181 (0.1069) loss_oracle 0.4357 (0.4254) acc 78.1250 (85.7031) alaph_mean 0.6336 (0.6321) alpha_val 0.6336 (0.6321) lr 1.1874e-03 eta 0:06:04
epoch [24/50] batch [100/160] time 0.075 (0.089) data 0.000 (0.003) loss 0.8553 (0.8977) teacher_loss 0.4162 (0.4170) loss_zs_kd 0.1548 (0.1092) loss_oracle 0.3617 (0.4261) acc 84.3750 (85.5000) alaph_mean 0.6346 (0.6325) alpha_val 0.6346 (0.6325) lr 1.1874e-03 eta 0:06:14
epoch [24/50] batch [120/160] time 0.087 (0.090) data 0.000 (0.003) loss 1.0574 (0.8997) teacher_loss 0.5478 (0.4163) loss_zs_kd 0.1310 (0.1114) loss_oracle 0.4441 (0.4277) acc 90.6250 (85.4688) alaph_mean 0.6353 (0.6329) alpha_val 0.6353 (0.6329) lr 1.1874e-03 eta 0:06:19
epoch [24/50] batch [140/160] time 0.081 (0.089) data 0.000 (0.002) loss 0.9838 (0.8997) teacher_loss 0.5428 (0.4172) loss_zs_kd 0.1271 (0.1132) loss_oracle 0.3775 (0.4259) acc 84.3750 (85.6027) alaph_mean 0.6359 (0.6333) alpha_val 0.6359 (0.6333) lr 1.1874e-03 eta 0:06:11
epoch [24/50] batch [160/160] time 0.078 (0.088) data 0.000 (0.002) loss 0.8828 (0.8952) teacher_loss 0.4151 (0.4130) loss_zs_kd 0.1159 (0.1163) loss_oracle 0.4098 (0.4240) acc 84.3750 (85.9961) alaph_mean 0.6363 (0.6336) alpha_val 0.6363 (0.6336) lr 1.1253e-03 eta 0:06:04
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,857
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,954
* accuracy: 87.5%
* error: 12.5%
* macro_f1: 88.8%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [25/50] batch [20/160] time 0.098 (0.107) data 0.000 (0.018) loss 0.9116 (0.8534) teacher_loss 0.4717 (0.3871) loss_zs_kd 0.0927 (0.1199) loss_oracle 0.3936 (0.4063) acc 84.3750 (87.6562) alaph_mean 0.6367 (0.6365) alpha_val 0.6367 (0.6365) lr 1.1253e-03 eta 0:07:21
epoch [25/50] batch [40/160] time 0.086 (0.096) data 0.000 (0.009) loss 0.7405 (0.8572) teacher_loss 0.2742 (0.3862) loss_zs_kd 0.1274 (0.1193) loss_oracle 0.4025 (0.4114) acc 90.6250 (87.5000) alaph_mean 0.6373 (0.6368) alpha_val 0.6373 (0.6368) lr 1.1253e-03 eta 0:06:36
epoch [25/50] batch [60/160] time 0.085 (0.093) data 0.001 (0.006) loss 0.6868 (0.8625) teacher_loss 0.2406 (0.3898) loss_zs_kd 0.0874 (0.1183) loss_oracle 0.4025 (0.4136) acc 93.7500 (87.4479) alaph_mean 0.6378 (0.6370) alpha_val 0.6378 (0.6370) lr 1.1253e-03 eta 0:06:19
epoch [25/50] batch [80/160] time 0.084 (0.092) data 0.000 (0.005) loss 0.8253 (0.8557) teacher_loss 0.3620 (0.3812) loss_zs_kd 0.1360 (0.1181) loss_oracle 0.3953 (0.4154) acc 84.3750 (87.7734) alaph_mean 0.6385 (0.6373) alpha_val 0.6385 (0.6373) lr 1.1253e-03 eta 0:06:14
epoch [25/50] batch [100/160] time 0.083 (0.091) data 0.000 (0.004) loss 0.8170 (0.8729) teacher_loss 0.3841 (0.3960) loss_zs_kd 0.1264 (0.1197) loss_oracle 0.3696 (0.4171) acc 84.3750 (87.0000) alaph_mean 0.6392 (0.6376) alpha_val 0.6392 (0.6376) lr 1.1253e-03 eta 0:06:08
epoch [25/50] batch [120/160] time 0.086 (0.090) data 0.000 (0.003) loss 1.0165 (0.8885) teacher_loss 0.4667 (0.4073) loss_zs_kd 0.1178 (0.1219) loss_oracle 0.4909 (0.4202) acc 84.3750 (86.3802) alaph_mean 0.6399 (0.6379) alpha_val 0.6399 (0.6379) lr 1.1253e-03 eta 0:06:03
epoch [25/50] batch [140/160] time 0.088 (0.090) data 0.000 (0.003) loss 0.8749 (0.8935) teacher_loss 0.3285 (0.4106) loss_zs_kd 0.0786 (0.1194) loss_oracle 0.5071 (0.4232) acc 90.6250 (86.2054) alaph_mean 0.6406 (0.6383) alpha_val 0.6406 (0.6383) lr 1.1253e-03 eta 0:06:00
epoch [25/50] batch [160/160] time 0.081 (0.088) data 0.000 (0.003) loss 0.8436 (0.8953) teacher_loss 0.3709 (0.4100) loss_zs_kd 0.1307 (0.1201) loss_oracle 0.4074 (0.4252) acc 84.3750 (86.1133) alaph_mean 0.6413 (0.6386) alpha_val 0.6413 (0.6386) lr 1.0628e-03 eta 0:05:53
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,852
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,968
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 89.0%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [26/50] batch [20/160] time 0.082 (0.099) data 0.000 (0.016) loss 0.9275 (0.9204) teacher_loss 0.4167 (0.4311) loss_zs_kd 0.1287 (0.1240) loss_oracle 0.4465 (0.4273) acc 81.2500 (83.4375) alaph_mean 0.6420 (0.6417) alpha_val 0.6420 (0.6417) lr 1.0628e-03 eta 0:06:32
epoch [26/50] batch [40/160] time 0.112 (0.093) data 0.000 (0.008) loss 0.8955 (0.9149) teacher_loss 0.4353 (0.4234) loss_zs_kd 0.1344 (0.1232) loss_oracle 0.3930 (0.4298) acc 84.3750 (84.7656) alaph_mean 0.6424 (0.6419) alpha_val 0.6424 (0.6419) lr 1.0628e-03 eta 0:06:08
epoch [26/50] batch [60/160] time 0.075 (0.090) data 0.000 (0.006) loss 0.8160 (0.9246) teacher_loss 0.3475 (0.4259) loss_zs_kd 0.1151 (0.1231) loss_oracle 0.4110 (0.4372) acc 90.6250 (85.0000) alaph_mean 0.6428 (0.6422) alpha_val 0.6428 (0.6422) lr 1.0628e-03 eta 0:05:54
epoch [26/50] batch [80/160] time 0.059 (0.085) data 0.000 (0.004) loss 0.9253 (0.9152) teacher_loss 0.3971 (0.4174) loss_zs_kd 0.1606 (0.1246) loss_oracle 0.4478 (0.4355) acc 87.5000 (85.7422) alaph_mean 0.6435 (0.6424) alpha_val 0.6435 (0.6424) lr 1.0628e-03 eta 0:05:33
epoch [26/50] batch [100/160] time 0.079 (0.083) data 0.000 (0.003) loss 0.7449 (0.9081) teacher_loss 0.3305 (0.4136) loss_zs_kd 0.0953 (0.1218) loss_oracle 0.3667 (0.4336) acc 87.5000 (86.1562) alaph_mean 0.6443 (0.6427) alpha_val 0.6443 (0.6427) lr 1.0628e-03 eta 0:05:23
epoch [26/50] batch [120/160] time 0.074 (0.081) data 0.000 (0.003) loss 0.9653 (0.8987) teacher_loss 0.4872 (0.4005) loss_zs_kd 0.1445 (0.1225) loss_oracle 0.4059 (0.4370) acc 87.5000 (86.7969) alaph_mean 0.6447 (0.6430) alpha_val 0.6447 (0.6430) lr 1.0628e-03 eta 0:05:16
epoch [26/50] batch [140/160] time 0.084 (0.080) data 0.000 (0.003) loss 0.9246 (0.8996) teacher_loss 0.4918 (0.4006) loss_zs_kd 0.1214 (0.1243) loss_oracle 0.3721 (0.4369) acc 84.3750 (86.6295) alaph_mean 0.6452 (0.6433) alpha_val 0.6452 (0.6433) lr 1.0628e-03 eta 0:05:10
epoch [26/50] batch [160/160] time 0.074 (0.079) data 0.000 (0.002) loss 0.8728 (0.9023) teacher_loss 0.3857 (0.4044) loss_zs_kd 0.1462 (0.1248) loss_oracle 0.4140 (0.4355) acc 81.2500 (86.4258) alaph_mean 0.6462 (0.6436) alpha_val 0.6462 (0.6436) lr 1.0000e-03 eta 0:05:04
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,854
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,987
* accuracy: 88.5%
* error: 11.5%
* macro_f1: 89.5%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [27/50] batch [20/160] time 0.076 (0.101) data 0.000 (0.019) loss 0.7898 (0.8789) teacher_loss 0.2367 (0.3823) loss_zs_kd 0.1353 (0.1069) loss_oracle 0.4855 (0.4432) acc 96.8750 (86.8750) alaph_mean 0.6468 (0.6466) alpha_val 0.6468 (0.6466) lr 1.0000e-03 eta 0:06:25
epoch [27/50] batch [40/160] time 0.064 (0.087) data 0.000 (0.010) loss 0.9673 (0.8882) teacher_loss 0.5661 (0.4028) loss_zs_kd 0.1152 (0.1078) loss_oracle 0.3436 (0.4315) acc 75.0000 (86.3281) alaph_mean 0.6473 (0.6468) alpha_val 0.6473 (0.6468) lr 1.0000e-03 eta 0:05:31
epoch [27/50] batch [60/160] time 0.093 (0.086) data 0.001 (0.006) loss 0.7541 (0.8931) teacher_loss 0.2797 (0.4061) loss_zs_kd 0.1333 (0.1110) loss_oracle 0.4077 (0.4315) acc 90.6250 (86.3542) alaph_mean 0.6480 (0.6471) alpha_val 0.6480 (0.6471) lr 1.0000e-03 eta 0:05:25
epoch [27/50] batch [80/160] time 0.078 (0.086) data 0.000 (0.005) loss 0.8563 (0.8922) teacher_loss 0.3218 (0.4057) loss_zs_kd 0.1224 (0.1128) loss_oracle 0.4733 (0.4301) acc 90.6250 (86.2891) alaph_mean 0.6485 (0.6474) alpha_val 0.6485 (0.6474) lr 1.0000e-03 eta 0:05:25
epoch [27/50] batch [100/160] time 0.099 (0.087) data 0.000 (0.004) loss 0.8629 (0.8977) teacher_loss 0.3729 (0.4119) loss_zs_kd 0.1355 (0.1120) loss_oracle 0.4223 (0.4297) acc 87.5000 (86.0625) alaph_mean 0.6491 (0.6477) alpha_val 0.6491 (0.6477) lr 1.0000e-03 eta 0:05:24
epoch [27/50] batch [120/160] time 0.093 (0.087) data 0.000 (0.003) loss 0.8429 (0.8965) teacher_loss 0.3499 (0.4109) loss_zs_kd 0.1078 (0.1139) loss_oracle 0.4391 (0.4286) acc 84.3750 (86.2500) alaph_mean 0.6496 (0.6479) alpha_val 0.6496 (0.6479) lr 1.0000e-03 eta 0:05:24
epoch [27/50] batch [140/160] time 0.087 (0.087) data 0.000 (0.003) loss 0.7576 (0.8977) teacher_loss 0.2626 (0.4117) loss_zs_kd 0.1368 (0.1157) loss_oracle 0.4266 (0.4281) acc 93.7500 (86.2054) alaph_mean 0.6501 (0.6482) alpha_val 0.6501 (0.6482) lr 1.0000e-03 eta 0:05:22
epoch [27/50] batch [160/160] time 0.079 (0.086) data 0.000 (0.003) loss 1.1785 (0.8959) teacher_loss 0.7428 (0.4089) loss_zs_kd 0.1350 (0.1163) loss_oracle 0.3682 (0.4288) acc 78.1250 (86.2500) alaph_mean 0.6507 (0.6485) alpha_val 0.6507 (0.6485) lr 9.3721e-04 eta 0:05:16
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,858
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,975
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 89.2%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [28/50] batch [20/160] time 0.077 (0.133) data 0.000 (0.022) loss 0.7488 (0.8836) teacher_loss 0.3163 (0.4039) loss_zs_kd 0.0888 (0.1238) loss_oracle 0.3881 (0.4178) acc 90.6250 (86.2500) alaph_mean 0.6514 (0.6511) alpha_val 0.6514 (0.6511) lr 9.3721e-04 eta 0:08:06
epoch [28/50] batch [40/160] time 0.095 (0.108) data 0.000 (0.011) loss 0.9701 (0.8914) teacher_loss 0.4913 (0.4079) loss_zs_kd 0.0922 (0.1160) loss_oracle 0.4327 (0.4254) acc 78.1250 (85.8594) alaph_mean 0.6520 (0.6514) alpha_val 0.6520 (0.6514) lr 9.3721e-04 eta 0:06:34
epoch [28/50] batch [60/160] time 0.086 (0.101) data 0.000 (0.007) loss 0.7545 (0.8940) teacher_loss 0.2423 (0.4042) loss_zs_kd 0.1271 (0.1170) loss_oracle 0.4487 (0.4314) acc 90.6250 (85.8854) alaph_mean 0.6524 (0.6517) alpha_val 0.6524 (0.6517) lr 9.3721e-04 eta 0:06:05
epoch [28/50] batch [80/160] time 0.081 (0.095) data 0.000 (0.006) loss 0.9085 (0.8880) teacher_loss 0.3386 (0.4019) loss_zs_kd 0.1161 (0.1177) loss_oracle 0.5119 (0.4273) acc 87.5000 (86.2109) alaph_mean 0.6529 (0.6519) alpha_val 0.6529 (0.6519) lr 9.3721e-04 eta 0:05:40
epoch [28/50] batch [100/160] time 0.074 (0.092) data 0.000 (0.005) loss 0.7849 (0.8927) teacher_loss 0.3254 (0.4050) loss_zs_kd 0.1280 (0.1170) loss_oracle 0.3955 (0.4292) acc 84.3750 (86.1875) alaph_mean 0.6532 (0.6521) alpha_val 0.6532 (0.6521) lr 9.3721e-04 eta 0:05:29
epoch [28/50] batch [120/160] time 0.081 (0.090) data 0.000 (0.004) loss 0.9060 (0.8952) teacher_loss 0.4344 (0.4055) loss_zs_kd 0.0957 (0.1180) loss_oracle 0.4237 (0.4307) acc 84.3750 (86.2500) alaph_mean 0.6536 (0.6524) alpha_val 0.6536 (0.6524) lr 9.3721e-04 eta 0:05:21
epoch [28/50] batch [140/160] time 0.074 (0.088) data 0.000 (0.003) loss 0.7717 (0.8933) teacher_loss 0.2531 (0.4003) loss_zs_kd 0.1613 (0.1210) loss_oracle 0.4379 (0.4326) acc 100.0000 (86.5402) alaph_mean 0.6541 (0.6526) alpha_val 0.6541 (0.6526) lr 9.3721e-04 eta 0:05:12
epoch [28/50] batch [160/160] time 0.071 (0.087) data 0.000 (0.003) loss 0.8202 (0.8976) teacher_loss 0.3102 (0.4047) loss_zs_kd 0.0947 (0.1216) loss_oracle 0.4626 (0.4322) acc 84.3750 (86.4062) alaph_mean 0.6547 (0.6528) alpha_val 0.6547 (0.6528) lr 8.7467e-04 eta 0:05:05
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,852
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,984
* accuracy: 88.4%
* error: 11.6%
* macro_f1: 89.3%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [29/50] batch [20/160] time 0.093 (0.098) data 0.000 (0.014) loss 0.9145 (0.9213) teacher_loss 0.4282 (0.4319) loss_zs_kd 0.1787 (0.1249) loss_oracle 0.3969 (0.4269) acc 87.5000 (85.6250) alaph_mean 0.6554 (0.6551) alpha_val 0.6554 (0.6551) lr 8.7467e-04 eta 0:05:43
epoch [29/50] batch [40/160] time 0.071 (0.092) data 0.000 (0.007) loss 0.8962 (0.8880) teacher_loss 0.4119 (0.4021) loss_zs_kd 0.1138 (0.1236) loss_oracle 0.4273 (0.4242) acc 87.5000 (86.5625) alaph_mean 0.6561 (0.6554) alpha_val 0.6561 (0.6554) lr 8.7467e-04 eta 0:05:20
epoch [29/50] batch [60/160] time 0.081 (0.089) data 0.001 (0.005) loss 0.9122 (0.8757) teacher_loss 0.5248 (0.3947) loss_zs_kd 0.1029 (0.1209) loss_oracle 0.3359 (0.4206) acc 78.1250 (86.4583) alaph_mean 0.6570 (0.6558) alpha_val 0.6570 (0.6558) lr 8.7467e-04 eta 0:05:06
epoch [29/50] batch [80/160] time 0.081 (0.088) data 0.000 (0.004) loss 0.7426 (0.8818) teacher_loss 0.2377 (0.3998) loss_zs_kd 0.0877 (0.1177) loss_oracle 0.4611 (0.4232) acc 93.7500 (86.2500) alaph_mean 0.6577 (0.6562) alpha_val 0.6577 (0.6562) lr 8.7467e-04 eta 0:05:02
epoch [29/50] batch [100/160] time 0.084 (0.087) data 0.000 (0.003) loss 0.8970 (0.8863) teacher_loss 0.4279 (0.4007) loss_zs_kd 0.1289 (0.1169) loss_oracle 0.4047 (0.4272) acc 87.5000 (86.1875) alaph_mean 0.6580 (0.6565) alpha_val 0.6580 (0.6565) lr 8.7467e-04 eta 0:04:58
epoch [29/50] batch [120/160] time 0.088 (0.087) data 0.000 (0.003) loss 0.9314 (0.8980) teacher_loss 0.5014 (0.4084) loss_zs_kd 0.1543 (0.1202) loss_oracle 0.3529 (0.4295) acc 84.3750 (85.9635) alaph_mean 0.6585 (0.6568) alpha_val 0.6585 (0.6568) lr 8.7467e-04 eta 0:04:54
epoch [29/50] batch [140/160] time 0.082 (0.086) data 0.000 (0.002) loss 1.0602 (0.9068) teacher_loss 0.6093 (0.4130) loss_zs_kd 0.0907 (0.1212) loss_oracle 0.4055 (0.4332) acc 78.1250 (85.9821) alaph_mean 0.6591 (0.6571) alpha_val 0.6591 (0.6571) lr 8.7467e-04 eta 0:04:51
epoch [29/50] batch [160/160] time 0.074 (0.085) data 0.000 (0.002) loss 0.8626 (0.9074) teacher_loss 0.3967 (0.4134) loss_zs_kd 0.1086 (0.1208) loss_oracle 0.4117 (0.4336) acc 81.2500 (86.0742) alaph_mean 0.6595 (0.6574) alpha_val 0.6595 (0.6574) lr 8.1262e-04 eta 0:04:45
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,852
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,971
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 89.1%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [30/50] batch [20/160] time 0.069 (0.093) data 0.000 (0.011) loss 1.0216 (0.9351) teacher_loss 0.5982 (0.4445) loss_zs_kd 0.1395 (0.1154) loss_oracle 0.3537 (0.4329) acc 78.1250 (86.2500) alaph_mean 0.6601 (0.6599) alpha_val 0.6601 (0.6599) lr 8.1262e-04 eta 0:05:10
epoch [30/50] batch [40/160] time 0.075 (0.085) data 0.000 (0.006) loss 1.0001 (0.9032) teacher_loss 0.4495 (0.4050) loss_zs_kd 0.1092 (0.1165) loss_oracle 0.4960 (0.4399) acc 75.0000 (87.4219) alaph_mean 0.6607 (0.6601) alpha_val 0.6607 (0.6601) lr 8.1262e-04 eta 0:04:43
epoch [30/50] batch [60/160] time 0.088 (0.084) data 0.001 (0.004) loss 0.8080 (0.9000) teacher_loss 0.2412 (0.4017) loss_zs_kd 0.1092 (0.1196) loss_oracle 0.5123 (0.4385) acc 93.7500 (87.2917) alaph_mean 0.6610 (0.6604) alpha_val 0.6610 (0.6604) lr 8.1262e-04 eta 0:04:38
epoch [30/50] batch [80/160] time 0.086 (0.084) data 0.001 (0.003) loss 1.0152 (0.9115) teacher_loss 0.4890 (0.4108) loss_zs_kd 0.1455 (0.1237) loss_oracle 0.4535 (0.4389) acc 81.2500 (87.1875) alaph_mean 0.6614 (0.6606) alpha_val 0.6614 (0.6606) lr 8.1262e-04 eta 0:04:35
epoch [30/50] batch [100/160] time 0.082 (0.083) data 0.000 (0.002) loss 0.9057 (0.9073) teacher_loss 0.4061 (0.4049) loss_zs_kd 0.1249 (0.1255) loss_oracle 0.4372 (0.4397) acc 81.2500 (87.2188) alaph_mean 0.6617 (0.6608) alpha_val 0.6617 (0.6608) lr 8.1262e-04 eta 0:04:31
epoch [30/50] batch [120/160] time 0.081 (0.083) data 0.000 (0.002) loss 0.7238 (0.9066) teacher_loss 0.2401 (0.4029) loss_zs_kd 0.0887 (0.1259) loss_oracle 0.4393 (0.4408) acc 93.7500 (87.2917) alaph_mean 0.6621 (0.6610) alpha_val 0.6621 (0.6610) lr 8.1262e-04 eta 0:04:27
epoch [30/50] batch [140/160] time 0.088 (0.082) data 0.000 (0.002) loss 0.8799 (0.9049) teacher_loss 0.3396 (0.4026) loss_zs_kd 0.1550 (0.1251) loss_oracle 0.4628 (0.4397) acc 84.3750 (87.2768) alaph_mean 0.6627 (0.6612) alpha_val 0.6627 (0.6612) lr 8.1262e-04 eta 0:04:23
epoch [30/50] batch [160/160] time 0.072 (0.081) data 0.000 (0.002) loss 1.0940 (0.9069) teacher_loss 0.5760 (0.4064) loss_zs_kd 0.1378 (0.1253) loss_oracle 0.4491 (0.4378) acc 84.3750 (87.1484) alaph_mean 0.6631 (0.6614) alpha_val 0.6631 (0.6614) lr 7.5131e-04 eta 0:04:19
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,847
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 84.8%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,996
* accuracy: 88.7%
* error: 11.3%
* macro_f1: 89.7%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [31/50] batch [20/160] time 0.094 (0.109) data 0.000 (0.020) loss 1.0732 (0.8842) teacher_loss 0.6218 (0.3865) loss_zs_kd 0.1478 (0.1179) loss_oracle 0.3776 (0.4388) acc 84.3750 (87.8125) alaph_mean 0.6634 (0.6633) alpha_val 0.6634 (0.6633) lr 7.5131e-04 eta 0:05:46
epoch [31/50] batch [40/160] time 0.085 (0.098) data 0.000 (0.010) loss 0.7496 (0.9148) teacher_loss 0.3150 (0.4173) loss_zs_kd 0.1498 (0.1320) loss_oracle 0.3598 (0.4315) acc 93.7500 (87.2656) alaph_mean 0.6638 (0.6634) alpha_val 0.6638 (0.6634) lr 7.5131e-04 eta 0:05:09
epoch [31/50] batch [60/160] time 0.084 (0.094) data 0.000 (0.007) loss 0.8399 (0.9015) teacher_loss 0.3090 (0.3982) loss_zs_kd 0.1651 (0.1317) loss_oracle 0.4483 (0.4375) acc 87.5000 (87.3958) alaph_mean 0.6642 (0.6636) alpha_val 0.6642 (0.6636) lr 7.5131e-04 eta 0:04:55
epoch [31/50] batch [80/160] time 0.089 (0.092) data 0.001 (0.005) loss 0.8341 (0.9053) teacher_loss 0.3160 (0.3983) loss_zs_kd 0.1511 (0.1330) loss_oracle 0.4426 (0.4405) acc 93.7500 (87.3047) alaph_mean 0.6646 (0.6638) alpha_val 0.6646 (0.6638) lr 7.5131e-04 eta 0:04:46
epoch [31/50] batch [100/160] time 0.083 (0.090) data 0.000 (0.004) loss 0.7567 (0.9038) teacher_loss 0.2355 (0.3967) loss_zs_kd 0.1027 (0.1337) loss_oracle 0.4698 (0.4403) acc 93.7500 (87.2812) alaph_mean 0.6650 (0.6640) alpha_val 0.6650 (0.6640) lr 7.5131e-04 eta 0:04:40
epoch [31/50] batch [120/160] time 0.090 (0.089) data 0.001 (0.004) loss 1.1159 (0.9065) teacher_loss 0.6494 (0.4037) loss_zs_kd 0.1027 (0.1310) loss_oracle 0.4152 (0.4373) acc 84.3750 (87.1354) alaph_mean 0.6655 (0.6642) alpha_val 0.6655 (0.6642) lr 7.5131e-04 eta 0:04:35
epoch [31/50] batch [140/160] time 0.084 (0.091) data 0.000 (0.003) loss 0.8247 (0.9072) teacher_loss 0.3928 (0.4067) loss_zs_kd 0.1068 (0.1288) loss_oracle 0.3785 (0.4361) acc 90.6250 (86.8973) alaph_mean 0.6660 (0.6644) alpha_val 0.6660 (0.6644) lr 7.5131e-04 eta 0:04:38
epoch [31/50] batch [160/160] time 0.079 (0.090) data 0.001 (0.003) loss 0.8051 (0.9001) teacher_loss 0.1780 (0.4009) loss_zs_kd 0.2279 (0.1296) loss_oracle 0.5132 (0.4343) acc 93.7500 (87.1094) alaph_mean 0.6665 (0.6647) alpha_val 0.6665 (0.6647) lr 6.9098e-04 eta 0:04:32
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,851
* accuracy: 83.9%
* error: 16.1%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,972
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 89.2%
******* Domain p best val acc:      84.3%, epoch: 18 *******
******* Domain p best val test acc: 87.4%, epoch: 18 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [32/50] batch [20/160] time 0.087 (0.098) data 0.000 (0.014) loss 0.8143 (0.8424) teacher_loss 0.3381 (0.3692) loss_zs_kd 0.1111 (0.1170) loss_oracle 0.4206 (0.4147) acc 87.5000 (87.0312) alaph_mean 0.6668 (0.6666) alpha_val 0.6668 (0.6666) lr 6.9098e-04 eta 0:04:57
epoch [32/50] batch [40/160] time 0.080 (0.092) data 0.000 (0.007) loss 1.0111 (0.9001) teacher_loss 0.4529 (0.4160) loss_zs_kd 0.1355 (0.1220) loss_oracle 0.4904 (0.4231) acc 84.3750 (85.1562) alaph_mean 0.6671 (0.6668) alpha_val 0.6671 (0.6668) lr 6.9098e-04 eta 0:04:34
epoch [32/50] batch [60/160] time 0.092 (0.089) data 0.001 (0.005) loss 0.7997 (0.9143) teacher_loss 0.3103 (0.4137) loss_zs_kd 0.1310 (0.1275) loss_oracle 0.4239 (0.4369) acc 87.5000 (85.8333) alaph_mean 0.6675 (0.6670) alpha_val 0.6675 (0.6670) lr 6.9098e-04 eta 0:04:26
epoch [32/50] batch [80/160] time 0.086 (0.088) data 0.000 (0.004) loss 0.8922 (0.8924) teacher_loss 0.3431 (0.3933) loss_zs_kd 0.1318 (0.1264) loss_oracle 0.4831 (0.4359) acc 84.3750 (86.7969) alaph_mean 0.6679 (0.6672) alpha_val 0.6679 (0.6672) lr 6.9098e-04 eta 0:04:21
epoch [32/50] batch [100/160] time 0.084 (0.088) data 0.000 (0.003) loss 0.9167 (0.9025) teacher_loss 0.2885 (0.3984) loss_zs_kd 0.1416 (0.1282) loss_oracle 0.5574 (0.4400) acc 93.7500 (86.5000) alaph_mean 0.6684 (0.6674) alpha_val 0.6684 (0.6674) lr 6.9098e-04 eta 0:04:18
epoch [32/50] batch [120/160] time 0.089 (0.088) data 0.000 (0.003) loss 0.9958 (0.9033) teacher_loss 0.4618 (0.4007) loss_zs_kd 0.1195 (0.1272) loss_oracle 0.4743 (0.4390) acc 87.5000 (86.7969) alaph_mean 0.6689 (0.6676) alpha_val 0.6689 (0.6676) lr 6.9098e-04 eta 0:04:15
epoch [32/50] batch [140/160] time 0.068 (0.087) data 0.000 (0.002) loss 0.8719 (0.9003) teacher_loss 0.2880 (0.3966) loss_zs_kd 0.1870 (0.1274) loss_oracle 0.4904 (0.4400) acc 93.7500 (87.0759) alaph_mean 0.6693 (0.6678) alpha_val 0.6693 (0.6678) lr 6.9098e-04 eta 0:04:11
epoch [32/50] batch [160/160] time 0.073 (0.085) data 0.000 (0.002) loss 0.8462 (0.9008) teacher_loss 0.3366 (0.3976) loss_zs_kd 0.1585 (0.1277) loss_oracle 0.4304 (0.4393) acc 81.2500 (87.0508) alaph_mean 0.6697 (0.6680) alpha_val 0.6697 (0.6680) lr 6.3188e-04 eta 0:04:03
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,863
* accuracy: 84.5%
* error: 15.5%
* macro_f1: 85.5%
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,983
* accuracy: 88.4%
* error: 11.6%
* macro_f1: 89.4%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [33/50] batch [20/160] time 0.070 (0.087) data 0.000 (0.013) loss 0.9397 (0.8968) teacher_loss 0.3783 (0.3834) loss_zs_kd 0.1273 (0.1415) loss_oracle 0.4978 (0.4426) acc 87.5000 (87.6562) alaph_mean 0.6701 (0.6699) alpha_val 0.6701 (0.6699) lr 6.3188e-04 eta 0:04:08
epoch [33/50] batch [40/160] time 0.078 (0.081) data 0.000 (0.007) loss 0.9462 (0.8937) teacher_loss 0.4854 (0.3899) loss_zs_kd 0.1434 (0.1358) loss_oracle 0.3892 (0.4359) acc 84.3750 (86.8750) alaph_mean 0.6703 (0.6700) alpha_val 0.6703 (0.6700) lr 6.3188e-04 eta 0:03:50
epoch [33/50] batch [60/160] time 0.074 (0.078) data 0.001 (0.004) loss 1.1566 (0.9126) teacher_loss 0.5977 (0.3964) loss_zs_kd 0.1374 (0.1376) loss_oracle 0.4903 (0.4474) acc 78.1250 (86.9271) alaph_mean 0.6705 (0.6701) alpha_val 0.6705 (0.6701) lr 6.3188e-04 eta 0:03:38
epoch [33/50] batch [80/160] time 0.095 (0.084) data 0.000 (0.003) loss 0.9810 (0.9052) teacher_loss 0.3427 (0.3857) loss_zs_kd 0.1404 (0.1352) loss_oracle 0.5681 (0.4519) acc 87.5000 (86.9531) alaph_mean 0.6708 (0.6703) alpha_val 0.6708 (0.6703) lr 6.3188e-04 eta 0:03:54
epoch [33/50] batch [100/160] time 0.070 (0.083) data 0.000 (0.003) loss 0.8241 (0.9072) teacher_loss 0.3448 (0.3870) loss_zs_kd 0.0998 (0.1359) loss_oracle 0.4294 (0.4522) acc 93.7500 (87.1875) alaph_mean 0.6712 (0.6704) alpha_val 0.6712 (0.6704) lr 6.3188e-04 eta 0:03:49
epoch [33/50] batch [120/160] time 0.088 (0.082) data 0.000 (0.002) loss 0.9948 (0.9126) teacher_loss 0.4748 (0.3924) loss_zs_kd 0.1926 (0.1360) loss_oracle 0.4237 (0.4523) acc 81.2500 (87.0833) alaph_mean 0.6716 (0.6706) alpha_val 0.6716 (0.6706) lr 6.3188e-04 eta 0:03:47
epoch [33/50] batch [140/160] time 0.078 (0.082) data 0.000 (0.002) loss 1.0041 (0.9132) teacher_loss 0.5657 (0.3962) loss_zs_kd 0.1161 (0.1348) loss_oracle 0.3803 (0.4496) acc 87.5000 (87.0312) alaph_mean 0.6721 (0.6708) alpha_val 0.6721 (0.6708) lr 6.3188e-04 eta 0:03:44
epoch [33/50] batch [160/160] time 0.073 (0.081) data 0.000 (0.002) loss 0.7212 (0.9088) teacher_loss 0.2029 (0.3928) loss_zs_kd 0.0930 (0.1340) loss_oracle 0.4718 (0.4490) acc 100.0000 (87.3438) alaph_mean 0.6725 (0.6710) alpha_val 0.6725 (0.6710) lr 5.7422e-04 eta 0:03:40
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,860
* accuracy: 84.3%
* error: 15.7%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,972
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 89.1%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [34/50] batch [20/160] time 0.078 (0.096) data 0.000 (0.013) loss 0.7711 (0.8660) teacher_loss 0.3022 (0.3704) loss_zs_kd 0.1274 (0.1151) loss_oracle 0.4052 (0.4381) acc 93.7500 (87.5000) alaph_mean 0.6728 (0.6727) alpha_val 0.6728 (0.6727) lr 5.7422e-04 eta 0:04:19
epoch [34/50] batch [40/160] time 0.077 (0.087) data 0.000 (0.007) loss 0.8028 (0.8897) teacher_loss 0.3425 (0.4013) loss_zs_kd 0.1659 (0.1203) loss_oracle 0.3773 (0.4283) acc 96.8750 (86.3281) alaph_mean 0.6732 (0.6729) alpha_val 0.6732 (0.6729) lr 5.7422e-04 eta 0:03:52
epoch [34/50] batch [60/160] time 0.085 (0.085) data 0.001 (0.005) loss 1.0995 (0.9085) teacher_loss 0.5387 (0.4127) loss_zs_kd 0.1273 (0.1240) loss_oracle 0.4971 (0.4338) acc 81.2500 (86.5104) alaph_mean 0.6735 (0.6730) alpha_val 0.6735 (0.6730) lr 5.7422e-04 eta 0:03:46
epoch [34/50] batch [80/160] time 0.078 (0.084) data 0.000 (0.004) loss 1.0030 (0.9088) teacher_loss 0.4838 (0.4067) loss_zs_kd 0.1365 (0.1260) loss_oracle 0.4510 (0.4391) acc 84.3750 (86.5234) alaph_mean 0.6737 (0.6732) alpha_val 0.6737 (0.6732) lr 5.7422e-04 eta 0:03:40
epoch [34/50] batch [100/160] time 0.083 (0.083) data 0.000 (0.003) loss 0.7724 (0.9131) teacher_loss 0.2203 (0.4041) loss_zs_kd 0.1233 (0.1275) loss_oracle 0.4905 (0.4453) acc 96.8750 (86.9688) alaph_mean 0.6739 (0.6733) alpha_val 0.6739 (0.6733) lr 5.7422e-04 eta 0:03:37
epoch [34/50] batch [120/160] time 0.084 (0.083) data 0.000 (0.002) loss 0.9137 (0.9173) teacher_loss 0.3907 (0.4035) loss_zs_kd 0.1412 (0.1286) loss_oracle 0.4525 (0.4494) acc 84.3750 (86.6667) alaph_mean 0.6742 (0.6734) alpha_val 0.6742 (0.6734) lr 5.7422e-04 eta 0:03:35
epoch [34/50] batch [140/160] time 0.085 (0.083) data 0.000 (0.002) loss 1.1404 (0.9063) teacher_loss 0.6629 (0.3944) loss_zs_kd 0.1543 (0.1283) loss_oracle 0.4004 (0.4477) acc 78.1250 (87.0982) alaph_mean 0.6745 (0.6736) alpha_val 0.6745 (0.6736) lr 5.7422e-04 eta 0:03:35
epoch [34/50] batch [160/160] time 0.073 (0.083) data 0.000 (0.002) loss 0.9831 (0.9014) teacher_loss 0.5333 (0.3928) loss_zs_kd 0.1246 (0.1280) loss_oracle 0.3876 (0.4445) acc 87.5000 (87.2070) alaph_mean 0.6748 (0.6737) alpha_val 0.6748 (0.6737) lr 5.1825e-04 eta 0:03:31
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,844
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 84.8%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,983
* accuracy: 88.4%
* error: 11.6%
* macro_f1: 89.4%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [35/50] batch [20/160] time 0.082 (0.101) data 0.000 (0.016) loss 0.7072 (0.9297) teacher_loss 0.2120 (0.4197) loss_zs_kd 0.1080 (0.1244) loss_oracle 0.4412 (0.4478) acc 96.8750 (86.5625) alaph_mean 0.6751 (0.6750) alpha_val 0.6751 (0.6750) lr 5.1825e-04 eta 0:04:16
epoch [35/50] batch [40/160] time 0.074 (0.102) data 0.000 (0.008) loss 0.7799 (0.8982) teacher_loss 0.3025 (0.3889) loss_zs_kd 0.0796 (0.1221) loss_oracle 0.4375 (0.4483) acc 87.5000 (87.1875) alaph_mean 0.6754 (0.6751) alpha_val 0.6754 (0.6751) lr 5.1825e-04 eta 0:04:16
epoch [35/50] batch [60/160] time 0.087 (0.099) data 0.001 (0.006) loss 0.8350 (0.8790) teacher_loss 0.2686 (0.3645) loss_zs_kd 0.1084 (0.1247) loss_oracle 0.5121 (0.4521) acc 96.8750 (88.4375) alaph_mean 0.6756 (0.6753) alpha_val 0.6756 (0.6753) lr 5.1825e-04 eta 0:04:07
epoch [35/50] batch [80/160] time 0.065 (0.092) data 0.000 (0.004) loss 1.0287 (0.9060) teacher_loss 0.5738 (0.3898) loss_zs_kd 0.1479 (0.1271) loss_oracle 0.3810 (0.4527) acc 81.2500 (87.4219) alaph_mean 0.6759 (0.6754) alpha_val 0.6759 (0.6754) lr 5.1825e-04 eta 0:03:49
epoch [35/50] batch [100/160] time 0.072 (0.088) data 0.000 (0.004) loss 0.8872 (0.9053) teacher_loss 0.4812 (0.3882) loss_zs_kd 0.0863 (0.1280) loss_oracle 0.3629 (0.4530) acc 87.5000 (87.4688) alaph_mean 0.6762 (0.6755) alpha_val 0.6762 (0.6755) lr 5.1825e-04 eta 0:03:37
epoch [35/50] batch [120/160] time 0.071 (0.086) data 0.000 (0.003) loss 0.9027 (0.9067) teacher_loss 0.3908 (0.3901) loss_zs_kd 0.1619 (0.1316) loss_oracle 0.4309 (0.4509) acc 84.3750 (87.3698) alaph_mean 0.6764 (0.6756) alpha_val 0.6764 (0.6756) lr 5.1825e-04 eta 0:03:30
epoch [35/50] batch [140/160] time 0.092 (0.086) data 0.000 (0.003) loss 1.1194 (0.9047) teacher_loss 0.6196 (0.3900) loss_zs_kd 0.1414 (0.1305) loss_oracle 0.4291 (0.4494) acc 78.1250 (87.3214) alaph_mean 0.6766 (0.6758) alpha_val 0.6766 (0.6758) lr 5.1825e-04 eta 0:03:27
epoch [35/50] batch [160/160] time 0.073 (0.084) data 0.000 (0.002) loss 0.9171 (0.9057) teacher_loss 0.3288 (0.3902) loss_zs_kd 0.1377 (0.1313) loss_oracle 0.5195 (0.4499) acc 87.5000 (87.1484) alaph_mean 0.6769 (0.6759) alpha_val 0.6769 (0.6759) lr 4.6417e-04 eta 0:03:22
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,847
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 85.1%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,973
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 89.2%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [36/50] batch [20/160] time 0.088 (0.093) data 0.001 (0.013) loss 0.8577 (0.8759) teacher_loss 0.4140 (0.3623) loss_zs_kd 0.1293 (0.1378) loss_oracle 0.3790 (0.4447) acc 78.1250 (88.1250) alaph_mean 0.6772 (0.6770) alpha_val 0.6772 (0.6770) lr 4.6417e-04 eta 0:03:41
epoch [36/50] batch [40/160] time 0.096 (0.087) data 0.001 (0.007) loss 0.9185 (0.8964) teacher_loss 0.3695 (0.3795) loss_zs_kd 0.1359 (0.1353) loss_oracle 0.4810 (0.4493) acc 90.6250 (87.4219) alaph_mean 0.6775 (0.6772) alpha_val 0.6775 (0.6772) lr 4.6417e-04 eta 0:03:26
epoch [36/50] batch [60/160] time 0.086 (0.087) data 0.001 (0.004) loss 0.7905 (0.9032) teacher_loss 0.2736 (0.3852) loss_zs_kd 0.1357 (0.1319) loss_oracle 0.4491 (0.4520) acc 90.6250 (87.2917) alaph_mean 0.6777 (0.6773) alpha_val 0.6777 (0.6773) lr 4.6417e-04 eta 0:03:23
epoch [36/50] batch [80/160] time 0.083 (0.087) data 0.000 (0.003) loss 0.8281 (0.9019) teacher_loss 0.2718 (0.3864) loss_zs_kd 0.1468 (0.1308) loss_oracle 0.4829 (0.4501) acc 90.6250 (87.2656) alaph_mean 0.6780 (0.6775) alpha_val 0.6780 (0.6775) lr 4.6417e-04 eta 0:03:21
epoch [36/50] batch [100/160] time 0.087 (0.086) data 0.000 (0.003) loss 0.9303 (0.9058) teacher_loss 0.3692 (0.3846) loss_zs_kd 0.1251 (0.1331) loss_oracle 0.4985 (0.4547) acc 90.6250 (87.2500) alaph_mean 0.6782 (0.6776) alpha_val 0.6782 (0.6776) lr 4.6417e-04 eta 0:03:18
epoch [36/50] batch [120/160] time 0.091 (0.086) data 0.001 (0.002) loss 0.8091 (0.9061) teacher_loss 0.2552 (0.3861) loss_zs_kd 0.1516 (0.1335) loss_oracle 0.4781 (0.4533) acc 90.6250 (87.3177) alaph_mean 0.6784 (0.6777) alpha_val 0.6784 (0.6777) lr 4.6417e-04 eta 0:03:16
epoch [36/50] batch [140/160] time 0.080 (0.087) data 0.000 (0.002) loss 0.9091 (0.9041) teacher_loss 0.3124 (0.3837) loss_zs_kd 0.1694 (0.1329) loss_oracle 0.5119 (0.4539) acc 87.5000 (87.3214) alaph_mean 0.6787 (0.6778) alpha_val 0.6787 (0.6778) lr 4.6417e-04 eta 0:03:16
epoch [36/50] batch [160/160] time 0.071 (0.085) data 0.000 (0.002) loss 1.0587 (0.8978) teacher_loss 0.4521 (0.3761) loss_zs_kd 0.1660 (0.1363) loss_oracle 0.5236 (0.4535) acc 81.2500 (87.8320) alaph_mean 0.6790 (0.6780) alpha_val 0.6790 (0.6780) lr 4.1221e-04 eta 0:03:11
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,853
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 84.9%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,981
* accuracy: 88.3%
* error: 11.7%
* macro_f1: 89.4%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [37/50] batch [20/160] time 0.082 (0.100) data 0.000 (0.016) loss 0.8605 (0.9117) teacher_loss 0.3329 (0.3975) loss_zs_kd 0.1725 (0.1508) loss_oracle 0.4414 (0.4388) acc 90.6250 (88.2812) alaph_mean 0.6792 (0.6791) alpha_val 0.6792 (0.6791) lr 4.1221e-04 eta 0:03:40
epoch [37/50] batch [40/160] time 0.083 (0.092) data 0.000 (0.008) loss 0.9009 (0.8754) teacher_loss 0.3609 (0.3667) loss_zs_kd 0.1283 (0.1353) loss_oracle 0.4759 (0.4410) acc 87.5000 (88.5938) alaph_mean 0.6795 (0.6792) alpha_val 0.6795 (0.6792) lr 4.1221e-04 eta 0:03:23
epoch [37/50] batch [60/160] time 0.094 (0.090) data 0.000 (0.006) loss 0.9953 (0.8767) teacher_loss 0.4741 (0.3614) loss_zs_kd 0.1109 (0.1373) loss_oracle 0.4657 (0.4467) acc 87.5000 (88.6979) alaph_mean 0.6797 (0.6794) alpha_val 0.6797 (0.6794) lr 4.1221e-04 eta 0:03:15
epoch [37/50] batch [80/160] time 0.083 (0.089) data 0.000 (0.004) loss 1.0098 (0.8875) teacher_loss 0.4464 (0.3716) loss_zs_kd 0.1271 (0.1369) loss_oracle 0.4999 (0.4474) acc 90.6250 (88.3594) alaph_mean 0.6800 (0.6795) alpha_val 0.6800 (0.6795) lr 4.1221e-04 eta 0:03:11
epoch [37/50] batch [100/160] time 0.085 (0.088) data 0.000 (0.004) loss 1.1972 (0.8797) teacher_loss 0.6979 (0.3672) loss_zs_kd 0.1747 (0.1350) loss_oracle 0.4120 (0.4450) acc 75.0000 (88.3438) alaph_mean 0.6803 (0.6796) alpha_val 0.6803 (0.6796) lr 4.1221e-04 eta 0:03:09
epoch [37/50] batch [120/160] time 0.079 (0.088) data 0.000 (0.003) loss 0.7136 (0.8942) teacher_loss 0.2831 (0.3788) loss_zs_kd 0.1694 (0.1388) loss_oracle 0.3458 (0.4460) acc 93.7500 (87.8385) alaph_mean 0.6805 (0.6798) alpha_val 0.6805 (0.6798) lr 4.1221e-04 eta 0:03:06
epoch [37/50] batch [140/160] time 0.084 (0.088) data 0.000 (0.003) loss 0.9694 (0.8930) teacher_loss 0.4278 (0.3764) loss_zs_kd 0.1396 (0.1390) loss_oracle 0.4718 (0.4471) acc 81.2500 (87.9018) alaph_mean 0.6808 (0.6799) alpha_val 0.6808 (0.6799) lr 4.1221e-04 eta 0:03:03
epoch [37/50] batch [160/160] time 0.075 (0.086) data 0.000 (0.002) loss 0.9524 (0.8976) teacher_loss 0.4623 (0.3832) loss_zs_kd 0.0999 (0.1388) loss_oracle 0.4402 (0.4451) acc 84.3750 (87.6758) alaph_mean 0.6810 (0.6800) alpha_val 0.6810 (0.6800) lr 3.6258e-04 eta 0:02:59
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,863
* accuracy: 84.5%
* error: 15.5%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,976
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 89.2%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [38/50] batch [20/160] time 0.078 (0.097) data 0.000 (0.013) loss 0.9609 (0.9305) teacher_loss 0.4586 (0.4112) loss_zs_kd 0.1349 (0.1352) loss_oracle 0.4348 (0.4517) acc 90.6250 (86.7188) alaph_mean 0.6813 (0.6811) alpha_val 0.6813 (0.6811) lr 3.6258e-04 eta 0:03:20
epoch [38/50] batch [40/160] time 0.086 (0.090) data 0.000 (0.007) loss 1.0805 (0.9330) teacher_loss 0.5099 (0.4194) loss_zs_kd 0.1504 (0.1377) loss_oracle 0.4954 (0.4447) acc 75.0000 (86.6406) alaph_mean 0.6815 (0.6813) alpha_val 0.6815 (0.6813) lr 3.6258e-04 eta 0:03:03
epoch [38/50] batch [60/160] time 0.083 (0.089) data 0.001 (0.004) loss 0.8671 (0.9191) teacher_loss 0.4035 (0.4089) loss_zs_kd 0.1324 (0.1364) loss_oracle 0.3974 (0.4420) acc 84.3750 (86.9792) alaph_mean 0.6818 (0.6814) alpha_val 0.6818 (0.6814) lr 3.6258e-04 eta 0:02:59
epoch [38/50] batch [80/160] time 0.087 (0.088) data 0.000 (0.003) loss 1.0122 (0.9113) teacher_loss 0.4684 (0.4021) loss_zs_kd 0.1234 (0.1364) loss_oracle 0.4822 (0.4410) acc 78.1250 (86.7578) alaph_mean 0.6820 (0.6815) alpha_val 0.6820 (0.6815) lr 3.6258e-04 eta 0:02:56
epoch [38/50] batch [100/160] time 0.081 (0.087) data 0.000 (0.003) loss 0.9696 (0.9064) teacher_loss 0.4301 (0.3973) loss_zs_kd 0.1020 (0.1346) loss_oracle 0.4885 (0.4417) acc 81.2500 (87.0938) alaph_mean 0.6822 (0.6816) alpha_val 0.6822 (0.6816) lr 3.6258e-04 eta 0:02:52
epoch [38/50] batch [120/160] time 0.091 (0.087) data 0.000 (0.002) loss 0.9269 (0.9031) teacher_loss 0.4369 (0.3944) loss_zs_kd 0.1432 (0.1325) loss_oracle 0.4183 (0.4424) acc 84.3750 (87.1354) alaph_mean 0.6824 (0.6817) alpha_val 0.6824 (0.6817) lr 3.6258e-04 eta 0:02:50
epoch [38/50] batch [140/160] time 0.088 (0.087) data 0.000 (0.002) loss 0.7597 (0.9030) teacher_loss 0.2898 (0.3964) loss_zs_kd 0.1242 (0.1322) loss_oracle 0.4078 (0.4405) acc 90.6250 (87.0312) alaph_mean 0.6827 (0.6819) alpha_val 0.6827 (0.6819) lr 3.6258e-04 eta 0:02:48
epoch [38/50] batch [160/160] time 0.098 (0.086) data 0.000 (0.002) loss 0.7569 (0.9039) teacher_loss 0.3199 (0.3974) loss_zs_kd 0.1233 (0.1326) loss_oracle 0.3754 (0.4401) acc 90.6250 (87.0703) alaph_mean 0.6831 (0.6820) alpha_val 0.6831 (0.6820) lr 3.1545e-04 eta 0:02:44
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,855
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 85.1%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,984
* accuracy: 88.4%
* error: 11.6%
* macro_f1: 89.4%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [39/50] batch [20/160] time 0.106 (0.111) data 0.000 (0.018) loss 0.7581 (0.8888) teacher_loss 0.1546 (0.3678) loss_zs_kd 0.0901 (0.1381) loss_oracle 0.5584 (0.4519) acc 93.7500 (88.7500) alaph_mean 0.6833 (0.6832) alpha_val 0.6833 (0.6832) lr 3.1545e-04 eta 0:03:31
epoch [39/50] batch [40/160] time 0.082 (0.101) data 0.001 (0.009) loss 0.9472 (0.9140) teacher_loss 0.4370 (0.3960) loss_zs_kd 0.1518 (0.1399) loss_oracle 0.4342 (0.4481) acc 87.5000 (87.6562) alaph_mean 0.6834 (0.6833) alpha_val 0.6834 (0.6833) lr 3.1545e-04 eta 0:03:10
epoch [39/50] batch [60/160] time 0.083 (0.095) data 0.001 (0.006) loss 0.8189 (0.9126) teacher_loss 0.3884 (0.3935) loss_zs_kd 0.0810 (0.1414) loss_oracle 0.3900 (0.4484) acc 90.6250 (87.6042) alaph_mean 0.6837 (0.6834) alpha_val 0.6837 (0.6834) lr 3.1545e-04 eta 0:02:56
epoch [39/50] batch [80/160] time 0.091 (0.092) data 0.000 (0.005) loss 0.9452 (0.9130) teacher_loss 0.3998 (0.3951) loss_zs_kd 0.1282 (0.1397) loss_oracle 0.4813 (0.4480) acc 93.7500 (87.6562) alaph_mean 0.6839 (0.6835) alpha_val 0.6839 (0.6835) lr 3.1545e-04 eta 0:02:48
epoch [39/50] batch [100/160] time 0.082 (0.091) data 0.000 (0.004) loss 0.7520 (0.9108) teacher_loss 0.2351 (0.3928) loss_zs_kd 0.1161 (0.1401) loss_oracle 0.4589 (0.4479) acc 93.7500 (87.8750) alaph_mean 0.6841 (0.6836) alpha_val 0.6841 (0.6836) lr 3.1545e-04 eta 0:02:44
epoch [39/50] batch [120/160] time 0.071 (0.087) data 0.000 (0.003) loss 0.9376 (0.9136) teacher_loss 0.4488 (0.3955) loss_zs_kd 0.1372 (0.1409) loss_oracle 0.4203 (0.4476) acc 90.6250 (87.8385) alaph_mean 0.6843 (0.6837) alpha_val 0.6843 (0.6837) lr 3.1545e-04 eta 0:02:37
epoch [39/50] batch [140/160] time 0.073 (0.085) data 0.000 (0.003) loss 0.7124 (0.9099) teacher_loss 0.1575 (0.3968) loss_zs_kd 0.0884 (0.1389) loss_oracle 0.5107 (0.4437) acc 93.7500 (87.5893) alaph_mean 0.6845 (0.6838) alpha_val 0.6845 (0.6838) lr 3.1545e-04 eta 0:02:30
epoch [39/50] batch [160/160] time 0.077 (0.083) data 0.000 (0.003) loss 0.7915 (0.9063) teacher_loss 0.2560 (0.3944) loss_zs_kd 0.1879 (0.1395) loss_oracle 0.4416 (0.4421) acc 96.8750 (87.8320) alaph_mean 0.6848 (0.6839) alpha_val 0.6848 (0.6839) lr 2.7103e-04 eta 0:02:26
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,852
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,970
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 89.1%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [40/50] batch [20/160] time 0.091 (0.103) data 0.000 (0.016) loss 0.8101 (0.8293) teacher_loss 0.2448 (0.3447) loss_zs_kd 0.1475 (0.1343) loss_oracle 0.4916 (0.4174) acc 90.6250 (90.1562) alaph_mean 0.6849 (0.6849) alpha_val 0.6849 (0.6849) lr 2.7103e-04 eta 0:02:59
epoch [40/50] batch [40/160] time 0.082 (0.093) data 0.000 (0.008) loss 0.9623 (0.8444) teacher_loss 0.4817 (0.3550) loss_zs_kd 0.1335 (0.1316) loss_oracle 0.4139 (0.4236) acc 87.5000 (89.4531) alaph_mean 0.6850 (0.6849) alpha_val 0.6850 (0.6849) lr 2.7103e-04 eta 0:02:40
epoch [40/50] batch [60/160] time 0.081 (0.089) data 0.001 (0.005) loss 0.6869 (0.8408) teacher_loss 0.1953 (0.3407) loss_zs_kd 0.1113 (0.1360) loss_oracle 0.4359 (0.4321) acc 96.8750 (89.7917) alaph_mean 0.6851 (0.6850) alpha_val 0.6851 (0.6850) lr 2.7103e-04 eta 0:02:31
epoch [40/50] batch [80/160] time 0.086 (0.088) data 0.001 (0.004) loss 0.7776 (0.8623) teacher_loss 0.3205 (0.3578) loss_zs_kd 0.1529 (0.1389) loss_oracle 0.3806 (0.4351) acc 90.6250 (88.9844) alaph_mean 0.6852 (0.6850) alpha_val 0.6852 (0.6850) lr 2.7103e-04 eta 0:02:28
epoch [40/50] batch [100/160] time 0.087 (0.088) data 0.000 (0.003) loss 0.9586 (0.8703) teacher_loss 0.4982 (0.3646) loss_zs_kd 0.0940 (0.1380) loss_oracle 0.4134 (0.4367) acc 78.1250 (88.7188) alaph_mean 0.6853 (0.6851) alpha_val 0.6853 (0.6851) lr 2.7103e-04 eta 0:02:26
epoch [40/50] batch [120/160] time 0.083 (0.090) data 0.000 (0.003) loss 0.9342 (0.8749) teacher_loss 0.4190 (0.3661) loss_zs_kd 0.1406 (0.1397) loss_oracle 0.4449 (0.4390) acc 87.5000 (88.5156) alaph_mean 0.6855 (0.6851) alpha_val 0.6855 (0.6851) lr 2.7103e-04 eta 0:02:28
epoch [40/50] batch [140/160] time 0.083 (0.090) data 0.000 (0.003) loss 0.7693 (0.8792) teacher_loss 0.2614 (0.3690) loss_zs_kd 0.1774 (0.1407) loss_oracle 0.4192 (0.4398) acc 96.8750 (88.4152) alaph_mean 0.6856 (0.6852) alpha_val 0.6856 (0.6852) lr 2.7103e-04 eta 0:02:25
epoch [40/50] batch [160/160] time 0.073 (0.088) data 0.000 (0.002) loss 0.8876 (0.8884) teacher_loss 0.3370 (0.3739) loss_zs_kd 0.1330 (0.1431) loss_oracle 0.4840 (0.4430) acc 87.5000 (88.1055) alaph_mean 0.6857 (0.6853) alpha_val 0.6857 (0.6853) lr 2.2949e-04 eta 0:02:21
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,846
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 85.1%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,976
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 89.3%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [41/50] batch [20/160] time 0.060 (0.101) data 0.000 (0.019) loss 0.9086 (0.8787) teacher_loss 0.3882 (0.3898) loss_zs_kd 0.1102 (0.1384) loss_oracle 0.4653 (0.4198) acc 87.5000 (89.2188) alaph_mean 0.6859 (0.6858) alpha_val 0.6859 (0.6858) lr 2.2949e-04 eta 0:02:38
epoch [41/50] batch [40/160] time 0.097 (0.091) data 0.000 (0.010) loss 0.9854 (0.8961) teacher_loss 0.4710 (0.3965) loss_zs_kd 0.1349 (0.1367) loss_oracle 0.4469 (0.4312) acc 81.2500 (88.2031) alaph_mean 0.6860 (0.6859) alpha_val 0.6860 (0.6859) lr 2.2949e-04 eta 0:02:22
epoch [41/50] batch [60/160] time 0.084 (0.089) data 0.001 (0.007) loss 0.8561 (0.8936) teacher_loss 0.3297 (0.3900) loss_zs_kd 0.1097 (0.1359) loss_oracle 0.4716 (0.4356) acc 90.6250 (88.2292) alaph_mean 0.6861 (0.6859) alpha_val 0.6861 (0.6859) lr 2.2949e-04 eta 0:02:17
epoch [41/50] batch [80/160] time 0.092 (0.088) data 0.000 (0.005) loss 0.6151 (0.8881) teacher_loss 0.2116 (0.3862) loss_zs_kd 0.1162 (0.1365) loss_oracle 0.3453 (0.4336) acc 93.7500 (88.0859) alaph_mean 0.6862 (0.6860) alpha_val 0.6862 (0.6860) lr 2.2949e-04 eta 0:02:14
epoch [41/50] batch [100/160] time 0.091 (0.088) data 0.000 (0.004) loss 0.9043 (0.8903) teacher_loss 0.3439 (0.3860) loss_zs_kd 0.1727 (0.1383) loss_oracle 0.4741 (0.4352) acc 87.5000 (88.2188) alaph_mean 0.6863 (0.6860) alpha_val 0.6863 (0.6860) lr 2.2949e-04 eta 0:02:11
epoch [41/50] batch [120/160] time 0.084 (0.088) data 0.000 (0.003) loss 0.9072 (0.8871) teacher_loss 0.4333 (0.3829) loss_zs_kd 0.1704 (0.1370) loss_oracle 0.3888 (0.4357) acc 93.7500 (88.1250) alaph_mean 0.6864 (0.6861) alpha_val 0.6864 (0.6861) lr 2.2949e-04 eta 0:02:10
epoch [41/50] batch [140/160] time 0.087 (0.088) data 0.000 (0.003) loss 0.9645 (0.8915) teacher_loss 0.4669 (0.3849) loss_zs_kd 0.1318 (0.1374) loss_oracle 0.4317 (0.4379) acc 87.5000 (88.0580) alaph_mean 0.6865 (0.6861) alpha_val 0.6865 (0.6861) lr 2.2949e-04 eta 0:02:08
epoch [41/50] batch [160/160] time 0.076 (0.087) data 0.000 (0.003) loss 0.9279 (0.8905) teacher_loss 0.3011 (0.3813) loss_zs_kd 0.1873 (0.1390) loss_oracle 0.5332 (0.4397) acc 90.6250 (88.2227) alaph_mean 0.6866 (0.6862) alpha_val 0.6866 (0.6862) lr 1.9098e-04 eta 0:02:05
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,850
* accuracy: 83.9%
* error: 16.1%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,974
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 89.2%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [42/50] batch [20/160] time 0.091 (0.096) data 0.000 (0.013) loss 1.1047 (0.9091) teacher_loss 0.6130 (0.3865) loss_zs_kd 0.1602 (0.1531) loss_oracle 0.4116 (0.4461) acc 75.0000 (89.0625) alaph_mean 0.6867 (0.6867) alpha_val 0.6867 (0.6867) lr 1.9098e-04 eta 0:02:16
epoch [42/50] batch [40/160] time 0.082 (0.090) data 0.000 (0.007) loss 0.7405 (0.9250) teacher_loss 0.2688 (0.3974) loss_zs_kd 0.1274 (0.1493) loss_oracle 0.4080 (0.4530) acc 93.7500 (88.0469) alaph_mean 0.6868 (0.6867) alpha_val 0.6868 (0.6867) lr 1.9098e-04 eta 0:02:05
epoch [42/50] batch [60/160] time 0.091 (0.093) data 0.001 (0.005) loss 0.9140 (0.9073) teacher_loss 0.4689 (0.3855) loss_zs_kd 0.1233 (0.1427) loss_oracle 0.3835 (0.4504) acc 84.3750 (88.1771) alaph_mean 0.6869 (0.6868) alpha_val 0.6869 (0.6868) lr 1.9098e-04 eta 0:02:08
epoch [42/50] batch [80/160] time 0.087 (0.091) data 0.001 (0.004) loss 0.7078 (0.8876) teacher_loss 0.2491 (0.3694) loss_zs_kd 0.1244 (0.1407) loss_oracle 0.3965 (0.4479) acc 96.8750 (88.7891) alaph_mean 0.6870 (0.6868) alpha_val 0.6870 (0.6868) lr 1.9098e-04 eta 0:02:03
epoch [42/50] batch [100/160] time 0.084 (0.090) data 0.000 (0.003) loss 0.8667 (0.8906) teacher_loss 0.3198 (0.3742) loss_zs_kd 0.1249 (0.1390) loss_oracle 0.4845 (0.4468) acc 90.6250 (88.5938) alaph_mean 0.6871 (0.6869) alpha_val 0.6871 (0.6869) lr 1.9098e-04 eta 0:02:00
epoch [42/50] batch [120/160] time 0.083 (0.089) data 0.000 (0.002) loss 0.8338 (0.8843) teacher_loss 0.3164 (0.3710) loss_zs_kd 0.1208 (0.1384) loss_oracle 0.4570 (0.4441) acc 87.5000 (88.5938) alaph_mean 0.6872 (0.6869) alpha_val 0.6872 (0.6869) lr 1.9098e-04 eta 0:01:57
epoch [42/50] batch [140/160] time 0.085 (0.089) data 0.000 (0.002) loss 0.9757 (0.8960) teacher_loss 0.4262 (0.3795) loss_zs_kd 0.1163 (0.1396) loss_oracle 0.4914 (0.4467) acc 81.2500 (88.1027) alaph_mean 0.6873 (0.6870) alpha_val 0.6873 (0.6870) lr 1.9098e-04 eta 0:01:55
epoch [42/50] batch [160/160] time 0.078 (0.088) data 0.000 (0.002) loss 0.8619 (0.8945) teacher_loss 0.3173 (0.3752) loss_zs_kd 0.1305 (0.1417) loss_oracle 0.4794 (0.4484) acc 93.7500 (88.1641) alaph_mean 0.6873 (0.6870) alpha_val 0.6873 (0.6870) lr 1.5567e-04 eta 0:01:52
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,847
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 84.9%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,972
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 89.1%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [43/50] batch [20/160] time 0.082 (0.107) data 0.000 (0.017) loss 1.0691 (0.8783) teacher_loss 0.4959 (0.3522) loss_zs_kd 0.1785 (0.1459) loss_oracle 0.4839 (0.4531) acc 87.5000 (90.1562) alaph_mean 0.6874 (0.6874) alpha_val 0.6874 (0.6874) lr 1.5567e-04 eta 0:02:14
epoch [43/50] batch [40/160] time 0.109 (0.098) data 0.001 (0.009) loss 0.8303 (0.8752) teacher_loss 0.3929 (0.3494) loss_zs_kd 0.1269 (0.1446) loss_oracle 0.3740 (0.4535) acc 84.3750 (90.0781) alaph_mean 0.6875 (0.6874) alpha_val 0.6875 (0.6874) lr 1.5567e-04 eta 0:02:01
epoch [43/50] batch [60/160] time 0.082 (0.093) data 0.001 (0.006) loss 0.9945 (0.8874) teacher_loss 0.3522 (0.3562) loss_zs_kd 0.2243 (0.1472) loss_oracle 0.5302 (0.4576) acc 90.6250 (89.6354) alaph_mean 0.6876 (0.6875) alpha_val 0.6876 (0.6875) lr 1.5567e-04 eta 0:01:52
epoch [43/50] batch [80/160] time 0.076 (0.091) data 0.000 (0.005) loss 0.8283 (0.8963) teacher_loss 0.3280 (0.3670) loss_zs_kd 0.2023 (0.1489) loss_oracle 0.3992 (0.4549) acc 90.6250 (89.2188) alaph_mean 0.6876 (0.6875) alpha_val 0.6876 (0.6875) lr 1.5567e-04 eta 0:01:48
epoch [43/50] batch [100/160] time 0.085 (0.089) data 0.000 (0.004) loss 0.7652 (0.8901) teacher_loss 0.2669 (0.3640) loss_zs_kd 0.1443 (0.1454) loss_oracle 0.4262 (0.4534) acc 90.6250 (89.0312) alaph_mean 0.6877 (0.6875) alpha_val 0.6877 (0.6875) lr 1.5567e-04 eta 0:01:44
epoch [43/50] batch [120/160] time 0.075 (0.088) data 0.000 (0.003) loss 0.7174 (0.8927) teacher_loss 0.1618 (0.3683) loss_zs_kd 0.1170 (0.1451) loss_oracle 0.4971 (0.4520) acc 96.8750 (88.8281) alaph_mean 0.6878 (0.6876) alpha_val 0.6878 (0.6876) lr 1.5567e-04 eta 0:01:42
epoch [43/50] batch [140/160] time 0.082 (0.088) data 0.000 (0.003) loss 0.7484 (0.8855) teacher_loss 0.2684 (0.3624) loss_zs_kd 0.1193 (0.1444) loss_oracle 0.4204 (0.4509) acc 93.7500 (88.9286) alaph_mean 0.6879 (0.6876) alpha_val 0.6879 (0.6876) lr 1.5567e-04 eta 0:01:40
epoch [43/50] batch [160/160] time 0.079 (0.087) data 0.000 (0.002) loss 0.8620 (0.8920) teacher_loss 0.3217 (0.3691) loss_zs_kd 0.1589 (0.1441) loss_oracle 0.4609 (0.4509) acc 90.6250 (88.4961) alaph_mean 0.6880 (0.6877) alpha_val 0.6880 (0.6877) lr 1.2369e-04 eta 0:01:37
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,847
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 84.9%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,980
* accuracy: 88.3%
* error: 11.7%
* macro_f1: 89.3%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [44/50] batch [20/160] time 0.076 (0.099) data 0.000 (0.014) loss 0.8662 (0.8956) teacher_loss 0.3914 (0.3762) loss_zs_kd 0.1306 (0.1420) loss_oracle 0.4095 (0.4484) acc 81.2500 (88.7500) alaph_mean 0.6881 (0.6880) alpha_val 0.6881 (0.6880) lr 1.2369e-04 eta 0:01:48
epoch [44/50] batch [40/160] time 0.098 (0.090) data 0.000 (0.007) loss 0.9296 (0.8948) teacher_loss 0.4267 (0.3719) loss_zs_kd 0.1490 (0.1416) loss_oracle 0.4284 (0.4520) acc 93.7500 (88.9844) alaph_mean 0.6882 (0.6881) alpha_val 0.6882 (0.6881) lr 1.2369e-04 eta 0:01:37
epoch [44/50] batch [60/160] time 0.080 (0.085) data 0.001 (0.005) loss 0.9349 (0.8953) teacher_loss 0.4643 (0.3758) loss_zs_kd 0.1261 (0.1422) loss_oracle 0.4076 (0.4484) acc 84.3750 (88.5938) alaph_mean 0.6882 (0.6881) alpha_val 0.6882 (0.6881) lr 1.2369e-04 eta 0:01:30
epoch [44/50] batch [80/160] time 0.081 (0.083) data 0.000 (0.004) loss 0.8473 (0.8955) teacher_loss 0.2997 (0.3767) loss_zs_kd 0.1527 (0.1429) loss_oracle 0.4713 (0.4474) acc 96.8750 (88.5938) alaph_mean 0.6883 (0.6882) alpha_val 0.6883 (0.6882) lr 1.2369e-04 eta 0:01:26
epoch [44/50] batch [100/160] time 0.084 (0.083) data 0.000 (0.003) loss 0.9304 (0.8939) teacher_loss 0.4282 (0.3751) loss_zs_kd 0.1306 (0.1436) loss_oracle 0.4369 (0.4470) acc 84.3750 (88.5000) alaph_mean 0.6884 (0.6882) alpha_val 0.6884 (0.6882) lr 1.2369e-04 eta 0:01:24
epoch [44/50] batch [120/160] time 0.087 (0.083) data 0.000 (0.003) loss 0.9187 (0.8931) teacher_loss 0.3697 (0.3746) loss_zs_kd 0.1172 (0.1415) loss_oracle 0.4904 (0.4477) acc 84.3750 (88.3073) alaph_mean 0.6885 (0.6882) alpha_val 0.6885 (0.6882) lr 1.2369e-04 eta 0:01:23
epoch [44/50] batch [140/160] time 0.092 (0.084) data 0.001 (0.002) loss 0.9440 (0.8894) teacher_loss 0.4346 (0.3728) loss_zs_kd 0.1952 (0.1426) loss_oracle 0.4118 (0.4453) acc 90.6250 (88.2589) alaph_mean 0.6886 (0.6883) alpha_val 0.6886 (0.6883) lr 1.2369e-04 eta 0:01:21
epoch [44/50] batch [160/160] time 0.076 (0.083) data 0.000 (0.002) loss 1.0180 (0.8939) teacher_loss 0.5093 (0.3757) loss_zs_kd 0.1498 (0.1433) loss_oracle 0.4339 (0.4465) acc 84.3750 (88.0664) alaph_mean 0.6887 (0.6883) alpha_val 0.6887 (0.6883) lr 9.5173e-05 eta 0:01:19
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,850
* accuracy: 83.9%
* error: 16.1%
* macro_f1: 85.0%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,971
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 89.1%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [45/50] batch [20/160] time 0.077 (0.092) data 0.000 (0.014) loss 0.8663 (0.8881) teacher_loss 0.3419 (0.3803) loss_zs_kd 0.1319 (0.1345) loss_oracle 0.4585 (0.4405) acc 87.5000 (87.8125) alaph_mean 0.6887 (0.6887) alpha_val 0.6887 (0.6887) lr 9.5173e-05 eta 0:01:26
epoch [45/50] batch [40/160] time 0.082 (0.086) data 0.000 (0.007) loss 0.9269 (0.8933) teacher_loss 0.4936 (0.3695) loss_zs_kd 0.0837 (0.1395) loss_oracle 0.3915 (0.4541) acc 90.6250 (88.6719) alaph_mean 0.6888 (0.6887) alpha_val 0.6888 (0.6887) lr 9.5173e-05 eta 0:01:19
epoch [45/50] batch [60/160] time 0.083 (0.084) data 0.001 (0.005) loss 0.8249 (0.8998) teacher_loss 0.2744 (0.3796) loss_zs_kd 0.1230 (0.1422) loss_oracle 0.4890 (0.4490) acc 96.8750 (88.5417) alaph_mean 0.6888 (0.6888) alpha_val 0.6888 (0.6888) lr 9.5173e-05 eta 0:01:15
epoch [45/50] batch [80/160] time 0.075 (0.083) data 0.000 (0.004) loss 0.7927 (0.8947) teacher_loss 0.3006 (0.3774) loss_zs_kd 0.1620 (0.1439) loss_oracle 0.4111 (0.4454) acc 93.7500 (88.4375) alaph_mean 0.6889 (0.6888) alpha_val 0.6889 (0.6888) lr 9.5173e-05 eta 0:01:13
epoch [45/50] batch [100/160] time 0.080 (0.083) data 0.000 (0.003) loss 0.9040 (0.8930) teacher_loss 0.4039 (0.3771) loss_zs_kd 0.1693 (0.1451) loss_oracle 0.4155 (0.4434) acc 93.7500 (88.6250) alaph_mean 0.6889 (0.6888) alpha_val 0.6889 (0.6888) lr 9.5173e-05 eta 0:01:11
epoch [45/50] batch [120/160] time 0.081 (0.083) data 0.000 (0.003) loss 0.8382 (0.8897) teacher_loss 0.2818 (0.3756) loss_zs_kd 0.1824 (0.1448) loss_oracle 0.4652 (0.4417) acc 90.6250 (88.5677) alaph_mean 0.6890 (0.6888) alpha_val 0.6890 (0.6888) lr 9.5173e-05 eta 0:01:10
epoch [45/50] batch [140/160] time 0.086 (0.083) data 0.001 (0.002) loss 0.8816 (0.8848) teacher_loss 0.4343 (0.3709) loss_zs_kd 0.1436 (0.1444) loss_oracle 0.3755 (0.4417) acc 81.2500 (88.7054) alaph_mean 0.6891 (0.6889) alpha_val 0.6891 (0.6889) lr 9.5173e-05 eta 0:01:08
epoch [45/50] batch [160/160] time 0.072 (0.083) data 0.000 (0.002) loss 0.8648 (0.8871) teacher_loss 0.4160 (0.3720) loss_zs_kd 0.1444 (0.1438) loss_oracle 0.3766 (0.4433) acc 87.5000 (88.7500) alaph_mean 0.6891 (0.6889) alpha_val 0.6891 (0.6889) lr 7.0224e-05 eta 0:01:06
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,848
* accuracy: 83.8%
* error: 16.2%
* macro_f1: 84.9%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,977
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 89.3%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [46/50] batch [20/160] time 0.091 (0.093) data 0.000 (0.013) loss 1.2464 (0.8931) teacher_loss 0.7282 (0.3595) loss_zs_kd 0.1399 (0.1528) loss_oracle 0.4482 (0.4572) acc 81.2500 (89.8438) alaph_mean 0.6892 (0.6891) alpha_val 0.6892 (0.6891) lr 7.0224e-05 eta 0:01:12
epoch [46/50] batch [40/160] time 0.077 (0.088) data 0.000 (0.007) loss 0.7694 (0.9237) teacher_loss 0.2674 (0.3933) loss_zs_kd 0.1636 (0.1534) loss_oracle 0.4202 (0.4537) acc 96.8750 (88.6719) alaph_mean 0.6892 (0.6892) alpha_val 0.6892 (0.6892) lr 7.0224e-05 eta 0:01:06
epoch [46/50] batch [60/160] time 0.087 (0.087) data 0.001 (0.005) loss 0.8333 (0.9087) teacher_loss 0.3247 (0.3771) loss_zs_kd 0.1174 (0.1510) loss_oracle 0.4499 (0.4561) acc 87.5000 (89.1146) alaph_mean 0.6892 (0.6892) alpha_val 0.6892 (0.6892) lr 7.0224e-05 eta 0:01:04
epoch [46/50] batch [80/160] time 0.080 (0.087) data 0.000 (0.003) loss 0.8470 (0.9001) teacher_loss 0.3290 (0.3727) loss_zs_kd 0.1082 (0.1494) loss_oracle 0.4639 (0.4527) acc 84.3750 (89.1797) alaph_mean 0.6893 (0.6892) alpha_val 0.6893 (0.6892) lr 7.0224e-05 eta 0:01:02
epoch [46/50] batch [100/160] time 0.081 (0.087) data 0.000 (0.003) loss 0.7321 (0.8972) teacher_loss 0.2095 (0.3686) loss_zs_kd 0.1208 (0.1493) loss_oracle 0.4621 (0.4539) acc 96.8750 (89.3750) alaph_mean 0.6893 (0.6892) alpha_val 0.6893 (0.6892) lr 7.0224e-05 eta 0:01:01
epoch [46/50] batch [120/160] time 0.087 (0.086) data 0.000 (0.002) loss 0.9553 (0.9029) teacher_loss 0.4437 (0.3734) loss_zs_kd 0.1625 (0.1513) loss_oracle 0.4303 (0.4539) acc 87.5000 (89.1146) alaph_mean 0.6894 (0.6892) alpha_val 0.6894 (0.6892) lr 7.0224e-05 eta 0:00:58
epoch [46/50] batch [140/160] time 0.084 (0.086) data 0.000 (0.002) loss 0.6790 (0.9041) teacher_loss 0.1885 (0.3783) loss_zs_kd 0.1087 (0.1490) loss_oracle 0.4362 (0.4513) acc 96.8750 (88.7054) alaph_mean 0.6894 (0.6893) alpha_val 0.6894 (0.6893) lr 7.0224e-05 eta 0:00:57
epoch [46/50] batch [160/160] time 0.075 (0.085) data 0.000 (0.002) loss 0.8174 (0.9012) teacher_loss 0.2910 (0.3766) loss_zs_kd 0.1494 (0.1475) loss_oracle 0.4517 (0.4509) acc 90.6250 (88.4961) alaph_mean 0.6894 (0.6893) alpha_val 0.6894 (0.6893) lr 4.8943e-05 eta 0:00:54
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,847
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 84.8%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,976
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 89.3%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [47/50] batch [20/160] time 0.091 (0.102) data 0.001 (0.015) loss 0.8821 (0.8808) teacher_loss 0.3814 (0.3786) loss_zs_kd 0.1077 (0.1367) loss_oracle 0.4468 (0.4338) acc 87.5000 (87.1875) alaph_mean 0.6895 (0.6895) alpha_val 0.6895 (0.6895) lr 4.8943e-05 eta 0:01:02
epoch [47/50] batch [40/160] time 0.086 (0.092) data 0.000 (0.008) loss 1.0340 (0.8832) teacher_loss 0.4182 (0.3751) loss_zs_kd 0.1632 (0.1400) loss_oracle 0.5342 (0.4381) acc 84.3750 (87.1094) alaph_mean 0.6895 (0.6895) alpha_val 0.6895 (0.6895) lr 4.8943e-05 eta 0:00:55
epoch [47/50] batch [60/160] time 0.085 (0.090) data 0.001 (0.005) loss 0.8991 (0.8863) teacher_loss 0.3781 (0.3745) loss_zs_kd 0.1217 (0.1414) loss_oracle 0.4602 (0.4410) acc 84.3750 (87.2396) alaph_mean 0.6895 (0.6895) alpha_val 0.6895 (0.6895) lr 4.8943e-05 eta 0:00:52
epoch [47/50] batch [80/160] time 0.089 (0.090) data 0.000 (0.004) loss 0.7388 (0.8816) teacher_loss 0.3277 (0.3739) loss_zs_kd 0.1014 (0.1401) loss_oracle 0.3604 (0.4377) acc 84.3750 (87.5781) alaph_mean 0.6896 (0.6895) alpha_val 0.6896 (0.6895) lr 4.8943e-05 eta 0:00:50
epoch [47/50] batch [100/160] time 0.090 (0.089) data 0.000 (0.003) loss 0.8887 (0.8880) teacher_loss 0.4643 (0.3795) loss_zs_kd 0.1033 (0.1433) loss_oracle 0.3728 (0.4369) acc 81.2500 (87.6250) alaph_mean 0.6896 (0.6895) alpha_val 0.6896 (0.6895) lr 4.8943e-05 eta 0:00:47
epoch [47/50] batch [120/160] time 0.115 (0.089) data 0.001 (0.003) loss 1.0660 (0.8917) teacher_loss 0.4896 (0.3784) loss_zs_kd 0.1651 (0.1433) loss_oracle 0.4939 (0.4416) acc 87.5000 (87.9167) alaph_mean 0.6896 (0.6895) alpha_val 0.6896 (0.6895) lr 4.8943e-05 eta 0:00:46
epoch [47/50] batch [140/160] time 0.083 (0.089) data 0.000 (0.002) loss 0.9276 (0.8909) teacher_loss 0.4150 (0.3767) loss_zs_kd 0.1773 (0.1448) loss_oracle 0.4239 (0.4418) acc 78.1250 (87.9464) alaph_mean 0.6897 (0.6896) alpha_val 0.6897 (0.6896) lr 4.8943e-05 eta 0:00:44
epoch [47/50] batch [160/160] time 0.078 (0.089) data 0.001 (0.002) loss 1.1213 (0.8936) teacher_loss 0.5483 (0.3788) loss_zs_kd 0.1683 (0.1455) loss_oracle 0.4889 (0.4420) acc 81.2500 (87.8711) alaph_mean 0.6897 (0.6896) alpha_val 0.6897 (0.6896) lr 3.1417e-05 eta 0:00:42
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,848
* accuracy: 83.8%
* error: 16.2%
* macro_f1: 84.9%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,977
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 89.3%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [48/50] batch [20/160] time 0.081 (0.100) data 0.000 (0.016) loss 0.8272 (0.9141) teacher_loss 0.3938 (0.4005) loss_zs_kd 0.1203 (0.1427) loss_oracle 0.3732 (0.4422) acc 90.6250 (87.6562) alaph_mean 0.6897 (0.6897) alpha_val 0.6897 (0.6897) lr 3.1417e-05 eta 0:00:45
epoch [48/50] batch [40/160] time 0.083 (0.090) data 0.000 (0.008) loss 0.9019 (0.9013) teacher_loss 0.3703 (0.3790) loss_zs_kd 0.1176 (0.1462) loss_oracle 0.4728 (0.4492) acc 93.7500 (88.8281) alaph_mean 0.6897 (0.6897) alpha_val 0.6897 (0.6897) lr 3.1417e-05 eta 0:00:39
epoch [48/50] batch [60/160] time 0.097 (0.087) data 0.001 (0.006) loss 1.0035 (0.9049) teacher_loss 0.4302 (0.3813) loss_zs_kd 0.1591 (0.1472) loss_oracle 0.4937 (0.4500) acc 84.3750 (88.4896) alaph_mean 0.6898 (0.6897) alpha_val 0.6898 (0.6897) lr 3.1417e-05 eta 0:00:36
epoch [48/50] batch [80/160] time 0.084 (0.086) data 0.000 (0.004) loss 0.9838 (0.9038) teacher_loss 0.4918 (0.3801) loss_zs_kd 0.1526 (0.1450) loss_oracle 0.4157 (0.4513) acc 81.2500 (88.3594) alaph_mean 0.6898 (0.6897) alpha_val 0.6898 (0.6897) lr 3.1417e-05 eta 0:00:34
epoch [48/50] batch [100/160] time 0.086 (0.085) data 0.000 (0.003) loss 0.9848 (0.8954) teacher_loss 0.5410 (0.3737) loss_zs_kd 0.1146 (0.1451) loss_oracle 0.3865 (0.4492) acc 87.5000 (88.6875) alaph_mean 0.6898 (0.6897) alpha_val 0.6898 (0.6897) lr 3.1417e-05 eta 0:00:32
epoch [48/50] batch [120/160] time 0.082 (0.085) data 0.000 (0.003) loss 0.7605 (0.8881) teacher_loss 0.2679 (0.3681) loss_zs_kd 0.1329 (0.1449) loss_oracle 0.4262 (0.4476) acc 90.6250 (88.8021) alaph_mean 0.6898 (0.6898) alpha_val 0.6898 (0.6898) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [140/160] time 0.088 (0.085) data 0.000 (0.003) loss 0.8229 (0.8843) teacher_loss 0.3307 (0.3634) loss_zs_kd 0.1589 (0.1455) loss_oracle 0.4127 (0.4481) acc 90.6250 (88.9286) alaph_mean 0.6898 (0.6898) alpha_val 0.6898 (0.6898) lr 3.1417e-05 eta 0:00:29
epoch [48/50] batch [160/160] time 0.076 (0.085) data 0.000 (0.002) loss 1.1004 (0.8905) teacher_loss 0.5272 (0.3725) loss_zs_kd 0.1660 (0.1448) loss_oracle 0.4902 (0.4456) acc 81.2500 (88.5156) alaph_mean 0.6899 (0.6898) alpha_val 0.6899 (0.6898) lr 1.7713e-05 eta 0:00:27
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,848
* accuracy: 83.8%
* error: 16.2%
* macro_f1: 84.8%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,974
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 89.2%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [49/50] batch [20/160] time 0.082 (0.104) data 0.000 (0.015) loss 1.1251 (0.8793) teacher_loss 0.6284 (0.3615) loss_zs_kd 0.1662 (0.1437) loss_oracle 0.4135 (0.4459) acc 78.1250 (88.7500) alaph_mean 0.6899 (0.6899) alpha_val 0.6899 (0.6899) lr 1.7713e-05 eta 0:00:31
epoch [49/50] batch [40/160] time 0.079 (0.097) data 0.000 (0.008) loss 0.8616 (0.9042) teacher_loss 0.3167 (0.3880) loss_zs_kd 0.1711 (0.1434) loss_oracle 0.4594 (0.4445) acc 87.5000 (87.5000) alaph_mean 0.6899 (0.6899) alpha_val 0.6899 (0.6899) lr 1.7713e-05 eta 0:00:27
epoch [49/50] batch [60/160] time 0.101 (0.094) data 0.001 (0.005) loss 1.1264 (0.8882) teacher_loss 0.6084 (0.3685) loss_zs_kd 0.1712 (0.1431) loss_oracle 0.4324 (0.4482) acc 78.1250 (88.3333) alaph_mean 0.6899 (0.6899) alpha_val 0.6899 (0.6899) lr 1.7713e-05 eta 0:00:24
epoch [49/50] batch [80/160] time 0.085 (0.094) data 0.000 (0.004) loss 0.8833 (0.8889) teacher_loss 0.3869 (0.3676) loss_zs_kd 0.1383 (0.1445) loss_oracle 0.4273 (0.4491) acc 87.5000 (88.4766) alaph_mean 0.6899 (0.6899) alpha_val 0.6899 (0.6899) lr 1.7713e-05 eta 0:00:22
epoch [49/50] batch [100/160] time 0.091 (0.092) data 0.000 (0.003) loss 0.9078 (0.9013) teacher_loss 0.4178 (0.3809) loss_zs_kd 0.1713 (0.1451) loss_oracle 0.4043 (0.4479) acc 87.5000 (88.1250) alaph_mean 0.6899 (0.6899) alpha_val 0.6899 (0.6899) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [120/160] time 0.094 (0.092) data 0.001 (0.003) loss 1.0484 (0.9031) teacher_loss 0.4985 (0.3838) loss_zs_kd 0.1390 (0.1457) loss_oracle 0.4804 (0.4465) acc 87.5000 (87.8906) alaph_mean 0.6899 (0.6899) alpha_val 0.6899 (0.6899) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [140/160] time 0.089 (0.091) data 0.000 (0.003) loss 0.9167 (0.9017) teacher_loss 0.4258 (0.3855) loss_zs_kd 0.1353 (0.1439) loss_oracle 0.4232 (0.4443) acc 87.5000 (87.8795) alaph_mean 0.6899 (0.6899) alpha_val 0.6899 (0.6899) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [160/160] time 0.076 (0.089) data 0.001 (0.002) loss 0.9037 (0.9005) teacher_loss 0.3936 (0.3836) loss_zs_kd 0.1836 (0.1438) loss_oracle 0.4183 (0.4450) acc 87.5000 (87.9688) alaph_mean 0.6899 (0.6899) alpha_val 0.6899 (0.6899) lr 7.8853e-06 eta 0:00:14
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,849
* accuracy: 83.8%
* error: 16.2%
* macro_f1: 85.0%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,977
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 89.3%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
epoch [50/50] batch [20/160] time 0.076 (0.093) data 0.000 (0.011) loss 1.0412 (0.9145) teacher_loss 0.4522 (0.3664) loss_zs_kd 0.1520 (0.1486) loss_oracle 0.5130 (0.4738) acc 90.6250 (88.7500) alaph_mean 0.6900 (0.6900) alpha_val 0.6900 (0.6900) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [40/160] time 0.091 (0.088) data 0.000 (0.006) loss 1.0089 (0.8915) teacher_loss 0.4656 (0.3533) loss_zs_kd 0.1461 (0.1487) loss_oracle 0.4702 (0.4639) acc 87.5000 (89.6875) alaph_mean 0.6900 (0.6900) alpha_val 0.6900 (0.6900) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [60/160] time 0.090 (0.088) data 0.001 (0.004) loss 0.9716 (0.8754) teacher_loss 0.4374 (0.3512) loss_zs_kd 0.1377 (0.1451) loss_oracle 0.4653 (0.4516) acc 87.5000 (89.6354) alaph_mean 0.6900 (0.6900) alpha_val 0.6900 (0.6900) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [80/160] time 0.089 (0.088) data 0.000 (0.003) loss 0.9036 (0.8789) teacher_loss 0.3669 (0.3577) loss_zs_kd 0.1941 (0.1454) loss_oracle 0.4396 (0.4485) acc 90.6250 (89.3750) alaph_mean 0.6900 (0.6900) alpha_val 0.6900 (0.6900) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [100/160] time 0.088 (0.087) data 0.001 (0.003) loss 1.0213 (0.8827) teacher_loss 0.5123 (0.3620) loss_zs_kd 0.0990 (0.1442) loss_oracle 0.4594 (0.4487) acc 84.3750 (89.0625) alaph_mean 0.6900 (0.6900) alpha_val 0.6900 (0.6900) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [120/160] time 0.098 (0.087) data 0.000 (0.002) loss 0.8051 (0.8827) teacher_loss 0.2675 (0.3630) loss_zs_kd 0.1253 (0.1451) loss_oracle 0.4750 (0.4471) acc 87.5000 (88.9323) alaph_mean 0.6900 (0.6900) alpha_val 0.6900 (0.6900) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [140/160] time 0.093 (0.088) data 0.001 (0.002) loss 0.8081 (0.8842) teacher_loss 0.2649 (0.3645) loss_zs_kd 0.1481 (0.1464) loss_oracle 0.4691 (0.4465) acc 90.6250 (88.7500) alaph_mean 0.6900 (0.6900) alpha_val 0.6900 (0.6900) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [160/160] time 0.074 (0.087) data 0.000 (0.002) loss 0.7743 (0.8853) teacher_loss 0.2791 (0.3663) loss_zs_kd 0.1385 (0.1450) loss_oracle 0.4260 (0.4465) acc 93.7500 (88.8672) alaph_mean 0.6900 (0.6900) alpha_val 0.6900 (0.6900) lr 1.9733e-06 eta 0:00:00
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,849
* accuracy: 83.8%
* error: 16.2%
* macro_f1: 85.0%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,973
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 89.2%
******* Domain p best val acc:      84.5%, epoch: 32 *******
******* Domain p best val test acc: 88.4%, epoch: 32 *******
******* Domain p best test acc:     88.7%, epoch: 30 *******
Checkpoint saved to icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model.pth.tar-50
Finish the whole training
Elapsed: 0:15:46
[Info] Hyperparameters saved to: icml/multi-dg/oracle/14_learnablealpha/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/hyperparameters.json
