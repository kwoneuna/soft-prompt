Loading trainer: TRIP
Loading dataset: SPG_OfficeHome
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------------------------------
Dataset    SPG_OfficeHome
Source     ['clipart', 'product', 'real_world']
Target     ['art']
# classes  65
# train_x  9,222
# val      3,939
# test     2,427
---------  ------------------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,195
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/288] time 0.099 (0.136) data 0.000 (0.018) loss 1.4363 (1.5660) teacher_loss 1.1123 (1.2382) loss_zs_kd 0.0000 (0.0000) loss_oracle 0.3239 (0.3278) acc 71.8750 (68.5938) alaph_mean 0.3499 (0.3457) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.5000) lr 1.0000e-05 eta 0:32:32
epoch [1/50] batch [40/288] time 0.115 (0.116) data 0.000 (0.009) loss 1.5970 (1.5759) teacher_loss 1.2158 (1.2595) loss_zs_kd 0.0002 (0.0001) loss_oracle 0.3811 (0.3163) acc 65.6250 (68.3594) alaph_mean 0.3246 (0.3503) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5000) lr 1.0000e-05 eta 0:27:52
epoch [1/50] batch [60/288] time 0.103 (0.113) data 0.001 (0.006) loss 1.6541 (1.5963) teacher_loss 1.4297 (1.2664) loss_zs_kd 0.0006 (0.0001) loss_oracle 0.2241 (0.3299) acc 59.3750 (67.6562) alaph_mean 0.3880 (0.3440) alpha_min -0.0000 (0.0000) alpha_max 0.5003 (0.5000) lr 1.0000e-05 eta 0:27:00
epoch [1/50] batch [80/288] time 0.093 (0.110) data 0.000 (0.005) loss 1.5705 (1.6170) teacher_loss 1.2539 (1.2704) loss_zs_kd 0.0009 (0.0002) loss_oracle 0.3162 (0.3466) acc 62.5000 (67.4609) alaph_mean 0.3518 (0.3362) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:26:19
epoch [1/50] batch [100/288] time 0.106 (0.108) data 0.001 (0.004) loss 1.4512 (1.5862) teacher_loss 1.0908 (1.2426) loss_zs_kd 0.0006 (0.0003) loss_oracle 0.3601 (0.3434) acc 62.5000 (68.1562) alaph_mean 0.3298 (0.3379) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:25:51
epoch [1/50] batch [120/288] time 0.105 (0.107) data 0.000 (0.003) loss 1.4023 (1.5845) teacher_loss 1.0332 (1.2364) loss_zs_kd 0.0007 (0.0004) loss_oracle 0.3688 (0.3479) acc 75.0000 (68.2292) alaph_mean 0.3234 (0.3360) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:25:28
epoch [1/50] batch [140/288] time 0.111 (0.106) data 0.002 (0.003) loss 1.8782 (1.5751) teacher_loss 1.4844 (1.2224) loss_zs_kd 0.0011 (0.0005) loss_oracle 0.3932 (0.3524) acc 56.2500 (68.3259) alaph_mean 0.3148 (0.3340) alpha_min -0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:25:11
epoch [1/50] batch [160/288] time 0.107 (0.105) data 0.000 (0.003) loss 1.2007 (1.5695) teacher_loss 1.0586 (1.2199) loss_zs_kd 0.0020 (0.0006) loss_oracle 0.1411 (0.3493) acc 71.8750 (68.6523) alaph_mean 0.4339 (0.3353) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:25:00
epoch [1/50] batch [180/288] time 0.106 (0.105) data 0.000 (0.002) loss 1.2836 (1.5709) teacher_loss 0.9946 (1.2235) loss_zs_kd 0.0024 (0.0007) loss_oracle 0.2878 (0.3470) acc 71.8750 (68.4722) alaph_mean 0.3632 (0.3364) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.5001) lr 1.0000e-05 eta 0:24:51
epoch [1/50] batch [200/288] time 0.109 (0.104) data 0.001 (0.002) loss 1.7613 (1.5668) teacher_loss 1.3369 (1.2202) loss_zs_kd 0.0020 (0.0008) loss_oracle 0.4233 (0.3462) acc 62.5000 (68.3594) alaph_mean 0.3015 (0.3368) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:24:41
epoch [1/50] batch [220/288] time 0.100 (0.104) data 0.000 (0.002) loss 1.4859 (1.5600) teacher_loss 1.4160 (1.2159) loss_zs_kd 0.0022 (0.0010) loss_oracle 0.0687 (0.3436) acc 65.6250 (68.6222) alaph_mean 0.4687 (0.3380) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:24:32
epoch [1/50] batch [240/288] time 0.102 (0.103) data 0.000 (0.002) loss 1.2778 (1.5559) teacher_loss 0.9849 (1.2135) loss_zs_kd 0.0011 (0.0011) loss_oracle 0.2924 (0.3419) acc 81.2500 (68.8802) alaph_mean 0.3568 (0.3389) alpha_min -0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:24:24
epoch [1/50] batch [260/288] time 0.095 (0.103) data 0.000 (0.002) loss 1.7934 (1.5616) teacher_loss 1.3027 (1.2190) loss_zs_kd 0.0055 (0.0013) loss_oracle 0.4879 (0.3419) acc 71.8750 (68.7380) alaph_mean 0.2738 (0.3389) alpha_min -0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:24:20
epoch [1/50] batch [280/288] time 0.102 (0.104) data 0.000 (0.002) loss 1.7307 (1.5608) teacher_loss 1.5205 (1.2189) loss_zs_kd 0.0053 (0.0015) loss_oracle 0.2075 (0.3412) acc 62.5000 (68.7946) alaph_mean 0.4030 (0.3391) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.5001) lr 1.0000e-05 eta 0:24:22
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,266
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 82.0%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,964
* accuracy: 80.9%
* error: 19.1%
* macro_f1: 76.7%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      82.9%, epoch: 1 *******
******* Domain a best val test acc: 80.9%, epoch: 1 *******
******* Domain a best test acc:     80.9%, epoch: 1 *******
epoch [2/50] batch [20/288] time 0.090 (0.118) data 0.000 (0.015) loss 1.5057 (1.7065) teacher_loss 0.9912 (1.1802) loss_zs_kd 0.1228 (0.0967) loss_oracle 0.4531 (0.4779) acc 68.7500 (69.5312) alaph_mean 0.3369 (0.3153) alpha_min 0.0000 (0.0000) alpha_max 0.5088 (0.5046) lr 2.0000e-03 eta 0:27:39
epoch [2/50] batch [40/288] time 0.199 (0.141) data 0.000 (0.008) loss 1.2429 (1.6681) teacher_loss 0.6973 (1.1202) loss_zs_kd 0.1144 (0.1207) loss_oracle 0.4885 (0.4875) acc 81.2500 (70.7031) alaph_mean 0.3287 (0.3175) alpha_min -0.0000 (0.0000) alpha_max 0.5066 (0.5050) lr 2.0000e-03 eta 0:33:10
epoch [2/50] batch [60/288] time 0.199 (0.161) data 0.001 (0.005) loss 1.8868 (1.6718) teacher_loss 1.2510 (1.1194) loss_zs_kd 0.1010 (0.1207) loss_oracle 0.5854 (0.4920) acc 65.6250 (70.8333) alaph_mean 0.2815 (0.3157) alpha_min -0.0000 (0.0083) alpha_max 0.5045 (0.5047) lr 2.0000e-03 eta 0:37:42
epoch [2/50] batch [80/288] time 0.196 (0.170) data 0.000 (0.004) loss 1.4683 (1.6618) teacher_loss 0.7603 (1.0992) loss_zs_kd 0.1147 (0.1189) loss_oracle 0.6507 (0.5032) acc 81.2500 (71.3281) alaph_mean 0.2892 (0.3138) alpha_min 0.0000 (0.0062) alpha_max 0.5065 (0.5048) lr 2.0000e-03 eta 0:39:45
epoch [2/50] batch [100/288] time 0.193 (0.175) data 0.000 (0.003) loss 1.8769 (1.6622) teacher_loss 1.3926 (1.0940) loss_zs_kd 0.1088 (0.1196) loss_oracle 0.4300 (0.5084) acc 65.6250 (71.4062) alaph_mean 0.3423 (0.3154) alpha_min 0.0000 (0.0050) alpha_max 0.5019 (0.5051) lr 2.0000e-03 eta 0:40:46
epoch [2/50] batch [120/288] time 0.195 (0.178) data 0.001 (0.003) loss 1.7040 (1.6591) teacher_loss 1.0713 (1.0876) loss_zs_kd 0.0885 (0.1176) loss_oracle 0.5884 (0.5126) acc 75.0000 (71.5365) alaph_mean 0.2676 (0.3153) alpha_min 0.0000 (0.0041) alpha_max 0.5093 (0.5055) lr 2.0000e-03 eta 0:41:24
epoch [2/50] batch [140/288] time 0.196 (0.180) data 0.000 (0.002) loss 1.4931 (1.6608) teacher_loss 1.0879 (1.0912) loss_zs_kd 0.1449 (0.1158) loss_oracle 0.3328 (0.5117) acc 75.0000 (71.3839) alaph_mean 0.4376 (0.3169) alpha_min -0.0000 (0.0035) alpha_max 0.5059 (0.5060) lr 2.0000e-03 eta 0:41:53
epoch [2/50] batch [160/288] time 0.195 (0.181) data 0.000 (0.002) loss 1.6385 (1.6670) teacher_loss 1.1211 (1.0930) loss_zs_kd 0.1934 (0.1169) loss_oracle 0.4207 (0.5155) acc 71.8750 (71.3281) alaph_mean 0.3577 (0.3149) alpha_min 0.0000 (0.0031) alpha_max 0.5036 (0.5058) lr 2.0000e-03 eta 0:42:10
epoch [2/50] batch [180/288] time 0.190 (0.182) data 0.000 (0.002) loss 1.6456 (1.6673) teacher_loss 0.9453 (1.0897) loss_zs_kd 0.1468 (0.1185) loss_oracle 0.6269 (0.5183) acc 78.1250 (71.4757) alaph_mean 0.2814 (0.3147) alpha_min -0.0000 (0.0028) alpha_max 0.5065 (0.5056) lr 2.0000e-03 eta 0:42:21
epoch [2/50] batch [200/288] time 0.194 (0.184) data 0.000 (0.002) loss 2.4720 (1.6749) teacher_loss 1.7832 (1.0941) loss_zs_kd 0.1816 (0.1218) loss_oracle 0.5980 (0.5199) acc 53.1250 (71.1719) alaph_mean 0.2726 (0.3159) alpha_min -0.0000 (0.0025) alpha_max 0.5029 (0.5056) lr 2.0000e-03 eta 0:42:33
epoch [2/50] batch [220/288] time 0.192 (0.184) data 0.000 (0.002) loss 1.7202 (1.6661) teacher_loss 1.0820 (1.0817) loss_zs_kd 0.1451 (0.1223) loss_oracle 0.5657 (0.5232) acc 75.0000 (71.4915) alaph_mean 0.3193 (0.3161) alpha_min 0.0000 (0.0023) alpha_max 0.5102 (0.5056) lr 2.0000e-03 eta 0:42:42
epoch [2/50] batch [240/288] time 0.199 (0.185) data 0.000 (0.001) loss 1.8931 (1.6629) teacher_loss 1.3730 (1.0773) loss_zs_kd 0.1239 (0.1229) loss_oracle 0.4581 (0.5242) acc 62.5000 (71.6016) alaph_mean 0.3757 (0.3163) alpha_min 0.0000 (0.0021) alpha_max 0.5108 (0.5056) lr 2.0000e-03 eta 0:42:49
epoch [2/50] batch [260/288] time 0.189 (0.186) data 0.000 (0.001) loss 1.7378 (1.6618) teacher_loss 1.0527 (1.0732) loss_zs_kd 0.1863 (0.1239) loss_oracle 0.5919 (0.5266) acc 71.8750 (71.6346) alaph_mean 0.3044 (0.3156) alpha_min -0.0000 (0.0019) alpha_max 0.5068 (0.5056) lr 2.0000e-03 eta 0:42:54
epoch [2/50] batch [280/288] time 0.202 (0.186) data 0.000 (0.001) loss 1.4584 (1.6607) teacher_loss 0.8086 (1.0686) loss_zs_kd 0.1389 (0.1241) loss_oracle 0.5804 (0.5301) acc 78.1250 (71.7188) alaph_mean 0.2578 (0.3150) alpha_min -0.0000 (0.0018) alpha_max 0.5010 (0.5057) lr 2.0000e-03 eta 0:42:57
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,361
* accuracy: 85.3%
* error: 14.7%
* macro_f1: 84.6%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,011
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.2%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      85.3%, epoch: 2 *******
******* Domain a best val test acc: 82.9%, epoch: 2 *******
******* Domain a best test acc:     82.9%, epoch: 2 *******
epoch [3/50] batch [20/288] time 0.524 (0.299) data 0.000 (0.014) loss 1.8848 (1.6368) teacher_loss 1.3271 (1.0471) loss_zs_kd 0.1532 (0.1284) loss_oracle 0.4810 (0.5255) acc 62.5000 (72.0312) alaph_mean 0.3758 (0.3185) alpha_min -0.0000 (0.0000) alpha_max 0.5071 (0.5058) lr 1.9980e-03 eta 1:08:44
epoch [3/50] batch [40/288] time 0.167 (0.236) data 0.000 (0.007) loss 1.5229 (1.5790) teacher_loss 0.8403 (0.9799) loss_zs_kd 0.0992 (0.1245) loss_oracle 0.6330 (0.5368) acc 75.0000 (73.8281) alaph_mean 0.2572 (0.3234) alpha_min 0.0000 (0.0000) alpha_max 0.5068 (0.5120) lr 1.9980e-03 eta 0:54:14
epoch [3/50] batch [60/288] time 0.198 (0.222) data 0.000 (0.005) loss 1.5202 (1.6677) teacher_loss 0.9189 (1.0286) loss_zs_kd 0.0989 (0.1248) loss_oracle 0.5518 (0.5767) acc 75.0000 (72.8646) alaph_mean 0.3676 (0.3179) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5167) lr 1.9980e-03 eta 0:50:56
epoch [3/50] batch [80/288] time 0.193 (0.213) data 0.000 (0.004) loss 1.6759 (1.6739) teacher_loss 1.0664 (1.0305) loss_zs_kd 0.1334 (0.1238) loss_oracle 0.5428 (0.5815) acc 75.0000 (72.6562) alaph_mean 0.3344 (0.3159) alpha_min 0.0000 (0.0000) alpha_max 0.5100 (0.5197) lr 1.9980e-03 eta 0:48:53
epoch [3/50] batch [100/288] time 0.194 (0.209) data 0.000 (0.003) loss 1.5687 (1.6847) teacher_loss 0.9683 (1.0417) loss_zs_kd 0.1233 (0.1251) loss_oracle 0.5388 (0.5804) acc 75.0000 (72.5625) alaph_mean 0.3118 (0.3135) alpha_min 0.0000 (0.0000) alpha_max 0.5045 (0.5227) lr 1.9980e-03 eta 0:47:52
epoch [3/50] batch [120/288] time 0.192 (0.207) data 0.001 (0.003) loss 1.3075 (1.6648) teacher_loss 0.6689 (1.0307) loss_zs_kd 0.1456 (0.1278) loss_oracle 0.5657 (0.5701) acc 71.8750 (72.5781) alaph_mean 0.2817 (0.3158) alpha_min 0.0000 (0.0000) alpha_max 0.5071 (0.5213) lr 1.9980e-03 eta 0:47:17
epoch [3/50] batch [140/288] time 0.192 (0.205) data 0.000 (0.002) loss 1.4419 (1.6531) teacher_loss 0.9277 (1.0284) loss_zs_kd 0.0866 (0.1280) loss_oracle 0.4708 (0.5607) acc 71.8750 (72.4554) alaph_mean 0.3135 (0.3162) alpha_min 0.0000 (0.0000) alpha_max 0.5025 (0.5191) lr 1.9980e-03 eta 0:46:41
epoch [3/50] batch [160/288] time 0.195 (0.203) data 0.000 (0.002) loss 1.9619 (1.6506) teacher_loss 1.4229 (1.0321) loss_zs_kd 0.1543 (0.1294) loss_oracle 0.4619 (0.5538) acc 62.5000 (72.2656) alaph_mean 0.3449 (0.3154) alpha_min 0.0000 (0.0000) alpha_max 0.5027 (0.5174) lr 1.9980e-03 eta 0:46:17
epoch [3/50] batch [180/288] time 0.197 (0.202) data 0.000 (0.002) loss 1.7066 (1.6507) teacher_loss 1.0869 (1.0324) loss_zs_kd 0.1501 (0.1294) loss_oracle 0.5446 (0.5535) acc 71.8750 (72.2917) alaph_mean 0.3282 (0.3140) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5159) lr 1.9980e-03 eta 0:45:58
epoch [3/50] batch [200/288] time 0.193 (0.201) data 0.000 (0.002) loss 1.5167 (1.6488) teacher_loss 0.9673 (1.0326) loss_zs_kd 0.1255 (0.1297) loss_oracle 0.4867 (0.5514) acc 78.1250 (72.3281) alaph_mean 0.3102 (0.3128) alpha_min 0.0000 (0.0000) alpha_max 0.5031 (0.5155) lr 1.9980e-03 eta 0:45:42
epoch [3/50] batch [220/288] time 0.197 (0.201) data 0.000 (0.001) loss 1.4791 (1.6539) teacher_loss 0.8228 (1.0425) loss_zs_kd 0.1341 (0.1308) loss_oracle 0.5893 (0.5461) acc 75.0000 (72.1733) alaph_mean 0.2716 (0.3132) alpha_min 0.0000 (0.0000) alpha_max 0.5027 (0.5145) lr 1.9980e-03 eta 0:45:30
epoch [3/50] batch [240/288] time 0.191 (0.200) data 0.000 (0.001) loss 1.8933 (1.6554) teacher_loss 1.3008 (1.0463) loss_zs_kd 0.1019 (0.1308) loss_oracle 0.5415 (0.5437) acc 65.6250 (72.1354) alaph_mean 0.2999 (0.3136) alpha_min 0.0000 (0.0000) alpha_max 0.5056 (0.5158) lr 1.9980e-03 eta 0:45:17
epoch [3/50] batch [260/288] time 0.194 (0.200) data 0.000 (0.001) loss 1.2119 (1.6467) teacher_loss 0.6724 (1.0421) loss_zs_kd 0.1473 (0.1302) loss_oracle 0.4658 (0.5395) acc 78.1250 (72.2115) alaph_mean 0.3600 (0.3146) alpha_min 0.0000 (0.0000) alpha_max 0.5162 (0.5153) lr 1.9980e-03 eta 0:45:07
epoch [3/50] batch [280/288] time 0.185 (0.199) data 0.000 (0.001) loss 1.7471 (1.6422) teacher_loss 1.0469 (1.0381) loss_zs_kd 0.2118 (0.1316) loss_oracle 0.5943 (0.5384) acc 71.8750 (72.3103) alaph_mean 0.3060 (0.3149) alpha_min 0.0000 (0.0000) alpha_max 0.5063 (0.5145) lr 1.9980e-03 eta 0:44:57
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,382
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.1%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.2%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      85.9%, epoch: 3 *******
******* Domain a best val test acc: 83.3%, epoch: 3 *******
******* Domain a best test acc:     83.3%, epoch: 3 *******
epoch [4/50] batch [20/288] time 0.414 (0.271) data 0.000 (0.012) loss 1.3377 (1.5670) teacher_loss 0.7861 (0.9786) loss_zs_kd 0.1133 (0.1334) loss_oracle 0.4949 (0.5216) acc 84.3750 (73.5938) alaph_mean 0.3348 (0.3177) alpha_min -0.0000 (0.0000) alpha_max 0.5064 (0.5064) lr 1.9921e-03 eta 1:01:05
epoch [4/50] batch [40/288] time 0.201 (0.241) data 0.000 (0.006) loss 1.6257 (1.5750) teacher_loss 1.1025 (0.9904) loss_zs_kd 0.2229 (0.1372) loss_oracle 0.4118 (0.5159) acc 71.8750 (73.5156) alaph_mean 0.3755 (0.3188) alpha_min -0.0000 (0.0000) alpha_max 0.5059 (0.5084) lr 1.9921e-03 eta 0:54:08
epoch [4/50] batch [60/288] time 0.199 (0.225) data 0.000 (0.004) loss 1.7762 (1.5615) teacher_loss 1.1562 (0.9805) loss_zs_kd 0.1100 (0.1344) loss_oracle 0.5649 (0.5138) acc 71.8750 (73.4896) alaph_mean 0.2850 (0.3205) alpha_min 0.0000 (0.0000) alpha_max 0.5053 (0.5072) lr 1.9921e-03 eta 0:50:33
epoch [4/50] batch [80/288] time 0.199 (0.217) data 0.000 (0.003) loss 1.9735 (1.5817) teacher_loss 1.4873 (0.9966) loss_zs_kd 0.1119 (0.1321) loss_oracle 0.4302 (0.5191) acc 62.5000 (73.6328) alaph_mean 0.3610 (0.3204) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5069) lr 1.9921e-03 eta 0:48:45
epoch [4/50] batch [100/288] time 0.198 (0.213) data 0.000 (0.003) loss 1.3663 (1.5975) teacher_loss 0.8696 (1.0124) loss_zs_kd 0.1071 (0.1288) loss_oracle 0.4431 (0.5207) acc 78.1250 (73.1875) alaph_mean 0.3408 (0.3205) alpha_min 0.0000 (0.0000) alpha_max 0.5044 (0.5074) lr 1.9921e-03 eta 0:47:36
epoch [4/50] batch [120/288] time 0.189 (0.209) data 0.000 (0.002) loss 1.6656 (1.6074) teacher_loss 0.8989 (1.0210) loss_zs_kd 0.1133 (0.1262) loss_oracle 0.7100 (0.5233) acc 81.2500 (73.0469) alaph_mean 0.2071 (0.3191) alpha_min -0.0000 (0.0000) alpha_max 0.5033 (0.5071) lr 1.9921e-03 eta 0:46:50
epoch [4/50] batch [140/288] time 0.188 (0.207) data 0.000 (0.002) loss 1.4937 (1.6261) teacher_loss 0.7554 (1.0278) loss_zs_kd 0.1140 (0.1265) loss_oracle 0.6813 (0.5351) acc 78.1250 (72.7902) alaph_mean 0.2531 (0.3161) alpha_min -0.0000 (0.0000) alpha_max 0.5030 (0.5078) lr 1.9921e-03 eta 0:46:16
epoch [4/50] batch [160/288] time 0.195 (0.205) data 0.000 (0.002) loss 1.7644 (1.6276) teacher_loss 1.2432 (1.0236) loss_zs_kd 0.1215 (0.1277) loss_oracle 0.4605 (0.5402) acc 68.7500 (72.7539) alaph_mean 0.3708 (0.3171) alpha_min -0.0000 (0.0000) alpha_max 0.5072 (0.5097) lr 1.9921e-03 eta 0:45:46
epoch [4/50] batch [180/288] time 0.195 (0.204) data 0.000 (0.002) loss 2.0179 (1.6252) teacher_loss 1.3652 (1.0214) loss_zs_kd 0.1756 (0.1278) loss_oracle 0.5649 (0.5399) acc 59.3750 (72.8125) alaph_mean 0.3093 (0.3178) alpha_min 0.0000 (0.0000) alpha_max 0.5052 (0.5103) lr 1.9921e-03 eta 0:45:28
epoch [4/50] batch [200/288] time 0.183 (0.203) data 0.000 (0.001) loss 1.8592 (1.6252) teacher_loss 1.1670 (1.0194) loss_zs_kd 0.1147 (0.1300) loss_oracle 0.6349 (0.5408) acc 65.6250 (72.9844) alaph_mean 0.2667 (0.3176) alpha_min 0.0000 (0.0000) alpha_max 0.5025 (0.5098) lr 1.9921e-03 eta 0:45:09
epoch [4/50] batch [220/288] time 0.191 (0.202) data 0.000 (0.001) loss 1.7650 (1.6263) teacher_loss 1.1758 (1.0218) loss_zs_kd 0.1144 (0.1310) loss_oracle 0.5320 (0.5389) acc 65.6250 (72.8693) alaph_mean 0.3095 (0.3184) alpha_min 0.0000 (0.0000) alpha_max 0.5066 (0.5095) lr 1.9921e-03 eta 0:44:53
epoch [4/50] batch [240/288] time 0.191 (0.202) data 0.000 (0.001) loss 1.9511 (1.6227) teacher_loss 1.2002 (1.0191) loss_zs_kd 0.1157 (0.1310) loss_oracle 0.6931 (0.5381) acc 65.6250 (73.0339) alaph_mean 0.2417 (0.3189) alpha_min -0.0000 (0.0000) alpha_max 0.5045 (0.5092) lr 1.9921e-03 eta 0:44:40
epoch [4/50] batch [260/288] time 0.194 (0.201) data 0.000 (0.001) loss 1.3662 (1.6259) teacher_loss 0.7563 (1.0225) loss_zs_kd 0.1301 (0.1315) loss_oracle 0.5448 (0.5376) acc 71.8750 (72.8245) alaph_mean 0.3050 (0.3194) alpha_min -0.0000 (0.0000) alpha_max 0.5052 (0.5097) lr 1.9921e-03 eta 0:44:28
epoch [4/50] batch [280/288] time 0.182 (0.200) data 0.000 (0.001) loss 1.3845 (1.6199) teacher_loss 0.7388 (1.0178) loss_zs_kd 0.1099 (0.1309) loss_oracle 0.5907 (0.5367) acc 78.1250 (73.0804) alaph_mean 0.2778 (0.3199) alpha_min 0.0000 (0.0000) alpha_max 0.5059 (0.5097) lr 1.9921e-03 eta 0:44:16
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,397
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,017
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.3%
******* Domain a best val acc:      86.2%, epoch: 4 *******
******* Domain a best val test acc: 83.1%, epoch: 4 *******
******* Domain a best test acc:     83.3%, epoch: 3 *******
epoch [5/50] batch [20/288] time 0.084 (0.251) data 0.000 (0.013) loss 2.1366 (1.7687) teacher_loss 1.4365 (1.1464) loss_zs_kd 0.1524 (0.1436) loss_oracle 0.6239 (0.5505) acc 71.8750 (70.3125) alaph_mean 0.2825 (0.3193) alpha_min 0.0000 (0.0000) alpha_max 0.5089 (0.5129) lr 1.9823e-03 eta 0:55:21
epoch [5/50] batch [40/288] time 0.190 (0.232) data 0.000 (0.007) loss 1.0265 (1.6846) teacher_loss 0.4834 (1.0464) loss_zs_kd 0.1939 (0.1455) loss_oracle 0.4462 (0.5654) acc 90.6250 (72.7344) alaph_mean 0.3441 (0.3198) alpha_min 0.0000 (0.0000) alpha_max 0.5024 (0.5121) lr 1.9823e-03 eta 0:51:07
epoch [5/50] batch [60/288] time 0.198 (0.220) data 0.000 (0.005) loss 1.6275 (1.6520) teacher_loss 1.1104 (1.0156) loss_zs_kd 0.1127 (0.1397) loss_oracle 0.4607 (0.5665) acc 65.6250 (73.5938) alaph_mean 0.4058 (0.3168) alpha_min 0.0000 (0.0000) alpha_max 0.5023 (0.5106) lr 1.9823e-03 eta 0:48:14
epoch [5/50] batch [80/288] time 0.194 (0.213) data 0.000 (0.003) loss 2.0099 (1.6387) teacher_loss 1.4062 (0.9977) loss_zs_kd 0.1280 (0.1428) loss_oracle 0.5397 (0.5695) acc 71.8750 (73.9062) alaph_mean 0.3393 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5073 (0.5094) lr 1.9823e-03 eta 0:46:47
epoch [5/50] batch [100/288] time 0.196 (0.210) data 0.000 (0.003) loss 1.3843 (1.6515) teacher_loss 0.6299 (1.0044) loss_zs_kd 0.1605 (0.1447) loss_oracle 0.6742 (0.5748) acc 84.3750 (73.7188) alaph_mean 0.3117 (0.3177) alpha_min 0.0000 (0.0000) alpha_max 0.5089 (0.5089) lr 1.9823e-03 eta 0:45:55
epoch [5/50] batch [120/288] time 0.184 (0.207) data 0.000 (0.002) loss 1.7751 (1.6430) teacher_loss 1.0381 (0.9931) loss_zs_kd 0.1739 (0.1449) loss_oracle 0.6501 (0.5774) acc 75.0000 (73.9583) alaph_mean 0.2912 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5094) lr 1.9823e-03 eta 0:45:11
epoch [5/50] batch [140/288] time 0.204 (0.204) data 0.000 (0.002) loss 1.5428 (1.6484) teacher_loss 1.0410 (0.9952) loss_zs_kd 0.1373 (0.1446) loss_oracle 0.4331 (0.5809) acc 71.8750 (73.9286) alaph_mean 0.4072 (0.3184) alpha_min 0.0000 (0.0000) alpha_max 0.5115 (0.5132) lr 1.9823e-03 eta 0:44:38
epoch [5/50] batch [160/288] time 0.194 (0.203) data 0.000 (0.002) loss 2.0064 (1.6505) teacher_loss 1.2773 (0.9972) loss_zs_kd 0.1410 (0.1439) loss_oracle 0.6586 (0.5814) acc 65.6250 (73.7891) alaph_mean 0.2820 (0.3194) alpha_min 0.0000 (0.0000) alpha_max 0.5034 (0.5126) lr 1.9823e-03 eta 0:44:17
epoch [5/50] batch [180/288] time 0.192 (0.202) data 0.000 (0.002) loss 1.6108 (1.6596) teacher_loss 0.9932 (1.0038) loss_zs_kd 0.1445 (0.1449) loss_oracle 0.5454 (0.5833) acc 68.7500 (73.5590) alaph_mean 0.3414 (0.3175) alpha_min 0.0000 (0.0000) alpha_max 0.5055 (0.5122) lr 1.9823e-03 eta 0:43:58
epoch [5/50] batch [200/288] time 0.193 (0.201) data 0.000 (0.001) loss 1.5861 (1.6620) teacher_loss 0.9565 (1.0071) loss_zs_kd 0.1126 (0.1438) loss_oracle 0.5733 (0.5830) acc 78.1250 (73.5625) alaph_mean 0.3137 (0.3171) alpha_min 0.0000 (0.0000) alpha_max 0.5039 (0.5131) lr 1.9823e-03 eta 0:43:43
epoch [5/50] batch [220/288] time 0.193 (0.200) data 0.000 (0.001) loss 1.9087 (1.6621) teacher_loss 1.1943 (1.0103) loss_zs_kd 0.1272 (0.1429) loss_oracle 0.6507 (0.5803) acc 75.0000 (73.3665) alaph_mean 0.2511 (0.3166) alpha_min -0.0000 (0.0000) alpha_max 0.5026 (0.5125) lr 1.9823e-03 eta 0:43:29
epoch [5/50] batch [240/288] time 0.194 (0.200) data 0.000 (0.001) loss 1.5673 (1.6577) teacher_loss 0.8472 (1.0086) loss_zs_kd 0.1228 (0.1427) loss_oracle 0.6587 (0.5777) acc 78.1250 (73.3073) alaph_mean 0.3004 (0.3179) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5124) lr 1.9823e-03 eta 0:43:18
epoch [5/50] batch [260/288] time 0.181 (0.199) data 0.000 (0.001) loss 1.7624 (1.6513) teacher_loss 1.0283 (1.0040) loss_zs_kd 0.0945 (0.1418) loss_oracle 0.6869 (0.5764) acc 68.7500 (73.2692) alaph_mean 0.2610 (0.3183) alpha_min 0.0000 (0.0000) alpha_max 0.5130 (0.5122) lr 1.9823e-03 eta 0:43:09
epoch [5/50] batch [280/288] time 0.190 (0.199) data 0.000 (0.001) loss 1.3268 (1.6393) teacher_loss 0.7300 (0.9952) loss_zs_kd 0.1476 (0.1411) loss_oracle 0.5231 (0.5735) acc 84.3750 (73.5379) alaph_mean 0.3105 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.5019 (0.5124) lr 1.9823e-03 eta 0:43:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,395
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.8%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.2%, epoch: 4 *******
******* Domain a best val test acc: 83.1%, epoch: 4 *******
******* Domain a best test acc:     83.3%, epoch: 5 *******
epoch [6/50] batch [20/288] time 0.089 (0.251) data 0.000 (0.012) loss 1.7206 (1.4988) teacher_loss 1.0889 (0.8975) loss_zs_kd 0.0928 (0.1191) loss_oracle 0.5853 (0.5418) acc 71.8750 (74.6875) alaph_mean 0.2864 (0.3188) alpha_min -0.0000 (0.0000) alpha_max 0.5067 (0.5114) lr 1.9686e-03 eta 0:54:03
epoch [6/50] batch [40/288] time 0.186 (0.234) data 0.000 (0.006) loss 1.6603 (1.5673) teacher_loss 0.9399 (0.9476) loss_zs_kd 0.1329 (0.1296) loss_oracle 0.6539 (0.5549) acc 75.0000 (73.9844) alaph_mean 0.2519 (0.3180) alpha_min -0.0000 (0.0000) alpha_max 0.5064 (0.5108) lr 1.9686e-03 eta 0:50:25
epoch [6/50] batch [60/288] time 0.193 (0.220) data 0.000 (0.004) loss 1.6654 (1.5608) teacher_loss 1.1006 (0.9396) loss_zs_kd 0.1593 (0.1324) loss_oracle 0.4852 (0.5550) acc 68.7500 (74.3229) alaph_mean 0.3405 (0.3198) alpha_min 0.0000 (0.0000) alpha_max 0.5059 (0.5097) lr 1.9686e-03 eta 0:47:15
epoch [6/50] batch [80/288] time 0.192 (0.213) data 0.000 (0.003) loss 1.5069 (1.5629) teacher_loss 0.8813 (0.9434) loss_zs_kd 0.1702 (0.1365) loss_oracle 0.5405 (0.5512) acc 75.0000 (74.4531) alaph_mean 0.3438 (0.3205) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5094) lr 1.9686e-03 eta 0:45:45
epoch [6/50] batch [100/288] time 0.195 (0.209) data 0.000 (0.003) loss 1.9460 (1.5836) teacher_loss 1.2012 (0.9591) loss_zs_kd 0.1705 (0.1374) loss_oracle 0.6596 (0.5558) acc 65.6250 (74.2188) alaph_mean 0.2800 (0.3186) alpha_min 0.0000 (0.0000) alpha_max 0.5036 (0.5100) lr 1.9686e-03 eta 0:44:51
epoch [6/50] batch [120/288] time 0.185 (0.207) data 0.000 (0.002) loss 1.7599 (1.5795) teacher_loss 1.1406 (0.9536) loss_zs_kd 0.1683 (0.1387) loss_oracle 0.5351 (0.5566) acc 68.7500 (74.6875) alaph_mean 0.3446 (0.3189) alpha_min 0.0000 (0.0000) alpha_max 0.5122 (0.5096) lr 1.9686e-03 eta 0:44:12
epoch [6/50] batch [140/288] time 0.197 (0.205) data 0.000 (0.002) loss 1.3370 (1.5709) teacher_loss 0.7773 (0.9542) loss_zs_kd 0.0989 (0.1378) loss_oracle 0.5102 (0.5478) acc 84.3750 (74.6429) alaph_mean 0.3438 (0.3232) alpha_min -0.0000 (0.0000) alpha_max 0.5043 (0.5103) lr 1.9686e-03 eta 0:43:46
epoch [6/50] batch [160/288] time 0.185 (0.203) data 0.000 (0.002) loss 1.3375 (1.5799) teacher_loss 0.6958 (0.9577) loss_zs_kd 0.1772 (0.1403) loss_oracle 0.5531 (0.5520) acc 75.0000 (74.4141) alaph_mean 0.3251 (0.3203) alpha_min 0.0000 (0.0000) alpha_max 0.5107 (0.5099) lr 1.9686e-03 eta 0:43:19
epoch [6/50] batch [180/288] time 0.188 (0.202) data 0.000 (0.002) loss 1.4481 (1.5821) teacher_loss 0.7397 (0.9614) loss_zs_kd 0.1961 (0.1417) loss_oracle 0.6103 (0.5499) acc 84.3750 (74.4097) alaph_mean 0.2997 (0.3209) alpha_min 0.0000 (0.0000) alpha_max 0.5038 (0.5102) lr 1.9686e-03 eta 0:42:55
epoch [6/50] batch [200/288] time 0.192 (0.200) data 0.000 (0.001) loss 1.4111 (1.5840) teacher_loss 0.9873 (0.9661) loss_zs_kd 0.1267 (0.1415) loss_oracle 0.3605 (0.5472) acc 68.7500 (74.3594) alaph_mean 0.4126 (0.3217) alpha_min 0.0000 (0.0000) alpha_max 0.5077 (0.5104) lr 1.9686e-03 eta 0:42:34
epoch [6/50] batch [220/288] time 0.195 (0.200) data 0.000 (0.001) loss 1.8955 (1.5880) teacher_loss 1.2031 (0.9694) loss_zs_kd 0.1759 (0.1420) loss_oracle 0.6044 (0.5476) acc 71.8750 (74.2756) alaph_mean 0.3128 (0.3219) alpha_min -0.0000 (0.0000) alpha_max 0.5058 (0.5101) lr 1.9686e-03 eta 0:42:27
epoch [6/50] batch [240/288] time 0.197 (0.200) data 0.000 (0.001) loss 1.3592 (1.5923) teacher_loss 0.9678 (0.9739) loss_zs_kd 0.1086 (0.1421) loss_oracle 0.3371 (0.5473) acc 78.1250 (74.1276) alaph_mean 0.4297 (0.3231) alpha_min 0.0000 (0.0000) alpha_max 0.7297 (0.5114) lr 1.9686e-03 eta 0:42:19
epoch [6/50] batch [260/288] time 0.192 (0.199) data 0.000 (0.001) loss 1.9736 (1.5988) teacher_loss 1.2168 (0.9776) loss_zs_kd 0.1274 (0.1430) loss_oracle 0.6932 (0.5497) acc 71.8750 (73.9904) alaph_mean 0.2345 (0.3207) alpha_min -0.0000 (0.0000) alpha_max 0.5081 (0.5116) lr 1.9686e-03 eta 0:42:07
epoch [6/50] batch [280/288] time 0.187 (0.199) data 0.000 (0.001) loss 1.8543 (1.5993) teacher_loss 1.2207 (0.9776) loss_zs_kd 0.1733 (0.1432) loss_oracle 0.5469 (0.5501) acc 75.0000 (74.1295) alaph_mean 0.3282 (0.3206) alpha_min 0.0000 (0.0000) alpha_max 0.5099 (0.5113) lr 1.9686e-03 eta 0:41:58
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,383
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,030
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.2%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.2%, epoch: 4 *******
******* Domain a best val test acc: 83.1%, epoch: 4 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [7/50] batch [20/288] time 0.107 (0.360) data 0.000 (0.017) loss 1.8450 (1.6979) teacher_loss 1.0996 (1.0299) loss_zs_kd 0.1183 (0.1304) loss_oracle 0.6862 (0.6027) acc 71.8750 (74.0625) alaph_mean 0.2737 (0.3103) alpha_min 0.0000 (0.0000) alpha_max 0.5047 (0.5062) lr 1.9511e-03 eta 1:15:53
epoch [7/50] batch [40/288] time 0.199 (0.278) data 0.000 (0.008) loss 1.5069 (1.6508) teacher_loss 0.7402 (0.9701) loss_zs_kd 0.1316 (0.1370) loss_oracle 0.7009 (0.6122) acc 84.3750 (75.1562) alaph_mean 0.2930 (0.3092) alpha_min -0.0000 (0.0000) alpha_max 0.5433 (0.5162) lr 1.9511e-03 eta 0:58:28
epoch [7/50] batch [60/288] time 0.199 (0.250) data 0.000 (0.006) loss 1.6800 (1.6203) teacher_loss 1.1221 (0.9499) loss_zs_kd 0.1344 (0.1392) loss_oracle 0.4907 (0.6007) acc 68.7500 (75.2083) alaph_mean 0.3435 (0.3129) alpha_min 0.0000 (0.0000) alpha_max 0.5024 (0.5168) lr 1.9511e-03 eta 0:52:34
epoch [7/50] batch [80/288] time 0.195 (0.236) data 0.000 (0.004) loss 1.5206 (1.6080) teacher_loss 0.8691 (0.9503) loss_zs_kd 0.1763 (0.1431) loss_oracle 0.5633 (0.5862) acc 78.1250 (74.9219) alaph_mean 0.3433 (0.3161) alpha_min -0.0000 (0.0000) alpha_max 0.5118 (0.5141) lr 1.9511e-03 eta 0:49:34
epoch [7/50] batch [100/288] time 0.192 (0.228) data 0.000 (0.003) loss 1.2932 (1.6024) teacher_loss 0.6089 (0.9448) loss_zs_kd 0.1060 (0.1451) loss_oracle 0.6313 (0.5850) acc 84.3750 (74.7500) alaph_mean 0.2838 (0.3151) alpha_min 0.0000 (0.0000) alpha_max 0.5707 (0.5163) lr 1.9511e-03 eta 0:47:44
epoch [7/50] batch [120/288] time 0.193 (0.222) data 0.000 (0.003) loss 1.9802 (1.6042) teacher_loss 1.2383 (0.9416) loss_zs_kd 0.1573 (0.1458) loss_oracle 0.6633 (0.5898) acc 71.8750 (74.7917) alaph_mean 0.2770 (0.3127) alpha_min 0.0000 (0.0000) alpha_max 0.5047 (0.5179) lr 1.9511e-03 eta 0:46:28
epoch [7/50] batch [140/288] time 0.189 (0.218) data 0.000 (0.003) loss 1.9208 (1.6208) teacher_loss 1.0693 (0.9527) loss_zs_kd 0.1637 (0.1476) loss_oracle 0.7696 (0.5943) acc 71.8750 (74.4420) alaph_mean 0.2508 (0.3131) alpha_min 0.0000 (0.0000) alpha_max 0.5074 (0.5192) lr 1.9511e-03 eta 0:45:35
epoch [7/50] batch [160/288] time 0.191 (0.215) data 0.000 (0.002) loss 1.7939 (1.6409) teacher_loss 0.9512 (0.9671) loss_zs_kd 0.1517 (0.1480) loss_oracle 0.7669 (0.5998) acc 78.1250 (74.0430) alaph_mean 0.2475 (0.3127) alpha_min 0.0000 (0.0000) alpha_max 0.5141 (0.5222) lr 1.9511e-03 eta 0:44:51
epoch [7/50] batch [180/288] time 0.199 (0.213) data 0.000 (0.002) loss 1.7137 (1.6478) teacher_loss 1.0488 (0.9716) loss_zs_kd 0.1678 (0.1480) loss_oracle 0.5810 (0.6022) acc 71.8750 (74.1667) alaph_mean 0.3678 (0.3147) alpha_min 0.0000 (0.0000) alpha_max 0.5717 (0.5240) lr 1.9511e-03 eta 0:44:17
epoch [7/50] batch [200/288] time 0.199 (0.211) data 0.000 (0.002) loss 1.2965 (1.6417) teacher_loss 0.7080 (0.9665) loss_zs_kd 0.1593 (0.1482) loss_oracle 0.5088 (0.6011) acc 78.1250 (74.3750) alaph_mean 0.3504 (0.3153) alpha_min 0.0000 (0.0000) alpha_max 0.5693 (0.5245) lr 1.9511e-03 eta 0:43:49
epoch [7/50] batch [220/288] time 0.191 (0.209) data 0.000 (0.002) loss 1.9204 (1.6416) teacher_loss 1.2871 (0.9681) loss_zs_kd 0.1409 (0.1471) loss_oracle 0.5628 (0.5999) acc 68.7500 (74.2614) alaph_mean 0.3095 (0.3152) alpha_min 0.0000 (0.0000) alpha_max 0.5508 (0.5244) lr 1.9511e-03 eta 0:43:26
epoch [7/50] batch [240/288] time 0.184 (0.208) data 0.000 (0.002) loss 2.1846 (1.6457) teacher_loss 1.4805 (0.9743) loss_zs_kd 0.1488 (0.1474) loss_oracle 0.6297 (0.5977) acc 56.2500 (74.0755) alaph_mean 0.2705 (0.3155) alpha_min 0.0000 (0.0000) alpha_max 0.5046 (0.5231) lr 1.9511e-03 eta 0:43:05
epoch [7/50] batch [260/288] time 0.192 (0.207) data 0.000 (0.001) loss 1.5474 (1.6474) teacher_loss 0.9692 (0.9765) loss_zs_kd 0.1179 (0.1461) loss_oracle 0.5192 (0.5978) acc 71.8750 (73.9904) alaph_mean 0.3460 (0.3144) alpha_min 0.0000 (0.0000) alpha_max 0.5067 (0.5227) lr 1.9511e-03 eta 0:42:47
epoch [7/50] batch [280/288] time 0.193 (0.206) data 0.000 (0.001) loss 1.2795 (1.6407) teacher_loss 0.6509 (0.9712) loss_zs_kd 0.1364 (0.1454) loss_oracle 0.5604 (0.5969) acc 84.3750 (74.2076) alaph_mean 0.3323 (0.3149) alpha_min -0.0000 (0.0000) alpha_max 0.6314 (0.5242) lr 1.9511e-03 eta 0:42:32
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,406
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.7%
******* Domain a best val acc:      86.5%, epoch: 7 *******
******* Domain a best val test acc: 83.2%, epoch: 7 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [8/50] batch [20/288] time 0.196 (0.262) data 0.000 (0.014) loss 1.5487 (1.6202) teacher_loss 0.8442 (0.9695) loss_zs_kd 0.2452 (0.1566) loss_oracle 0.5819 (0.5723) acc 87.5000 (74.8438) alaph_mean 0.3120 (0.3124) alpha_min 0.0000 (0.0000) alpha_max 0.5023 (0.5093) lr 1.9298e-03 eta 0:54:04
epoch [8/50] batch [40/288] time 0.194 (0.228) data 0.000 (0.007) loss 1.4882 (1.6005) teacher_loss 0.8418 (0.9557) loss_zs_kd 0.1819 (0.1484) loss_oracle 0.5555 (0.5706) acc 78.1250 (74.4531) alaph_mean 0.3319 (0.3199) alpha_min 0.0000 (0.0000) alpha_max 0.5049 (0.5101) lr 1.9298e-03 eta 0:46:57
epoch [8/50] batch [60/288] time 0.190 (0.217) data 0.000 (0.005) loss 2.3861 (1.5937) teacher_loss 1.5977 (0.9424) loss_zs_kd 0.0963 (0.1455) loss_oracle 0.7403 (0.5786) acc 59.3750 (74.4792) alaph_mean 0.2093 (0.3164) alpha_min 0.0000 (0.0000) alpha_max 0.5024 (0.5088) lr 1.9298e-03 eta 0:44:29
epoch [8/50] batch [80/288] time 0.193 (0.211) data 0.000 (0.004) loss 1.8267 (1.5774) teacher_loss 1.1494 (0.9337) loss_zs_kd 0.0996 (0.1443) loss_oracle 0.6275 (0.5716) acc 68.7500 (75.0391) alaph_mean 0.2536 (0.3202) alpha_min -0.0000 (0.0000) alpha_max 0.5074 (0.5090) lr 1.9298e-03 eta 0:43:17
epoch [8/50] batch [100/288] time 0.197 (0.208) data 0.000 (0.003) loss 1.5158 (1.5796) teacher_loss 0.8174 (0.9395) loss_zs_kd 0.2011 (0.1450) loss_oracle 0.5979 (0.5676) acc 81.2500 (74.9688) alaph_mean 0.3445 (0.3226) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5104) lr 1.9298e-03 eta 0:42:34
epoch [8/50] batch [120/288] time 0.190 (0.206) data 0.000 (0.002) loss 1.7795 (1.5789) teacher_loss 0.8965 (0.9312) loss_zs_kd 0.1687 (0.1478) loss_oracle 0.7986 (0.5738) acc 78.1250 (74.9219) alaph_mean 0.2399 (0.3227) alpha_min -0.0000 (0.0000) alpha_max 0.5085 (0.5124) lr 1.9298e-03 eta 0:42:02
epoch [8/50] batch [140/288] time 0.197 (0.204) data 0.000 (0.002) loss 1.3351 (1.5909) teacher_loss 0.8623 (0.9420) loss_zs_kd 0.2095 (0.1489) loss_oracle 0.3681 (0.5745) acc 75.0000 (74.6652) alaph_mean 0.4246 (0.3220) alpha_min 0.0000 (0.0000) alpha_max 0.5077 (0.5159) lr 1.9298e-03 eta 0:41:41
epoch [8/50] batch [160/288] time 0.194 (0.203) data 0.001 (0.002) loss 1.8022 (1.6054) teacher_loss 1.1553 (0.9578) loss_zs_kd 0.1671 (0.1490) loss_oracle 0.5634 (0.5731) acc 62.5000 (74.3750) alaph_mean 0.3432 (0.3209) alpha_min 0.0000 (0.0000) alpha_max 0.5084 (0.5149) lr 1.9298e-03 eta 0:41:23
epoch [8/50] batch [180/288] time 0.206 (0.203) data 0.000 (0.002) loss 1.9623 (1.6113) teacher_loss 1.3330 (0.9603) loss_zs_kd 0.1332 (0.1476) loss_oracle 0.5627 (0.5772) acc 65.6250 (74.2708) alaph_mean 0.3438 (0.3197) alpha_min 0.0000 (0.0000) alpha_max 0.5060 (0.5169) lr 1.9298e-03 eta 0:41:13
epoch [8/50] batch [200/288] time 0.200 (0.202) data 0.000 (0.002) loss 1.7857 (1.6145) teacher_loss 1.1875 (0.9625) loss_zs_kd 0.1481 (0.1477) loss_oracle 0.5242 (0.5782) acc 62.5000 (74.1250) alaph_mean 0.3232 (0.3195) alpha_min 0.0000 (0.0000) alpha_max 0.6078 (0.5183) lr 1.9298e-03 eta 0:40:59
epoch [8/50] batch [220/288] time 0.194 (0.201) data 0.000 (0.001) loss 1.3425 (1.6108) teacher_loss 0.6982 (0.9577) loss_zs_kd 0.1331 (0.1477) loss_oracle 0.5777 (0.5792) acc 81.2500 (74.3182) alaph_mean 0.3507 (0.3191) alpha_min 0.0000 (0.0000) alpha_max 0.5092 (0.5183) lr 1.9298e-03 eta 0:40:46
epoch [8/50] batch [240/288] time 0.193 (0.201) data 0.000 (0.001) loss 1.4982 (1.6125) teacher_loss 0.9131 (0.9617) loss_zs_kd 0.1769 (0.1483) loss_oracle 0.4966 (0.5766) acc 75.0000 (74.1406) alaph_mean 0.3699 (0.3210) alpha_min 0.0000 (0.0000) alpha_max 0.5094 (0.5190) lr 1.9298e-03 eta 0:40:35
epoch [8/50] batch [260/288] time 0.195 (0.200) data 0.000 (0.001) loss 1.5197 (1.6151) teacher_loss 0.8638 (0.9618) loss_zs_kd 0.1854 (0.1478) loss_oracle 0.5633 (0.5794) acc 78.1250 (74.2428) alaph_mean 0.3488 (0.3206) alpha_min 0.0000 (0.0000) alpha_max 0.5106 (0.5206) lr 1.9298e-03 eta 0:40:25
epoch [8/50] batch [280/288] time 0.189 (0.200) data 0.000 (0.001) loss 2.0977 (1.6229) teacher_loss 1.4336 (0.9674) loss_zs_kd 0.1527 (0.1476) loss_oracle 0.5878 (0.5817) acc 56.2500 (74.1406) alaph_mean 0.3471 (0.3210) alpha_min -0.0000 (0.0000) alpha_max 0.5057 (0.5204) lr 1.9298e-03 eta 0:40:15
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,414
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.0%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,027
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 79.9%
******* Domain a best val acc:      86.7%, epoch: 8 *******
******* Domain a best val test acc: 83.5%, epoch: 8 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [9/50] batch [20/288] time 0.203 (0.235) data 0.000 (0.013) loss 1.5733 (1.6136) teacher_loss 1.0137 (0.9659) loss_zs_kd 0.1308 (0.1385) loss_oracle 0.4942 (0.5784) acc 71.8750 (73.1250) alaph_mean 0.3834 (0.3327) alpha_min -0.0000 (0.0000) alpha_max 0.5078 (0.5470) lr 1.9048e-03 eta 0:47:15
epoch [9/50] batch [40/288] time 0.187 (0.216) data 0.000 (0.007) loss 1.1401 (1.5757) teacher_loss 0.5957 (0.9137) loss_zs_kd 0.0920 (0.1416) loss_oracle 0.4983 (0.5912) acc 81.2500 (75.3125) alaph_mean 0.3941 (0.3251) alpha_min -0.0000 (0.0000) alpha_max 0.5075 (0.5431) lr 1.9048e-03 eta 0:43:27
epoch [9/50] batch [60/288] time 0.196 (0.209) data 0.001 (0.005) loss 1.7652 (1.5821) teacher_loss 1.1719 (0.9203) loss_zs_kd 0.1519 (0.1425) loss_oracle 0.5173 (0.5906) acc 65.6250 (75.1562) alaph_mean 0.3245 (0.3228) alpha_min -0.0000 (0.0000) alpha_max 0.5073 (0.5356) lr 1.9048e-03 eta 0:41:52
epoch [9/50] batch [80/288] time 0.195 (0.205) data 0.000 (0.003) loss 1.4303 (1.6116) teacher_loss 0.8198 (0.9525) loss_zs_kd 0.1463 (0.1466) loss_oracle 0.5374 (0.5858) acc 81.2500 (74.4141) alaph_mean 0.3264 (0.3244) alpha_min -0.0000 (0.0000) alpha_max 0.5062 (0.5303) lr 1.9048e-03 eta 0:41:04
epoch [9/50] batch [100/288] time 0.195 (0.203) data 0.000 (0.003) loss 1.4184 (1.6124) teacher_loss 0.7871 (0.9559) loss_zs_kd 0.1466 (0.1464) loss_oracle 0.5580 (0.5832) acc 71.8750 (74.5312) alaph_mean 0.3282 (0.3244) alpha_min 0.0000 (0.0000) alpha_max 0.5931 (0.5284) lr 1.9048e-03 eta 0:40:35
epoch [9/50] batch [120/288] time 0.194 (0.202) data 0.000 (0.002) loss 1.6281 (1.6167) teacher_loss 0.9985 (0.9662) loss_zs_kd 0.1421 (0.1471) loss_oracle 0.5585 (0.5769) acc 68.7500 (73.9844) alaph_mean 0.3287 (0.3257) alpha_min 0.0000 (0.0000) alpha_max 0.5038 (0.5259) lr 1.9048e-03 eta 0:40:14
epoch [9/50] batch [140/288] time 0.196 (0.201) data 0.000 (0.002) loss 1.5073 (1.6212) teacher_loss 0.9766 (0.9741) loss_zs_kd 0.0795 (0.1470) loss_oracle 0.4910 (0.5736) acc 75.0000 (73.9955) alaph_mean 0.3487 (0.3260) alpha_min 0.0000 (0.0000) alpha_max 0.5017 (0.5264) lr 1.9048e-03 eta 0:39:57
epoch [9/50] batch [160/288] time 0.191 (0.200) data 0.000 (0.002) loss 1.5031 (1.6182) teacher_loss 0.7310 (0.9671) loss_zs_kd 0.1769 (0.1511) loss_oracle 0.6837 (0.5755) acc 84.3750 (74.0625) alaph_mean 0.2396 (0.3238) alpha_min -0.0000 (0.0000) alpha_max 0.6213 (0.5246) lr 1.9048e-03 eta 0:39:43
epoch [9/50] batch [180/288] time 0.198 (0.199) data 0.000 (0.002) loss 2.1782 (1.6168) teacher_loss 1.5332 (0.9678) loss_zs_kd 0.1722 (0.1508) loss_oracle 0.5589 (0.5736) acc 68.7500 (73.9931) alaph_mean 0.3814 (0.3244) alpha_min 0.0000 (0.0000) alpha_max 0.5124 (0.5233) lr 1.9048e-03 eta 0:39:31
epoch [9/50] batch [200/288] time 0.199 (0.199) data 0.000 (0.002) loss 1.7639 (1.6237) teacher_loss 1.1357 (0.9744) loss_zs_kd 0.1634 (0.1521) loss_oracle 0.5464 (0.5732) acc 71.8750 (73.9062) alaph_mean 0.3751 (0.3256) alpha_min -0.0000 (0.0000) alpha_max 0.5075 (0.5249) lr 1.9048e-03 eta 0:39:28
epoch [9/50] batch [220/288] time 0.191 (0.199) data 0.000 (0.001) loss 1.4228 (1.6257) teacher_loss 0.6641 (0.9754) loss_zs_kd 0.1784 (0.1517) loss_oracle 0.6695 (0.5745) acc 81.2500 (73.7074) alaph_mean 0.2670 (0.3247) alpha_min -0.0000 (0.0000) alpha_max 0.5109 (0.5250) lr 1.9048e-03 eta 0:39:20
epoch [9/50] batch [240/288] time 0.198 (0.198) data 0.000 (0.001) loss 1.5754 (1.6255) teacher_loss 1.0205 (0.9736) loss_zs_kd 0.2099 (0.1512) loss_oracle 0.4499 (0.5763) acc 75.0000 (73.8802) alaph_mean 0.4061 (0.3240) alpha_min 0.0000 (0.0000) alpha_max 0.5065 (0.5263) lr 1.9048e-03 eta 0:39:12
epoch [9/50] batch [260/288] time 0.196 (0.198) data 0.000 (0.001) loss 1.5274 (1.6252) teacher_loss 0.9043 (0.9761) loss_zs_kd 0.1285 (0.1514) loss_oracle 0.5588 (0.5734) acc 65.6250 (73.7500) alaph_mean 0.3310 (0.3246) alpha_min 0.0000 (0.0000) alpha_max 0.5119 (0.5260) lr 1.9048e-03 eta 0:39:04
epoch [9/50] batch [280/288] time 0.193 (0.198) data 0.000 (0.001) loss 1.4222 (1.6188) teacher_loss 0.8560 (0.9698) loss_zs_kd 0.0876 (0.1516) loss_oracle 0.5225 (0.5732) acc 71.8750 (73.8728) alaph_mean 0.3438 (0.3243) alpha_min 0.0000 (0.0000) alpha_max 0.5060 (0.5247) lr 1.9048e-03 eta 0:38:56
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,417
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.1%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.5%
******* Domain a best val acc:      86.7%, epoch: 9 *******
******* Domain a best val test acc: 83.1%, epoch: 9 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [10/50] batch [20/288] time 0.192 (0.243) data 0.000 (0.015) loss 1.4306 (1.6466) teacher_loss 0.7061 (1.0050) loss_zs_kd 0.1012 (0.1516) loss_oracle 0.6739 (0.5658) acc 81.2500 (74.3750) alaph_mean 0.2824 (0.3225) alpha_min 0.0000 (0.0000) alpha_max 0.5076 (0.5172) lr 1.8763e-03 eta 0:47:40
epoch [10/50] batch [40/288] time 0.198 (0.221) data 0.000 (0.007) loss 1.8500 (1.5922) teacher_loss 1.1377 (0.9574) loss_zs_kd 0.1949 (0.1527) loss_oracle 0.6149 (0.5584) acc 62.5000 (75.3906) alaph_mean 0.2968 (0.3275) alpha_min 0.0000 (0.0000) alpha_max 0.5091 (0.5216) lr 1.8763e-03 eta 0:43:25
epoch [10/50] batch [60/288] time 0.194 (0.213) data 0.001 (0.005) loss 1.5256 (1.5628) teacher_loss 0.9004 (0.9235) loss_zs_kd 0.1316 (0.1532) loss_oracle 0.5594 (0.5627) acc 75.0000 (75.6771) alaph_mean 0.3138 (0.3228) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5234) lr 1.8763e-03 eta 0:41:40
epoch [10/50] batch [80/288] time 0.194 (0.208) data 0.000 (0.004) loss 1.6685 (1.5877) teacher_loss 1.0791 (0.9401) loss_zs_kd 0.1196 (0.1569) loss_oracle 0.5297 (0.5692) acc 78.1250 (75.3125) alaph_mean 0.3396 (0.3225) alpha_min 0.0000 (0.0000) alpha_max 0.5087 (0.5218) lr 1.8763e-03 eta 0:40:41
epoch [10/50] batch [100/288] time 0.199 (0.206) data 0.000 (0.003) loss 1.9136 (1.5711) teacher_loss 1.2461 (0.9308) loss_zs_kd 0.1472 (0.1532) loss_oracle 0.5939 (0.5637) acc 68.7500 (75.4688) alaph_mean 0.3240 (0.3247) alpha_min 0.0000 (0.0000) alpha_max 0.5135 (0.5216) lr 1.8763e-03 eta 0:40:06
epoch [10/50] batch [120/288] time 0.196 (0.204) data 0.000 (0.003) loss 1.6930 (1.5723) teacher_loss 0.8672 (0.9284) loss_zs_kd 0.1348 (0.1517) loss_oracle 0.7584 (0.5681) acc 81.2500 (75.6250) alaph_mean 0.2500 (0.3242) alpha_min 0.0000 (0.0000) alpha_max 0.5032 (0.5218) lr 1.8763e-03 eta 0:39:40
epoch [10/50] batch [140/288] time 0.183 (0.202) data 0.000 (0.002) loss 1.8287 (1.5915) teacher_loss 1.1943 (0.9437) loss_zs_kd 0.1327 (0.1532) loss_oracle 0.5680 (0.5711) acc 75.0000 (75.3348) alaph_mean 0.3297 (0.3247) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5230) lr 1.8763e-03 eta 0:39:20
epoch [10/50] batch [160/288] time 0.203 (0.201) data 0.000 (0.002) loss 1.1800 (1.5903) teacher_loss 0.6631 (0.9430) loss_zs_kd 0.1125 (0.1522) loss_oracle 0.4607 (0.5712) acc 87.5000 (75.2344) alaph_mean 0.4055 (0.3249) alpha_min 0.0000 (0.0000) alpha_max 0.5070 (0.5237) lr 1.8763e-03 eta 0:39:05
epoch [10/50] batch [180/288] time 0.192 (0.200) data 0.000 (0.002) loss 1.6894 (1.6033) teacher_loss 1.0840 (0.9519) loss_zs_kd 0.1174 (0.1525) loss_oracle 0.5467 (0.5751) acc 75.0000 (74.9306) alaph_mean 0.3242 (0.3234) alpha_min 0.0000 (0.0000) alpha_max 0.5085 (0.5252) lr 1.8763e-03 eta 0:38:51
epoch [10/50] batch [200/288] time 0.191 (0.200) data 0.001 (0.002) loss 1.9672 (1.6144) teacher_loss 1.1777 (0.9604) loss_zs_kd 0.1280 (0.1518) loss_oracle 0.7255 (0.5780) acc 65.6250 (74.6875) alaph_mean 0.2263 (0.3221) alpha_min -0.0000 (0.0000) alpha_max 0.5045 (0.5248) lr 1.8763e-03 eta 0:38:41
epoch [10/50] batch [220/288] time 0.192 (0.199) data 0.000 (0.002) loss 1.6688 (1.6162) teacher_loss 1.0859 (0.9621) loss_zs_kd 0.1355 (0.1511) loss_oracle 0.5152 (0.5786) acc 65.6250 (74.6875) alaph_mean 0.3754 (0.3225) alpha_min 0.0000 (0.0000) alpha_max 0.5102 (0.5233) lr 1.8763e-03 eta 0:38:29
epoch [10/50] batch [240/288] time 0.198 (0.199) data 0.000 (0.001) loss 1.4851 (1.6233) teacher_loss 0.8960 (0.9658) loss_zs_kd 0.1721 (0.1509) loss_oracle 0.5030 (0.5820) acc 78.1250 (74.5703) alaph_mean 0.3619 (0.3207) alpha_min -0.0000 (0.0000) alpha_max 0.5051 (0.5230) lr 1.8763e-03 eta 0:38:20
epoch [10/50] batch [260/288] time 0.206 (0.199) data 0.000 (0.001) loss 1.5551 (1.6260) teacher_loss 1.0615 (0.9661) loss_zs_kd 0.1482 (0.1511) loss_oracle 0.4195 (0.5843) acc 75.0000 (74.4471) alaph_mean 0.4692 (0.3211) alpha_min 0.0000 (0.0000) alpha_max 0.5098 (0.5221) lr 1.8763e-03 eta 0:38:12
epoch [10/50] batch [280/288] time 0.189 (0.198) data 0.000 (0.001) loss 2.3293 (1.6386) teacher_loss 1.5420 (0.9747) loss_zs_kd 0.2070 (0.1519) loss_oracle 0.6837 (0.5879) acc 68.7500 (74.1964) alaph_mean 0.2738 (0.3207) alpha_min 0.0000 (0.0000) alpha_max 0.6126 (0.5242) lr 1.8763e-03 eta 0:38:04
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,422
* accuracy: 86.9%
* error: 13.1%
* macro_f1: 86.3%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.9%
******* Domain a best val acc:      86.9%, epoch: 10 *******
******* Domain a best val test acc: 83.4%, epoch: 10 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [11/50] batch [20/288] time 0.193 (0.226) data 0.000 (0.017) loss 2.3265 (1.6671) teacher_loss 1.5518 (0.9799) loss_zs_kd 0.1381 (0.1571) loss_oracle 0.7057 (0.6087) acc 65.6250 (73.1250) alaph_mean 0.3152 (0.3352) alpha_min 0.0000 (0.0000) alpha_max 0.5365 (0.5622) lr 1.8443e-03 eta 0:43:13
epoch [11/50] batch [40/288] time 0.195 (0.210) data 0.000 (0.008) loss 1.8220 (1.6918) teacher_loss 1.0703 (0.9804) loss_zs_kd 0.1945 (0.1555) loss_oracle 0.6544 (0.6336) acc 65.6250 (72.4219) alaph_mean 0.2846 (0.3169) alpha_min -0.0000 (0.0000) alpha_max 0.5048 (0.5433) lr 1.8443e-03 eta 0:40:09
epoch [11/50] batch [60/288] time 0.198 (0.205) data 0.000 (0.006) loss 1.4329 (1.6819) teacher_loss 0.7974 (0.9787) loss_zs_kd 0.1799 (0.1558) loss_oracle 0.5456 (0.6254) acc 78.1250 (72.8125) alaph_mean 0.4069 (0.3205) alpha_min -0.0000 (0.0000) alpha_max 0.5109 (0.5386) lr 1.8443e-03 eta 0:39:06
epoch [11/50] batch [80/288] time 0.195 (0.202) data 0.000 (0.004) loss 1.8428 (1.6576) teacher_loss 1.1309 (0.9555) loss_zs_kd 0.1078 (0.1534) loss_oracle 0.6581 (0.6255) acc 68.7500 (73.7891) alaph_mean 0.3033 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.5100 (0.5320) lr 1.8443e-03 eta 0:38:31
epoch [11/50] batch [100/288] time 0.197 (0.201) data 0.000 (0.004) loss 1.8188 (1.6412) teacher_loss 1.1387 (0.9424) loss_zs_kd 0.1683 (0.1512) loss_oracle 0.5960 (0.6232) acc 68.7500 (74.2812) alaph_mean 0.3577 (0.3212) alpha_min 0.0000 (0.0000) alpha_max 0.5140 (0.5305) lr 1.8443e-03 eta 0:38:15
epoch [11/50] batch [120/288] time 0.191 (0.200) data 0.000 (0.003) loss 2.1199 (1.6464) teacher_loss 1.3369 (0.9466) loss_zs_kd 0.1427 (0.1507) loss_oracle 0.7117 (0.6245) acc 56.2500 (74.2188) alaph_mean 0.3183 (0.3211) alpha_min 0.0000 (0.0000) alpha_max 0.6093 (0.5295) lr 1.8443e-03 eta 0:37:54
epoch [11/50] batch [140/288] time 0.195 (0.199) data 0.000 (0.003) loss 1.4543 (1.6416) teacher_loss 0.8257 (0.9491) loss_zs_kd 0.1177 (0.1502) loss_oracle 0.5698 (0.6175) acc 81.2500 (74.1741) alaph_mean 0.3129 (0.3236) alpha_min -0.0000 (0.0000) alpha_max 0.5027 (0.5299) lr 1.8443e-03 eta 0:37:41
epoch [11/50] batch [160/288] time 0.198 (0.198) data 0.000 (0.002) loss 1.9272 (1.6529) teacher_loss 1.2490 (0.9641) loss_zs_kd 0.1369 (0.1499) loss_oracle 0.6097 (0.6138) acc 62.5000 (73.7109) alaph_mean 0.3334 (0.3232) alpha_min 0.0000 (0.0000) alpha_max 0.5959 (0.5300) lr 1.8443e-03 eta 0:37:34
epoch [11/50] batch [180/288] time 0.197 (0.198) data 0.000 (0.002) loss 1.5929 (1.6429) teacher_loss 0.9849 (0.9587) loss_zs_kd 0.1777 (0.1501) loss_oracle 0.5192 (0.6092) acc 75.0000 (73.9410) alaph_mean 0.3288 (0.3234) alpha_min 0.0000 (0.0000) alpha_max 0.5048 (0.5292) lr 1.8443e-03 eta 0:37:27
epoch [11/50] batch [200/288] time 0.191 (0.198) data 0.000 (0.002) loss 1.6774 (1.6383) teacher_loss 1.0059 (0.9554) loss_zs_kd 0.2054 (0.1502) loss_oracle 0.5688 (0.6077) acc 78.1250 (74.1875) alaph_mean 0.3517 (0.3229) alpha_min -0.0000 (0.0000) alpha_max 0.5091 (0.5300) lr 1.8443e-03 eta 0:37:18
epoch [11/50] batch [220/288] time 0.194 (0.198) data 0.000 (0.002) loss 1.6564 (1.6427) teacher_loss 0.9229 (0.9559) loss_zs_kd 0.1463 (0.1523) loss_oracle 0.6604 (0.6107) acc 84.3750 (74.1761) alaph_mean 0.3278 (0.3203) alpha_min -0.0000 (0.0000) alpha_max 0.5068 (0.5291) lr 1.8443e-03 eta 0:37:12
epoch [11/50] batch [240/288] time 0.182 (0.197) data 0.000 (0.002) loss 1.7110 (1.6439) teacher_loss 0.9810 (0.9542) loss_zs_kd 0.1076 (0.1527) loss_oracle 0.6762 (0.6133) acc 75.0000 (74.3620) alaph_mean 0.2631 (0.3181) alpha_min -0.0000 (0.0000) alpha_max 0.7330 (0.5291) lr 1.8443e-03 eta 0:37:02
epoch [11/50] batch [260/288] time 0.190 (0.197) data 0.000 (0.001) loss 1.5193 (1.6452) teacher_loss 0.8535 (0.9546) loss_zs_kd 0.1591 (0.1530) loss_oracle 0.5862 (0.6141) acc 81.2500 (74.2788) alaph_mean 0.3278 (0.3178) alpha_min 0.0000 (0.0000) alpha_max 0.5065 (0.5298) lr 1.8443e-03 eta 0:36:55
epoch [11/50] batch [280/288] time 0.196 (0.197) data 0.000 (0.001) loss 1.3098 (1.6366) teacher_loss 0.6030 (0.9468) loss_zs_kd 0.1085 (0.1527) loss_oracle 0.6525 (0.6135) acc 81.2500 (74.4754) alaph_mean 0.3432 (0.3183) alpha_min 0.0000 (0.0000) alpha_max 0.5076 (0.5287) lr 1.8443e-03 eta 0:36:49
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,427
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.4%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.1%
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [12/50] batch [20/288] time 0.202 (0.199) data 0.000 (0.013) loss 1.3764 (1.6392) teacher_loss 0.9155 (0.9641) loss_zs_kd 0.1086 (0.1619) loss_oracle 0.4066 (0.5941) acc 81.2500 (73.5938) alaph_mean 0.4136 (0.3180) alpha_min 0.0000 (0.0000) alpha_max 0.5105 (0.5440) lr 1.8090e-03 eta 0:37:09
epoch [12/50] batch [40/288] time 0.188 (0.196) data 0.000 (0.006) loss 1.7056 (1.6234) teacher_loss 1.0312 (0.9390) loss_zs_kd 0.1040 (0.1589) loss_oracle 0.6224 (0.6050) acc 75.0000 (74.0625) alaph_mean 0.2760 (0.3140) alpha_min -0.0000 (0.0000) alpha_max 0.5061 (0.5467) lr 1.8090e-03 eta 0:36:35
epoch [12/50] batch [60/288] time 0.193 (0.195) data 0.001 (0.004) loss 1.7443 (1.6516) teacher_loss 1.0762 (0.9638) loss_zs_kd 0.1604 (0.1575) loss_oracle 0.5879 (0.6090) acc 78.1250 (73.6458) alaph_mean 0.3228 (0.3137) alpha_min 0.0000 (0.0000) alpha_max 0.5091 (0.5392) lr 1.8090e-03 eta 0:36:16
epoch [12/50] batch [80/288] time 0.194 (0.195) data 0.000 (0.003) loss 2.6398 (1.6496) teacher_loss 1.8643 (0.9649) loss_zs_kd 0.1909 (0.1595) loss_oracle 0.6801 (0.6050) acc 53.1250 (74.1016) alaph_mean 0.3036 (0.3186) alpha_min -0.0000 (0.0000) alpha_max 0.6673 (0.5372) lr 1.8090e-03 eta 0:36:10
epoch [12/50] batch [100/288] time 0.198 (0.194) data 0.000 (0.003) loss 1.7089 (1.6316) teacher_loss 0.9995 (0.9474) loss_zs_kd 0.1492 (0.1572) loss_oracle 0.6348 (0.6055) acc 68.7500 (74.5938) alaph_mean 0.3133 (0.3170) alpha_min 0.0000 (0.0000) alpha_max 0.5073 (0.5356) lr 1.8090e-03 eta 0:36:04
epoch [12/50] batch [120/288] time 0.197 (0.195) data 0.000 (0.002) loss 1.9557 (1.6247) teacher_loss 1.2510 (0.9420) loss_zs_kd 0.1499 (0.1564) loss_oracle 0.6297 (0.6045) acc 65.6250 (74.8438) alaph_mean 0.2664 (0.3174) alpha_min -0.0000 (0.0000) alpha_max 0.5076 (0.5312) lr 1.8090e-03 eta 0:36:04
epoch [12/50] batch [140/288] time 0.192 (0.194) data 0.000 (0.002) loss 1.1543 (1.6281) teacher_loss 0.4741 (0.9471) loss_zs_kd 0.1261 (0.1581) loss_oracle 0.6171 (0.6019) acc 90.6250 (74.5536) alaph_mean 0.3422 (0.3195) alpha_min 0.0000 (0.0000) alpha_max 0.5123 (0.5284) lr 1.8090e-03 eta 0:35:57
epoch [12/50] batch [160/288] time 0.193 (0.194) data 0.000 (0.002) loss 1.6795 (1.6263) teacher_loss 0.9399 (0.9451) loss_zs_kd 0.2259 (0.1592) loss_oracle 0.6266 (0.6016) acc 75.0000 (74.5312) alaph_mean 0.3367 (0.3209) alpha_min -0.0000 (0.0000) alpha_max 0.6500 (0.5288) lr 1.8090e-03 eta 0:35:53
epoch [12/50] batch [180/288] time 0.194 (0.194) data 0.000 (0.002) loss 1.5375 (1.6441) teacher_loss 0.7881 (0.9583) loss_zs_kd 0.1836 (0.1603) loss_oracle 0.6576 (0.6057) acc 84.3750 (74.1493) alaph_mean 0.3141 (0.3199) alpha_min 0.0000 (0.0000) alpha_max 0.5143 (0.5306) lr 1.8090e-03 eta 0:35:48
epoch [12/50] batch [200/288] time 0.215 (0.194) data 0.000 (0.001) loss 1.9977 (1.6505) teacher_loss 1.1768 (0.9612) loss_zs_kd 0.1635 (0.1596) loss_oracle 0.7392 (0.6095) acc 62.5000 (74.2500) alaph_mean 0.2276 (0.3175) alpha_min -0.0000 (0.0000) alpha_max 0.5055 (0.5294) lr 1.8090e-03 eta 0:35:43
epoch [12/50] batch [220/288] time 0.196 (0.194) data 0.000 (0.001) loss 1.4634 (1.6484) teacher_loss 0.7256 (0.9588) loss_zs_kd 0.2114 (0.1596) loss_oracle 0.6321 (0.6098) acc 78.1250 (74.3324) alaph_mean 0.2655 (0.3160) alpha_min 0.0000 (0.0000) alpha_max 0.5090 (0.5282) lr 1.8090e-03 eta 0:35:38
epoch [12/50] batch [240/288] time 0.195 (0.194) data 0.000 (0.001) loss 1.8221 (1.6519) teacher_loss 1.2783 (0.9632) loss_zs_kd 0.1832 (0.1594) loss_oracle 0.4522 (0.6090) acc 59.3750 (74.1797) alaph_mean 0.4220 (0.3158) alpha_min 0.0000 (0.0000) alpha_max 0.5092 (0.5264) lr 1.8090e-03 eta 0:35:34
epoch [12/50] batch [260/288] time 0.193 (0.194) data 0.000 (0.001) loss 1.5597 (1.6475) teacher_loss 0.8130 (0.9597) loss_zs_kd 0.1734 (0.1590) loss_oracle 0.6601 (0.6083) acc 75.0000 (74.2188) alaph_mean 0.2664 (0.3163) alpha_min 0.0000 (0.0000) alpha_max 0.5088 (0.5258) lr 1.8090e-03 eta 0:35:30
epoch [12/50] batch [280/288] time 0.174 (0.194) data 0.000 (0.001) loss 1.5568 (1.6424) teacher_loss 0.8101 (0.9566) loss_zs_kd 0.1216 (0.1587) loss_oracle 0.6859 (0.6065) acc 78.1250 (74.3080) alaph_mean 0.2655 (0.3179) alpha_min -0.0000 (0.0000) alpha_max 0.5041 (0.5245) lr 1.8090e-03 eta 0:35:26
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.5%
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [13/50] batch [20/288] time 0.195 (0.204) data 0.000 (0.014) loss 1.3871 (1.5753) teacher_loss 0.7656 (0.9241) loss_zs_kd 0.1808 (0.1624) loss_oracle 0.5311 (0.5700) acc 78.1250 (73.7500) alaph_mean 0.3443 (0.3369) alpha_min 0.0000 (0.0000) alpha_max 0.5151 (0.5096) lr 1.7705e-03 eta 0:37:10
epoch [13/50] batch [40/288] time 0.196 (0.199) data 0.000 (0.007) loss 1.2889 (1.6428) teacher_loss 0.6953 (0.9805) loss_zs_kd 0.1957 (0.1644) loss_oracle 0.4958 (0.5802) acc 87.5000 (72.5781) alaph_mean 0.3454 (0.3316) alpha_min 0.0000 (0.0000) alpha_max 0.5106 (0.5164) lr 1.7705e-03 eta 0:36:14
epoch [13/50] batch [60/288] time 0.191 (0.197) data 0.001 (0.005) loss 2.3445 (1.7189) teacher_loss 1.4648 (1.0292) loss_zs_kd 0.1988 (0.1643) loss_oracle 0.7803 (0.6075) acc 65.6250 (72.1354) alaph_mean 0.1888 (0.3167) alpha_min -0.0000 (0.0000) alpha_max 0.5093 (0.5166) lr 1.7705e-03 eta 0:35:46
epoch [13/50] batch [80/288] time 0.195 (0.197) data 0.000 (0.004) loss 1.9267 (1.6997) teacher_loss 1.3047 (1.0071) loss_zs_kd 0.2366 (0.1644) loss_oracle 0.5037 (0.6104) acc 68.7500 (72.8125) alaph_mean 0.3677 (0.3172) alpha_min 0.0000 (0.0000) alpha_max 0.5107 (0.5172) lr 1.7705e-03 eta 0:35:34
epoch [13/50] batch [100/288] time 0.195 (0.196) data 0.000 (0.003) loss 1.2267 (1.6624) teacher_loss 0.5737 (0.9731) loss_zs_kd 0.1004 (0.1608) loss_oracle 0.6028 (0.6089) acc 87.5000 (73.7188) alaph_mean 0.3310 (0.3157) alpha_min 0.0000 (0.0000) alpha_max 0.5072 (0.5160) lr 1.7705e-03 eta 0:35:24
epoch [13/50] batch [120/288] time 0.189 (0.196) data 0.000 (0.003) loss 1.7879 (1.6528) teacher_loss 0.9814 (0.9652) loss_zs_kd 0.1503 (0.1608) loss_oracle 0.7313 (0.6072) acc 75.0000 (74.1146) alaph_mean 0.2529 (0.3164) alpha_min -0.0000 (0.0000) alpha_max 0.5113 (0.5185) lr 1.7705e-03 eta 0:35:18
epoch [13/50] batch [140/288] time 0.192 (0.195) data 0.000 (0.002) loss 1.6155 (1.6593) teacher_loss 0.9165 (0.9750) loss_zs_kd 0.1620 (0.1596) loss_oracle 0.6180 (0.6046) acc 75.0000 (73.9509) alaph_mean 0.3012 (0.3167) alpha_min 0.0000 (0.0000) alpha_max 0.5049 (0.5190) lr 1.7705e-03 eta 0:35:11
epoch [13/50] batch [160/288] time 0.194 (0.195) data 0.000 (0.002) loss 1.7961 (1.6461) teacher_loss 1.1025 (0.9631) loss_zs_kd 0.1349 (0.1582) loss_oracle 0.6261 (0.6039) acc 71.8750 (74.2578) alaph_mean 0.2977 (0.3171) alpha_min 0.0000 (0.0000) alpha_max 0.5158 (0.5200) lr 1.7705e-03 eta 0:35:06
epoch [13/50] batch [180/288] time 0.196 (0.195) data 0.000 (0.002) loss 1.2950 (1.6516) teacher_loss 0.6519 (0.9694) loss_zs_kd 0.1112 (0.1597) loss_oracle 0.5875 (0.6023) acc 78.1250 (74.1146) alaph_mean 0.3600 (0.3178) alpha_min 0.0000 (0.0000) alpha_max 0.5067 (0.5198) lr 1.7705e-03 eta 0:35:01
epoch [13/50] batch [200/288] time 0.196 (0.195) data 0.000 (0.002) loss 2.0260 (1.6486) teacher_loss 1.3779 (0.9659) loss_zs_kd 0.1179 (0.1624) loss_oracle 0.5891 (0.6015) acc 65.6250 (74.1406) alaph_mean 0.3231 (0.3184) alpha_min -0.0000 (0.0000) alpha_max 0.5142 (0.5192) lr 1.7705e-03 eta 0:34:55
epoch [13/50] batch [220/288] time 0.177 (0.195) data 0.000 (0.001) loss 1.5360 (1.6430) teacher_loss 0.7915 (0.9610) loss_zs_kd 0.2154 (0.1630) loss_oracle 0.6368 (0.6005) acc 81.2500 (74.4034) alaph_mean 0.2454 (0.3187) alpha_min -0.0000 (0.0000) alpha_max 0.5498 (0.5212) lr 1.7705e-03 eta 0:34:49
epoch [13/50] batch [240/288] time 0.193 (0.195) data 0.000 (0.001) loss 1.8154 (1.6388) teacher_loss 1.2021 (0.9589) loss_zs_kd 0.1727 (0.1611) loss_oracle 0.5269 (0.5993) acc 68.7500 (74.4401) alaph_mean 0.3606 (0.3189) alpha_min 0.0000 (0.0000) alpha_max 0.5050 (0.5208) lr 1.7705e-03 eta 0:34:45
epoch [13/50] batch [260/288] time 0.195 (0.195) data 0.000 (0.001) loss 1.6441 (1.6259) teacher_loss 1.0439 (0.9491) loss_zs_kd 0.1490 (0.1591) loss_oracle 0.5257 (0.5972) acc 71.8750 (74.6755) alaph_mean 0.3406 (0.3194) alpha_min 0.0000 (0.0000) alpha_max 0.5074 (0.5208) lr 1.7705e-03 eta 0:34:41
epoch [13/50] batch [280/288] time 0.202 (0.195) data 0.001 (0.001) loss 1.3238 (1.6263) teacher_loss 0.7056 (0.9508) loss_zs_kd 0.1236 (0.1582) loss_oracle 0.5564 (0.5964) acc 84.3750 (74.6540) alaph_mean 0.3547 (0.3201) alpha_min -0.0000 (0.0000) alpha_max 0.5975 (0.5209) lr 1.7705e-03 eta 0:34:36
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,420
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,028
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.1%
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [14/50] batch [20/288] time 0.202 (0.209) data 0.000 (0.013) loss 1.5758 (1.6611) teacher_loss 0.9990 (0.9917) loss_zs_kd 0.1495 (0.1477) loss_oracle 0.5020 (0.5955) acc 68.7500 (72.9688) alaph_mean 0.3786 (0.3170) alpha_min 0.0000 (0.0000) alpha_max 0.5067 (0.5109) lr 1.7290e-03 eta 0:37:05
epoch [14/50] batch [40/288] time 0.197 (0.202) data 0.000 (0.006) loss 1.5562 (1.6300) teacher_loss 0.7578 (0.9470) loss_zs_kd 0.1916 (0.1506) loss_oracle 0.7027 (0.6077) acc 78.1250 (74.3750) alaph_mean 0.2470 (0.3150) alpha_min 0.0000 (0.0000) alpha_max 0.5034 (0.5190) lr 1.7290e-03 eta 0:35:39
epoch [14/50] batch [60/288] time 0.193 (0.199) data 0.000 (0.004) loss 1.4841 (1.6025) teacher_loss 0.8750 (0.9241) loss_zs_kd 0.1345 (0.1535) loss_oracle 0.5418 (0.6016) acc 71.8750 (75.0000) alaph_mean 0.3446 (0.3138) alpha_min -0.0000 (0.0000) alpha_max 0.5083 (0.5190) lr 1.7290e-03 eta 0:35:05
epoch [14/50] batch [80/288] time 0.188 (0.198) data 0.000 (0.003) loss 1.8567 (1.6243) teacher_loss 1.1084 (0.9401) loss_zs_kd 0.1456 (0.1569) loss_oracle 0.6755 (0.6058) acc 78.1250 (74.4922) alaph_mean 0.2600 (0.3120) alpha_min 0.0000 (0.0000) alpha_max 0.5051 (0.5217) lr 1.7290e-03 eta 0:34:49
epoch [14/50] batch [100/288] time 0.207 (0.197) data 0.000 (0.003) loss 1.7401 (1.6185) teacher_loss 1.1006 (0.9378) loss_zs_kd 0.1781 (0.1593) loss_oracle 0.5504 (0.6010) acc 68.7500 (74.5625) alaph_mean 0.3513 (0.3124) alpha_min 0.0000 (0.0000) alpha_max 0.5151 (0.5195) lr 1.7290e-03 eta 0:34:38
epoch [14/50] batch [120/288] time 0.197 (0.196) data 0.000 (0.002) loss 1.3369 (1.6318) teacher_loss 0.7017 (0.9494) loss_zs_kd 0.1649 (0.1575) loss_oracle 0.5528 (0.6037) acc 84.3750 (74.4271) alaph_mean 0.3651 (0.3120) alpha_min 0.0000 (0.0000) alpha_max 0.5141 (0.5226) lr 1.7290e-03 eta 0:34:27
epoch [14/50] batch [140/288] time 0.200 (0.196) data 0.000 (0.002) loss 1.3975 (1.6409) teacher_loss 0.8140 (0.9570) loss_zs_kd 0.1348 (0.1574) loss_oracle 0.5161 (0.6053) acc 81.2500 (74.4643) alaph_mean 0.3634 (0.3121) alpha_min 0.0000 (0.0000) alpha_max 0.5148 (0.5211) lr 1.7290e-03 eta 0:34:21
epoch [14/50] batch [160/288] time 0.198 (0.196) data 0.000 (0.002) loss 1.2246 (1.6285) teacher_loss 0.6187 (0.9450) loss_zs_kd 0.1528 (0.1565) loss_oracle 0.5295 (0.6052) acc 84.3750 (74.7656) alaph_mean 0.3412 (0.3130) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5244) lr 1.7290e-03 eta 0:34:15
epoch [14/50] batch [180/288] time 0.198 (0.196) data 0.000 (0.002) loss 1.1701 (1.6186) teacher_loss 0.6343 (0.9344) loss_zs_kd 0.1060 (0.1555) loss_oracle 0.4829 (0.6065) acc 84.3750 (74.9653) alaph_mean 0.4058 (0.3130) alpha_min 0.0000 (0.0000) alpha_max 0.5079 (0.5233) lr 1.7290e-03 eta 0:34:10
epoch [14/50] batch [200/288] time 0.180 (0.196) data 0.000 (0.001) loss 1.8920 (1.6179) teacher_loss 1.1484 (0.9395) loss_zs_kd 0.1797 (0.1548) loss_oracle 0.6537 (0.6010) acc 65.6250 (74.9062) alaph_mean 0.2632 (0.3163) alpha_min 0.0000 (0.0000) alpha_max 0.5107 (0.5245) lr 1.7290e-03 eta 0:34:04
epoch [14/50] batch [220/288] time 0.195 (0.195) data 0.000 (0.001) loss 1.0691 (1.6106) teacher_loss 0.4160 (0.9326) loss_zs_kd 0.1328 (0.1549) loss_oracle 0.5867 (0.6005) acc 87.5000 (75.1705) alaph_mean 0.3280 (0.3172) alpha_min -0.0000 (0.0000) alpha_max 0.5038 (0.5281) lr 1.7290e-03 eta 0:33:59
epoch [14/50] batch [240/288] time 0.191 (0.195) data 0.000 (0.001) loss 1.5568 (1.6070) teacher_loss 0.7896 (0.9284) loss_zs_kd 0.1925 (0.1558) loss_oracle 0.6710 (0.6007) acc 78.1250 (75.2083) alaph_mean 0.2691 (0.3170) alpha_min 0.0000 (0.0000) alpha_max 0.5143 (0.5267) lr 1.7290e-03 eta 0:33:54
epoch [14/50] batch [260/288] time 0.205 (0.195) data 0.000 (0.001) loss 1.8354 (1.6078) teacher_loss 1.1973 (0.9282) loss_zs_kd 0.1724 (0.1570) loss_oracle 0.5519 (0.6012) acc 75.0000 (75.2404) alaph_mean 0.3229 (0.3174) alpha_min 0.0000 (0.0000) alpha_max 0.5086 (0.5276) lr 1.7290e-03 eta 0:33:49
epoch [14/50] batch [280/288] time 0.190 (0.195) data 0.000 (0.001) loss 1.2038 (1.6065) teacher_loss 0.5283 (0.9270) loss_zs_kd 0.1404 (0.1568) loss_oracle 0.6053 (0.6011) acc 87.5000 (75.3237) alaph_mean 0.3313 (0.3182) alpha_min 0.0000 (0.0000) alpha_max 0.5137 (0.5284) lr 1.7290e-03 eta 0:33:44
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,412
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,019
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.4%
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [15/50] batch [20/288] time 0.189 (0.211) data 0.000 (0.016) loss 1.8927 (1.6352) teacher_loss 1.0908 (0.9407) loss_zs_kd 0.1204 (0.1549) loss_oracle 0.7417 (0.6170) acc 75.0000 (75.1562) alaph_mean 0.2530 (0.3185) alpha_min -0.0000 (0.0000) alpha_max 0.5058 (0.5436) lr 1.6845e-03 eta 0:36:23
epoch [15/50] batch [40/288] time 0.185 (0.203) data 0.000 (0.008) loss 1.8738 (1.6842) teacher_loss 1.0371 (0.9960) loss_zs_kd 0.1567 (0.1557) loss_oracle 0.7583 (0.6103) acc 78.1250 (74.2969) alaph_mean 0.2503 (0.3252) alpha_min 0.0000 (0.0000) alpha_max 0.5056 (0.5305) lr 1.6845e-03 eta 0:34:57
epoch [15/50] batch [60/288] time 0.195 (0.201) data 0.000 (0.006) loss 1.2728 (1.6330) teacher_loss 0.6021 (0.9517) loss_zs_kd 0.1370 (0.1566) loss_oracle 0.6023 (0.6030) acc 78.1250 (75.2083) alaph_mean 0.3381 (0.3275) alpha_min 0.0000 (0.0000) alpha_max 0.5116 (0.5382) lr 1.6845e-03 eta 0:34:28
epoch [15/50] batch [80/288] time 0.198 (0.199) data 0.001 (0.004) loss 1.6874 (1.5963) teacher_loss 0.9219 (0.9154) loss_zs_kd 0.1354 (0.1549) loss_oracle 0.6978 (0.6034) acc 84.3750 (76.1328) alaph_mean 0.2978 (0.3254) alpha_min 0.0000 (0.0000) alpha_max 0.5096 (0.5340) lr 1.6845e-03 eta 0:34:07
epoch [15/50] batch [100/288] time 0.212 (0.198) data 0.000 (0.003) loss 1.6590 (1.5863) teacher_loss 0.9375 (0.9070) loss_zs_kd 0.1262 (0.1568) loss_oracle 0.6584 (0.6009) acc 75.0000 (76.1250) alaph_mean 0.3218 (0.3273) alpha_min 0.0000 (0.0000) alpha_max 0.5101 (0.5316) lr 1.6845e-03 eta 0:33:54
epoch [15/50] batch [120/288] time 0.174 (0.197) data 0.000 (0.003) loss 1.9074 (1.5857) teacher_loss 1.1797 (0.9072) loss_zs_kd 0.1999 (0.1553) loss_oracle 0.6277 (0.6009) acc 71.8750 (75.9375) alaph_mean 0.3294 (0.3290) alpha_min -0.0000 (0.0000) alpha_max 0.5119 (0.5359) lr 1.6845e-03 eta 0:33:43
epoch [15/50] batch [140/288] time 0.195 (0.197) data 0.000 (0.003) loss 1.7248 (1.5960) teacher_loss 1.0098 (0.9162) loss_zs_kd 0.1366 (0.1554) loss_oracle 0.6467 (0.6021) acc 75.0000 (75.5580) alaph_mean 0.3451 (0.3289) alpha_min -0.0000 (0.0000) alpha_max 0.5157 (0.5348) lr 1.6845e-03 eta 0:33:36
epoch [15/50] batch [160/288] time 0.183 (0.197) data 0.000 (0.002) loss 1.6769 (1.6037) teacher_loss 1.0742 (0.9222) loss_zs_kd 0.1241 (0.1565) loss_oracle 0.5406 (0.6032) acc 68.7500 (75.2930) alaph_mean 0.3402 (0.3292) alpha_min 0.0000 (0.0000) alpha_max 0.6609 (0.5341) lr 1.6845e-03 eta 0:33:29
epoch [15/50] batch [180/288] time 0.196 (0.197) data 0.000 (0.002) loss 2.0280 (1.6142) teacher_loss 1.3555 (0.9317) loss_zs_kd 0.1710 (0.1560) loss_oracle 0.5870 (0.6045) acc 71.8750 (75.0347) alaph_mean 0.3320 (0.3293) alpha_min -0.0000 (0.0000) alpha_max 0.5102 (0.5345) lr 1.6845e-03 eta 0:33:23
epoch [15/50] batch [200/288] time 0.197 (0.197) data 0.000 (0.002) loss 1.4211 (1.6220) teacher_loss 0.7412 (0.9388) loss_zs_kd 0.1920 (0.1557) loss_oracle 0.5839 (0.6054) acc 75.0000 (74.7969) alaph_mean 0.3698 (0.3301) alpha_min -0.0000 (0.0000) alpha_max 0.5081 (0.5371) lr 1.6845e-03 eta 0:33:18
epoch [15/50] batch [220/288] time 0.188 (0.196) data 0.000 (0.002) loss 1.8814 (1.6284) teacher_loss 1.1318 (0.9400) loss_zs_kd 0.1416 (0.1553) loss_oracle 0.6787 (0.6107) acc 71.8750 (74.7443) alaph_mean 0.2822 (0.3283) alpha_min 0.0000 (0.0000) alpha_max 0.5107 (0.5372) lr 1.6845e-03 eta 0:33:08
epoch [15/50] batch [240/288] time 0.193 (0.196) data 0.000 (0.002) loss 1.6320 (1.6245) teacher_loss 0.8018 (0.9343) loss_zs_kd 0.2721 (0.1564) loss_oracle 0.6942 (0.6120) acc 75.0000 (74.8438) alaph_mean 0.2655 (0.3282) alpha_min 0.0000 (0.0000) alpha_max 0.5065 (0.5374) lr 1.6845e-03 eta 0:33:04
epoch [15/50] batch [260/288] time 0.198 (0.196) data 0.000 (0.001) loss 1.5625 (1.6294) teacher_loss 1.0107 (0.9378) loss_zs_kd 0.1202 (0.1571) loss_oracle 0.4916 (0.6131) acc 78.1250 (74.7236) alaph_mean 0.3789 (0.3282) alpha_min 0.0000 (0.0000) alpha_max 0.5048 (0.5372) lr 1.6845e-03 eta 0:32:59
epoch [15/50] batch [280/288] time 0.196 (0.196) data 0.000 (0.001) loss 1.4591 (1.6269) teacher_loss 0.9258 (0.9361) loss_zs_kd 0.1273 (0.1563) loss_oracle 0.4697 (0.6126) acc 68.7500 (74.6875) alaph_mean 0.3604 (0.3278) alpha_min 0.0000 (0.0000) alpha_max 0.7091 (0.5381) lr 1.6845e-03 eta 0:32:53
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,411
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.7%
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [16/50] batch [20/288] time 0.186 (0.209) data 0.000 (0.012) loss 2.2564 (1.6579) teacher_loss 1.3164 (0.9765) loss_zs_kd 0.1772 (0.1512) loss_oracle 0.8514 (0.6058) acc 65.6250 (74.3750) alaph_mean 0.1852 (0.3209) alpha_min 0.0000 (0.0000) alpha_max 0.5017 (0.5070) lr 1.6374e-03 eta 0:35:02
epoch [16/50] batch [40/288] time 0.195 (0.202) data 0.001 (0.006) loss 1.7904 (1.6296) teacher_loss 1.0986 (0.9400) loss_zs_kd 0.1699 (0.1557) loss_oracle 0.6069 (0.6117) acc 68.7500 (73.9062) alaph_mean 0.3281 (0.3187) alpha_min 0.0000 (0.0000) alpha_max 0.5065 (0.5143) lr 1.6374e-03 eta 0:33:50
epoch [16/50] batch [60/288] time 0.185 (0.200) data 0.000 (0.004) loss 2.2651 (1.6444) teacher_loss 1.1387 (0.9379) loss_zs_kd 0.2095 (0.1559) loss_oracle 1.0217 (0.6285) acc 75.0000 (74.1667) alaph_mean 0.1294 (0.3146) alpha_min -0.0000 (0.0000) alpha_max 0.7312 (0.5200) lr 1.6374e-03 eta 0:33:20
epoch [16/50] batch [80/288] time 0.192 (0.198) data 0.000 (0.003) loss 2.0173 (1.6487) teacher_loss 1.3037 (0.9366) loss_zs_kd 0.1605 (0.1550) loss_oracle 0.6334 (0.6345) acc 68.7500 (74.6094) alaph_mean 0.2720 (0.3101) alpha_min 0.0000 (0.0000) alpha_max 0.5038 (0.5174) lr 1.6374e-03 eta 0:33:01
epoch [16/50] batch [100/288] time 0.194 (0.198) data 0.000 (0.003) loss 1.5374 (1.6551) teacher_loss 1.0195 (0.9457) loss_zs_kd 0.1764 (0.1574) loss_oracle 0.4297 (0.6307) acc 65.6250 (74.2188) alaph_mean 0.3965 (0.3125) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5187) lr 1.6374e-03 eta 0:32:51
epoch [16/50] batch [120/288] time 0.183 (0.197) data 0.000 (0.002) loss 1.4263 (1.6475) teacher_loss 0.6797 (0.9395) loss_zs_kd 0.1989 (0.1559) loss_oracle 0.6472 (0.6301) acc 87.5000 (74.4271) alaph_mean 0.2810 (0.3126) alpha_min 0.0000 (0.0000) alpha_max 0.5093 (0.5217) lr 1.6374e-03 eta 0:32:41
epoch [16/50] batch [140/288] time 0.185 (0.197) data 0.000 (0.002) loss 1.6824 (1.6226) teacher_loss 1.0146 (0.9208) loss_zs_kd 0.1360 (0.1551) loss_oracle 0.5997 (0.6243) acc 71.8750 (74.9330) alaph_mean 0.3345 (0.3160) alpha_min 0.0000 (0.0000) alpha_max 0.5043 (0.5203) lr 1.6374e-03 eta 0:32:33
epoch [16/50] batch [160/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.7483 (1.6200) teacher_loss 1.0537 (0.9177) loss_zs_kd 0.1410 (0.1562) loss_oracle 0.6241 (0.6243) acc 75.0000 (75.1172) alaph_mean 0.3134 (0.3156) alpha_min 0.0000 (0.0000) alpha_max 0.5087 (0.5198) lr 1.6374e-03 eta 0:32:26
epoch [16/50] batch [180/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.4685 (1.6251) teacher_loss 0.8081 (0.9265) loss_zs_kd 0.1287 (0.1570) loss_oracle 0.5960 (0.6202) acc 78.1250 (75.0694) alaph_mean 0.3308 (0.3175) alpha_min 0.0000 (0.0000) alpha_max 0.5084 (0.5190) lr 1.6374e-03 eta 0:32:20
epoch [16/50] batch [200/288] time 0.195 (0.196) data 0.000 (0.001) loss 1.3768 (1.6251) teacher_loss 0.7744 (0.9305) loss_zs_kd 0.1821 (0.1563) loss_oracle 0.5114 (0.6164) acc 81.2500 (74.8594) alaph_mean 0.3587 (0.3177) alpha_min -0.0000 (0.0000) alpha_max 0.5066 (0.5185) lr 1.6374e-03 eta 0:32:14
epoch [16/50] batch [220/288] time 0.213 (0.196) data 0.000 (0.001) loss 1.2779 (1.6242) teacher_loss 0.6523 (0.9298) loss_zs_kd 0.1563 (0.1567) loss_oracle 0.5474 (0.6160) acc 84.3750 (74.7869) alaph_mean 0.3982 (0.3168) alpha_min 0.0000 (0.0000) alpha_max 0.7928 (0.5217) lr 1.6374e-03 eta 0:32:08
epoch [16/50] batch [240/288] time 0.184 (0.196) data 0.000 (0.001) loss 1.4689 (1.6167) teacher_loss 0.7876 (0.9226) loss_zs_kd 0.2120 (0.1573) loss_oracle 0.5753 (0.6154) acc 78.1250 (74.9609) alaph_mean 0.3267 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5370 (0.5218) lr 1.6374e-03 eta 0:32:04
epoch [16/50] batch [260/288] time 0.197 (0.195) data 0.000 (0.001) loss 1.5957 (1.6207) teacher_loss 0.9287 (0.9272) loss_zs_kd 0.1392 (0.1580) loss_oracle 0.5974 (0.6145) acc 78.1250 (74.8438) alaph_mean 0.3210 (0.3176) alpha_min 0.0000 (0.0000) alpha_max 0.5087 (0.5208) lr 1.6374e-03 eta 0:31:59
epoch [16/50] batch [280/288] time 0.183 (0.195) data 0.000 (0.001) loss 2.1629 (1.6284) teacher_loss 1.3721 (0.9331) loss_zs_kd 0.2430 (0.1594) loss_oracle 0.6693 (0.6156) acc 59.3750 (74.6652) alaph_mean 0.2916 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.7701 (0.5234) lr 1.6374e-03 eta 0:31:54
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,419
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,013
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.5%
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [17/50] batch [20/288] time 0.193 (0.197) data 0.000 (0.013) loss 2.0419 (1.5282) teacher_loss 1.3506 (0.8004) loss_zs_kd 0.1490 (0.1502) loss_oracle 0.6169 (0.6526) acc 75.0000 (79.8438) alaph_mean 0.3360 (0.3065) alpha_min 0.0000 (0.0000) alpha_max 0.5043 (0.5452) lr 1.5878e-03 eta 0:32:03
epoch [17/50] batch [40/288] time 0.188 (0.196) data 0.000 (0.006) loss 1.4719 (1.5926) teacher_loss 0.8281 (0.8924) loss_zs_kd 0.1858 (0.1509) loss_oracle 0.5509 (0.6248) acc 81.2500 (77.7344) alaph_mean 0.3450 (0.3130) alpha_min 0.0000 (0.0000) alpha_max 0.5104 (0.5383) lr 1.5878e-03 eta 0:31:48
epoch [17/50] batch [60/288] time 0.198 (0.196) data 0.001 (0.004) loss 2.1365 (1.6108) teacher_loss 1.3291 (0.9086) loss_zs_kd 0.1433 (0.1546) loss_oracle 0.7358 (0.6249) acc 65.6250 (76.5104) alaph_mean 0.2357 (0.3115) alpha_min 0.0000 (0.0000) alpha_max 0.5095 (0.5340) lr 1.5878e-03 eta 0:31:46
epoch [17/50] batch [80/288] time 0.192 (0.196) data 0.000 (0.003) loss 1.4206 (1.6039) teacher_loss 0.7529 (0.9050) loss_zs_kd 0.1630 (0.1554) loss_oracle 0.5861 (0.6212) acc 81.2500 (76.4062) alaph_mean 0.2918 (0.3096) alpha_min 0.0000 (0.0000) alpha_max 0.5091 (0.5361) lr 1.5878e-03 eta 0:31:38
epoch [17/50] batch [100/288] time 0.194 (0.195) data 0.000 (0.003) loss 1.6444 (1.5925) teacher_loss 1.0596 (0.9049) loss_zs_kd 0.1388 (0.1544) loss_oracle 0.5155 (0.6105) acc 68.7500 (76.1250) alaph_mean 0.3493 (0.3124) alpha_min -0.0000 (0.0000) alpha_max 0.5110 (0.5353) lr 1.5878e-03 eta 0:31:30
epoch [17/50] batch [120/288] time 0.196 (0.195) data 0.000 (0.002) loss 1.3609 (1.5942) teacher_loss 0.9282 (0.9101) loss_zs_kd 0.1053 (0.1538) loss_oracle 0.3800 (0.6072) acc 78.1250 (75.8073) alaph_mean 0.4256 (0.3131) alpha_min -0.0000 (0.0000) alpha_max 0.6159 (0.5318) lr 1.5878e-03 eta 0:31:24
epoch [17/50] batch [140/288] time 0.196 (0.195) data 0.000 (0.002) loss 1.7250 (1.6038) teacher_loss 1.0977 (0.9296) loss_zs_kd 0.1470 (0.1558) loss_oracle 0.5539 (0.5962) acc 68.7500 (75.3571) alaph_mean 0.3449 (0.3172) alpha_min 0.0000 (0.0000) alpha_max 0.5085 (0.5308) lr 1.5878e-03 eta 0:31:20
epoch [17/50] batch [160/288] time 0.195 (0.195) data 0.000 (0.002) loss 1.7325 (1.6021) teacher_loss 1.0098 (0.9286) loss_zs_kd 0.1426 (0.1563) loss_oracle 0.6514 (0.5954) acc 75.0000 (75.2344) alaph_mean 0.3030 (0.3185) alpha_min 0.0000 (0.0000) alpha_max 0.5099 (0.5305) lr 1.5878e-03 eta 0:31:15
epoch [17/50] batch [180/288] time 0.194 (0.195) data 0.000 (0.002) loss 1.5526 (1.5973) teacher_loss 0.8330 (0.9192) loss_zs_kd 0.1239 (0.1551) loss_oracle 0.6577 (0.6005) acc 71.8750 (75.4688) alaph_mean 0.3055 (0.3185) alpha_min 0.0000 (0.0000) alpha_max 0.7554 (0.5315) lr 1.5878e-03 eta 0:31:13
epoch [17/50] batch [200/288] time 0.194 (0.195) data 0.000 (0.001) loss 1.4278 (1.5986) teacher_loss 0.7852 (0.9185) loss_zs_kd 0.1349 (0.1555) loss_oracle 0.5752 (0.6023) acc 75.0000 (75.4688) alaph_mean 0.3498 (0.3184) alpha_min 0.0000 (0.0000) alpha_max 0.5044 (0.5308) lr 1.5878e-03 eta 0:31:07
epoch [17/50] batch [220/288] time 0.192 (0.195) data 0.000 (0.001) loss 1.3684 (1.6006) teacher_loss 0.7651 (0.9180) loss_zs_kd 0.0927 (0.1562) loss_oracle 0.5570 (0.6045) acc 81.2500 (75.4545) alaph_mean 0.3601 (0.3175) alpha_min 0.0000 (0.0000) alpha_max 0.5096 (0.5317) lr 1.5878e-03 eta 0:31:02
epoch [17/50] batch [240/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.4287 (1.5982) teacher_loss 0.8857 (0.9158) loss_zs_kd 0.1445 (0.1564) loss_oracle 0.4707 (0.6042) acc 75.0000 (75.4948) alaph_mean 0.3839 (0.3179) alpha_min 0.0000 (0.0000) alpha_max 0.5047 (0.5312) lr 1.5878e-03 eta 0:30:58
epoch [17/50] batch [260/288] time 0.192 (0.195) data 0.000 (0.001) loss 2.0549 (1.6015) teacher_loss 1.2285 (0.9180) loss_zs_kd 0.1708 (0.1575) loss_oracle 0.7410 (0.6047) acc 68.7500 (75.4447) alaph_mean 0.2235 (0.3176) alpha_min -0.0000 (0.0000) alpha_max 0.5057 (0.5318) lr 1.5878e-03 eta 0:30:55
epoch [17/50] batch [280/288] time 0.193 (0.195) data 0.000 (0.001) loss 1.9749 (1.6014) teacher_loss 1.1982 (0.9159) loss_zs_kd 0.2253 (0.1588) loss_oracle 0.6640 (0.6061) acc 68.7500 (75.5580) alaph_mean 0.2940 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5042 (0.5321) lr 1.5878e-03 eta 0:30:50
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,418
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,016
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.4%
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [18/50] batch [20/288] time 0.191 (0.217) data 0.000 (0.018) loss 2.0340 (1.5911) teacher_loss 1.2500 (0.9105) loss_zs_kd 0.1671 (0.1670) loss_oracle 0.7005 (0.5971) acc 68.7500 (75.7812) alaph_mean 0.2669 (0.3270) alpha_min 0.0000 (0.0000) alpha_max 0.5076 (0.5138) lr 1.5358e-03 eta 0:34:21
epoch [18/50] batch [40/288] time 0.195 (0.206) data 0.000 (0.009) loss 2.0505 (1.6402) teacher_loss 1.3359 (0.9432) loss_zs_kd 0.2302 (0.1685) loss_oracle 0.5995 (0.6128) acc 68.7500 (75.2344) alaph_mean 0.3328 (0.3268) alpha_min 0.0000 (0.0122) alpha_max 0.5054 (0.5108) lr 1.5358e-03 eta 0:32:29
epoch [18/50] batch [60/288] time 0.191 (0.202) data 0.000 (0.006) loss 1.4475 (1.6476) teacher_loss 0.7090 (0.9462) loss_zs_kd 0.1682 (0.1634) loss_oracle 0.6544 (0.6197) acc 84.3750 (75.0521) alaph_mean 0.3180 (0.3207) alpha_min 0.0000 (0.0081) alpha_max 0.6654 (0.5149) lr 1.5358e-03 eta 0:31:47
epoch [18/50] batch [80/288] time 0.192 (0.200) data 0.000 (0.005) loss 2.5294 (1.6647) teacher_loss 1.7832 (0.9660) loss_zs_kd 0.1939 (0.1631) loss_oracle 0.6493 (0.6171) acc 59.3750 (74.7266) alaph_mean 0.3198 (0.3235) alpha_min -0.0000 (0.0061) alpha_max 0.6141 (0.5191) lr 1.5358e-03 eta 0:31:26
epoch [18/50] batch [100/288] time 0.189 (0.199) data 0.000 (0.004) loss 1.7559 (1.6560) teacher_loss 0.8779 (0.9550) loss_zs_kd 0.1274 (0.1623) loss_oracle 0.8143 (0.6199) acc 81.2500 (75.0000) alaph_mean 0.2279 (0.3240) alpha_min 0.0000 (0.0049) alpha_max 0.5062 (0.5222) lr 1.5358e-03 eta 0:31:10
epoch [18/50] batch [120/288] time 0.196 (0.198) data 0.000 (0.003) loss 1.5856 (1.6420) teacher_loss 1.0762 (0.9439) loss_zs_kd 0.1714 (0.1611) loss_oracle 0.4238 (0.6175) acc 78.1250 (75.3646) alaph_mean 0.4115 (0.3250) alpha_min 0.0000 (0.0041) alpha_max 0.5044 (0.5238) lr 1.5358e-03 eta 0:30:59
epoch [18/50] batch [140/288] time 0.202 (0.198) data 0.000 (0.003) loss 1.5241 (1.6466) teacher_loss 0.9419 (0.9518) loss_zs_kd 0.1527 (0.1614) loss_oracle 0.5059 (0.6141) acc 75.0000 (74.9777) alaph_mean 0.3796 (0.3259) alpha_min 0.0000 (0.0035) alpha_max 0.5083 (0.5220) lr 1.5358e-03 eta 0:30:50
epoch [18/50] batch [160/288] time 0.195 (0.197) data 0.000 (0.002) loss 1.5872 (1.6382) teacher_loss 0.7896 (0.9438) loss_zs_kd 0.2248 (0.1631) loss_oracle 0.6852 (0.6129) acc 75.0000 (75.0195) alaph_mean 0.2897 (0.3253) alpha_min 0.0000 (0.0031) alpha_max 0.5081 (0.5216) lr 1.5358e-03 eta 0:30:43
epoch [18/50] batch [180/288] time 0.195 (0.197) data 0.000 (0.002) loss 1.5041 (1.6315) teacher_loss 0.7729 (0.9364) loss_zs_kd 0.2312 (0.1656) loss_oracle 0.6156 (0.6124) acc 71.8750 (75.1562) alaph_mean 0.3349 (0.3251) alpha_min 0.0000 (0.0027) alpha_max 0.5058 (0.5229) lr 1.5358e-03 eta 0:30:36
epoch [18/50] batch [200/288] time 0.183 (0.197) data 0.000 (0.002) loss 1.8215 (1.6194) teacher_loss 1.2119 (0.9267) loss_zs_kd 0.1708 (0.1654) loss_oracle 0.5242 (0.6100) acc 65.6250 (75.3125) alaph_mean 0.3604 (0.3250) alpha_min 0.0000 (0.0024) alpha_max 0.5096 (0.5244) lr 1.5358e-03 eta 0:30:30
epoch [18/50] batch [220/288] time 0.199 (0.196) data 0.000 (0.002) loss 2.1052 (1.6217) teacher_loss 1.4268 (0.9302) loss_zs_kd 0.1460 (0.1656) loss_oracle 0.6055 (0.6087) acc 62.5000 (75.1420) alaph_mean 0.3254 (0.3245) alpha_min -0.0000 (0.0022) alpha_max 0.5088 (0.5228) lr 1.5358e-03 eta 0:30:23
epoch [18/50] batch [240/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.3012 (1.6207) teacher_loss 0.7144 (0.9312) loss_zs_kd 0.1558 (0.1658) loss_oracle 0.5089 (0.6066) acc 84.3750 (75.0000) alaph_mean 0.3755 (0.3236) alpha_min 0.0000 (0.0020) alpha_max 0.5059 (0.5215) lr 1.5358e-03 eta 0:30:17
epoch [18/50] batch [260/288] time 0.197 (0.196) data 0.000 (0.002) loss 1.5198 (1.6166) teacher_loss 0.9023 (0.9309) loss_zs_kd 0.0952 (0.1648) loss_oracle 0.5698 (0.6033) acc 78.1250 (75.1923) alaph_mean 0.3443 (0.3236) alpha_min 0.0000 (0.0019) alpha_max 0.5080 (0.5219) lr 1.5358e-03 eta 0:30:12
epoch [18/50] batch [280/288] time 0.195 (0.196) data 0.000 (0.001) loss 1.8808 (1.6188) teacher_loss 1.1572 (0.9349) loss_zs_kd 0.2364 (0.1652) loss_oracle 0.6053 (0.6013) acc 68.7500 (75.1451) alaph_mean 0.3479 (0.3235) alpha_min 0.0000 (0.0017) alpha_max 0.6283 (0.5218) lr 1.5358e-03 eta 0:30:07
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,420
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,029
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.3%
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [19/50] batch [20/288] time 0.200 (0.207) data 0.000 (0.014) loss 1.6718 (1.5779) teacher_loss 0.9707 (0.8752) loss_zs_kd 0.1247 (0.1617) loss_oracle 0.6388 (0.6219) acc 71.8750 (77.3438) alaph_mean 0.2944 (0.3101) alpha_min 0.0000 (0.0000) alpha_max 0.5069 (0.5282) lr 1.4818e-03 eta 0:31:44
epoch [19/50] batch [40/288] time 0.193 (0.201) data 0.000 (0.007) loss 1.8215 (1.6320) teacher_loss 1.0586 (0.9354) loss_zs_kd 0.1654 (0.1628) loss_oracle 0.6802 (0.6152) acc 68.7500 (75.4688) alaph_mean 0.2820 (0.3120) alpha_min 0.0000 (0.0000) alpha_max 0.5057 (0.5347) lr 1.4818e-03 eta 0:30:44
epoch [19/50] batch [60/288] time 0.202 (0.199) data 0.000 (0.005) loss 1.8947 (1.6765) teacher_loss 1.1807 (0.9780) loss_zs_kd 0.2004 (0.1679) loss_oracle 0.6139 (0.6145) acc 68.7500 (74.4792) alaph_mean 0.2991 (0.3114) alpha_min 0.0000 (0.0000) alpha_max 0.5043 (0.5256) lr 1.4818e-03 eta 0:30:19
epoch [19/50] batch [80/288] time 0.198 (0.198) data 0.000 (0.004) loss 1.6019 (1.6246) teacher_loss 0.8755 (0.9364) loss_zs_kd 0.1450 (0.1667) loss_oracle 0.6539 (0.6049) acc 71.8750 (75.3125) alaph_mean 0.2615 (0.3126) alpha_min -0.0000 (0.0000) alpha_max 0.5109 (0.5265) lr 1.4818e-03 eta 0:30:13
epoch [19/50] batch [100/288] time 0.187 (0.198) data 0.000 (0.003) loss 1.6862 (1.6220) teacher_loss 1.1084 (0.9391) loss_zs_kd 0.1652 (0.1652) loss_oracle 0.4952 (0.6002) acc 71.8750 (75.3125) alaph_mean 0.4062 (0.3146) alpha_min 0.0000 (0.0000) alpha_max 0.5085 (0.5233) lr 1.4818e-03 eta 0:30:01
epoch [19/50] batch [120/288] time 0.198 (0.197) data 0.000 (0.002) loss 1.9301 (1.6198) teacher_loss 1.3613 (0.9381) loss_zs_kd 0.1507 (0.1639) loss_oracle 0.4934 (0.5998) acc 68.7500 (75.3906) alaph_mean 0.3592 (0.3155) alpha_min -0.0000 (0.0000) alpha_max 0.5100 (0.5215) lr 1.4818e-03 eta 0:29:52
epoch [19/50] batch [140/288] time 0.195 (0.197) data 0.000 (0.002) loss 1.8404 (1.6161) teacher_loss 1.2871 (0.9401) loss_zs_kd 0.1230 (0.1627) loss_oracle 0.4918 (0.5946) acc 78.1250 (75.2902) alaph_mean 0.3464 (0.3150) alpha_min -0.0000 (0.0000) alpha_max 0.5781 (0.5217) lr 1.4818e-03 eta 0:29:45
epoch [19/50] batch [160/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.6213 (1.6182) teacher_loss 0.9526 (0.9441) loss_zs_kd 0.1567 (0.1595) loss_oracle 0.5903 (0.5944) acc 78.1250 (75.0000) alaph_mean 0.3075 (0.3134) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5233) lr 1.4818e-03 eta 0:29:38
epoch [19/50] batch [180/288] time 0.191 (0.196) data 0.000 (0.002) loss 1.7317 (1.6195) teacher_loss 1.0195 (0.9465) loss_zs_kd 0.1499 (0.1588) loss_oracle 0.6373 (0.5936) acc 75.0000 (74.8090) alaph_mean 0.2907 (0.3133) alpha_min 0.0000 (0.0000) alpha_max 0.5043 (0.5214) lr 1.4818e-03 eta 0:29:32
epoch [19/50] batch [200/288] time 0.187 (0.196) data 0.000 (0.002) loss 1.1940 (1.6115) teacher_loss 0.6177 (0.9391) loss_zs_kd 0.1818 (0.1592) loss_oracle 0.4854 (0.5928) acc 84.3750 (74.8906) alaph_mean 0.3590 (0.3136) alpha_min 0.0000 (0.0000) alpha_max 0.5113 (0.5221) lr 1.4818e-03 eta 0:29:25
epoch [19/50] batch [220/288] time 0.199 (0.196) data 0.000 (0.001) loss 1.4547 (1.6178) teacher_loss 0.8726 (0.9466) loss_zs_kd 0.1914 (0.1620) loss_oracle 0.4864 (0.5902) acc 78.1250 (74.8153) alaph_mean 0.4090 (0.3151) alpha_min 0.0000 (0.0000) alpha_max 0.5065 (0.5215) lr 1.4818e-03 eta 0:29:20
epoch [19/50] batch [240/288] time 0.188 (0.196) data 0.000 (0.001) loss 1.2968 (1.6165) teacher_loss 0.6792 (0.9457) loss_zs_kd 0.2133 (0.1625) loss_oracle 0.5109 (0.5896) acc 84.3750 (74.7917) alaph_mean 0.3017 (0.3143) alpha_min 0.0000 (0.0000) alpha_max 0.5545 (0.5206) lr 1.4818e-03 eta 0:29:15
epoch [19/50] batch [260/288] time 0.195 (0.195) data 0.000 (0.001) loss 1.7278 (1.6110) teacher_loss 1.0449 (0.9420) loss_zs_kd 0.1672 (0.1624) loss_oracle 0.5993 (0.5878) acc 75.0000 (74.8798) alaph_mean 0.3204 (0.3146) alpha_min 0.0000 (0.0000) alpha_max 0.5103 (0.5214) lr 1.4818e-03 eta 0:29:10
epoch [19/50] batch [280/288] time 0.184 (0.195) data 0.000 (0.001) loss 2.2963 (1.6108) teacher_loss 1.5039 (0.9429) loss_zs_kd 0.2084 (0.1625) loss_oracle 0.6882 (0.5867) acc 62.5000 (74.9665) alaph_mean 0.2658 (0.3149) alpha_min 0.0000 (0.0000) alpha_max 0.5069 (0.5216) lr 1.4818e-03 eta 0:29:05
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,427
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.7%
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     83.6%, epoch: 6 *******
epoch [20/50] batch [20/288] time 0.193 (0.207) data 0.000 (0.013) loss 2.1217 (1.5743) teacher_loss 1.3760 (0.9094) loss_zs_kd 0.1586 (0.1630) loss_oracle 0.6664 (0.5835) acc 53.1250 (74.5312) alaph_mean 0.2498 (0.3186) alpha_min 0.0000 (0.0000) alpha_max 0.6172 (0.5133) lr 1.4258e-03 eta 0:30:40
epoch [20/50] batch [40/288] time 0.178 (0.200) data 0.000 (0.007) loss 1.6948 (1.5848) teacher_loss 0.9351 (0.9140) loss_zs_kd 0.2451 (0.1695) loss_oracle 0.6372 (0.5861) acc 78.1250 (74.7656) alaph_mean 0.3442 (0.3232) alpha_min -0.0000 (0.0000) alpha_max 0.5083 (0.5118) lr 1.4258e-03 eta 0:29:42
epoch [20/50] batch [60/288] time 0.179 (0.199) data 0.000 (0.005) loss 1.3746 (1.5935) teacher_loss 0.8062 (0.9057) loss_zs_kd 0.1058 (0.1687) loss_oracle 0.5156 (0.6035) acc 81.2500 (75.0521) alaph_mean 0.3555 (0.3148) alpha_min 0.0000 (0.0000) alpha_max 0.5100 (0.5100) lr 1.4258e-03 eta 0:29:21
epoch [20/50] batch [80/288] time 0.218 (0.199) data 0.000 (0.004) loss 1.4253 (1.5912) teacher_loss 0.7129 (0.9070) loss_zs_kd 0.1529 (0.1688) loss_oracle 0.6360 (0.5998) acc 84.3750 (75.1172) alaph_mean 0.3026 (0.3175) alpha_min 0.0000 (0.0000) alpha_max 0.5047 (0.5100) lr 1.4258e-03 eta 0:29:18
epoch [20/50] batch [100/288] time 0.204 (0.198) data 0.000 (0.003) loss 1.6638 (1.5835) teacher_loss 1.0996 (0.9030) loss_zs_kd 0.1599 (0.1677) loss_oracle 0.4842 (0.5966) acc 71.8750 (75.2188) alaph_mean 0.3937 (0.3163) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5093) lr 1.4258e-03 eta 0:29:08
epoch [20/50] batch [120/288] time 0.188 (0.197) data 0.000 (0.002) loss 1.1530 (1.5972) teacher_loss 0.5811 (0.9202) loss_zs_kd 0.0907 (0.1672) loss_oracle 0.5265 (0.5934) acc 87.5000 (74.9740) alaph_mean 0.3308 (0.3154) alpha_min 0.0000 (0.0000) alpha_max 0.6218 (0.5113) lr 1.4258e-03 eta 0:28:56
epoch [20/50] batch [140/288] time 0.191 (0.197) data 0.000 (0.002) loss 1.4016 (1.6029) teacher_loss 0.7046 (0.9285) loss_zs_kd 0.1935 (0.1676) loss_oracle 0.6002 (0.5906) acc 81.2500 (74.7098) alaph_mean 0.2985 (0.3150) alpha_min 0.0000 (0.0000) alpha_max 0.5046 (0.5109) lr 1.4258e-03 eta 0:28:48
epoch [20/50] batch [160/288] time 0.193 (0.196) data 0.000 (0.002) loss 1.5298 (1.5951) teacher_loss 0.9746 (0.9248) loss_zs_kd 0.1235 (0.1667) loss_oracle 0.4934 (0.5870) acc 71.8750 (75.0195) alaph_mean 0.3551 (0.3168) alpha_min 0.0000 (0.0000) alpha_max 0.5048 (0.5134) lr 1.4258e-03 eta 0:28:41
epoch [20/50] batch [180/288] time 0.192 (0.196) data 0.001 (0.002) loss 1.2782 (1.5861) teacher_loss 0.6987 (0.9173) loss_zs_kd 0.1185 (0.1657) loss_oracle 0.5202 (0.5859) acc 81.2500 (75.1736) alaph_mean 0.3725 (0.3174) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5127) lr 1.4258e-03 eta 0:28:35
epoch [20/50] batch [200/288] time 0.190 (0.196) data 0.000 (0.002) loss 2.0394 (1.5880) teacher_loss 1.3682 (0.9198) loss_zs_kd 0.2836 (0.1662) loss_oracle 0.5295 (0.5852) acc 65.6250 (75.1250) alaph_mean 0.3464 (0.3198) alpha_min -0.0000 (0.0000) alpha_max 0.5106 (0.5124) lr 1.4258e-03 eta 0:28:30
epoch [20/50] batch [220/288] time 0.194 (0.196) data 0.000 (0.001) loss 1.4264 (1.5802) teacher_loss 0.8071 (0.9092) loss_zs_kd 0.1448 (0.1660) loss_oracle 0.5469 (0.5879) acc 78.1250 (75.2131) alaph_mean 0.3656 (0.3202) alpha_min 0.0000 (0.0000) alpha_max 0.5079 (0.5180) lr 1.4258e-03 eta 0:28:24
epoch [20/50] batch [240/288] time 0.193 (0.196) data 0.000 (0.001) loss 1.8411 (1.5804) teacher_loss 1.0469 (0.9071) loss_zs_kd 0.1459 (0.1666) loss_oracle 0.7212 (0.5901) acc 68.7500 (75.1953) alaph_mean 0.2827 (0.3197) alpha_min -0.0000 (0.0000) alpha_max 0.5146 (0.5199) lr 1.4258e-03 eta 0:28:20
epoch [20/50] batch [260/288] time 0.195 (0.196) data 0.000 (0.001) loss 1.9513 (1.5751) teacher_loss 1.2461 (0.9029) loss_zs_kd 0.1317 (0.1665) loss_oracle 0.6394 (0.5889) acc 68.7500 (75.2885) alaph_mean 0.3286 (0.3207) alpha_min 0.0000 (0.0000) alpha_max 0.5086 (0.5202) lr 1.4258e-03 eta 0:28:14
epoch [20/50] batch [280/288] time 0.201 (0.195) data 0.000 (0.001) loss 1.2799 (1.5728) teacher_loss 0.7134 (0.9008) loss_zs_kd 0.1331 (0.1658) loss_oracle 0.4999 (0.5891) acc 84.3750 (75.3348) alaph_mean 0.3689 (0.3207) alpha_min 0.0000 (0.0000) alpha_max 0.8158 (0.5219) lr 1.4258e-03 eta 0:28:10
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,418
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,036
* accuracy: 83.9%
* error: 16.1%
* macro_f1: 80.1%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     83.9%, epoch: 20 *******
epoch [21/50] batch [20/288] time 0.187 (0.207) data 0.000 (0.015) loss 1.1635 (1.5492) teacher_loss 0.4612 (0.8576) loss_zs_kd 0.1637 (0.1740) loss_oracle 0.6204 (0.6046) acc 87.5000 (77.5000) alaph_mean 0.2831 (0.3135) alpha_min 0.0000 (0.0000) alpha_max 0.5026 (0.5089) lr 1.3681e-03 eta 0:29:47
epoch [21/50] batch [40/288] time 0.195 (0.200) data 0.000 (0.008) loss 1.2500 (1.5522) teacher_loss 0.6216 (0.8675) loss_zs_kd 0.1458 (0.1684) loss_oracle 0.5556 (0.6006) acc 81.2500 (76.0156) alaph_mean 0.3245 (0.3117) alpha_min 0.0000 (0.0000) alpha_max 0.5068 (0.5181) lr 1.3681e-03 eta 0:28:42
epoch [21/50] batch [60/288] time 0.191 (0.199) data 0.001 (0.005) loss 1.4234 (1.5616) teacher_loss 0.7417 (0.8885) loss_zs_kd 0.1837 (0.1662) loss_oracle 0.5898 (0.5901) acc 75.0000 (75.4688) alaph_mean 0.2864 (0.3115) alpha_min 0.0000 (0.0000) alpha_max 0.5049 (0.5144) lr 1.3681e-03 eta 0:28:30
epoch [21/50] batch [80/288] time 0.184 (0.198) data 0.000 (0.004) loss 1.2679 (1.5798) teacher_loss 0.5332 (0.9115) loss_zs_kd 0.1187 (0.1628) loss_oracle 0.6754 (0.5869) acc 84.3750 (75.1562) alaph_mean 0.2359 (0.3114) alpha_min 0.0000 (0.0000) alpha_max 0.5062 (0.5135) lr 1.3681e-03 eta 0:28:15
epoch [21/50] batch [100/288] time 0.207 (0.197) data 0.000 (0.003) loss 1.4079 (1.5620) teacher_loss 0.9150 (0.8963) loss_zs_kd 0.1017 (0.1616) loss_oracle 0.4420 (0.5848) acc 75.0000 (75.4688) alaph_mean 0.3367 (0.3115) alpha_min 0.0000 (0.0000) alpha_max 0.5062 (0.5135) lr 1.3681e-03 eta 0:28:05
epoch [21/50] batch [120/288] time 0.188 (0.196) data 0.000 (0.003) loss 1.4840 (1.5682) teacher_loss 0.7070 (0.9021) loss_zs_kd 0.1133 (0.1629) loss_oracle 0.7203 (0.5846) acc 84.3750 (75.5990) alaph_mean 0.2018 (0.3090) alpha_min 0.0000 (0.0000) alpha_max 0.5027 (0.5131) lr 1.3681e-03 eta 0:27:52
epoch [21/50] batch [140/288] time 0.197 (0.196) data 0.000 (0.002) loss 1.5705 (1.5752) teacher_loss 0.8398 (0.9125) loss_zs_kd 0.1801 (0.1618) loss_oracle 0.6406 (0.5818) acc 81.2500 (75.4018) alaph_mean 0.2868 (0.3095) alpha_min -0.0000 (0.0000) alpha_max 0.5076 (0.5133) lr 1.3681e-03 eta 0:27:45
epoch [21/50] batch [160/288] time 0.205 (0.196) data 0.001 (0.002) loss 1.5393 (1.5919) teacher_loss 0.9258 (0.9275) loss_zs_kd 0.1499 (0.1641) loss_oracle 0.5386 (0.5823) acc 71.8750 (74.9023) alaph_mean 0.3432 (0.3102) alpha_min 0.0000 (0.0000) alpha_max 0.5066 (0.5133) lr 1.3681e-03 eta 0:27:40
epoch [21/50] batch [180/288] time 0.192 (0.196) data 0.000 (0.002) loss 1.9488 (1.5976) teacher_loss 1.1768 (0.9322) loss_zs_kd 0.1352 (0.1648) loss_oracle 0.7045 (0.5831) acc 71.8750 (74.7049) alaph_mean 0.2436 (0.3096) alpha_min 0.0000 (0.0000) alpha_max 0.5045 (0.5139) lr 1.3681e-03 eta 0:27:34
epoch [21/50] batch [200/288] time 0.187 (0.195) data 0.000 (0.002) loss 1.9520 (1.5944) teacher_loss 1.2363 (0.9264) loss_zs_kd 0.1420 (0.1651) loss_oracle 0.6447 (0.5854) acc 65.6250 (74.8906) alaph_mean 0.2424 (0.3096) alpha_min 0.0000 (0.0000) alpha_max 0.5042 (0.5157) lr 1.3681e-03 eta 0:27:29
epoch [21/50] batch [220/288] time 0.169 (0.195) data 0.000 (0.002) loss 1.3288 (1.5953) teacher_loss 0.7686 (0.9241) loss_zs_kd 0.1410 (0.1652) loss_oracle 0.4898 (0.5886) acc 78.1250 (74.9148) alaph_mean 0.3249 (0.3089) alpha_min 0.0000 (0.0000) alpha_max 0.5081 (0.5167) lr 1.3681e-03 eta 0:27:24
epoch [21/50] batch [240/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.6278 (1.6059) teacher_loss 0.9409 (0.9361) loss_zs_kd 0.2147 (0.1650) loss_oracle 0.5796 (0.5873) acc 84.3750 (74.7135) alaph_mean 0.3127 (0.3098) alpha_min 0.0000 (0.0000) alpha_max 0.5127 (0.5170) lr 1.3681e-03 eta 0:27:18
epoch [21/50] batch [260/288] time 0.194 (0.195) data 0.000 (0.001) loss 1.5138 (1.6014) teacher_loss 0.9136 (0.9333) loss_zs_kd 0.1493 (0.1651) loss_oracle 0.5256 (0.5856) acc 75.0000 (74.7476) alaph_mean 0.3252 (0.3104) alpha_min -0.0000 (0.0000) alpha_max 0.5088 (0.5168) lr 1.3681e-03 eta 0:27:14
epoch [21/50] batch [280/288] time 0.195 (0.195) data 0.000 (0.001) loss 1.5185 (1.6021) teacher_loss 0.8306 (0.9320) loss_zs_kd 0.1198 (0.1648) loss_oracle 0.6280 (0.5876) acc 78.1250 (74.7321) alaph_mean 0.3092 (0.3098) alpha_min -0.0000 (0.0000) alpha_max 0.5117 (0.5176) lr 1.3681e-03 eta 0:27:09
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,417
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,032
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 80.2%
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     83.9%, epoch: 20 *******
epoch [22/50] batch [20/288] time 0.189 (0.209) data 0.000 (0.013) loss 2.2579 (1.6132) teacher_loss 1.4189 (0.9074) loss_zs_kd 0.1772 (0.1716) loss_oracle 0.7504 (0.6200) acc 56.2500 (74.2188) alaph_mean 0.2375 (0.3057) alpha_min 0.0000 (0.0000) alpha_max 0.5106 (0.5285) lr 1.3090e-03 eta 0:29:01
epoch [22/50] batch [40/288] time 0.198 (0.203) data 0.000 (0.007) loss 1.3932 (1.5828) teacher_loss 0.8120 (0.8975) loss_zs_kd 0.1364 (0.1637) loss_oracle 0.5130 (0.6035) acc 84.3750 (74.4531) alaph_mean 0.3986 (0.3200) alpha_min 0.0000 (0.0000) alpha_max 0.5105 (0.5307) lr 1.3090e-03 eta 0:28:04
epoch [22/50] batch [60/288] time 0.192 (0.200) data 0.001 (0.005) loss 1.4942 (1.6044) teacher_loss 0.6709 (0.9163) loss_zs_kd 0.1942 (0.1654) loss_oracle 0.7262 (0.6054) acc 75.0000 (73.9583) alaph_mean 0.2863 (0.3252) alpha_min 0.0000 (0.0000) alpha_max 0.5132 (0.5374) lr 1.3090e-03 eta 0:27:37
epoch [22/50] batch [80/288] time 0.189 (0.198) data 0.000 (0.004) loss 2.0158 (1.5965) teacher_loss 1.2754 (0.9050) loss_zs_kd 0.1750 (0.1668) loss_oracle 0.6529 (0.6082) acc 62.5000 (74.3359) alaph_mean 0.2581 (0.3233) alpha_min -0.0000 (0.0000) alpha_max 0.5385 (0.5304) lr 1.3090e-03 eta 0:27:20
epoch [22/50] batch [100/288] time 0.174 (0.197) data 0.000 (0.003) loss 1.1754 (1.5951) teacher_loss 0.6484 (0.9022) loss_zs_kd 0.0932 (0.1632) loss_oracle 0.4804 (0.6113) acc 78.1250 (74.4062) alaph_mean 0.3607 (0.3195) alpha_min -0.0000 (0.0000) alpha_max 0.5394 (0.5319) lr 1.3090e-03 eta 0:27:08
epoch [22/50] batch [120/288] time 0.193 (0.197) data 0.000 (0.002) loss 1.5602 (1.5990) teacher_loss 0.8950 (0.9095) loss_zs_kd 0.1772 (0.1656) loss_oracle 0.5766 (0.6066) acc 81.2500 (74.5573) alaph_mean 0.3143 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.5067 (0.5295) lr 1.3090e-03 eta 0:26:59
epoch [22/50] batch [140/288] time 0.193 (0.196) data 0.000 (0.002) loss 1.5639 (1.5866) teacher_loss 0.8989 (0.9048) loss_zs_kd 0.1738 (0.1627) loss_oracle 0.5781 (0.6005) acc 81.2500 (75.0000) alaph_mean 0.3164 (0.3206) alpha_min 0.0000 (0.0000) alpha_max 0.5050 (0.5289) lr 1.3090e-03 eta 0:26:52
epoch [22/50] batch [160/288] time 0.196 (0.196) data 0.000 (0.002) loss 1.0952 (1.5976) teacher_loss 0.5576 (0.9165) loss_zs_kd 0.1096 (0.1628) loss_oracle 0.4828 (0.5996) acc 87.5000 (74.9219) alaph_mean 0.3816 (0.3191) alpha_min 0.0000 (0.0000) alpha_max 0.5067 (0.5289) lr 1.3090e-03 eta 0:26:46
epoch [22/50] batch [180/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.5741 (1.5913) teacher_loss 0.9053 (0.9127) loss_zs_kd 0.0976 (0.1625) loss_oracle 0.6200 (0.5974) acc 81.2500 (75.1389) alaph_mean 0.3128 (0.3192) alpha_min -0.0000 (0.0000) alpha_max 0.5068 (0.5281) lr 1.3090e-03 eta 0:26:40
epoch [22/50] batch [200/288] time 0.177 (0.196) data 0.000 (0.002) loss 1.5615 (1.5952) teacher_loss 0.8247 (0.9176) loss_zs_kd 0.1535 (0.1624) loss_oracle 0.6601 (0.5964) acc 75.0000 (75.0938) alaph_mean 0.2817 (0.3187) alpha_min 0.0000 (0.0000) alpha_max 0.5076 (0.5276) lr 1.3090e-03 eta 0:26:35
epoch [22/50] batch [220/288] time 0.190 (0.196) data 0.000 (0.001) loss 1.0255 (1.5952) teacher_loss 0.4807 (0.9163) loss_zs_kd 0.1739 (0.1641) loss_oracle 0.4578 (0.5969) acc 90.6250 (75.0000) alaph_mean 0.3824 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.5125 (0.5324) lr 1.3090e-03 eta 0:26:30
epoch [22/50] batch [240/288] time 0.196 (0.196) data 0.000 (0.001) loss 1.6074 (1.5979) teacher_loss 0.9780 (0.9190) loss_zs_kd 0.1758 (0.1646) loss_oracle 0.5415 (0.5967) acc 75.0000 (75.0000) alaph_mean 0.3662 (0.3193) alpha_min 0.0000 (0.0000) alpha_max 0.7677 (0.5314) lr 1.3090e-03 eta 0:26:26
epoch [22/50] batch [260/288] time 0.193 (0.196) data 0.000 (0.001) loss 1.5588 (1.5966) teacher_loss 0.8198 (0.9188) loss_zs_kd 0.1754 (0.1639) loss_oracle 0.6513 (0.5958) acc 81.2500 (75.0120) alaph_mean 0.2797 (0.3197) alpha_min 0.0000 (0.0000) alpha_max 0.5073 (0.5303) lr 1.3090e-03 eta 0:26:22
epoch [22/50] batch [280/288] time 0.201 (0.195) data 0.000 (0.001) loss 1.1117 (1.6019) teacher_loss 0.5640 (0.9220) loss_zs_kd 0.1570 (0.1641) loss_oracle 0.4693 (0.5978) acc 87.5000 (74.8549) alaph_mean 0.3794 (0.3181) alpha_min 0.0000 (0.0000) alpha_max 0.5059 (0.5295) lr 1.3090e-03 eta 0:26:16
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,420
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,028
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 79.8%
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     83.9%, epoch: 20 *******
epoch [23/50] batch [20/288] time 0.201 (0.211) data 0.000 (0.012) loss 1.6800 (1.5685) teacher_loss 1.0977 (0.8814) loss_zs_kd 0.1431 (0.1577) loss_oracle 0.5107 (0.6083) acc 68.7500 (76.8750) alaph_mean 0.3498 (0.3226) alpha_min 0.0000 (0.0000) alpha_max 0.5058 (0.5364) lr 1.2487e-03 eta 0:28:16
epoch [23/50] batch [40/288] time 0.221 (0.204) data 0.000 (0.006) loss 1.4299 (1.5694) teacher_loss 0.8794 (0.8830) loss_zs_kd 0.1361 (0.1602) loss_oracle 0.4824 (0.6063) acc 75.0000 (76.3281) alaph_mean 0.3761 (0.3231) alpha_min 0.0000 (0.0000) alpha_max 0.5034 (0.5380) lr 1.2487e-03 eta 0:27:14
epoch [23/50] batch [60/288] time 0.193 (0.201) data 0.000 (0.004) loss 1.9650 (1.5967) teacher_loss 1.2432 (0.9118) loss_zs_kd 0.2018 (0.1650) loss_oracle 0.6209 (0.6024) acc 62.5000 (75.6250) alaph_mean 0.2822 (0.3233) alpha_min -0.0000 (0.0000) alpha_max 0.5036 (0.5337) lr 1.2487e-03 eta 0:26:52
epoch [23/50] batch [80/288] time 0.197 (0.200) data 0.000 (0.003) loss 1.8038 (1.5916) teacher_loss 1.0654 (0.9054) loss_zs_kd 0.1982 (0.1679) loss_oracle 0.6393 (0.6023) acc 68.7500 (75.7031) alaph_mean 0.3388 (0.3220) alpha_min -0.0000 (0.0000) alpha_max 0.5081 (0.5312) lr 1.2487e-03 eta 0:26:34
epoch [23/50] batch [100/288] time 0.195 (0.198) data 0.000 (0.003) loss 1.4499 (1.5851) teacher_loss 0.7231 (0.8975) loss_zs_kd 0.1847 (0.1686) loss_oracle 0.6344 (0.6033) acc 81.2500 (75.8750) alaph_mean 0.3294 (0.3216) alpha_min 0.0000 (0.0000) alpha_max 0.5257 (0.5293) lr 1.2487e-03 eta 0:26:20
epoch [23/50] batch [120/288] time 0.200 (0.198) data 0.000 (0.002) loss 1.1961 (1.5921) teacher_loss 0.5796 (0.9015) loss_zs_kd 0.1931 (0.1703) loss_oracle 0.5200 (0.6054) acc 84.3750 (75.5990) alaph_mean 0.3679 (0.3209) alpha_min 0.0000 (0.0000) alpha_max 0.5095 (0.5276) lr 1.2487e-03 eta 0:26:10
epoch [23/50] batch [140/288] time 0.194 (0.197) data 0.000 (0.002) loss 1.2882 (1.5857) teacher_loss 0.8755 (0.8974) loss_zs_kd 0.1291 (0.1683) loss_oracle 0.3481 (0.6042) acc 84.3750 (75.6696) alaph_mean 0.4692 (0.3227) alpha_min 0.0000 (0.0000) alpha_max 0.5095 (0.5279) lr 1.2487e-03 eta 0:26:02
epoch [23/50] batch [160/288] time 0.193 (0.197) data 0.000 (0.002) loss 1.7342 (1.5949) teacher_loss 0.9341 (0.9086) loss_zs_kd 0.1432 (0.1676) loss_oracle 0.7286 (0.6025) acc 78.1250 (75.4688) alaph_mean 0.2435 (0.3239) alpha_min 0.0000 (0.0000) alpha_max 0.5087 (0.5264) lr 1.2487e-03 eta 0:25:54
epoch [23/50] batch [180/288] time 0.191 (0.196) data 0.000 (0.002) loss 1.4011 (1.5913) teacher_loss 0.7871 (0.9044) loss_zs_kd 0.1019 (0.1664) loss_oracle 0.5631 (0.6037) acc 84.3750 (75.6944) alaph_mean 0.3561 (0.3232) alpha_min 0.0000 (0.0000) alpha_max 0.5156 (0.5244) lr 1.2487e-03 eta 0:25:48
epoch [23/50] batch [200/288] time 0.199 (0.196) data 0.000 (0.001) loss 1.4440 (1.5963) teacher_loss 0.7539 (0.9054) loss_zs_kd 0.1763 (0.1663) loss_oracle 0.6019 (0.6078) acc 81.2500 (75.9688) alaph_mean 0.3538 (0.3225) alpha_min 0.0000 (0.0000) alpha_max 0.5784 (0.5240) lr 1.2487e-03 eta 0:25:42
epoch [23/50] batch [220/288] time 0.195 (0.196) data 0.000 (0.001) loss 1.8255 (1.6041) teacher_loss 1.1201 (0.9100) loss_zs_kd 0.1471 (0.1664) loss_oracle 0.6319 (0.6109) acc 71.8750 (75.7528) alaph_mean 0.3312 (0.3206) alpha_min -0.0000 (0.0000) alpha_max 0.5396 (0.5233) lr 1.2487e-03 eta 0:25:36
epoch [23/50] batch [240/288] time 0.191 (0.196) data 0.000 (0.001) loss 1.4920 (1.6090) teacher_loss 0.8525 (0.9179) loss_zs_kd 0.1868 (0.1649) loss_oracle 0.5460 (0.6087) acc 71.8750 (75.6250) alaph_mean 0.3527 (0.3211) alpha_min 0.0000 (0.0000) alpha_max 0.5050 (0.5231) lr 1.2487e-03 eta 0:25:31
epoch [23/50] batch [260/288] time 0.192 (0.196) data 0.000 (0.001) loss 1.3791 (1.6208) teacher_loss 0.6045 (0.9256) loss_zs_kd 0.1579 (0.1655) loss_oracle 0.6957 (0.6124) acc 81.2500 (75.3846) alaph_mean 0.2821 (0.3192) alpha_min -0.0000 (0.0000) alpha_max 0.5092 (0.5226) lr 1.2487e-03 eta 0:25:27
epoch [23/50] batch [280/288] time 0.167 (0.196) data 0.000 (0.001) loss 1.5949 (1.6136) teacher_loss 0.8418 (0.9195) loss_zs_kd 0.1897 (0.1651) loss_oracle 0.6582 (0.6116) acc 78.1250 (75.5580) alaph_mean 0.2882 (0.3195) alpha_min -0.0000 (0.0000) alpha_max 0.7153 (0.5237) lr 1.2487e-03 eta 0:25:21
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,425
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,038
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 80.4%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      87.0%, epoch: 11 *******
******* Domain a best val test acc: 83.4%, epoch: 11 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [24/50] batch [20/288] time 0.193 (0.207) data 0.000 (0.014) loss 1.5961 (1.6210) teacher_loss 1.0283 (0.9032) loss_zs_kd 0.1382 (0.1527) loss_oracle 0.4986 (0.6414) acc 78.1250 (76.7188) alaph_mean 0.3819 (0.2976) alpha_min 0.0000 (0.0000) alpha_max 0.5065 (0.5221) lr 1.1874e-03 eta 0:26:45
epoch [24/50] batch [40/288] time 0.199 (0.201) data 0.000 (0.007) loss 1.4995 (1.6345) teacher_loss 0.9556 (0.9306) loss_zs_kd 0.2060 (0.1571) loss_oracle 0.4410 (0.6254) acc 71.8750 (75.6250) alaph_mean 0.3938 (0.3061) alpha_min 0.0000 (0.0000) alpha_max 0.5151 (0.5289) lr 1.1874e-03 eta 0:25:52
epoch [24/50] batch [60/288] time 0.194 (0.199) data 0.000 (0.005) loss 1.6223 (1.6347) teacher_loss 0.8779 (0.9372) loss_zs_kd 0.1550 (0.1646) loss_oracle 0.6668 (0.6152) acc 78.1250 (75.3646) alaph_mean 0.2660 (0.3128) alpha_min 0.0000 (0.0000) alpha_max 0.5109 (0.5336) lr 1.1874e-03 eta 0:25:31
epoch [24/50] batch [80/288] time 0.185 (0.199) data 0.000 (0.004) loss 1.4682 (1.6184) teacher_loss 0.8252 (0.9150) loss_zs_kd 0.1977 (0.1646) loss_oracle 0.5441 (0.6211) acc 78.1250 (75.7422) alaph_mean 0.3667 (0.3126) alpha_min -0.0000 (0.0000) alpha_max 0.5103 (0.5307) lr 1.1874e-03 eta 0:25:30
epoch [24/50] batch [100/288] time 0.186 (0.198) data 0.000 (0.003) loss 1.4969 (1.6200) teacher_loss 0.8452 (0.9108) loss_zs_kd 0.2140 (0.1636) loss_oracle 0.5447 (0.6274) acc 81.2500 (75.7812) alaph_mean 0.3820 (0.3118) alpha_min 0.0000 (0.0000) alpha_max 0.5850 (0.5269) lr 1.1874e-03 eta 0:25:18
epoch [24/50] batch [120/288] time 0.194 (0.197) data 0.000 (0.002) loss 1.5375 (1.6131) teacher_loss 0.8516 (0.9011) loss_zs_kd 0.1414 (0.1646) loss_oracle 0.6152 (0.6297) acc 75.0000 (75.9375) alaph_mean 0.3488 (0.3137) alpha_min 0.0000 (0.0000) alpha_max 0.6429 (0.5265) lr 1.1874e-03 eta 0:25:10
epoch [24/50] batch [140/288] time 0.195 (0.197) data 0.000 (0.002) loss 1.7865 (1.6106) teacher_loss 1.0068 (0.8943) loss_zs_kd 0.1498 (0.1668) loss_oracle 0.7047 (0.6329) acc 78.1250 (76.1384) alaph_mean 0.2957 (0.3154) alpha_min -0.0000 (0.0000) alpha_max 0.5060 (0.5275) lr 1.1874e-03 eta 0:25:03
epoch [24/50] batch [160/288] time 0.194 (0.197) data 0.000 (0.002) loss 1.7749 (1.6161) teacher_loss 1.1299 (0.8997) loss_zs_kd 0.1552 (0.1712) loss_oracle 0.5674 (0.6308) acc 62.5000 (75.9375) alaph_mean 0.3526 (0.3175) alpha_min 0.0000 (0.0000) alpha_max 0.7630 (0.5293) lr 1.1874e-03 eta 0:24:57
epoch [24/50] batch [180/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.6042 (1.6261) teacher_loss 0.8237 (0.9112) loss_zs_kd 0.1325 (0.1700) loss_oracle 0.7142 (0.6299) acc 75.0000 (75.5035) alaph_mean 0.2360 (0.3169) alpha_min -0.0000 (0.0000) alpha_max 0.5040 (0.5272) lr 1.1874e-03 eta 0:24:51
epoch [24/50] batch [200/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.7028 (1.6311) teacher_loss 0.8906 (0.9150) loss_zs_kd 0.1529 (0.1690) loss_oracle 0.7358 (0.6316) acc 75.0000 (75.3281) alaph_mean 0.2390 (0.3147) alpha_min 0.0000 (0.0000) alpha_max 0.5046 (0.5268) lr 1.1874e-03 eta 0:24:45
epoch [24/50] batch [220/288] time 0.193 (0.196) data 0.000 (0.001) loss 1.4530 (1.6258) teacher_loss 0.7739 (0.9162) loss_zs_kd 0.1957 (0.1683) loss_oracle 0.5813 (0.6254) acc 78.1250 (75.3977) alaph_mean 0.3345 (0.3168) alpha_min 0.0000 (0.0000) alpha_max 0.5081 (0.5251) lr 1.1874e-03 eta 0:24:39
epoch [24/50] batch [240/288] time 0.193 (0.196) data 0.000 (0.001) loss 1.5345 (1.6234) teacher_loss 0.7432 (0.9162) loss_zs_kd 0.1695 (0.1676) loss_oracle 0.7066 (0.6233) acc 84.3750 (75.3646) alaph_mean 0.2659 (0.3171) alpha_min -0.0000 (0.0000) alpha_max 0.6055 (0.5245) lr 1.1874e-03 eta 0:24:34
epoch [24/50] batch [260/288] time 0.193 (0.196) data 0.000 (0.001) loss 1.1517 (1.6247) teacher_loss 0.5161 (0.9193) loss_zs_kd 0.1719 (0.1673) loss_oracle 0.5496 (0.6218) acc 87.5000 (75.1803) alaph_mean 0.3286 (0.3171) alpha_min -0.0000 (0.0000) alpha_max 0.5076 (0.5238) lr 1.1874e-03 eta 0:24:29
epoch [24/50] batch [280/288] time 0.082 (0.195) data 0.000 (0.001) loss 1.3839 (1.6254) teacher_loss 0.8818 (0.9218) loss_zs_kd 0.1608 (0.1678) loss_oracle 0.4216 (0.6197) acc 75.0000 (75.0893) alaph_mean 0.4066 (0.3176) alpha_min -0.0000 (0.0000) alpha_max 0.5049 (0.5227) lr 1.1874e-03 eta 0:24:21
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,430
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.5%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,029
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 79.9%
******* Domain a best val acc:      87.1%, epoch: 24 *******
******* Domain a best val test acc: 83.6%, epoch: 24 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [25/50] batch [20/288] time 0.190 (0.213) data 0.000 (0.013) loss 1.9609 (1.6082) teacher_loss 1.2373 (0.9288) loss_zs_kd 0.1654 (0.1711) loss_oracle 0.6408 (0.5939) acc 65.6250 (75.7812) alaph_mean 0.3283 (0.3342) alpha_min 0.0000 (0.0000) alpha_max 0.5095 (0.5103) lr 1.1253e-03 eta 0:26:32
epoch [25/50] batch [40/288] time 0.208 (0.204) data 0.000 (0.007) loss 1.3716 (1.6325) teacher_loss 0.7319 (0.9397) loss_zs_kd 0.1263 (0.1677) loss_oracle 0.5765 (0.6089) acc 84.3750 (75.0000) alaph_mean 0.3180 (0.3236) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5129) lr 1.1253e-03 eta 0:25:19
epoch [25/50] batch [60/288] time 0.196 (0.200) data 0.000 (0.005) loss 1.9656 (1.6340) teacher_loss 1.3447 (0.9523) loss_zs_kd 0.1483 (0.1635) loss_oracle 0.5467 (0.5999) acc 56.2500 (74.2188) alaph_mean 0.3598 (0.3251) alpha_min 0.0000 (0.0000) alpha_max 0.6988 (0.5142) lr 1.1253e-03 eta 0:24:49
epoch [25/50] batch [80/288] time 0.196 (0.199) data 0.000 (0.003) loss 1.5033 (1.6283) teacher_loss 0.9126 (0.9527) loss_zs_kd 0.2385 (0.1634) loss_oracle 0.4714 (0.5940) acc 71.8750 (74.8438) alaph_mean 0.3951 (0.3247) alpha_min 0.0000 (0.0000) alpha_max 0.5041 (0.5166) lr 1.1253e-03 eta 0:24:34
epoch [25/50] batch [100/288] time 0.194 (0.197) data 0.000 (0.003) loss 1.4415 (1.6325) teacher_loss 0.7837 (0.9496) loss_zs_kd 0.1967 (0.1681) loss_oracle 0.5595 (0.5989) acc 71.8750 (74.9062) alaph_mean 0.3288 (0.3212) alpha_min 0.0000 (0.0000) alpha_max 0.5044 (0.5168) lr 1.1253e-03 eta 0:24:14
epoch [25/50] batch [120/288] time 0.190 (0.196) data 0.000 (0.002) loss 1.8032 (1.6214) teacher_loss 1.1279 (0.9379) loss_zs_kd 0.1498 (0.1659) loss_oracle 0.6004 (0.6005) acc 68.7500 (75.0260) alaph_mean 0.2839 (0.3192) alpha_min 0.0000 (0.0000) alpha_max 0.5063 (0.5214) lr 1.1253e-03 eta 0:24:07
epoch [25/50] batch [140/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.0083 (1.6292) teacher_loss 0.3792 (0.9405) loss_zs_kd 0.1145 (0.1658) loss_oracle 0.5719 (0.6059) acc 93.7500 (75.0670) alaph_mean 0.3776 (0.3174) alpha_min -0.0000 (0.0000) alpha_max 0.7975 (0.5230) lr 1.1253e-03 eta 0:24:00
epoch [25/50] batch [160/288] time 0.192 (0.196) data 0.000 (0.002) loss 1.5497 (1.6224) teacher_loss 0.7881 (0.9276) loss_zs_kd 0.1421 (0.1650) loss_oracle 0.6906 (0.6123) acc 81.2500 (75.1953) alaph_mean 0.2820 (0.3149) alpha_min 0.0000 (0.0000) alpha_max 0.5081 (0.5213) lr 1.1253e-03 eta 0:23:53
epoch [25/50] batch [180/288] time 0.189 (0.195) data 0.000 (0.002) loss 1.8752 (1.6269) teacher_loss 1.0371 (0.9320) loss_zs_kd 0.1747 (0.1652) loss_oracle 0.7507 (0.6123) acc 71.8750 (75.1736) alaph_mean 0.2495 (0.3157) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5217) lr 1.1253e-03 eta 0:23:47
epoch [25/50] batch [200/288] time 0.195 (0.195) data 0.000 (0.001) loss 1.3085 (1.6158) teacher_loss 0.6060 (0.9267) loss_zs_kd 0.2001 (0.1651) loss_oracle 0.6025 (0.6066) acc 81.2500 (75.3594) alaph_mean 0.3460 (0.3184) alpha_min 0.0000 (0.0000) alpha_max 0.5135 (0.5209) lr 1.1253e-03 eta 0:23:43
epoch [25/50] batch [220/288] time 0.191 (0.195) data 0.000 (0.001) loss 1.6229 (1.6096) teacher_loss 0.8525 (0.9206) loss_zs_kd 0.1848 (0.1651) loss_oracle 0.6779 (0.6065) acc 75.0000 (75.4688) alaph_mean 0.2441 (0.3185) alpha_min 0.0000 (0.0000) alpha_max 0.5055 (0.5207) lr 1.1253e-03 eta 0:23:38
epoch [25/50] batch [240/288] time 0.192 (0.195) data 0.000 (0.001) loss 2.1192 (1.6076) teacher_loss 1.4248 (0.9210) loss_zs_kd 0.1678 (0.1647) loss_oracle 0.6105 (0.6042) acc 68.7500 (75.4948) alaph_mean 0.3128 (0.3194) alpha_min 0.0000 (0.0000) alpha_max 0.5031 (0.5207) lr 1.1253e-03 eta 0:23:34
epoch [25/50] batch [260/288] time 0.215 (0.195) data 0.000 (0.001) loss 1.5673 (1.6071) teacher_loss 0.8325 (0.9183) loss_zs_kd 0.1594 (0.1660) loss_oracle 0.6550 (0.6058) acc 75.0000 (75.6130) alaph_mean 0.3378 (0.3186) alpha_min 0.0000 (0.0000) alpha_max 0.5027 (0.5203) lr 1.1253e-03 eta 0:23:30
epoch [25/50] batch [280/288] time 0.193 (0.195) data 0.000 (0.001) loss 1.5239 (1.6094) teacher_loss 0.9033 (0.9171) loss_zs_kd 0.1215 (0.1661) loss_oracle 0.5598 (0.6092) acc 75.0000 (75.7031) alaph_mean 0.3751 (0.3172) alpha_min 0.0000 (0.0000) alpha_max 0.5090 (0.5211) lr 1.1253e-03 eta 0:23:25
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,433
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.5%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,027
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.1%
******* Domain a best val acc:      87.2%, epoch: 25 *******
******* Domain a best val test acc: 83.5%, epoch: 25 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [26/50] batch [20/288] time 0.194 (0.210) data 0.000 (0.014) loss 1.3055 (1.5147) teacher_loss 0.6465 (0.7825) loss_zs_kd 0.1088 (0.1669) loss_oracle 0.6046 (0.6487) acc 81.2500 (80.1562) alaph_mean 0.3355 (0.3122) alpha_min 0.0000 (0.0000) alpha_max 0.6370 (0.5234) lr 1.0628e-03 eta 0:25:08
epoch [26/50] batch [40/288] time 0.169 (0.202) data 0.000 (0.007) loss 1.7253 (1.5693) teacher_loss 0.9927 (0.8517) loss_zs_kd 0.1030 (0.1672) loss_oracle 0.6811 (0.6340) acc 75.0000 (77.1094) alaph_mean 0.2671 (0.3171) alpha_min -0.0000 (0.0000) alpha_max 0.5061 (0.5199) lr 1.0628e-03 eta 0:24:08
epoch [26/50] batch [60/288] time 0.194 (0.199) data 0.000 (0.005) loss 1.4973 (1.5777) teacher_loss 0.8877 (0.8641) loss_zs_kd 0.1567 (0.1687) loss_oracle 0.5313 (0.6293) acc 68.7500 (76.9792) alaph_mean 0.3603 (0.3178) alpha_min 0.0000 (0.0000) alpha_max 0.5091 (0.5308) lr 1.0628e-03 eta 0:23:43
epoch [26/50] batch [80/288] time 0.195 (0.198) data 0.000 (0.004) loss 1.8705 (1.5823) teacher_loss 1.1836 (0.8703) loss_zs_kd 0.1660 (0.1684) loss_oracle 0.6039 (0.6278) acc 68.7500 (76.5625) alaph_mean 0.3465 (0.3146) alpha_min 0.0000 (0.0000) alpha_max 0.5044 (0.5255) lr 1.0628e-03 eta 0:23:30
epoch [26/50] batch [100/288] time 0.224 (0.198) data 0.000 (0.003) loss 1.2619 (1.5896) teacher_loss 0.7324 (0.8861) loss_zs_kd 0.1325 (0.1676) loss_oracle 0.4632 (0.6197) acc 84.3750 (75.8750) alaph_mean 0.3749 (0.3156) alpha_min 0.0000 (0.0000) alpha_max 0.5032 (0.5239) lr 1.0628e-03 eta 0:23:24
epoch [26/50] batch [120/288] time 0.193 (0.197) data 0.000 (0.002) loss 1.5732 (1.5847) teacher_loss 0.9414 (0.8900) loss_zs_kd 0.2326 (0.1656) loss_oracle 0.5154 (0.6119) acc 71.8750 (75.7552) alaph_mean 0.3448 (0.3175) alpha_min 0.0000 (0.0000) alpha_max 0.5255 (0.5229) lr 1.0628e-03 eta 0:23:11
epoch [26/50] batch [140/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.6540 (1.5777) teacher_loss 0.9971 (0.8864) loss_zs_kd 0.1690 (0.1651) loss_oracle 0.5725 (0.6088) acc 78.1250 (76.0491) alaph_mean 0.3216 (0.3160) alpha_min -0.0000 (0.0000) alpha_max 0.5123 (0.5223) lr 1.0628e-03 eta 0:23:05
epoch [26/50] batch [160/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.7598 (1.5720) teacher_loss 1.0742 (0.8858) loss_zs_kd 0.2129 (0.1659) loss_oracle 0.5791 (0.6031) acc 71.8750 (76.0742) alaph_mean 0.3423 (0.3188) alpha_min 0.0000 (0.0000) alpha_max 0.5102 (0.5208) lr 1.0628e-03 eta 0:23:00
epoch [26/50] batch [180/288] time 0.197 (0.196) data 0.000 (0.002) loss 1.3801 (1.5719) teacher_loss 0.6738 (0.8843) loss_zs_kd 0.1370 (0.1664) loss_oracle 0.6377 (0.6043) acc 87.5000 (76.0243) alaph_mean 0.3126 (0.3175) alpha_min 0.0000 (0.0000) alpha_max 0.5043 (0.5206) lr 1.0628e-03 eta 0:22:55
epoch [26/50] batch [200/288] time 0.193 (0.196) data 0.000 (0.002) loss 1.6218 (1.5770) teacher_loss 0.9038 (0.8883) loss_zs_kd 0.1598 (0.1671) loss_oracle 0.6381 (0.6051) acc 87.5000 (75.9688) alaph_mean 0.2839 (0.3166) alpha_min -0.0000 (0.0000) alpha_max 0.5059 (0.5202) lr 1.0628e-03 eta 0:22:51
epoch [26/50] batch [220/288] time 0.191 (0.196) data 0.000 (0.001) loss 1.0433 (1.5812) teacher_loss 0.3342 (0.8911) loss_zs_kd 0.1120 (0.1671) loss_oracle 0.6530 (0.6066) acc 90.6250 (75.9375) alaph_mean 0.3003 (0.3158) alpha_min 0.0000 (0.0000) alpha_max 0.5037 (0.5194) lr 1.0628e-03 eta 0:22:46
epoch [26/50] batch [240/288] time 0.195 (0.196) data 0.000 (0.001) loss 1.5392 (1.5837) teacher_loss 0.9214 (0.8945) loss_zs_kd 0.1882 (0.1674) loss_oracle 0.5237 (0.6056) acc 78.1250 (75.8724) alaph_mean 0.3413 (0.3167) alpha_min -0.0000 (0.0000) alpha_max 0.5021 (0.5191) lr 1.0628e-03 eta 0:22:42
epoch [26/50] batch [260/288] time 0.195 (0.196) data 0.000 (0.001) loss 1.6541 (1.5941) teacher_loss 0.8906 (0.9017) loss_zs_kd 0.1989 (0.1692) loss_oracle 0.6641 (0.6078) acc 75.0000 (75.6250) alaph_mean 0.2881 (0.3157) alpha_min 0.0000 (0.0000) alpha_max 0.5111 (0.5193) lr 1.0628e-03 eta 0:22:37
epoch [26/50] batch [280/288] time 0.188 (0.195) data 0.000 (0.001) loss 1.8479 (1.5974) teacher_loss 1.0098 (0.9052) loss_zs_kd 0.2067 (0.1703) loss_oracle 0.7348 (0.6070) acc 68.7500 (75.5692) alaph_mean 0.2243 (0.3155) alpha_min 0.0000 (0.0000) alpha_max 0.5020 (0.5187) lr 1.0628e-03 eta 0:22:31
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,425
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.9%
******* Domain a best val acc:      87.2%, epoch: 25 *******
******* Domain a best val test acc: 83.5%, epoch: 25 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [27/50] batch [20/288] time 0.228 (0.215) data 0.000 (0.015) loss 1.0711 (1.5145) teacher_loss 0.6201 (0.8712) loss_zs_kd 0.1980 (0.1905) loss_oracle 0.3520 (0.5480) acc 84.3750 (76.8750) alaph_mean 0.4691 (0.3409) alpha_min 0.0000 (0.0000) alpha_max 0.5093 (0.5481) lr 1.0000e-03 eta 0:24:41
epoch [27/50] batch [40/288] time 0.187 (0.204) data 0.000 (0.007) loss 1.1187 (1.5602) teacher_loss 0.4192 (0.9125) loss_zs_kd 0.1237 (0.1762) loss_oracle 0.6376 (0.5596) acc 90.6250 (75.9375) alaph_mean 0.2892 (0.3352) alpha_min 0.0000 (0.0000) alpha_max 0.5116 (0.5365) lr 1.0000e-03 eta 0:23:23
epoch [27/50] batch [60/288] time 0.193 (0.202) data 0.001 (0.005) loss 1.9512 (1.5464) teacher_loss 1.2832 (0.8974) loss_zs_kd 0.2630 (0.1748) loss_oracle 0.5365 (0.5616) acc 68.7500 (76.1979) alaph_mean 0.3500 (0.3330) alpha_min 0.0000 (0.0000) alpha_max 0.5096 (0.5269) lr 1.0000e-03 eta 0:23:05
epoch [27/50] batch [80/288] time 0.181 (0.201) data 0.000 (0.004) loss 1.6036 (1.5550) teacher_loss 0.7588 (0.8907) loss_zs_kd 0.1822 (0.1704) loss_oracle 0.7536 (0.5791) acc 78.1250 (76.2109) alaph_mean 0.2339 (0.3257) alpha_min -0.0000 (0.0000) alpha_max 0.5202 (0.5276) lr 1.0000e-03 eta 0:22:50
epoch [27/50] batch [100/288] time 0.191 (0.199) data 0.000 (0.003) loss 1.3859 (1.5678) teacher_loss 0.6709 (0.8983) loss_zs_kd 0.2268 (0.1714) loss_oracle 0.6016 (0.5837) acc 84.3750 (76.0000) alaph_mean 0.3329 (0.3254) alpha_min -0.0000 (0.0000) alpha_max 0.5094 (0.5247) lr 1.0000e-03 eta 0:22:37
epoch [27/50] batch [120/288] time 0.187 (0.199) data 0.000 (0.003) loss 1.1968 (1.5450) teacher_loss 0.7192 (0.8797) loss_zs_kd 0.1387 (0.1691) loss_oracle 0.4083 (0.5807) acc 87.5000 (76.6406) alaph_mean 0.4221 (0.3276) alpha_min -0.0000 (0.0000) alpha_max 0.5062 (0.5297) lr 1.0000e-03 eta 0:22:28
epoch [27/50] batch [140/288] time 0.186 (0.198) data 0.000 (0.002) loss 1.5360 (1.5596) teacher_loss 0.9116 (0.8913) loss_zs_kd 0.2509 (0.1721) loss_oracle 0.4989 (0.5823) acc 75.0000 (76.4062) alaph_mean 0.4218 (0.3274) alpha_min 0.0000 (0.0000) alpha_max 0.8161 (0.5299) lr 1.0000e-03 eta 0:22:21
epoch [27/50] batch [160/288] time 0.189 (0.197) data 0.000 (0.002) loss 1.8123 (1.5646) teacher_loss 0.9663 (0.8853) loss_zs_kd 0.2097 (0.1734) loss_oracle 0.7411 (0.5926) acc 65.6250 (76.4844) alaph_mean 0.2450 (0.3232) alpha_min 0.0000 (0.0000) alpha_max 0.6162 (0.5291) lr 1.0000e-03 eta 0:22:13
epoch [27/50] batch [180/288] time 0.192 (0.197) data 0.000 (0.002) loss 1.3905 (1.5683) teacher_loss 0.6948 (0.8879) loss_zs_kd 0.1641 (0.1734) loss_oracle 0.6136 (0.5937) acc 71.8750 (76.4236) alaph_mean 0.3306 (0.3235) alpha_min 0.0000 (0.0000) alpha_max 0.5076 (0.5284) lr 1.0000e-03 eta 0:22:06
epoch [27/50] batch [200/288] time 0.191 (0.197) data 0.000 (0.002) loss 1.5812 (1.5606) teacher_loss 0.8955 (0.8799) loss_zs_kd 0.1441 (0.1734) loss_oracle 0.6136 (0.5940) acc 81.2500 (76.6094) alaph_mean 0.3217 (0.3237) alpha_min 0.0000 (0.0000) alpha_max 0.5104 (0.5283) lr 1.0000e-03 eta 0:21:59
epoch [27/50] batch [220/288] time 0.196 (0.196) data 0.000 (0.002) loss 1.7049 (1.5599) teacher_loss 0.9712 (0.8795) loss_zs_kd 0.3127 (0.1729) loss_oracle 0.5774 (0.5940) acc 75.0000 (76.6193) alaph_mean 0.3463 (0.3229) alpha_min 0.0000 (0.0000) alpha_max 0.5115 (0.5272) lr 1.0000e-03 eta 0:21:54
epoch [27/50] batch [240/288] time 0.197 (0.196) data 0.000 (0.001) loss 1.2136 (1.5743) teacher_loss 0.6245 (0.8933) loss_zs_kd 0.1078 (0.1729) loss_oracle 0.5353 (0.5946) acc 84.3750 (76.1068) alaph_mean 0.3906 (0.3224) alpha_min -0.0000 (0.0000) alpha_max 0.5087 (0.5273) lr 1.0000e-03 eta 0:21:48
epoch [27/50] batch [260/288] time 0.195 (0.196) data 0.000 (0.001) loss 1.2633 (1.5773) teacher_loss 0.6763 (0.8951) loss_zs_kd 0.1443 (0.1733) loss_oracle 0.5149 (0.5956) acc 78.1250 (76.1058) alaph_mean 0.3590 (0.3226) alpha_min 0.0000 (0.0000) alpha_max 0.5053 (0.5268) lr 1.0000e-03 eta 0:21:44
epoch [27/50] batch [280/288] time 0.193 (0.196) data 0.000 (0.001) loss 2.1384 (1.5830) teacher_loss 1.2881 (0.8991) loss_zs_kd 0.2543 (0.1740) loss_oracle 0.7232 (0.5969) acc 59.3750 (75.9598) alaph_mean 0.2752 (0.3225) alpha_min -0.0000 (0.0000) alpha_max 0.5066 (0.5255) lr 1.0000e-03 eta 0:21:39
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,431
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.6%
******* Domain a best val acc:      87.2%, epoch: 25 *******
******* Domain a best val test acc: 83.5%, epoch: 25 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [28/50] batch [20/288] time 0.198 (0.218) data 0.000 (0.016) loss 1.2158 (1.5648) teacher_loss 0.6772 (0.8624) loss_zs_kd 0.1453 (0.1815) loss_oracle 0.4659 (0.6117) acc 84.3750 (77.1875) alaph_mean 0.4093 (0.3145) alpha_min 0.0000 (0.0000) alpha_max 0.6941 (0.5274) lr 9.3721e-04 eta 0:24:01
epoch [28/50] batch [40/288] time 0.201 (0.205) data 0.000 (0.008) loss 1.1980 (1.5904) teacher_loss 0.6045 (0.9116) loss_zs_kd 0.1246 (0.1650) loss_oracle 0.5312 (0.5963) acc 84.3750 (75.3906) alaph_mean 0.3365 (0.3168) alpha_min -0.0000 (0.0000) alpha_max 0.5105 (0.5180) lr 9.3721e-04 eta 0:22:32
epoch [28/50] batch [60/288] time 0.193 (0.202) data 0.000 (0.006) loss 1.4533 (1.6042) teacher_loss 0.7539 (0.9249) loss_zs_kd 0.2042 (0.1669) loss_oracle 0.5973 (0.5958) acc 78.1250 (74.7396) alaph_mean 0.3139 (0.3170) alpha_min 0.0000 (0.0000) alpha_max 0.5141 (0.5263) lr 9.3721e-04 eta 0:22:04
epoch [28/50] batch [80/288] time 0.203 (0.200) data 0.000 (0.004) loss 1.3237 (1.6139) teacher_loss 0.7461 (0.9320) loss_zs_kd 0.1427 (0.1694) loss_oracle 0.5062 (0.5972) acc 75.0000 (74.6094) alaph_mean 0.3755 (0.3170) alpha_min 0.0000 (0.0000) alpha_max 0.5041 (0.5229) lr 9.3721e-04 eta 0:21:50
epoch [28/50] batch [100/288] time 0.193 (0.198) data 0.000 (0.003) loss 1.5246 (1.6084) teacher_loss 0.7568 (0.9217) loss_zs_kd 0.1167 (0.1704) loss_oracle 0.7094 (0.6016) acc 81.2500 (74.9688) alaph_mean 0.2667 (0.3142) alpha_min -0.0000 (0.0000) alpha_max 0.5066 (0.5250) lr 9.3721e-04 eta 0:21:31
epoch [28/50] batch [120/288] time 0.196 (0.197) data 0.000 (0.003) loss 1.4098 (1.5952) teacher_loss 0.8032 (0.9083) loss_zs_kd 0.1446 (0.1678) loss_oracle 0.5343 (0.6031) acc 81.2500 (75.1042) alaph_mean 0.3431 (0.3140) alpha_min 0.0000 (0.0000) alpha_max 0.5090 (0.5249) lr 9.3721e-04 eta 0:21:24
epoch [28/50] batch [140/288] time 0.197 (0.197) data 0.000 (0.003) loss 1.0065 (1.5884) teacher_loss 0.4290 (0.9033) loss_zs_kd 0.1354 (0.1683) loss_oracle 0.5098 (0.6010) acc 90.6250 (75.2679) alaph_mean 0.3537 (0.3151) alpha_min 0.0000 (0.0000) alpha_max 0.5044 (0.5231) lr 9.3721e-04 eta 0:21:17
epoch [28/50] batch [160/288] time 0.197 (0.197) data 0.000 (0.002) loss 1.8190 (1.5744) teacher_loss 1.1875 (0.8933) loss_zs_kd 0.1835 (0.1674) loss_oracle 0.5398 (0.5975) acc 65.6250 (75.5469) alaph_mean 0.3669 (0.3173) alpha_min -0.0000 (0.0000) alpha_max 0.5121 (0.5215) lr 9.3721e-04 eta 0:21:12
epoch [28/50] batch [180/288] time 0.195 (0.197) data 0.000 (0.002) loss 1.3725 (1.5698) teacher_loss 0.7407 (0.8874) loss_zs_kd 0.2181 (0.1682) loss_oracle 0.5227 (0.5984) acc 78.1250 (75.8333) alaph_mean 0.3463 (0.3174) alpha_min -0.0000 (0.0000) alpha_max 0.5073 (0.5201) lr 9.3721e-04 eta 0:21:06
epoch [28/50] batch [200/288] time 0.195 (0.196) data 0.000 (0.002) loss 2.0808 (1.5812) teacher_loss 1.2783 (0.8959) loss_zs_kd 0.2325 (0.1694) loss_oracle 0.6863 (0.6007) acc 68.7500 (75.6875) alaph_mean 0.3127 (0.3166) alpha_min 0.0000 (0.0000) alpha_max 0.5058 (0.5188) lr 9.3721e-04 eta 0:21:00
epoch [28/50] batch [220/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.5644 (1.5809) teacher_loss 0.8955 (0.8951) loss_zs_kd 0.1476 (0.1700) loss_oracle 0.5951 (0.6007) acc 81.2500 (75.8239) alaph_mean 0.3287 (0.3175) alpha_min -0.0000 (0.0000) alpha_max 0.5082 (0.5183) lr 9.3721e-04 eta 0:20:56
epoch [28/50] batch [240/288] time 0.200 (0.196) data 0.000 (0.002) loss 0.9807 (1.5832) teacher_loss 0.3911 (0.8958) loss_zs_kd 0.1413 (0.1707) loss_oracle 0.5189 (0.6021) acc 96.8750 (75.7552) alaph_mean 0.3615 (0.3171) alpha_min 0.0000 (0.0000) alpha_max 0.5068 (0.5176) lr 9.3721e-04 eta 0:20:51
epoch [28/50] batch [260/288] time 0.194 (0.196) data 0.000 (0.001) loss 1.3584 (1.5846) teacher_loss 0.6802 (0.8976) loss_zs_kd 0.1640 (0.1708) loss_oracle 0.5962 (0.6016) acc 87.5000 (75.8654) alaph_mean 0.3288 (0.3174) alpha_min -0.0000 (0.0000) alpha_max 0.5107 (0.5188) lr 9.3721e-04 eta 0:20:47
epoch [28/50] batch [280/288] time 0.194 (0.196) data 0.000 (0.001) loss 1.4921 (1.5842) teacher_loss 0.7822 (0.8955) loss_zs_kd 0.1827 (0.1716) loss_oracle 0.6186 (0.6029) acc 78.1250 (75.8817) alaph_mean 0.3137 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5048 (0.5189) lr 9.3721e-04 eta 0:20:42
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,436
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.6%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,026
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.1%
******* Domain a best val acc:      87.2%, epoch: 28 *******
******* Domain a best val test acc: 83.5%, epoch: 28 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [29/50] batch [20/288] time 0.189 (0.205) data 0.000 (0.013) loss 1.4188 (1.4783) teacher_loss 0.5635 (0.7683) loss_zs_kd 0.2402 (0.1733) loss_oracle 0.7352 (0.6234) acc 81.2500 (80.1562) alaph_mean 0.2756 (0.3160) alpha_min -0.0000 (0.0000) alpha_max 0.5100 (0.5148) lr 8.7467e-04 eta 0:21:36
epoch [29/50] batch [40/288] time 0.188 (0.199) data 0.000 (0.007) loss 1.4732 (1.5707) teacher_loss 0.8721 (0.8752) loss_zs_kd 0.1618 (0.1736) loss_oracle 0.5202 (0.6087) acc 78.1250 (76.9531) alaph_mean 0.3660 (0.3245) alpha_min -0.0000 (0.0000) alpha_max 0.5064 (0.5164) lr 8.7467e-04 eta 0:20:55
epoch [29/50] batch [60/288] time 0.198 (0.198) data 0.001 (0.004) loss 2.3058 (1.6009) teacher_loss 1.6484 (0.9190) loss_zs_kd 0.1754 (0.1698) loss_oracle 0.5697 (0.5970) acc 59.3750 (75.2604) alaph_mean 0.3452 (0.3275) alpha_min -0.0000 (0.0000) alpha_max 0.5101 (0.5228) lr 8.7467e-04 eta 0:20:43
epoch [29/50] batch [80/288] time 0.193 (0.197) data 0.000 (0.003) loss 1.3348 (1.5980) teacher_loss 0.6748 (0.9158) loss_zs_kd 0.2240 (0.1763) loss_oracle 0.5480 (0.5941) acc 90.6250 (75.1953) alaph_mean 0.3344 (0.3254) alpha_min -0.0000 (0.0000) alpha_max 0.5057 (0.5187) lr 8.7467e-04 eta 0:20:32
epoch [29/50] batch [100/288] time 0.194 (0.197) data 0.000 (0.003) loss 2.0556 (1.6156) teacher_loss 1.2891 (0.9322) loss_zs_kd 0.1608 (0.1794) loss_oracle 0.6861 (0.5936) acc 71.8750 (75.0938) alaph_mean 0.3129 (0.3245) alpha_min 0.0000 (0.0000) alpha_max 0.5094 (0.5168) lr 8.7467e-04 eta 0:20:25
epoch [29/50] batch [120/288] time 0.198 (0.196) data 0.000 (0.002) loss 1.5138 (1.6050) teacher_loss 0.8086 (0.9181) loss_zs_kd 0.1126 (0.1769) loss_oracle 0.6489 (0.5984) acc 75.0000 (75.6250) alaph_mean 0.3034 (0.3216) alpha_min -0.0000 (0.0000) alpha_max 0.5087 (0.5160) lr 8.7467e-04 eta 0:20:19
epoch [29/50] batch [140/288] time 0.192 (0.196) data 0.000 (0.002) loss 1.7560 (1.5976) teacher_loss 0.9502 (0.9083) loss_zs_kd 0.1057 (0.1746) loss_oracle 0.7529 (0.6021) acc 65.6250 (75.6696) alaph_mean 0.2180 (0.3206) alpha_min 0.0000 (0.0000) alpha_max 0.5074 (0.5147) lr 8.7467e-04 eta 0:20:13
epoch [29/50] batch [160/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.4207 (1.5977) teacher_loss 0.7002 (0.9062) loss_zs_kd 0.2073 (0.1753) loss_oracle 0.6169 (0.6039) acc 78.1250 (75.6250) alaph_mean 0.3131 (0.3210) alpha_min -0.0000 (0.0000) alpha_max 0.5077 (0.5165) lr 8.7467e-04 eta 0:20:08
epoch [29/50] batch [180/288] time 0.193 (0.195) data 0.000 (0.002) loss 1.7278 (1.6053) teacher_loss 1.0371 (0.9129) loss_zs_kd 0.2117 (0.1770) loss_oracle 0.5849 (0.6039) acc 75.0000 (75.5035) alaph_mean 0.3425 (0.3229) alpha_min -0.0000 (0.0000) alpha_max 0.5083 (0.5178) lr 8.7467e-04 eta 0:20:03
epoch [29/50] batch [200/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.8414 (1.6039) teacher_loss 1.0469 (0.9083) loss_zs_kd 0.1712 (0.1773) loss_oracle 0.7089 (0.6069) acc 68.7500 (75.7031) alaph_mean 0.2992 (0.3220) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5186) lr 8.7467e-04 eta 0:19:58
epoch [29/50] batch [220/288] time 0.189 (0.195) data 0.000 (0.001) loss 1.8459 (1.6101) teacher_loss 0.9458 (0.9112) loss_zs_kd 0.1793 (0.1760) loss_oracle 0.8105 (0.6110) acc 78.1250 (75.7244) alaph_mean 0.1738 (0.3195) alpha_min 0.0000 (0.0000) alpha_max 0.5079 (0.5196) lr 8.7467e-04 eta 0:19:54
epoch [29/50] batch [240/288] time 0.192 (0.195) data 0.000 (0.001) loss 1.5625 (1.6106) teacher_loss 0.8096 (0.9109) loss_zs_kd 0.1536 (0.1754) loss_oracle 0.6761 (0.6120) acc 81.2500 (75.6771) alaph_mean 0.3141 (0.3188) alpha_min 0.0000 (0.0000) alpha_max 0.5074 (0.5200) lr 8.7467e-04 eta 0:19:49
epoch [29/50] batch [260/288] time 0.190 (0.195) data 0.000 (0.001) loss 1.5172 (1.6100) teacher_loss 0.7520 (0.9103) loss_zs_kd 0.2106 (0.1762) loss_oracle 0.6599 (0.6116) acc 75.0000 (75.6851) alaph_mean 0.3279 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.7626 (0.5208) lr 8.7467e-04 eta 0:19:45
epoch [29/50] batch [280/288] time 0.194 (0.195) data 0.000 (0.001) loss 2.0987 (1.6128) teacher_loss 1.3105 (0.9106) loss_zs_kd 0.1737 (0.1767) loss_oracle 0.7013 (0.6138) acc 59.3750 (75.5915) alaph_mean 0.2866 (0.3182) alpha_min 0.0000 (0.0000) alpha_max 0.5108 (0.5199) lr 8.7467e-04 eta 0:19:40
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,435
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,026
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.0%
******* Domain a best val acc:      87.2%, epoch: 28 *******
******* Domain a best val test acc: 83.5%, epoch: 28 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [30/50] batch [20/288] time 0.202 (0.211) data 0.000 (0.017) loss 1.3144 (1.5561) teacher_loss 0.6401 (0.8671) loss_zs_kd 0.1811 (0.1704) loss_oracle 0.5837 (0.6038) acc 78.1250 (75.0000) alaph_mean 0.3126 (0.3218) alpha_min -0.0000 (0.0000) alpha_max 0.5048 (0.5119) lr 8.1262e-04 eta 0:21:14
epoch [30/50] batch [40/288] time 0.184 (0.203) data 0.000 (0.008) loss 1.5449 (1.5016) teacher_loss 0.8311 (0.8140) loss_zs_kd 0.1523 (0.1685) loss_oracle 0.6377 (0.6034) acc 75.0000 (77.2656) alaph_mean 0.2975 (0.3198) alpha_min 0.0000 (0.0000) alpha_max 0.5124 (0.5155) lr 8.1262e-04 eta 0:20:20
epoch [30/50] batch [60/288] time 0.200 (0.200) data 0.001 (0.006) loss 1.6276 (1.5484) teacher_loss 1.0137 (0.8566) loss_zs_kd 0.2273 (0.1712) loss_oracle 0.5003 (0.6062) acc 71.8750 (76.5625) alaph_mean 0.3592 (0.3153) alpha_min 0.0000 (0.0000) alpha_max 0.5051 (0.5149) lr 8.1262e-04 eta 0:19:58
epoch [30/50] batch [80/288] time 0.196 (0.199) data 0.000 (0.004) loss 1.4813 (1.5653) teacher_loss 0.7329 (0.8745) loss_zs_kd 0.1540 (0.1692) loss_oracle 0.6714 (0.6062) acc 84.3750 (76.4453) alaph_mean 0.2678 (0.3139) alpha_min -0.0000 (0.0000) alpha_max 0.5680 (0.5178) lr 8.1262e-04 eta 0:19:44
epoch [30/50] batch [100/288] time 0.196 (0.198) data 0.000 (0.004) loss 1.2013 (1.5841) teacher_loss 0.5645 (0.8931) loss_zs_kd 0.1452 (0.1679) loss_oracle 0.5643 (0.6071) acc 90.6250 (76.0312) alaph_mean 0.2957 (0.3131) alpha_min 0.0000 (0.0000) alpha_max 0.5040 (0.5189) lr 8.1262e-04 eta 0:19:37
epoch [30/50] batch [120/288] time 0.196 (0.198) data 0.000 (0.003) loss 1.6237 (1.5886) teacher_loss 0.9966 (0.9035) loss_zs_kd 0.1299 (0.1681) loss_oracle 0.5621 (0.6010) acc 78.1250 (75.8854) alaph_mean 0.3460 (0.3154) alpha_min 0.0000 (0.0000) alpha_max 0.5053 (0.5174) lr 8.1262e-04 eta 0:19:31
epoch [30/50] batch [140/288] time 0.194 (0.197) data 0.000 (0.003) loss 1.7503 (1.5986) teacher_loss 1.2197 (0.9162) loss_zs_kd 0.1243 (0.1703) loss_oracle 0.4685 (0.5972) acc 59.3750 (75.5580) alaph_mean 0.3835 (0.3170) alpha_min 0.0000 (0.0000) alpha_max 0.5095 (0.5162) lr 8.1262e-04 eta 0:19:22
epoch [30/50] batch [160/288] time 0.191 (0.196) data 0.000 (0.002) loss 1.3493 (1.5919) teacher_loss 0.7085 (0.9128) loss_zs_kd 0.1630 (0.1712) loss_oracle 0.5593 (0.5935) acc 78.1250 (75.5859) alaph_mean 0.3167 (0.3185) alpha_min 0.0000 (0.0000) alpha_max 0.5063 (0.5173) lr 8.1262e-04 eta 0:19:16
epoch [30/50] batch [180/288] time 0.193 (0.196) data 0.000 (0.002) loss 1.7246 (1.6010) teacher_loss 1.0928 (0.9219) loss_zs_kd 0.1879 (0.1718) loss_oracle 0.5378 (0.5932) acc 65.6250 (75.3472) alaph_mean 0.3833 (0.3193) alpha_min 0.0000 (0.0000) alpha_max 0.6540 (0.5171) lr 8.1262e-04 eta 0:19:11
epoch [30/50] batch [200/288] time 0.199 (0.196) data 0.000 (0.002) loss 1.6787 (1.6017) teacher_loss 1.1699 (0.9203) loss_zs_kd 0.1816 (0.1717) loss_oracle 0.4180 (0.5956) acc 65.6250 (75.2344) alaph_mean 0.4100 (0.3191) alpha_min 0.0000 (0.0000) alpha_max 0.5091 (0.5194) lr 8.1262e-04 eta 0:19:06
epoch [30/50] batch [220/288] time 0.189 (0.196) data 0.000 (0.002) loss 1.7926 (1.6036) teacher_loss 0.9990 (0.9204) loss_zs_kd 0.1827 (0.1724) loss_oracle 0.7022 (0.5970) acc 71.8750 (75.1989) alaph_mean 0.2416 (0.3193) alpha_min -0.0000 (0.0000) alpha_max 0.5038 (0.5186) lr 8.1262e-04 eta 0:19:00
epoch [30/50] batch [240/288] time 0.196 (0.196) data 0.000 (0.002) loss 1.1295 (1.6035) teacher_loss 0.5957 (0.9218) loss_zs_kd 0.1415 (0.1718) loss_oracle 0.4631 (0.5958) acc 87.5000 (75.1302) alaph_mean 0.3659 (0.3205) alpha_min -0.0000 (0.0000) alpha_max 0.5103 (0.5179) lr 8.1262e-04 eta 0:18:55
epoch [30/50] batch [260/288] time 0.190 (0.195) data 0.000 (0.001) loss 1.5710 (1.6117) teacher_loss 0.7637 (0.9293) loss_zs_kd 0.1568 (0.1719) loss_oracle 0.7290 (0.5965) acc 78.1250 (75.0481) alaph_mean 0.2367 (0.3209) alpha_min -0.0000 (0.0000) alpha_max 0.5056 (0.5181) lr 8.1262e-04 eta 0:18:51
epoch [30/50] batch [280/288] time 0.192 (0.195) data 0.000 (0.001) loss 1.3694 (1.6061) teacher_loss 0.6196 (0.9221) loss_zs_kd 0.1396 (0.1710) loss_oracle 0.6800 (0.5984) acc 84.3750 (75.2679) alaph_mean 0.2660 (0.3202) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5173) lr 8.1262e-04 eta 0:18:46
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,433
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.6%
******* Domain a best val acc:      87.2%, epoch: 28 *******
******* Domain a best val test acc: 83.5%, epoch: 28 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [31/50] batch [20/288] time 0.194 (0.212) data 0.000 (0.018) loss 0.8586 (1.5279) teacher_loss 0.3403 (0.8505) loss_zs_kd 0.1178 (0.1803) loss_oracle 0.4594 (0.5872) acc 90.6250 (76.7188) alaph_mean 0.3774 (0.3229) alpha_min 0.0000 (0.0000) alpha_max 0.5105 (0.5160) lr 7.5131e-04 eta 0:20:16
epoch [31/50] batch [40/288] time 0.203 (0.206) data 0.001 (0.009) loss 1.2736 (1.5774) teacher_loss 0.6069 (0.9023) loss_zs_kd 0.1704 (0.1709) loss_oracle 0.5815 (0.5896) acc 87.5000 (75.7812) alaph_mean 0.3211 (0.3197) alpha_min 0.0000 (0.0000) alpha_max 0.5070 (0.5244) lr 7.5131e-04 eta 0:19:36
epoch [31/50] batch [60/288] time 0.196 (0.201) data 0.001 (0.006) loss 1.6276 (1.5972) teacher_loss 0.9497 (0.9172) loss_zs_kd 0.1680 (0.1684) loss_oracle 0.5939 (0.5957) acc 75.0000 (75.2083) alaph_mean 0.3284 (0.3208) alpha_min 0.0000 (0.0000) alpha_max 0.5075 (0.5195) lr 7.5131e-04 eta 0:19:08
epoch [31/50] batch [80/288] time 0.198 (0.200) data 0.000 (0.005) loss 1.5081 (1.5797) teacher_loss 0.8223 (0.8964) loss_zs_kd 0.1362 (0.1696) loss_oracle 0.6177 (0.5985) acc 81.2500 (75.7812) alaph_mean 0.3123 (0.3203) alpha_min 0.0000 (0.0000) alpha_max 0.5097 (0.5179) lr 7.5131e-04 eta 0:18:53
epoch [31/50] batch [100/288] time 0.193 (0.199) data 0.000 (0.004) loss 1.7748 (1.5684) teacher_loss 1.0156 (0.8853) loss_zs_kd 0.2206 (0.1677) loss_oracle 0.6489 (0.5992) acc 68.7500 (75.8125) alaph_mean 0.2821 (0.3231) alpha_min 0.0000 (0.0000) alpha_max 0.5104 (0.5180) lr 7.5131e-04 eta 0:18:44
epoch [31/50] batch [120/288] time 0.194 (0.198) data 0.000 (0.003) loss 1.3214 (1.5879) teacher_loss 0.5117 (0.8919) loss_zs_kd 0.1530 (0.1719) loss_oracle 0.7331 (0.6100) acc 87.5000 (75.7292) alaph_mean 0.2988 (0.3210) alpha_min 0.0000 (0.0000) alpha_max 0.5115 (0.5232) lr 7.5131e-04 eta 0:18:34
epoch [31/50] batch [140/288] time 0.196 (0.197) data 0.000 (0.003) loss 1.1326 (1.5870) teacher_loss 0.5991 (0.8899) loss_zs_kd 0.2423 (0.1726) loss_oracle 0.4123 (0.6107) acc 84.3750 (76.0045) alaph_mean 0.4296 (0.3222) alpha_min 0.0000 (0.0000) alpha_max 0.5079 (0.5249) lr 7.5131e-04 eta 0:18:28
epoch [31/50] batch [160/288] time 0.188 (0.197) data 0.000 (0.002) loss 1.6034 (1.5960) teacher_loss 0.8008 (0.8942) loss_zs_kd 0.1928 (0.1731) loss_oracle 0.7062 (0.6153) acc 81.2500 (76.0352) alaph_mean 0.2768 (0.3216) alpha_min 0.0000 (0.0000) alpha_max 0.8001 (0.5252) lr 7.5131e-04 eta 0:18:22
epoch [31/50] batch [180/288] time 0.200 (0.197) data 0.000 (0.002) loss 1.8214 (1.6014) teacher_loss 1.1719 (0.8980) loss_zs_kd 0.1861 (0.1713) loss_oracle 0.5565 (0.6178) acc 68.7500 (75.8160) alaph_mean 0.3612 (0.3220) alpha_min 0.0000 (0.0000) alpha_max 0.5106 (0.5233) lr 7.5131e-04 eta 0:18:17
epoch [31/50] batch [200/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.5272 (1.6013) teacher_loss 0.8677 (0.8999) loss_zs_kd 0.1336 (0.1719) loss_oracle 0.5927 (0.6154) acc 81.2500 (75.8750) alaph_mean 0.3432 (0.3238) alpha_min -0.0000 (0.0000) alpha_max 0.5075 (0.5237) lr 7.5131e-04 eta 0:18:12
epoch [31/50] batch [220/288] time 0.190 (0.196) data 0.000 (0.002) loss 1.8344 (1.6027) teacher_loss 0.9780 (0.9031) loss_zs_kd 0.1826 (0.1717) loss_oracle 0.7651 (0.6138) acc 62.5000 (75.7955) alaph_mean 0.2514 (0.3246) alpha_min -0.0000 (0.0000) alpha_max 0.5064 (0.5264) lr 7.5131e-04 eta 0:18:07
epoch [31/50] batch [240/288] time 0.189 (0.196) data 0.000 (0.002) loss 1.7290 (1.6041) teacher_loss 0.8843 (0.9023) loss_zs_kd 0.1760 (0.1719) loss_oracle 0.7567 (0.6158) acc 71.8750 (75.7812) alaph_mean 0.2448 (0.3238) alpha_min 0.0000 (0.0000) alpha_max 0.5036 (0.5268) lr 7.5131e-04 eta 0:18:02
epoch [31/50] batch [260/288] time 0.193 (0.196) data 0.000 (0.002) loss 1.3196 (1.6077) teacher_loss 0.5928 (0.9040) loss_zs_kd 0.1130 (0.1711) loss_oracle 0.6703 (0.6181) acc 90.6250 (75.6971) alaph_mean 0.2726 (0.3228) alpha_min 0.0000 (0.0000) alpha_max 0.5072 (0.5265) lr 7.5131e-04 eta 0:17:57
epoch [31/50] batch [280/288] time 0.192 (0.196) data 0.000 (0.001) loss 1.4322 (1.5988) teacher_loss 0.7422 (0.8955) loss_zs_kd 0.1189 (0.1698) loss_oracle 0.6305 (0.6184) acc 81.2500 (76.0714) alaph_mean 0.3179 (0.3229) alpha_min 0.0000 (0.0000) alpha_max 0.5037 (0.5252) lr 7.5131e-04 eta 0:17:53
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,428
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,031
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 79.9%
******* Domain a best val acc:      87.2%, epoch: 28 *******
******* Domain a best val test acc: 83.5%, epoch: 28 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [32/50] batch [20/288] time 0.193 (0.208) data 0.000 (0.015) loss 1.5387 (1.6490) teacher_loss 0.8726 (0.9230) loss_zs_kd 0.1194 (0.1638) loss_oracle 0.6065 (0.6441) acc 78.1250 (77.5000) alaph_mean 0.3177 (0.3026) alpha_min 0.0000 (0.0000) alpha_max 0.5021 (0.5353) lr 6.9098e-04 eta 0:18:56
epoch [32/50] batch [40/288] time 0.196 (0.203) data 0.000 (0.008) loss 1.0011 (1.6211) teacher_loss 0.4541 (0.9119) loss_zs_kd 0.0907 (0.1712) loss_oracle 0.5017 (0.6236) acc 87.5000 (77.1875) alaph_mean 0.3752 (0.3146) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5324) lr 6.9098e-04 eta 0:18:21
epoch [32/50] batch [60/288] time 0.174 (0.200) data 0.001 (0.005) loss 1.3479 (1.6476) teacher_loss 0.7046 (0.9316) loss_zs_kd 0.1991 (0.1740) loss_oracle 0.5437 (0.6290) acc 78.1250 (76.0938) alaph_mean 0.3635 (0.3089) alpha_min 0.0000 (0.0000) alpha_max 0.5095 (0.5284) lr 6.9098e-04 eta 0:18:01
epoch [32/50] batch [80/288] time 0.169 (0.199) data 0.000 (0.004) loss 1.9756 (1.6136) teacher_loss 1.3291 (0.9102) loss_zs_kd 0.1475 (0.1714) loss_oracle 0.5727 (0.6177) acc 68.7500 (76.5625) alaph_mean 0.3160 (0.3128) alpha_min -0.0000 (0.0000) alpha_max 0.5047 (0.5245) lr 6.9098e-04 eta 0:17:51
epoch [32/50] batch [100/288] time 0.195 (0.198) data 0.000 (0.003) loss 1.1709 (1.6074) teacher_loss 0.4956 (0.9095) loss_zs_kd 0.1752 (0.1690) loss_oracle 0.5877 (0.6135) acc 87.5000 (76.5938) alaph_mean 0.3128 (0.3128) alpha_min -0.0000 (0.0000) alpha_max 0.5053 (0.5229) lr 6.9098e-04 eta 0:17:42
epoch [32/50] batch [120/288] time 0.194 (0.198) data 0.000 (0.003) loss 1.1678 (1.5982) teacher_loss 0.5742 (0.9061) loss_zs_kd 0.1524 (0.1695) loss_oracle 0.5174 (0.6073) acc 78.1250 (76.3542) alaph_mean 0.3710 (0.3152) alpha_min -0.0000 (0.0000) alpha_max 0.5067 (0.5226) lr 6.9098e-04 eta 0:17:39
epoch [32/50] batch [140/288] time 0.193 (0.197) data 0.000 (0.002) loss 1.5559 (1.5951) teacher_loss 0.8833 (0.9053) loss_zs_kd 0.2355 (0.1686) loss_oracle 0.5548 (0.6055) acc 81.2500 (76.3616) alaph_mean 0.3499 (0.3156) alpha_min -0.0000 (0.0000) alpha_max 0.5059 (0.5225) lr 6.9098e-04 eta 0:17:32
epoch [32/50] batch [160/288] time 0.193 (0.197) data 0.000 (0.002) loss 1.2597 (1.5938) teacher_loss 0.6597 (0.9024) loss_zs_kd 0.1800 (0.1685) loss_oracle 0.5101 (0.6072) acc 87.5000 (76.2695) alaph_mean 0.3630 (0.3145) alpha_min -0.0000 (0.0000) alpha_max 0.6617 (0.5230) lr 6.9098e-04 eta 0:17:26
epoch [32/50] batch [180/288] time 0.194 (0.197) data 0.000 (0.002) loss 1.4326 (1.6032) teacher_loss 0.8462 (0.9122) loss_zs_kd 0.1997 (0.1687) loss_oracle 0.4866 (0.6066) acc 78.1250 (75.8333) alaph_mean 0.3667 (0.3146) alpha_min -0.0000 (0.0000) alpha_max 0.5098 (0.5222) lr 6.9098e-04 eta 0:17:20
epoch [32/50] batch [200/288] time 0.200 (0.196) data 0.000 (0.002) loss 1.6165 (1.5975) teacher_loss 0.8213 (0.9075) loss_zs_kd 0.1778 (0.1687) loss_oracle 0.7063 (0.6057) acc 75.0000 (75.9688) alaph_mean 0.2707 (0.3145) alpha_min 0.0000 (0.0000) alpha_max 0.5083 (0.5225) lr 6.9098e-04 eta 0:17:15
epoch [32/50] batch [220/288] time 0.193 (0.196) data 0.000 (0.002) loss 1.4451 (1.5949) teacher_loss 0.7139 (0.9096) loss_zs_kd 0.1217 (0.1682) loss_oracle 0.6704 (0.6012) acc 78.1250 (75.8807) alaph_mean 0.2788 (0.3172) alpha_min 0.0000 (0.0000) alpha_max 0.5095 (0.5215) lr 6.9098e-04 eta 0:17:11
epoch [32/50] batch [240/288] time 0.194 (0.196) data 0.000 (0.001) loss 1.5086 (1.5857) teacher_loss 0.8936 (0.9016) loss_zs_kd 0.1429 (0.1676) loss_oracle 0.5436 (0.6003) acc 81.2500 (76.1589) alaph_mean 0.3447 (0.3177) alpha_min -0.0000 (0.0000) alpha_max 0.5109 (0.5208) lr 6.9098e-04 eta 0:17:06
epoch [32/50] batch [260/288] time 0.178 (0.196) data 0.000 (0.001) loss 1.5643 (1.5909) teacher_loss 0.9253 (0.9084) loss_zs_kd 0.1166 (0.1686) loss_oracle 0.5807 (0.5982) acc 78.1250 (76.0457) alaph_mean 0.3289 (0.3183) alpha_min -0.0000 (0.0000) alpha_max 0.5052 (0.5208) lr 6.9098e-04 eta 0:17:02
epoch [32/50] batch [280/288] time 0.195 (0.196) data 0.000 (0.001) loss 1.1635 (1.5878) teacher_loss 0.6724 (0.9058) loss_zs_kd 0.1419 (0.1689) loss_oracle 0.4202 (0.5975) acc 71.8750 (76.0714) alaph_mean 0.4007 (0.3187) alpha_min 0.0000 (0.0000) alpha_max 0.5068 (0.5213) lr 6.9098e-04 eta 0:16:57
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,431
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,032
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 80.0%
******* Domain a best val acc:      87.2%, epoch: 28 *******
******* Domain a best val test acc: 83.5%, epoch: 28 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [33/50] batch [20/288] time 0.193 (0.210) data 0.000 (0.013) loss 1.5579 (1.6216) teacher_loss 1.0195 (0.9634) loss_zs_kd 0.1664 (0.1807) loss_oracle 0.4552 (0.5678) acc 81.2500 (75.1562) alaph_mean 0.4078 (0.3244) alpha_min 0.0000 (0.0000) alpha_max 0.5069 (0.5286) lr 6.3188e-04 eta 0:18:04
epoch [33/50] batch [40/288] time 0.187 (0.201) data 0.000 (0.007) loss 1.6066 (1.6606) teacher_loss 1.0049 (0.9755) loss_zs_kd 0.1413 (0.1744) loss_oracle 0.5310 (0.5979) acc 78.1250 (74.4531) alaph_mean 0.3413 (0.3117) alpha_min 0.0000 (0.0000) alpha_max 0.5067 (0.5246) lr 6.3188e-04 eta 0:17:13
epoch [33/50] batch [60/288] time 0.198 (0.198) data 0.001 (0.005) loss 1.6353 (1.6430) teacher_loss 0.9028 (0.9518) loss_zs_kd 0.1920 (0.1731) loss_oracle 0.6365 (0.6046) acc 71.8750 (74.6354) alaph_mean 0.2811 (0.3078) alpha_min -0.0000 (0.0000) alpha_max 0.5043 (0.5190) lr 6.3188e-04 eta 0:16:56
epoch [33/50] batch [80/288] time 0.192 (0.197) data 0.000 (0.004) loss 1.5017 (1.6359) teacher_loss 0.7681 (0.9497) loss_zs_kd 0.1161 (0.1696) loss_oracle 0.6756 (0.6014) acc 78.1250 (74.6875) alaph_mean 0.2665 (0.3094) alpha_min 0.0000 (0.0000) alpha_max 0.5063 (0.5170) lr 6.3188e-04 eta 0:16:47
epoch [33/50] batch [100/288] time 0.189 (0.197) data 0.000 (0.003) loss 1.4978 (1.6078) teacher_loss 0.7964 (0.9247) loss_zs_kd 0.1446 (0.1720) loss_oracle 0.6290 (0.5971) acc 84.3750 (75.1562) alaph_mean 0.3003 (0.3147) alpha_min 0.0000 (0.0000) alpha_max 0.5091 (0.5215) lr 6.3188e-04 eta 0:16:40
epoch [33/50] batch [120/288] time 0.193 (0.197) data 0.000 (0.002) loss 2.1962 (1.6044) teacher_loss 1.5664 (0.9243) loss_zs_kd 0.1646 (0.1723) loss_oracle 0.5475 (0.5939) acc 56.2500 (75.2865) alaph_mean 0.3756 (0.3174) alpha_min 0.0000 (0.0000) alpha_max 0.5099 (0.5210) lr 6.3188e-04 eta 0:16:35
epoch [33/50] batch [140/288] time 0.197 (0.196) data 0.000 (0.002) loss 1.5935 (1.6019) teacher_loss 1.0869 (0.9215) loss_zs_kd 0.2138 (0.1733) loss_oracle 0.3997 (0.5937) acc 71.8750 (75.4688) alaph_mean 0.4076 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5056 (0.5199) lr 6.3188e-04 eta 0:16:28
epoch [33/50] batch [160/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.8169 (1.6002) teacher_loss 1.0156 (0.9170) loss_zs_kd 0.2177 (0.1728) loss_oracle 0.6924 (0.5968) acc 68.7500 (75.4883) alaph_mean 0.2815 (0.3158) alpha_min -0.0000 (0.0000) alpha_max 0.5039 (0.5187) lr 6.3188e-04 eta 0:16:23
epoch [33/50] batch [180/288] time 0.203 (0.195) data 0.000 (0.002) loss 1.8171 (1.6034) teacher_loss 1.0693 (0.9169) loss_zs_kd 0.1651 (0.1715) loss_oracle 0.6652 (0.6007) acc 75.0000 (75.6076) alaph_mean 0.3138 (0.3147) alpha_min 0.0000 (0.0000) alpha_max 0.5060 (0.5208) lr 6.3188e-04 eta 0:16:18
epoch [33/50] batch [200/288] time 0.198 (0.195) data 0.000 (0.002) loss 1.6023 (1.5989) teacher_loss 0.9121 (0.9113) loss_zs_kd 0.1351 (0.1708) loss_oracle 0.6226 (0.6022) acc 75.0000 (75.7188) alaph_mean 0.3309 (0.3152) alpha_min 0.0000 (0.0000) alpha_max 0.5127 (0.5199) lr 6.3188e-04 eta 0:16:13
epoch [33/50] batch [220/288] time 0.191 (0.195) data 0.000 (0.001) loss 1.6365 (1.5965) teacher_loss 1.0908 (0.9082) loss_zs_kd 0.0869 (0.1709) loss_oracle 0.5023 (0.6028) acc 71.8750 (75.8097) alaph_mean 0.3592 (0.3153) alpha_min 0.0000 (0.0000) alpha_max 0.5062 (0.5204) lr 6.3188e-04 eta 0:16:09
epoch [33/50] batch [240/288] time 0.193 (0.195) data 0.000 (0.001) loss 1.2717 (1.5882) teacher_loss 0.6050 (0.8992) loss_zs_kd 0.1556 (0.1711) loss_oracle 0.5889 (0.6035) acc 78.1250 (75.9766) alaph_mean 0.3315 (0.3161) alpha_min 0.0000 (0.0000) alpha_max 0.5075 (0.5196) lr 6.3188e-04 eta 0:16:04
epoch [33/50] batch [260/288] time 0.199 (0.195) data 0.000 (0.001) loss 1.4339 (1.5930) teacher_loss 0.8301 (0.9046) loss_zs_kd 0.1285 (0.1716) loss_oracle 0.5396 (0.6026) acc 87.5000 (75.9495) alaph_mean 0.3345 (0.3177) alpha_min 0.0000 (0.0000) alpha_max 0.5110 (0.5190) lr 6.3188e-04 eta 0:16:01
epoch [33/50] batch [280/288] time 0.187 (0.195) data 0.000 (0.001) loss 1.4194 (1.5962) teacher_loss 0.8110 (0.9058) loss_zs_kd 0.1506 (0.1719) loss_oracle 0.5330 (0.6045) acc 78.1250 (75.8817) alaph_mean 0.3658 (0.3174) alpha_min 0.0000 (0.0000) alpha_max 0.6771 (0.5190) lr 6.3188e-04 eta 0:15:56
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,427
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,034
* accuracy: 83.8%
* error: 16.2%
* macro_f1: 80.2%
******* Domain a best val acc:      87.2%, epoch: 28 *******
******* Domain a best val test acc: 83.5%, epoch: 28 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [34/50] batch [20/288] time 0.229 (0.213) data 0.000 (0.014) loss 1.7096 (1.4942) teacher_loss 1.0869 (0.8179) loss_zs_kd 0.1929 (0.1694) loss_oracle 0.5263 (0.5916) acc 78.1250 (77.1875) alaph_mean 0.3603 (0.3228) alpha_min -0.0000 (0.0000) alpha_max 0.5063 (0.5486) lr 5.7422e-04 eta 0:17:18
epoch [34/50] batch [40/288] time 0.192 (0.203) data 0.000 (0.007) loss 1.5875 (1.5592) teacher_loss 0.8091 (0.8773) loss_zs_kd 0.1218 (0.1689) loss_oracle 0.7175 (0.5974) acc 78.1250 (76.4844) alaph_mean 0.2476 (0.3254) alpha_min -0.0000 (0.0000) alpha_max 0.5020 (0.5435) lr 5.7422e-04 eta 0:16:26
epoch [34/50] batch [60/288] time 0.188 (0.200) data 0.001 (0.005) loss 1.4778 (1.5630) teacher_loss 0.7451 (0.8842) loss_zs_kd 0.1180 (0.1694) loss_oracle 0.6737 (0.5941) acc 81.2500 (75.9375) alaph_mean 0.2809 (0.3276) alpha_min -0.0000 (0.0000) alpha_max 0.5053 (0.5387) lr 5.7422e-04 eta 0:16:07
epoch [34/50] batch [80/288] time 0.202 (0.199) data 0.000 (0.004) loss 1.5585 (1.5668) teacher_loss 0.9004 (0.8852) loss_zs_kd 0.1914 (0.1690) loss_oracle 0.5624 (0.5971) acc 78.1250 (76.1719) alaph_mean 0.3538 (0.3291) alpha_min 0.0000 (0.0000) alpha_max 0.5106 (0.5340) lr 5.7422e-04 eta 0:15:57
epoch [34/50] batch [100/288] time 0.197 (0.198) data 0.000 (0.003) loss 1.7782 (1.5620) teacher_loss 1.0488 (0.8751) loss_zs_kd 0.2078 (0.1676) loss_oracle 0.6254 (0.6031) acc 75.0000 (76.4375) alaph_mean 0.3168 (0.3255) alpha_min 0.0000 (0.0000) alpha_max 0.5070 (0.5311) lr 5.7422e-04 eta 0:15:49
epoch [34/50] batch [120/288] time 0.188 (0.197) data 0.000 (0.002) loss 1.7157 (1.5761) teacher_loss 0.9448 (0.8854) loss_zs_kd 0.1400 (0.1672) loss_oracle 0.7008 (0.6072) acc 71.8750 (76.0938) alaph_mean 0.2389 (0.3235) alpha_min -0.0000 (0.0000) alpha_max 0.5062 (0.5285) lr 5.7422e-04 eta 0:15:42
epoch [34/50] batch [140/288] time 0.194 (0.197) data 0.000 (0.002) loss 1.8030 (1.5704) teacher_loss 0.9448 (0.8782) loss_zs_kd 0.1857 (0.1686) loss_oracle 0.7653 (0.6080) acc 65.6250 (76.3616) alaph_mean 0.2377 (0.3240) alpha_min 0.0000 (0.0000) alpha_max 0.5103 (0.5343) lr 5.7422e-04 eta 0:15:36
epoch [34/50] batch [160/288] time 0.194 (0.197) data 0.000 (0.002) loss 0.9898 (1.5701) teacher_loss 0.5742 (0.8789) loss_zs_kd 0.1523 (0.1699) loss_oracle 0.3394 (0.6063) acc 84.3750 (76.3477) alaph_mean 0.4835 (0.3239) alpha_min 0.0000 (0.0000) alpha_max 0.5059 (0.5317) lr 5.7422e-04 eta 0:15:31
epoch [34/50] batch [180/288] time 0.185 (0.196) data 0.000 (0.002) loss 1.5463 (1.5751) teacher_loss 0.8818 (0.8859) loss_zs_kd 0.1575 (0.1712) loss_oracle 0.5857 (0.6036) acc 71.8750 (76.0590) alaph_mean 0.3091 (0.3250) alpha_min 0.0000 (0.0000) alpha_max 0.5072 (0.5289) lr 5.7422e-04 eta 0:15:25
epoch [34/50] batch [200/288] time 0.191 (0.196) data 0.000 (0.002) loss 2.2864 (1.5909) teacher_loss 1.4805 (0.8984) loss_zs_kd 0.2002 (0.1712) loss_oracle 0.7058 (0.6069) acc 62.5000 (75.7969) alaph_mean 0.2505 (0.3231) alpha_min 0.0000 (0.0000) alpha_max 0.5048 (0.5274) lr 5.7422e-04 eta 0:15:20
epoch [34/50] batch [220/288] time 0.194 (0.196) data 0.000 (0.001) loss 1.7836 (1.5943) teacher_loss 1.0605 (0.9003) loss_zs_kd 0.1298 (0.1717) loss_oracle 0.6581 (0.6082) acc 68.7500 (75.7528) alaph_mean 0.2971 (0.3224) alpha_min -0.0000 (0.0000) alpha_max 0.5042 (0.5257) lr 5.7422e-04 eta 0:15:15
epoch [34/50] batch [240/288] time 0.195 (0.196) data 0.000 (0.001) loss 1.7502 (1.6004) teacher_loss 0.9077 (0.9048) loss_zs_kd 0.1811 (0.1717) loss_oracle 0.7520 (0.6098) acc 71.8750 (75.7031) alaph_mean 0.2540 (0.3224) alpha_min -0.0000 (0.0000) alpha_max 0.5071 (0.5245) lr 5.7422e-04 eta 0:15:11
epoch [34/50] batch [260/288] time 0.200 (0.196) data 0.000 (0.001) loss 1.3881 (1.5990) teacher_loss 0.7642 (0.9022) loss_zs_kd 0.2014 (0.1716) loss_oracle 0.5232 (0.6110) acc 84.3750 (75.9255) alaph_mean 0.3903 (0.3222) alpha_min 0.0000 (0.0000) alpha_max 0.5077 (0.5249) lr 5.7422e-04 eta 0:15:06
epoch [34/50] batch [280/288] time 0.187 (0.195) data 0.000 (0.001) loss 1.9514 (1.5942) teacher_loss 1.0869 (0.8982) loss_zs_kd 0.2350 (0.1708) loss_oracle 0.7470 (0.6106) acc 65.6250 (75.9710) alaph_mean 0.2448 (0.3219) alpha_min 0.0000 (0.0000) alpha_max 0.5025 (0.5251) lr 5.7422e-04 eta 0:15:02
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,425
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.8%
******* Domain a best val acc:      87.2%, epoch: 28 *******
******* Domain a best val test acc: 83.5%, epoch: 28 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [35/50] batch [20/288] time 0.212 (0.216) data 0.000 (0.011) loss 1.8478 (1.6552) teacher_loss 1.1846 (0.9539) loss_zs_kd 0.2072 (0.1803) loss_oracle 0.5596 (0.6111) acc 71.8750 (75.1562) alaph_mean 0.3756 (0.3269) alpha_min 0.0000 (0.0000) alpha_max 0.5111 (0.5389) lr 5.1825e-04 eta 0:16:31
epoch [35/50] batch [40/288] time 0.194 (0.204) data 0.000 (0.006) loss 1.6793 (1.6919) teacher_loss 0.9805 (0.9844) loss_zs_kd 0.1497 (0.1807) loss_oracle 0.6240 (0.6171) acc 68.7500 (73.6719) alaph_mean 0.3281 (0.3204) alpha_min 0.0000 (0.0000) alpha_max 0.5063 (0.5305) lr 5.1825e-04 eta 0:15:34
epoch [35/50] batch [60/288] time 0.176 (0.201) data 0.001 (0.004) loss 2.0971 (1.6737) teacher_loss 1.3945 (0.9596) loss_zs_kd 0.1401 (0.1758) loss_oracle 0.6325 (0.6263) acc 68.7500 (74.5833) alaph_mean 0.3004 (0.3137) alpha_min 0.0000 (0.0000) alpha_max 0.5063 (0.5265) lr 5.1825e-04 eta 0:15:13
epoch [35/50] batch [80/288] time 0.197 (0.199) data 0.000 (0.003) loss 1.8423 (1.6668) teacher_loss 1.0430 (0.9591) loss_zs_kd 0.1976 (0.1746) loss_oracle 0.7005 (0.6204) acc 75.0000 (74.6484) alaph_mean 0.2972 (0.3167) alpha_min 0.0000 (0.0000) alpha_max 0.5066 (0.5309) lr 5.1825e-04 eta 0:15:02
epoch [35/50] batch [100/288] time 0.196 (0.198) data 0.000 (0.002) loss 1.6397 (1.6591) teacher_loss 1.0205 (0.9540) loss_zs_kd 0.1475 (0.1755) loss_oracle 0.5454 (0.6174) acc 71.8750 (74.5938) alaph_mean 0.4022 (0.3191) alpha_min 0.0000 (0.0000) alpha_max 0.7598 (0.5313) lr 5.1825e-04 eta 0:14:53
epoch [35/50] batch [120/288] time 0.193 (0.198) data 0.000 (0.002) loss 1.5318 (1.6517) teacher_loss 0.7417 (0.9422) loss_zs_kd 0.2232 (0.1757) loss_oracle 0.6785 (0.6216) acc 78.1250 (74.9219) alaph_mean 0.3469 (0.3189) alpha_min 0.0000 (0.0000) alpha_max 0.5377 (0.5306) lr 5.1825e-04 eta 0:14:46
epoch [35/50] batch [140/288] time 0.196 (0.197) data 0.000 (0.002) loss 1.1794 (1.6511) teacher_loss 0.5342 (0.9399) loss_zs_kd 0.0800 (0.1744) loss_oracle 0.6052 (0.6240) acc 87.5000 (75.2455) alaph_mean 0.2967 (0.3168) alpha_min 0.0000 (0.0000) alpha_max 0.5090 (0.5308) lr 5.1825e-04 eta 0:14:40
epoch [35/50] batch [160/288] time 0.190 (0.197) data 0.000 (0.002) loss 1.6241 (1.6430) teacher_loss 0.9214 (0.9347) loss_zs_kd 0.1734 (0.1747) loss_oracle 0.6160 (0.6209) acc 75.0000 (75.3125) alaph_mean 0.3161 (0.3174) alpha_min 0.0000 (0.0000) alpha_max 0.5056 (0.5279) lr 5.1825e-04 eta 0:14:34
epoch [35/50] batch [180/288] time 0.194 (0.196) data 0.000 (0.001) loss 1.2348 (1.6273) teacher_loss 0.6128 (0.9176) loss_zs_kd 0.1096 (0.1725) loss_oracle 0.5672 (0.6235) acc 78.1250 (75.8333) alaph_mean 0.3593 (0.3145) alpha_min 0.0000 (0.0000) alpha_max 0.5093 (0.5261) lr 5.1825e-04 eta 0:14:29
epoch [35/50] batch [200/288] time 0.198 (0.196) data 0.000 (0.001) loss 1.1584 (1.6240) teacher_loss 0.5601 (0.9137) loss_zs_kd 0.1725 (0.1728) loss_oracle 0.5121 (0.6238) acc 81.2500 (75.8438) alaph_mean 0.3730 (0.3138) alpha_min 0.0000 (0.0000) alpha_max 0.5100 (0.5267) lr 5.1825e-04 eta 0:14:24
epoch [35/50] batch [220/288] time 0.190 (0.196) data 0.000 (0.001) loss 1.2870 (1.6170) teacher_loss 0.5708 (0.9092) loss_zs_kd 0.1328 (0.1720) loss_oracle 0.6498 (0.6218) acc 87.5000 (75.9659) alaph_mean 0.2662 (0.3147) alpha_min 0.0000 (0.0000) alpha_max 0.5083 (0.5281) lr 5.1825e-04 eta 0:14:19
epoch [35/50] batch [240/288] time 0.160 (0.196) data 0.000 (0.001) loss 1.2544 (1.6128) teacher_loss 0.4265 (0.9049) loss_zs_kd 0.2092 (0.1716) loss_oracle 0.7233 (0.6221) acc 90.6250 (75.9766) alaph_mean 0.2482 (0.3147) alpha_min 0.0000 (0.0000) alpha_max 0.5045 (0.5272) lr 5.1825e-04 eta 0:14:15
epoch [35/50] batch [260/288] time 0.196 (0.196) data 0.000 (0.001) loss 1.3601 (1.6020) teacher_loss 0.6914 (0.8932) loss_zs_kd 0.1340 (0.1710) loss_oracle 0.6017 (0.6232) acc 81.2500 (76.2740) alaph_mean 0.3350 (0.3143) alpha_min 0.0000 (0.0000) alpha_max 0.5066 (0.5272) lr 5.1825e-04 eta 0:14:11
epoch [35/50] batch [280/288] time 0.195 (0.196) data 0.000 (0.001) loss 1.5101 (1.6020) teacher_loss 0.9126 (0.8961) loss_zs_kd 0.1361 (0.1716) loss_oracle 0.5294 (0.6201) acc 75.0000 (76.1384) alaph_mean 0.3779 (0.3158) alpha_min 0.0000 (0.0000) alpha_max 0.5150 (0.5277) lr 5.1825e-04 eta 0:14:06
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,437
* accuracy: 87.3%
* error: 12.7%
* macro_f1: 86.6%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.0%
******* Domain a best val acc:      87.3%, epoch: 35 *******
******* Domain a best val test acc: 83.4%, epoch: 35 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [36/50] batch [20/288] time 0.186 (0.212) data 0.000 (0.015) loss 2.0494 (1.5734) teacher_loss 1.2842 (0.8412) loss_zs_kd 0.1342 (0.1622) loss_oracle 0.6981 (0.6511) acc 62.5000 (77.6562) alaph_mean 0.2652 (0.3006) alpha_min -0.0000 (0.0000) alpha_max 0.5069 (0.5069) lr 4.6417e-04 eta 0:15:13
epoch [36/50] batch [40/288] time 0.194 (0.203) data 0.000 (0.008) loss 1.5081 (1.6133) teacher_loss 0.8564 (0.8933) loss_zs_kd 0.1263 (0.1736) loss_oracle 0.5885 (0.6332) acc 78.1250 (76.3281) alaph_mean 0.3754 (0.3115) alpha_min -0.0000 (0.0000) alpha_max 0.5092 (0.5190) lr 4.6417e-04 eta 0:14:29
epoch [36/50] batch [60/288] time 0.194 (0.200) data 0.000 (0.005) loss 1.3483 (1.6107) teacher_loss 0.6489 (0.8996) loss_zs_kd 0.2344 (0.1759) loss_oracle 0.5822 (0.6232) acc 78.1250 (75.9375) alaph_mean 0.3342 (0.3165) alpha_min 0.0000 (0.0000) alpha_max 0.5134 (0.5174) lr 4.6417e-04 eta 0:14:12
epoch [36/50] batch [80/288] time 0.195 (0.199) data 0.000 (0.004) loss 1.5607 (1.5828) teacher_loss 0.9204 (0.8815) loss_zs_kd 0.1560 (0.1739) loss_oracle 0.5623 (0.6143) acc 71.8750 (76.4062) alaph_mean 0.3010 (0.3193) alpha_min 0.0000 (0.0000) alpha_max 0.5083 (0.5163) lr 4.6417e-04 eta 0:14:02
epoch [36/50] batch [100/288] time 0.199 (0.198) data 0.000 (0.003) loss 1.8031 (1.5527) teacher_loss 1.0791 (0.8526) loss_zs_kd 0.1564 (0.1743) loss_oracle 0.6459 (0.6129) acc 68.7500 (76.9688) alaph_mean 0.2986 (0.3183) alpha_min -0.0000 (0.0000) alpha_max 0.5088 (0.5148) lr 4.6417e-04 eta 0:13:54
epoch [36/50] batch [120/288] time 0.197 (0.197) data 0.000 (0.003) loss 1.1748 (1.5599) teacher_loss 0.5234 (0.8625) loss_zs_kd 0.1600 (0.1715) loss_oracle 0.5714 (0.6117) acc 90.6250 (76.8750) alaph_mean 0.3671 (0.3201) alpha_min 0.0000 (0.0000) alpha_max 0.6418 (0.5166) lr 4.6417e-04 eta 0:13:48
epoch [36/50] batch [140/288] time 0.199 (0.197) data 0.000 (0.002) loss 1.4118 (1.5680) teacher_loss 0.8706 (0.8691) loss_zs_kd 0.1584 (0.1716) loss_oracle 0.4620 (0.6130) acc 71.8750 (76.6518) alaph_mean 0.4040 (0.3189) alpha_min 0.0000 (0.0000) alpha_max 0.5375 (0.5165) lr 4.6417e-04 eta 0:13:42
epoch [36/50] batch [160/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.3834 (1.5643) teacher_loss 0.7832 (0.8623) loss_zs_kd 0.1403 (0.1740) loss_oracle 0.5300 (0.6150) acc 78.1250 (76.7383) alaph_mean 0.3435 (0.3192) alpha_min -0.0000 (0.0000) alpha_max 0.5033 (0.5159) lr 4.6417e-04 eta 0:13:37
epoch [36/50] batch [180/288] time 0.191 (0.196) data 0.000 (0.002) loss 1.9899 (1.5724) teacher_loss 1.2422 (0.8686) loss_zs_kd 0.1510 (0.1737) loss_oracle 0.6722 (0.6169) acc 71.8750 (76.6667) alaph_mean 0.2683 (0.3185) alpha_min -0.0000 (0.0000) alpha_max 0.5048 (0.5155) lr 4.6417e-04 eta 0:13:32
epoch [36/50] batch [200/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.6368 (1.5864) teacher_loss 0.9683 (0.8786) loss_zs_kd 0.1626 (0.1741) loss_oracle 0.5872 (0.6208) acc 68.7500 (76.4375) alaph_mean 0.3502 (0.3165) alpha_min 0.0000 (0.0000) alpha_max 0.5056 (0.5161) lr 4.6417e-04 eta 0:13:27
epoch [36/50] batch [220/288] time 0.192 (0.196) data 0.000 (0.002) loss 1.7776 (1.5840) teacher_loss 0.9878 (0.8788) loss_zs_kd 0.1853 (0.1732) loss_oracle 0.6971 (0.6186) acc 75.0000 (76.4205) alaph_mean 0.2853 (0.3171) alpha_min -0.0000 (0.0000) alpha_max 0.5052 (0.5185) lr 4.6417e-04 eta 0:13:22
epoch [36/50] batch [240/288] time 0.194 (0.196) data 0.000 (0.001) loss 1.4406 (1.5815) teacher_loss 0.7100 (0.8760) loss_zs_kd 0.1716 (0.1726) loss_oracle 0.6449 (0.6192) acc 81.2500 (76.4714) alaph_mean 0.2651 (0.3166) alpha_min -0.0000 (0.0000) alpha_max 0.5064 (0.5210) lr 4.6417e-04 eta 0:13:18
epoch [36/50] batch [260/288] time 0.193 (0.195) data 0.000 (0.001) loss 1.2898 (1.5770) teacher_loss 0.6255 (0.8720) loss_zs_kd 0.1686 (0.1719) loss_oracle 0.5800 (0.6191) acc 84.3750 (76.5385) alaph_mean 0.3159 (0.3171) alpha_min 0.0000 (0.0000) alpha_max 0.5037 (0.5218) lr 4.6417e-04 eta 0:13:13
epoch [36/50] batch [280/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.5620 (1.5829) teacher_loss 0.8237 (0.8760) loss_zs_kd 0.1584 (0.1734) loss_oracle 0.6590 (0.6202) acc 84.3750 (76.4286) alaph_mean 0.2578 (0.3177) alpha_min -0.0000 (0.0000) alpha_max 0.5059 (0.5222) lr 4.6417e-04 eta 0:13:09
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,429
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,030
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.1%
******* Domain a best val acc:      87.3%, epoch: 35 *******
******* Domain a best val test acc: 83.4%, epoch: 35 *******
******* Domain a best test acc:     84.0%, epoch: 23 *******
epoch [37/50] batch [20/288] time 0.194 (0.216) data 0.000 (0.016) loss 1.6548 (1.6156) teacher_loss 0.9390 (0.9030) loss_zs_kd 0.1403 (0.1851) loss_oracle 0.6457 (0.6201) acc 75.0000 (77.0312) alaph_mean 0.2813 (0.3183) alpha_min 0.0000 (0.0000) alpha_max 0.5042 (0.5334) lr 4.1221e-04 eta 0:14:27
epoch [37/50] batch [40/288] time 0.197 (0.206) data 0.000 (0.008) loss 1.6165 (1.6545) teacher_loss 0.8730 (0.9343) loss_zs_kd 0.1829 (0.1748) loss_oracle 0.6520 (0.6328) acc 78.1250 (76.1719) alaph_mean 0.3102 (0.3144) alpha_min 0.0000 (0.0000) alpha_max 0.5098 (0.5225) lr 4.1221e-04 eta 0:13:43
epoch [37/50] batch [60/288] time 0.180 (0.202) data 0.000 (0.006) loss 1.4807 (1.6261) teacher_loss 0.7021 (0.9162) loss_zs_kd 0.1867 (0.1685) loss_oracle 0.6852 (0.6256) acc 84.3750 (76.3021) alaph_mean 0.2710 (0.3150) alpha_min -0.0000 (0.0000) alpha_max 0.5046 (0.5176) lr 4.1221e-04 eta 0:13:24
epoch [37/50] batch [80/288] time 0.200 (0.201) data 0.000 (0.004) loss 1.1385 (1.6186) teacher_loss 0.5444 (0.9015) loss_zs_kd 0.1229 (0.1718) loss_oracle 0.5327 (0.6312) acc 87.5000 (76.6406) alaph_mean 0.3592 (0.3125) alpha_min -0.0000 (0.0000) alpha_max 0.5026 (0.5219) lr 4.1221e-04 eta 0:13:15
epoch [37/50] batch [100/288] time 0.214 (0.201) data 0.000 (0.003) loss 1.4512 (1.6020) teacher_loss 0.8193 (0.8864) loss_zs_kd 0.1526 (0.1716) loss_oracle 0.5556 (0.6298) acc 78.1250 (77.0625) alaph_mean 0.3315 (0.3139) alpha_min 0.0000 (0.0000) alpha_max 0.5032 (0.5241) lr 4.1221e-04 eta 0:13:11
epoch [37/50] batch [120/288] time 0.200 (0.200) data 0.000 (0.003) loss 1.7771 (1.6182) teacher_loss 1.2051 (0.9019) loss_zs_kd 0.1118 (0.1728) loss_oracle 0.5161 (0.6299) acc 71.8750 (76.4062) alaph_mean 0.3617 (0.3132) alpha_min -0.0000 (0.0000) alpha_max 0.5033 (0.5240) lr 4.1221e-04 eta 0:13:01
epoch [37/50] batch [140/288] time 0.191 (0.199) data 0.000 (0.003) loss 1.8034 (1.6182) teacher_loss 1.0137 (0.9028) loss_zs_kd 0.1318 (0.1730) loss_oracle 0.7239 (0.6289) acc 75.0000 (76.3616) alaph_mean 0.2553 (0.3130) alpha_min -0.0000 (0.0000) alpha_max 0.5984 (0.5255) lr 4.1221e-04 eta 0:12:54
epoch [37/50] batch [160/288] time 0.189 (0.198) data 0.000 (0.002) loss 1.5640 (1.6097) teacher_loss 0.7739 (0.8955) loss_zs_kd 0.1432 (0.1736) loss_oracle 0.7185 (0.6274) acc 84.3750 (76.6211) alaph_mean 0.2449 (0.3149) alpha_min 0.0000 (0.0000) alpha_max 0.5021 (0.5270) lr 4.1221e-04 eta 0:12:47
epoch [37/50] batch [180/288] time 0.192 (0.198) data 0.000 (0.002) loss 1.5906 (1.6004) teacher_loss 0.7715 (0.8831) loss_zs_kd 0.1824 (0.1733) loss_oracle 0.7280 (0.6307) acc 81.2500 (76.9444) alaph_mean 0.2971 (0.3149) alpha_min 0.0000 (0.0000) alpha_max 0.5083 (0.5278) lr 4.1221e-04 eta 0:12:41
epoch [37/50] batch [200/288] time 0.203 (0.197) data 0.000 (0.002) loss 1.5029 (1.6057) teacher_loss 0.8174 (0.8839) loss_zs_kd 0.1643 (0.1737) loss_oracle 0.6034 (0.6350) acc 78.1250 (76.9531) alaph_mean 0.3437 (0.3137) alpha_min 0.0000 (0.0000) alpha_max 0.5049 (0.5270) lr 4.1221e-04 eta 0:12:36
epoch [37/50] batch [220/288] time 0.194 (0.197) data 0.000 (0.002) loss 1.9713 (1.6090) teacher_loss 1.1406 (0.8869) loss_zs_kd 0.2433 (0.1740) loss_oracle 0.7090 (0.6351) acc 71.8750 (76.7330) alaph_mean 0.3294 (0.3147) alpha_min 0.0000 (0.0000) alpha_max 0.5151 (0.5284) lr 4.1221e-04 eta 0:12:31
epoch [37/50] batch [240/288] time 0.195 (0.197) data 0.000 (0.002) loss 1.2745 (1.6101) teacher_loss 0.6855 (0.8872) loss_zs_kd 0.1969 (0.1739) loss_oracle 0.4905 (0.6359) acc 84.3750 (76.6667) alaph_mean 0.3886 (0.3142) alpha_min 0.0000 (0.0000) alpha_max 0.5058 (0.5276) lr 4.1221e-04 eta 0:12:26
epoch [37/50] batch [260/288] time 0.195 (0.197) data 0.000 (0.001) loss 1.8016 (1.6160) teacher_loss 1.0400 (0.8926) loss_zs_kd 0.1990 (0.1741) loss_oracle 0.6620 (0.6363) acc 75.0000 (76.3942) alaph_mean 0.3072 (0.3144) alpha_min -0.0000 (0.0000) alpha_max 0.5041 (0.5288) lr 4.1221e-04 eta 0:12:21
epoch [37/50] batch [280/288] time 0.082 (0.196) data 0.000 (0.001) loss 1.4010 (1.6143) teacher_loss 0.7031 (0.8886) loss_zs_kd 0.1757 (0.1745) loss_oracle 0.6100 (0.6384) acc 84.3750 (76.5513) alaph_mean 0.3147 (0.3129) alpha_min 0.0000 (0.0000) alpha_max 0.5574 (0.5276) lr 4.1221e-04 eta 0:12:15
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,429
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,040
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 80.5%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      87.3%, epoch: 35 *******
******* Domain a best val test acc: 83.4%, epoch: 35 *******
******* Domain a best test acc:     84.1%, epoch: 37 *******
epoch [38/50] batch [20/288] time 0.198 (0.208) data 0.000 (0.012) loss 1.7545 (1.6779) teacher_loss 1.0918 (0.9578) loss_zs_kd 0.1277 (0.1741) loss_oracle 0.5989 (0.6330) acc 65.6250 (73.7500) alaph_mean 0.3439 (0.3093) alpha_min 0.0000 (0.0000) alpha_max 0.5055 (0.5363) lr 3.6258e-04 eta 0:12:54
epoch [38/50] batch [40/288] time 0.195 (0.201) data 0.000 (0.006) loss 1.0574 (1.6043) teacher_loss 0.3159 (0.8772) loss_zs_kd 0.2924 (0.1739) loss_oracle 0.5954 (0.6402) acc 87.5000 (75.7031) alaph_mean 0.3307 (0.3066) alpha_min 0.0000 (0.0000) alpha_max 0.5105 (0.5350) lr 3.6258e-04 eta 0:12:23
epoch [38/50] batch [60/288] time 0.195 (0.199) data 0.000 (0.004) loss 1.3487 (1.6097) teacher_loss 0.6143 (0.8765) loss_zs_kd 0.1909 (0.1741) loss_oracle 0.6390 (0.6461) acc 84.3750 (76.3021) alaph_mean 0.3143 (0.3058) alpha_min 0.0000 (0.0000) alpha_max 0.5105 (0.5318) lr 3.6258e-04 eta 0:12:11
epoch [38/50] batch [80/288] time 0.197 (0.197) data 0.000 (0.003) loss 1.7444 (1.6236) teacher_loss 1.0234 (0.8897) loss_zs_kd 0.1215 (0.1739) loss_oracle 0.6602 (0.6469) acc 75.0000 (75.8594) alaph_mean 0.3127 (0.3067) alpha_min 0.0000 (0.0000) alpha_max 0.5425 (0.5269) lr 3.6258e-04 eta 0:12:03
epoch [38/50] batch [100/288] time 0.193 (0.197) data 0.000 (0.003) loss 1.6662 (1.6379) teacher_loss 0.9131 (0.9067) loss_zs_kd 0.1400 (0.1726) loss_oracle 0.6832 (0.6449) acc 78.1250 (75.6875) alaph_mean 0.2640 (0.3085) alpha_min 0.0000 (0.0000) alpha_max 0.5072 (0.5237) lr 3.6258e-04 eta 0:11:56
epoch [38/50] batch [120/288] time 0.201 (0.196) data 0.000 (0.002) loss 1.9607 (1.6478) teacher_loss 1.3262 (0.9154) loss_zs_kd 0.1943 (0.1709) loss_oracle 0.5374 (0.6470) acc 62.5000 (75.1823) alaph_mean 0.3910 (0.3075) alpha_min 0.0000 (0.0000) alpha_max 0.5193 (0.5227) lr 3.6258e-04 eta 0:11:51
epoch [38/50] batch [140/288] time 0.198 (0.196) data 0.000 (0.002) loss 1.5994 (1.6564) teacher_loss 0.8354 (0.9228) loss_zs_kd 0.1658 (0.1729) loss_oracle 0.6810 (0.6472) acc 78.1250 (74.9777) alaph_mean 0.2822 (0.3083) alpha_min 0.0000 (0.0000) alpha_max 0.5068 (0.5227) lr 3.6258e-04 eta 0:11:46
epoch [38/50] batch [160/288] time 0.191 (0.196) data 0.000 (0.002) loss 1.2827 (1.6648) teacher_loss 0.5854 (0.9317) loss_zs_kd 0.1223 (0.1741) loss_oracle 0.6361 (0.6461) acc 87.5000 (74.6875) alaph_mean 0.3433 (0.3113) alpha_min 0.0000 (0.0000) alpha_max 0.5053 (0.5225) lr 3.6258e-04 eta 0:11:41
epoch [38/50] batch [180/288] time 0.197 (0.196) data 0.000 (0.002) loss 1.8908 (1.6746) teacher_loss 1.0332 (0.9399) loss_zs_kd 0.2195 (0.1766) loss_oracle 0.7479 (0.6464) acc 75.0000 (74.7396) alaph_mean 0.2429 (0.3121) alpha_min -0.0000 (0.0000) alpha_max 0.5113 (0.5219) lr 3.6258e-04 eta 0:11:37
epoch [38/50] batch [200/288] time 0.193 (0.195) data 0.000 (0.001) loss 1.3392 (1.6587) teacher_loss 0.6353 (0.9271) loss_zs_kd 0.1235 (0.1753) loss_oracle 0.6423 (0.6440) acc 75.0000 (74.9844) alaph_mean 0.3089 (0.3135) alpha_min 0.0000 (0.0000) alpha_max 0.5079 (0.5232) lr 3.6258e-04 eta 0:11:32
epoch [38/50] batch [220/288] time 0.192 (0.195) data 0.000 (0.001) loss 1.3776 (1.6552) teacher_loss 0.4897 (0.9230) loss_zs_kd 0.1737 (0.1749) loss_oracle 0.8010 (0.6447) acc 87.5000 (75.0994) alaph_mean 0.2380 (0.3142) alpha_min -0.0000 (0.0000) alpha_max 0.5071 (0.5242) lr 3.6258e-04 eta 0:11:28
epoch [38/50] batch [240/288] time 0.191 (0.195) data 0.000 (0.001) loss 1.2630 (1.6547) teacher_loss 0.4912 (0.9222) loss_zs_kd 0.1722 (0.1759) loss_oracle 0.6857 (0.6446) acc 90.6250 (75.2604) alaph_mean 0.2837 (0.3144) alpha_min -0.0000 (0.0000) alpha_max 0.5062 (0.5247) lr 3.6258e-04 eta 0:11:24
epoch [38/50] batch [260/288] time 0.193 (0.195) data 0.000 (0.001) loss 1.6928 (1.6542) teacher_loss 0.9219 (0.9208) loss_zs_kd 0.1391 (0.1754) loss_oracle 0.7014 (0.6457) acc 81.2500 (75.3486) alaph_mean 0.2657 (0.3131) alpha_min 0.0000 (0.0000) alpha_max 0.5084 (0.5252) lr 3.6258e-04 eta 0:11:20
epoch [38/50] batch [280/288] time 0.086 (0.194) data 0.000 (0.001) loss 1.2451 (1.6470) teacher_loss 0.5825 (0.9137) loss_zs_kd 0.1495 (0.1746) loss_oracle 0.5878 (0.6459) acc 84.3750 (75.5246) alaph_mean 0.2893 (0.3132) alpha_min -0.0000 (0.0000) alpha_max 0.5056 (0.5270) lr 3.6258e-04 eta 0:11:12
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,434
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,040
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 80.6%
******* Domain a best val acc:      87.3%, epoch: 35 *******
******* Domain a best val test acc: 83.4%, epoch: 35 *******
******* Domain a best test acc:     84.1%, epoch: 37 *******
epoch [39/50] batch [20/288] time 0.192 (0.216) data 0.000 (0.017) loss 1.7086 (1.5888) teacher_loss 0.8564 (0.8486) loss_zs_kd 0.1536 (0.1651) loss_oracle 0.7753 (0.6577) acc 75.0000 (74.8438) alaph_mean 0.2813 (0.3153) alpha_min -0.0000 (0.0000) alpha_max 0.5141 (0.5364) lr 3.1545e-04 eta 0:12:21
epoch [39/50] batch [40/288] time 0.192 (0.205) data 0.000 (0.009) loss 1.7406 (1.6151) teacher_loss 0.9990 (0.8822) loss_zs_kd 0.2153 (0.1759) loss_oracle 0.6340 (0.6450) acc 68.7500 (74.8438) alaph_mean 0.3128 (0.3211) alpha_min 0.0000 (0.0000) alpha_max 0.5069 (0.5261) lr 3.1545e-04 eta 0:11:38
epoch [39/50] batch [60/288] time 0.190 (0.201) data 0.000 (0.006) loss 1.4521 (1.6086) teacher_loss 0.7354 (0.8773) loss_zs_kd 0.1210 (0.1729) loss_oracle 0.6563 (0.6449) acc 75.0000 (74.5833) alaph_mean 0.3188 (0.3227) alpha_min 0.0000 (0.0000) alpha_max 0.5084 (0.5219) lr 3.1545e-04 eta 0:11:23
epoch [39/50] batch [80/288] time 0.199 (0.200) data 0.000 (0.004) loss 1.8458 (1.6119) teacher_loss 1.2715 (0.8806) loss_zs_kd 0.1608 (0.1739) loss_oracle 0.4939 (0.6443) acc 59.3750 (74.7656) alaph_mean 0.4006 (0.3196) alpha_min 0.0000 (0.0000) alpha_max 0.7524 (0.5255) lr 3.1545e-04 eta 0:11:14
epoch [39/50] batch [100/288] time 0.192 (0.199) data 0.000 (0.004) loss 1.3424 (1.6122) teacher_loss 0.8701 (0.8852) loss_zs_kd 0.1527 (0.1728) loss_oracle 0.3960 (0.6405) acc 75.0000 (74.6875) alaph_mean 0.4160 (0.3195) alpha_min 0.0000 (0.0000) alpha_max 0.5037 (0.5246) lr 3.1545e-04 eta 0:11:07
epoch [39/50] batch [120/288] time 0.190 (0.198) data 0.000 (0.003) loss 1.2751 (1.6219) teacher_loss 0.5874 (0.8921) loss_zs_kd 0.1217 (0.1725) loss_oracle 0.6269 (0.6436) acc 84.3750 (74.8177) alaph_mean 0.3237 (0.3186) alpha_min -0.0000 (0.0000) alpha_max 0.5117 (0.5246) lr 3.1545e-04 eta 0:10:59
epoch [39/50] batch [140/288] time 0.186 (0.197) data 0.000 (0.003) loss 1.7898 (1.6164) teacher_loss 1.0449 (0.8914) loss_zs_kd 0.2038 (0.1760) loss_oracle 0.6430 (0.6370) acc 65.6250 (75.0670) alaph_mean 0.3282 (0.3215) alpha_min 0.0000 (0.0000) alpha_max 0.5090 (0.5231) lr 3.1545e-04 eta 0:10:54
epoch [39/50] batch [160/288] time 0.193 (0.197) data 0.000 (0.002) loss 1.6999 (1.6074) teacher_loss 0.9868 (0.8869) loss_zs_kd 0.1860 (0.1762) loss_oracle 0.6201 (0.6323) acc 78.1250 (75.3516) alaph_mean 0.2946 (0.3243) alpha_min 0.0000 (0.0000) alpha_max 0.6481 (0.5237) lr 3.1545e-04 eta 0:10:49
epoch [39/50] batch [180/288] time 0.198 (0.197) data 0.000 (0.002) loss 1.3809 (1.6037) teacher_loss 0.6543 (0.8828) loss_zs_kd 0.2101 (0.1753) loss_oracle 0.6215 (0.6333) acc 87.5000 (75.4167) alaph_mean 0.3447 (0.3239) alpha_min 0.0000 (0.0000) alpha_max 0.5046 (0.5222) lr 3.1545e-04 eta 0:10:44
epoch [39/50] batch [200/288] time 0.193 (0.197) data 0.000 (0.002) loss 1.5675 (1.6169) teacher_loss 0.6797 (0.8944) loss_zs_kd 0.1964 (0.1759) loss_oracle 0.7896 (0.6345) acc 78.1250 (75.2188) alaph_mean 0.2191 (0.3209) alpha_min -0.0000 (0.0000) alpha_max 0.5108 (0.5251) lr 3.1545e-04 eta 0:10:40
epoch [39/50] batch [220/288] time 0.193 (0.197) data 0.000 (0.002) loss 1.9646 (1.6141) teacher_loss 1.1777 (0.8943) loss_zs_kd 0.1829 (0.1755) loss_oracle 0.6954 (0.6321) acc 78.1250 (75.4972) alaph_mean 0.3262 (0.3214) alpha_min -0.0000 (0.0000) alpha_max 0.5146 (0.5259) lr 3.1545e-04 eta 0:10:36
epoch [39/50] batch [240/288] time 0.186 (0.197) data 0.000 (0.002) loss 1.5919 (1.6155) teacher_loss 0.7607 (0.8960) loss_zs_kd 0.2084 (0.1775) loss_oracle 0.7270 (0.6308) acc 68.7500 (75.2865) alaph_mean 0.2332 (0.3209) alpha_min 0.0000 (0.0000) alpha_max 0.5043 (0.5255) lr 3.1545e-04 eta 0:10:32
epoch [39/50] batch [260/288] time 0.193 (0.197) data 0.000 (0.002) loss 1.1740 (1.6137) teacher_loss 0.5142 (0.8959) loss_zs_kd 0.1237 (0.1767) loss_oracle 0.5979 (0.6294) acc 87.5000 (75.2885) alaph_mean 0.2976 (0.3213) alpha_min 0.0000 (0.0000) alpha_max 0.5086 (0.5262) lr 3.1545e-04 eta 0:10:29
epoch [39/50] batch [280/288] time 0.472 (0.198) data 0.001 (0.001) loss 1.5761 (1.6099) teacher_loss 0.7983 (0.8940) loss_zs_kd 0.1544 (0.1767) loss_oracle 0.7005 (0.6275) acc 75.0000 (75.3571) alaph_mean 0.2832 (0.3215) alpha_min -0.0000 (0.0000) alpha_max 0.5051 (0.5258) lr 3.1545e-04 eta 0:10:28
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,426
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,031
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 80.1%
******* Domain a best val acc:      87.3%, epoch: 35 *******
******* Domain a best val test acc: 83.4%, epoch: 35 *******
******* Domain a best test acc:     84.1%, epoch: 37 *******
epoch [40/50] batch [20/288] time 0.191 (0.217) data 0.000 (0.019) loss 1.6750 (1.6143) teacher_loss 0.7900 (0.8886) loss_zs_kd 0.1640 (0.1713) loss_oracle 0.8029 (0.6400) acc 81.2500 (76.2500) alaph_mean 0.2191 (0.3011) alpha_min -0.0000 (0.0000) alpha_max 0.5077 (0.5270) lr 2.7103e-04 eta 0:11:23
epoch [40/50] batch [40/288] time 0.192 (0.206) data 0.000 (0.010) loss 2.1322 (1.6394) teacher_loss 1.2578 (0.9128) loss_zs_kd 0.2387 (0.1797) loss_oracle 0.7551 (0.6367) acc 68.7500 (75.5469) alaph_mean 0.2349 (0.3063) alpha_min -0.0000 (0.0000) alpha_max 0.5179 (0.5308) lr 2.7103e-04 eta 0:10:44
epoch [40/50] batch [60/288] time 0.190 (0.201) data 0.000 (0.007) loss 1.7174 (1.6129) teacher_loss 0.7739 (0.8906) loss_zs_kd 0.1925 (0.1766) loss_oracle 0.8472 (0.6341) acc 81.2500 (76.3542) alaph_mean 0.1946 (0.3092) alpha_min -0.0000 (0.0000) alpha_max 0.5114 (0.5250) lr 2.7103e-04 eta 0:10:23
epoch [40/50] batch [80/288] time 0.193 (0.198) data 0.000 (0.005) loss 1.9580 (1.6203) teacher_loss 1.2041 (0.8968) loss_zs_kd 0.2013 (0.1788) loss_oracle 0.6533 (0.6342) acc 75.0000 (76.2891) alaph_mean 0.2984 (0.3104) alpha_min 0.0000 (0.0000) alpha_max 0.5155 (0.5303) lr 2.7103e-04 eta 0:10:11
epoch [40/50] batch [100/288] time 0.195 (0.197) data 0.000 (0.004) loss 1.5679 (1.6221) teacher_loss 0.8491 (0.9026) loss_zs_kd 0.1462 (0.1764) loss_oracle 0.6456 (0.6313) acc 78.1250 (75.8750) alaph_mean 0.3131 (0.3111) alpha_min 0.0000 (0.0000) alpha_max 0.5091 (0.5303) lr 2.7103e-04 eta 0:10:05
epoch [40/50] batch [120/288] time 0.194 (0.197) data 0.000 (0.003) loss 1.6956 (1.6134) teacher_loss 0.9609 (0.9004) loss_zs_kd 0.2062 (0.1767) loss_oracle 0.6316 (0.6246) acc 75.0000 (76.1198) alaph_mean 0.3142 (0.3151) alpha_min -0.0000 (0.0000) alpha_max 0.5112 (0.5277) lr 2.7103e-04 eta 0:10:00
epoch [40/50] batch [140/288] time 0.195 (0.196) data 0.000 (0.003) loss 1.7119 (1.6187) teacher_loss 0.9385 (0.9039) loss_zs_kd 0.1613 (0.1758) loss_oracle 0.6927 (0.6269) acc 68.7500 (75.8705) alaph_mean 0.2843 (0.3137) alpha_min 0.0000 (0.0000) alpha_max 0.5038 (0.5306) lr 2.7103e-04 eta 0:09:54
epoch [40/50] batch [160/288] time 0.189 (0.196) data 0.000 (0.003) loss 1.9249 (1.6147) teacher_loss 1.2178 (0.9047) loss_zs_kd 0.2002 (0.1762) loss_oracle 0.6070 (0.6219) acc 78.1250 (75.9570) alaph_mean 0.3213 (0.3170) alpha_min 0.0000 (0.0000) alpha_max 0.5109 (0.5288) lr 2.7103e-04 eta 0:09:50
epoch [40/50] batch [180/288] time 0.189 (0.196) data 0.000 (0.002) loss 1.4148 (1.6267) teacher_loss 0.7075 (0.9169) loss_zs_kd 0.1513 (0.1761) loss_oracle 0.6316 (0.6218) acc 81.2500 (75.7812) alaph_mean 0.3067 (0.3161) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5269) lr 2.7103e-04 eta 0:09:46
epoch [40/50] batch [200/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.6048 (1.6178) teacher_loss 0.8613 (0.9099) loss_zs_kd 0.2254 (0.1751) loss_oracle 0.6308 (0.6204) acc 68.7500 (76.0625) alaph_mean 0.3062 (0.3170) alpha_min 0.0000 (0.0000) alpha_max 0.5082 (0.5259) lr 2.7103e-04 eta 0:09:42
epoch [40/50] batch [220/288] time 0.205 (0.196) data 0.000 (0.002) loss 1.3613 (1.6144) teacher_loss 0.7510 (0.9089) loss_zs_kd 0.0788 (0.1754) loss_oracle 0.5709 (0.6178) acc 81.2500 (76.0795) alaph_mean 0.3439 (0.3185) alpha_min -0.0000 (0.0000) alpha_max 0.5086 (0.5250) lr 2.7103e-04 eta 0:09:37
epoch [40/50] batch [240/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.2811 (1.6033) teacher_loss 0.5381 (0.8996) loss_zs_kd 0.2074 (0.1746) loss_oracle 0.6393 (0.6164) acc 78.1250 (76.3281) alaph_mean 0.3017 (0.3193) alpha_min 0.0000 (0.0000) alpha_max 0.5105 (0.5237) lr 2.7103e-04 eta 0:09:33
epoch [40/50] batch [260/288] time 0.185 (0.196) data 0.000 (0.002) loss 1.8331 (1.5984) teacher_loss 1.0742 (0.8938) loss_zs_kd 0.2083 (0.1758) loss_oracle 0.6547 (0.6166) acc 75.0000 (76.4784) alaph_mean 0.2805 (0.3193) alpha_min 0.0000 (0.0000) alpha_max 0.5030 (0.5248) lr 2.7103e-04 eta 0:09:29
epoch [40/50] batch [280/288] time 0.469 (0.195) data 0.000 (0.002) loss 1.8369 (1.5967) teacher_loss 1.1113 (0.8915) loss_zs_kd 0.2238 (0.1750) loss_oracle 0.6136 (0.6177) acc 71.8750 (76.5513) alaph_mean 0.3409 (0.3182) alpha_min 0.0000 (0.0000) alpha_max 0.7274 (0.5267) lr 2.7103e-04 eta 0:09:24
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,431
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,041
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 80.4%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_teacherlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      87.3%, epoch: 35 *******
******* Domain a best val test acc: 83.4%, epoch: 35 *******
******* Domain a best test acc:     84.1%, epoch: 40 *******
epoch [41/50] batch [20/288] time 0.192 (0.213) data 0.000 (0.017) loss 1.3947 (1.6274) teacher_loss 0.7393 (0.8917) loss_zs_kd 0.1756 (0.1919) loss_oracle 0.5677 (0.6397) acc 78.1250 (74.8438) alaph_mean 0.3370 (0.3049) alpha_min -0.0000 (0.0000) alpha_max 0.5086 (0.5248) lr 2.2949e-04 eta 0:10:09
epoch [41/50] batch [40/288] time 0.157 (0.204) data 0.000 (0.009) loss 1.5476 (1.6125) teacher_loss 0.7729 (0.8976) loss_zs_kd 0.1800 (0.1780) loss_oracle 0.6846 (0.6258) acc 78.1250 (74.7656) alaph_mean 0.2497 (0.3113) alpha_min -0.0000 (0.0000) alpha_max 0.5020 (0.5219) lr 2.2949e-04 eta 0:09:38
epoch [41/50] batch [60/288] time 0.198 (0.200) data 0.000 (0.006) loss 1.5980 (1.6122) teacher_loss 1.0918 (0.9023) loss_zs_kd 0.1880 (0.1727) loss_oracle 0.4121 (0.6236) acc 71.8750 (75.2083) alaph_mean 0.4315 (0.3146) alpha_min 0.0000 (0.0000) alpha_max 0.5062 (0.5226) lr 2.2949e-04 eta 0:09:24
epoch [41/50] batch [80/288] time 0.195 (0.198) data 0.000 (0.004) loss 1.2568 (1.6220) teacher_loss 0.5391 (0.9047) loss_zs_kd 0.1436 (0.1715) loss_oracle 0.6460 (0.6315) acc 78.1250 (75.4688) alaph_mean 0.3127 (0.3121) alpha_min 0.0000 (0.0000) alpha_max 0.5066 (0.5237) lr 2.2949e-04 eta 0:09:15
epoch [41/50] batch [100/288] time 0.190 (0.198) data 0.000 (0.004) loss 2.1748 (1.6228) teacher_loss 1.2441 (0.9045) loss_zs_kd 0.2135 (0.1744) loss_oracle 0.8239 (0.6312) acc 65.6250 (75.4688) alaph_mean 0.2100 (0.3141) alpha_min -0.0000 (0.0000) alpha_max 0.7106 (0.5267) lr 2.2949e-04 eta 0:09:09
epoch [41/50] batch [120/288] time 0.199 (0.197) data 0.000 (0.003) loss 1.8516 (1.6246) teacher_loss 1.2031 (0.9043) loss_zs_kd 0.2006 (0.1742) loss_oracle 0.5481 (0.6332) acc 68.7500 (75.5729) alaph_mean 0.3460 (0.3139) alpha_min -0.0000 (0.0000) alpha_max 0.5106 (0.5273) lr 2.2949e-04 eta 0:09:03
epoch [41/50] batch [140/288] time 0.196 (0.197) data 0.000 (0.003) loss 1.6947 (1.6166) teacher_loss 1.0654 (0.8977) loss_zs_kd 0.1471 (0.1740) loss_oracle 0.5557 (0.6320) acc 71.8750 (75.9821) alaph_mean 0.3750 (0.3133) alpha_min -0.0000 (0.0000) alpha_max 0.5079 (0.5245) lr 2.2949e-04 eta 0:08:58
epoch [41/50] batch [160/288] time 0.188 (0.196) data 0.000 (0.002) loss 1.4043 (1.6065) teacher_loss 0.6685 (0.8888) loss_zs_kd 0.1077 (0.1725) loss_oracle 0.6820 (0.6315) acc 81.2500 (76.1133) alaph_mean 0.2416 (0.3128) alpha_min 0.0000 (0.0000) alpha_max 0.5586 (0.5242) lr 2.2949e-04 eta 0:08:54
epoch [41/50] batch [180/288] time 0.194 (0.196) data 0.000 (0.002) loss 2.3349 (1.6072) teacher_loss 1.6338 (0.8876) loss_zs_kd 0.2685 (0.1733) loss_oracle 0.5668 (0.6330) acc 62.5000 (76.1979) alaph_mean 0.3158 (0.3135) alpha_min 0.0000 (0.0000) alpha_max 0.5286 (0.5242) lr 2.2949e-04 eta 0:08:49
epoch [41/50] batch [200/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.6423 (1.6075) teacher_loss 0.9268 (0.8887) loss_zs_kd 0.1656 (0.1738) loss_oracle 0.6327 (0.6319) acc 75.0000 (76.2500) alaph_mean 0.2800 (0.3133) alpha_min -0.0000 (0.0000) alpha_max 0.5070 (0.5226) lr 2.2949e-04 eta 0:08:45
epoch [41/50] batch [220/288] time 0.200 (0.196) data 0.000 (0.002) loss 1.5586 (1.6060) teacher_loss 0.8789 (0.8863) loss_zs_kd 0.2225 (0.1746) loss_oracle 0.5684 (0.6324) acc 78.1250 (76.4489) alaph_mean 0.3434 (0.3134) alpha_min 0.0000 (0.0000) alpha_max 0.5028 (0.5219) lr 2.2949e-04 eta 0:08:40
epoch [41/50] batch [240/288] time 0.196 (0.196) data 0.000 (0.002) loss 1.6653 (1.6041) teacher_loss 0.8945 (0.8858) loss_zs_kd 0.1990 (0.1734) loss_oracle 0.6712 (0.6316) acc 78.1250 (76.4974) alaph_mean 0.2970 (0.3137) alpha_min 0.0000 (0.0000) alpha_max 0.5057 (0.5223) lr 2.2949e-04 eta 0:08:36
epoch [41/50] batch [260/288] time 0.194 (0.195) data 0.000 (0.001) loss 2.1867 (1.6076) teacher_loss 1.5137 (0.8871) loss_zs_kd 0.2201 (0.1738) loss_oracle 0.5630 (0.6336) acc 62.5000 (76.4183) alaph_mean 0.3285 (0.3135) alpha_min 0.0000 (0.0000) alpha_max 0.5080 (0.5228) lr 2.2949e-04 eta 0:08:32
epoch [41/50] batch [280/288] time 0.475 (0.200) data 0.000 (0.001) loss 1.5196 (1.6096) teacher_loss 0.7246 (0.8886) loss_zs_kd 0.1502 (0.1737) loss_oracle 0.7199 (0.6342) acc 81.2500 (76.4062) alaph_mean 0.2504 (0.3139) alpha_min -0.0000 (0.0000) alpha_max 0.5088 (0.5245) lr 2.2949e-04 eta 0:08:40
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,428
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,030
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.0%
******* Domain a best val acc:      87.3%, epoch: 35 *******
******* Domain a best val test acc: 83.4%, epoch: 35 *******
******* Domain a best test acc:     84.1%, epoch: 40 *******
epoch [42/50] batch [20/288] time 0.107 (0.162) data 0.000 (0.016) loss 1.4908 (1.5862) teacher_loss 0.7363 (0.8660) loss_zs_kd 0.1458 (0.1614) loss_oracle 0.6816 (0.6395) acc 78.1250 (76.8750) alaph_mean 0.3121 (0.3317) alpha_min 0.0000 (0.0000) alpha_max 0.5072 (0.5303) lr 1.9098e-04 eta 0:06:55
epoch [42/50] batch [40/288] time 0.095 (0.128) data 0.000 (0.008) loss 1.5620 (1.6098) teacher_loss 0.8330 (0.8964) loss_zs_kd 0.1657 (0.1658) loss_oracle 0.6462 (0.6305) acc 75.0000 (76.0938) alaph_mean 0.3094 (0.3320) alpha_min -0.0000 (0.0000) alpha_max 0.5082 (0.5347) lr 1.9098e-04 eta 0:05:27
epoch [42/50] batch [60/288] time 0.095 (0.118) data 0.001 (0.006) loss 1.9841 (1.6403) teacher_loss 1.2139 (0.9138) loss_zs_kd 0.1665 (0.1730) loss_oracle 0.6870 (0.6400) acc 75.0000 (75.5729) alaph_mean 0.3123 (0.3262) alpha_min 0.0000 (0.0000) alpha_max 0.5078 (0.5415) lr 1.9098e-04 eta 0:04:58
epoch [42/50] batch [80/288] time 0.095 (0.112) data 0.000 (0.004) loss 1.4501 (1.6264) teacher_loss 0.6489 (0.8956) loss_zs_kd 0.1743 (0.1753) loss_oracle 0.7140 (0.6432) acc 78.1250 (75.3125) alaph_mean 0.2814 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5351) lr 1.9098e-04 eta 0:04:41
epoch [42/50] batch [100/288] time 0.101 (0.109) data 0.000 (0.003) loss 1.6303 (1.6377) teacher_loss 0.8013 (0.9099) loss_zs_kd 0.1852 (0.1781) loss_oracle 0.7364 (0.6388) acc 81.2500 (75.3438) alaph_mean 0.2734 (0.3199) alpha_min 0.0000 (0.0000) alpha_max 0.5069 (0.5319) lr 1.9098e-04 eta 0:04:31
epoch [42/50] batch [120/288] time 0.093 (0.107) data 0.000 (0.003) loss 1.4838 (1.6122) teacher_loss 0.7212 (0.8853) loss_zs_kd 0.1790 (0.1744) loss_oracle 0.6731 (0.6397) acc 75.0000 (75.9115) alaph_mean 0.2808 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5092 (0.5279) lr 1.9098e-04 eta 0:04:23
epoch [42/50] batch [140/288] time 0.094 (0.105) data 0.000 (0.003) loss 1.8910 (1.6064) teacher_loss 1.0352 (0.8810) loss_zs_kd 0.1738 (0.1763) loss_oracle 0.7690 (0.6372) acc 62.5000 (76.0268) alaph_mean 0.2277 (0.3183) alpha_min 0.0000 (0.0000) alpha_max 0.5056 (0.5305) lr 1.9098e-04 eta 0:04:17
epoch [42/50] batch [160/288] time 0.099 (0.104) data 0.000 (0.002) loss 1.7903 (1.6108) teacher_loss 1.0664 (0.8828) loss_zs_kd 0.2208 (0.1775) loss_oracle 0.6135 (0.6392) acc 71.8750 (75.9570) alaph_mean 0.3442 (0.3175) alpha_min 0.0000 (0.0000) alpha_max 0.5116 (0.5331) lr 1.9098e-04 eta 0:04:12
epoch [42/50] batch [180/288] time 0.090 (0.103) data 0.000 (0.002) loss 1.6039 (1.6045) teacher_loss 0.9531 (0.8780) loss_zs_kd 0.2419 (0.1780) loss_oracle 0.5298 (0.6375) acc 78.1250 (76.0938) alaph_mean 0.3520 (0.3182) alpha_min 0.0000 (0.0000) alpha_max 0.5048 (0.5341) lr 1.9098e-04 eta 0:04:08
epoch [42/50] batch [200/288] time 0.092 (0.102) data 0.000 (0.002) loss 1.8799 (1.6058) teacher_loss 0.9780 (0.8786) loss_zs_kd 0.1379 (0.1780) loss_oracle 0.8330 (0.6381) acc 71.8750 (76.1875) alaph_mean 0.1749 (0.3177) alpha_min -0.0000 (0.0000) alpha_max 0.5058 (0.5340) lr 1.9098e-04 eta 0:04:04
epoch [42/50] batch [220/288] time 0.093 (0.102) data 0.000 (0.002) loss 1.3803 (1.6076) teacher_loss 0.8984 (0.8818) loss_zs_kd 0.1434 (0.1775) loss_oracle 0.4101 (0.6370) acc 78.1250 (76.0938) alaph_mean 0.4378 (0.3180) alpha_min 0.0000 (0.0000) alpha_max 0.5138 (0.5337) lr 1.9098e-04 eta 0:04:00
epoch [42/50] batch [240/288] time 0.096 (0.102) data 0.000 (0.002) loss 1.7505 (1.6059) teacher_loss 0.9800 (0.8817) loss_zs_kd 0.1639 (0.1765) loss_oracle 0.6886 (0.6359) acc 75.0000 (76.1849) alaph_mean 0.3123 (0.3182) alpha_min 0.0000 (0.0000) alpha_max 0.5092 (0.5340) lr 1.9098e-04 eta 0:03:59
epoch [42/50] batch [260/288] time 0.098 (0.102) data 0.000 (0.001) loss 1.9872 (1.6109) teacher_loss 1.1865 (0.8874) loss_zs_kd 0.1706 (0.1763) loss_oracle 0.7153 (0.6353) acc 75.0000 (76.0817) alaph_mean 0.2479 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5075 (0.5333) lr 1.9098e-04 eta 0:03:57
epoch [42/50] batch [280/288] time 0.085 (0.101) data 0.000 (0.001) loss 1.7069 (1.6052) teacher_loss 0.8408 (0.8827) loss_zs_kd 0.1391 (0.1752) loss_oracle 0.7965 (0.6349) acc 84.3750 (76.3170) alaph_mean 0.2190 (0.3165) alpha_min -0.0000 (0.0000) alpha_max 0.5095 (0.5330) lr 1.9098e-04 eta 0:03:53
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,427
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,029
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.0%
******* Domain a best val acc:      87.3%, epoch: 35 *******
******* Domain a best val test acc: 83.4%, epoch: 35 *******
******* Domain a best test acc:     84.1%, epoch: 40 *******
epoch [43/50] batch [20/288] time 0.093 (0.107) data 0.000 (0.015) loss 1.8323 (1.6876) teacher_loss 1.1367 (0.9415) loss_zs_kd 0.1503 (0.1775) loss_oracle 0.6204 (0.6573) acc 68.7500 (74.5312) alaph_mean 0.2817 (0.2944) alpha_min 0.0000 (0.0000) alpha_max 0.5053 (0.5369) lr 1.5567e-04 eta 0:04:03
epoch [43/50] batch [40/288] time 0.096 (0.103) data 0.000 (0.007) loss 1.7214 (1.6413) teacher_loss 1.0166 (0.9239) loss_zs_kd 0.1595 (0.1769) loss_oracle 0.6251 (0.6289) acc 68.7500 (75.2344) alaph_mean 0.3303 (0.3165) alpha_min -0.0000 (0.0000) alpha_max 0.5060 (0.5223) lr 1.5567e-04 eta 0:03:52
epoch [43/50] batch [60/288] time 0.096 (0.101) data 0.001 (0.005) loss 2.4945 (1.6675) teacher_loss 1.6289 (0.9413) loss_zs_kd 0.1871 (0.1814) loss_oracle 0.7720 (0.6355) acc 56.2500 (74.3750) alaph_mean 0.2230 (0.3143) alpha_min 0.0000 (0.0000) alpha_max 0.5015 (0.5192) lr 1.5567e-04 eta 0:03:47
epoch [43/50] batch [80/288] time 0.097 (0.101) data 0.000 (0.004) loss 1.1529 (1.6243) teacher_loss 0.5454 (0.8959) loss_zs_kd 0.0741 (0.1725) loss_oracle 0.5704 (0.6422) acc 87.5000 (75.7422) alaph_mean 0.2994 (0.3099) alpha_min 0.0000 (0.0000) alpha_max 0.5047 (0.5276) lr 1.5567e-04 eta 0:03:43
epoch [43/50] batch [100/288] time 0.098 (0.100) data 0.000 (0.003) loss 1.7428 (1.6298) teacher_loss 1.2002 (0.9031) loss_zs_kd 0.2033 (0.1708) loss_oracle 0.4410 (0.6414) acc 68.7500 (75.4062) alaph_mean 0.4061 (0.3096) alpha_min -0.0000 (0.0000) alpha_max 0.5065 (0.5237) lr 1.5567e-04 eta 0:03:39
epoch [43/50] batch [120/288] time 0.114 (0.100) data 0.000 (0.003) loss 1.3833 (1.6266) teacher_loss 0.6851 (0.9028) loss_zs_kd 0.1579 (0.1731) loss_oracle 0.6193 (0.6372) acc 81.2500 (75.6250) alaph_mean 0.2976 (0.3108) alpha_min 0.0000 (0.0000) alpha_max 0.5032 (0.5212) lr 1.5567e-04 eta 0:03:38
epoch [43/50] batch [140/288] time 0.097 (0.100) data 0.000 (0.002) loss 1.2239 (1.6206) teacher_loss 0.6406 (0.8999) loss_zs_kd 0.2095 (0.1739) loss_oracle 0.4785 (0.6337) acc 90.6250 (75.7366) alaph_mean 0.3902 (0.3138) alpha_min 0.0000 (0.0000) alpha_max 0.5062 (0.5199) lr 1.5567e-04 eta 0:03:36
epoch [43/50] batch [160/288] time 0.098 (0.100) data 0.000 (0.002) loss 1.6006 (1.6128) teacher_loss 0.8755 (0.8927) loss_zs_kd 0.1632 (0.1744) loss_oracle 0.6435 (0.6329) acc 75.0000 (75.8789) alaph_mean 0.3120 (0.3144) alpha_min -0.0000 (0.0000) alpha_max 0.5040 (0.5212) lr 1.5567e-04 eta 0:03:34
epoch [43/50] batch [180/288] time 0.114 (0.101) data 0.000 (0.002) loss 1.2691 (1.6027) teacher_loss 0.6685 (0.8826) loss_zs_kd 0.1481 (0.1736) loss_oracle 0.5266 (0.6333) acc 81.2500 (76.0764) alaph_mean 0.3792 (0.3144) alpha_min -0.0000 (0.0000) alpha_max 0.5091 (0.5205) lr 1.5567e-04 eta 0:03:33
epoch [43/50] batch [200/288] time 0.099 (0.101) data 0.000 (0.002) loss 1.4162 (1.6039) teacher_loss 0.8145 (0.8847) loss_zs_kd 0.1834 (0.1742) loss_oracle 0.5100 (0.6322) acc 81.2500 (76.1562) alaph_mean 0.4064 (0.3165) alpha_min 0.0000 (0.0000) alpha_max 0.5051 (0.5202) lr 1.5567e-04 eta 0:03:31
epoch [43/50] batch [220/288] time 0.103 (0.101) data 0.000 (0.002) loss 1.4240 (1.6054) teacher_loss 0.8110 (0.8854) loss_zs_kd 0.1889 (0.1744) loss_oracle 0.5185 (0.6328) acc 78.1250 (76.1932) alaph_mean 0.3845 (0.3169) alpha_min 0.0000 (0.0000) alpha_max 0.8368 (0.5232) lr 1.5567e-04 eta 0:03:31
epoch [43/50] batch [240/288] time 0.103 (0.101) data 0.000 (0.001) loss 1.5942 (1.6013) teacher_loss 0.9375 (0.8826) loss_zs_kd 0.2566 (0.1741) loss_oracle 0.5284 (0.6317) acc 75.0000 (76.2109) alaph_mean 0.3777 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5439 (0.5239) lr 1.5567e-04 eta 0:03:28
epoch [43/50] batch [260/288] time 0.096 (0.101) data 0.000 (0.001) loss 1.5092 (1.6017) teacher_loss 0.6694 (0.8832) loss_zs_kd 0.2023 (0.1746) loss_oracle 0.7386 (0.6312) acc 81.2500 (76.1538) alaph_mean 0.2661 (0.3181) alpha_min -0.0000 (0.0000) alpha_max 0.5057 (0.5242) lr 1.5567e-04 eta 0:03:27
epoch [43/50] batch [280/288] time 0.105 (0.101) data 0.000 (0.001) loss 1.7683 (1.6013) teacher_loss 0.9194 (0.8828) loss_zs_kd 0.1852 (0.1744) loss_oracle 0.7562 (0.6313) acc 78.1250 (76.2723) alaph_mean 0.2628 (0.3178) alpha_min -0.0000 (0.0000) alpha_max 0.5057 (0.5246) lr 1.5567e-04 eta 0:03:25
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,429
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,030
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.1%
******* Domain a best val acc:      87.3%, epoch: 35 *******
******* Domain a best val test acc: 83.4%, epoch: 35 *******
******* Domain a best test acc:     84.1%, epoch: 40 *******
epoch [44/50] batch [20/288] time 0.105 (0.121) data 0.000 (0.014) loss 1.7365 (1.5822) teacher_loss 1.0840 (0.8827) loss_zs_kd 0.1546 (0.1780) loss_oracle 0.5752 (0.6105) acc 68.7500 (75.1562) alaph_mean 0.3711 (0.3274) alpha_min -0.0000 (0.0000) alpha_max 0.5046 (0.5148) lr 1.2369e-04 eta 0:04:02
epoch [44/50] batch [40/288] time 0.100 (0.110) data 0.000 (0.007) loss 1.2885 (1.6151) teacher_loss 0.6465 (0.9115) loss_zs_kd 0.1537 (0.1761) loss_oracle 0.5651 (0.6156) acc 84.3750 (75.2344) alaph_mean 0.3184 (0.3229) alpha_min 0.0000 (0.0000) alpha_max 0.5027 (0.5121) lr 1.2369e-04 eta 0:03:37
epoch [44/50] batch [60/288] time 0.098 (0.107) data 0.000 (0.005) loss 1.4690 (1.6014) teacher_loss 0.7207 (0.8966) loss_zs_kd 0.1049 (0.1793) loss_oracle 0.6958 (0.6151) acc 81.2500 (76.0417) alaph_mean 0.2816 (0.3230) alpha_min 0.0000 (0.0000) alpha_max 0.5039 (0.5191) lr 1.2369e-04 eta 0:03:29
epoch [44/50] batch [80/288] time 0.098 (0.105) data 0.000 (0.004) loss 1.7616 (1.6038) teacher_loss 1.0371 (0.8949) loss_zs_kd 0.2410 (0.1788) loss_oracle 0.6040 (0.6195) acc 75.0000 (76.0156) alaph_mean 0.3251 (0.3217) alpha_min -0.0000 (0.0000) alpha_max 0.6654 (0.5195) lr 1.2369e-04 eta 0:03:22
epoch [44/50] batch [100/288] time 0.089 (0.102) data 0.000 (0.003) loss 1.6845 (1.5992) teacher_loss 1.0312 (0.8906) loss_zs_kd 0.1490 (0.1775) loss_oracle 0.5787 (0.6198) acc 75.0000 (76.2500) alaph_mean 0.3137 (0.3204) alpha_min 0.0000 (0.0000) alpha_max 0.5076 (0.5185) lr 1.2369e-04 eta 0:03:16
epoch [44/50] batch [120/288] time 0.097 (0.101) data 0.000 (0.002) loss 1.6729 (1.5910) teacher_loss 0.8926 (0.8804) loss_zs_kd 0.2312 (0.1765) loss_oracle 0.6647 (0.6223) acc 75.0000 (76.7448) alaph_mean 0.3113 (0.3194) alpha_min -0.0000 (0.0000) alpha_max 0.6208 (0.5238) lr 1.2369e-04 eta 0:03:12
epoch [44/50] batch [140/288] time 0.103 (0.101) data 0.000 (0.002) loss 1.7354 (1.5957) teacher_loss 0.8804 (0.8819) loss_zs_kd 0.2050 (0.1766) loss_oracle 0.7525 (0.6255) acc 68.7500 (76.6741) alaph_mean 0.2319 (0.3179) alpha_min -0.0000 (0.0000) alpha_max 0.5038 (0.5285) lr 1.2369e-04 eta 0:03:08
epoch [44/50] batch [160/288] time 0.098 (0.100) data 0.000 (0.002) loss 1.9699 (1.6032) teacher_loss 1.1426 (0.8863) loss_zs_kd 0.1687 (0.1774) loss_oracle 0.7430 (0.6282) acc 62.5000 (76.5234) alaph_mean 0.2966 (0.3186) alpha_min 0.0000 (0.0000) alpha_max 0.5080 (0.5299) lr 1.2369e-04 eta 0:03:05
epoch [44/50] batch [180/288] time 0.091 (0.100) data 0.000 (0.002) loss 1.6143 (1.6147) teacher_loss 0.7715 (0.8972) loss_zs_kd 0.1537 (0.1774) loss_oracle 0.7659 (0.6288) acc 78.1250 (76.3715) alaph_mean 0.2142 (0.3176) alpha_min -0.0000 (0.0000) alpha_max 0.5055 (0.5310) lr 1.2369e-04 eta 0:03:03
epoch [44/50] batch [200/288] time 0.092 (0.100) data 0.000 (0.002) loss 1.5160 (1.6187) teacher_loss 0.7622 (0.8965) loss_zs_kd 0.1655 (0.1788) loss_oracle 0.6710 (0.6328) acc 81.2500 (76.4219) alaph_mean 0.3290 (0.3157) alpha_min 0.0000 (0.0000) alpha_max 0.5134 (0.5306) lr 1.2369e-04 eta 0:03:02
epoch [44/50] batch [220/288] time 0.098 (0.100) data 0.000 (0.001) loss 1.5245 (1.6215) teacher_loss 0.7705 (0.8985) loss_zs_kd 0.1872 (0.1782) loss_oracle 0.6604 (0.6339) acc 75.0000 (76.3068) alaph_mean 0.2708 (0.3151) alpha_min -0.0000 (0.0000) alpha_max 0.5083 (0.5299) lr 1.2369e-04 eta 0:02:59
epoch [44/50] batch [240/288] time 0.095 (0.100) data 0.000 (0.001) loss 1.6015 (1.6322) teacher_loss 0.8872 (0.9118) loss_zs_kd 0.1748 (0.1777) loss_oracle 0.6269 (0.6316) acc 75.0000 (75.8984) alaph_mean 0.3105 (0.3159) alpha_min 0.0000 (0.0000) alpha_max 0.5026 (0.5300) lr 1.2369e-04 eta 0:02:56
epoch [44/50] batch [260/288] time 0.101 (0.099) data 0.000 (0.001) loss 1.9517 (1.6278) teacher_loss 1.2061 (0.9084) loss_zs_kd 0.1433 (0.1768) loss_oracle 0.6740 (0.6309) acc 68.7500 (75.8534) alaph_mean 0.3059 (0.3165) alpha_min 0.0000 (0.0000) alpha_max 0.6107 (0.5338) lr 1.2369e-04 eta 0:02:54
epoch [44/50] batch [280/288] time 0.107 (0.099) data 0.000 (0.001) loss 1.8398 (1.6239) teacher_loss 1.1719 (0.9065) loss_zs_kd 0.1484 (0.1775) loss_oracle 0.5938 (0.6287) acc 65.6250 (75.7478) alaph_mean 0.3474 (0.3177) alpha_min 0.0000 (0.0000) alpha_max 0.5085 (0.5337) lr 1.2369e-04 eta 0:02:51
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,429
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,032
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 80.3%
******* Domain a best val acc:      87.3%, epoch: 35 *******
******* Domain a best val test acc: 83.4%, epoch: 35 *******
******* Domain a best test acc:     84.1%, epoch: 40 *******
epoch [45/50] batch [20/288] time 0.119 (0.143) data 0.001 (0.020) loss 1.8348 (1.6060) teacher_loss 1.1484 (0.8997) loss_zs_kd 0.1368 (0.1798) loss_oracle 0.6180 (0.6164) acc 62.5000 (75.9375) alaph_mean 0.2916 (0.3176) alpha_min 0.0000 (0.0000) alpha_max 0.5021 (0.5129) lr 9.5173e-05 eta 0:04:03
epoch [45/50] batch [40/288] time 0.098 (0.126) data 0.002 (0.010) loss 1.3658 (1.5526) teacher_loss 0.7056 (0.8432) loss_zs_kd 0.1078 (0.1755) loss_oracle 0.6063 (0.6216) acc 71.8750 (77.3438) alaph_mean 0.3275 (0.3157) alpha_min 0.0000 (0.0000) alpha_max 0.5032 (0.5213) lr 9.5173e-05 eta 0:03:32
epoch [45/50] batch [60/288] time 0.094 (0.118) data 0.001 (0.007) loss 1.2397 (1.5402) teacher_loss 0.5029 (0.8376) loss_zs_kd 0.1541 (0.1711) loss_oracle 0.6597 (0.6170) acc 87.5000 (77.7083) alaph_mean 0.2755 (0.3224) alpha_min -0.0000 (0.0000) alpha_max 0.5052 (0.5227) lr 9.5173e-05 eta 0:03:16
epoch [45/50] batch [80/288] time 0.101 (0.114) data 0.000 (0.005) loss 1.0970 (1.5363) teacher_loss 0.5684 (0.8462) loss_zs_kd 0.1426 (0.1693) loss_oracle 0.4573 (0.6055) acc 84.3750 (77.3828) alaph_mean 0.3988 (0.3284) alpha_min 0.0000 (0.0000) alpha_max 0.5136 (0.5211) lr 9.5173e-05 eta 0:03:07
epoch [45/50] batch [100/288] time 0.120 (0.113) data 0.001 (0.004) loss 2.1325 (1.5340) teacher_loss 1.3887 (0.8442) loss_zs_kd 0.1636 (0.1693) loss_oracle 0.6621 (0.6052) acc 59.3750 (77.4062) alaph_mean 0.2477 (0.3276) alpha_min -0.0000 (0.0000) alpha_max 0.5078 (0.5248) lr 9.5173e-05 eta 0:03:04
epoch [45/50] batch [120/288] time 0.109 (0.113) data 0.000 (0.004) loss 1.4214 (1.5542) teacher_loss 0.8447 (0.8555) loss_zs_kd 0.1512 (0.1716) loss_oracle 0.5011 (0.6129) acc 78.1250 (77.1615) alaph_mean 0.3401 (0.3235) alpha_min 0.0000 (0.0000) alpha_max 0.5073 (0.5257) lr 9.5173e-05 eta 0:03:01
epoch [45/50] batch [140/288] time 0.110 (0.112) data 0.000 (0.003) loss 1.0480 (1.5612) teacher_loss 0.3367 (0.8618) loss_zs_kd 0.1378 (0.1713) loss_oracle 0.6424 (0.6137) acc 90.6250 (76.9643) alaph_mean 0.2785 (0.3232) alpha_min 0.0000 (0.0000) alpha_max 0.5045 (0.5278) lr 9.5173e-05 eta 0:02:57
epoch [45/50] batch [160/288] time 0.096 (0.113) data 0.000 (0.003) loss 1.4224 (1.5678) teacher_loss 0.8311 (0.8661) loss_zs_kd 0.1977 (0.1733) loss_oracle 0.4925 (0.6151) acc 78.1250 (76.5430) alaph_mean 0.3673 (0.3225) alpha_min 0.0000 (0.0000) alpha_max 0.5047 (0.5258) lr 9.5173e-05 eta 0:02:56
epoch [45/50] batch [180/288] time 0.119 (0.112) data 0.000 (0.003) loss 1.8168 (1.5733) teacher_loss 1.0947 (0.8712) loss_zs_kd 0.2444 (0.1745) loss_oracle 0.5999 (0.6148) acc 75.0000 (76.3021) alaph_mean 0.3507 (0.3240) alpha_min 0.0000 (0.0000) alpha_max 0.5093 (0.5254) lr 9.5173e-05 eta 0:02:54
epoch [45/50] batch [200/288] time 0.106 (0.112) data 0.001 (0.002) loss 1.3559 (1.5780) teacher_loss 0.5396 (0.8758) loss_zs_kd 0.1449 (0.1748) loss_oracle 0.7439 (0.6148) acc 87.5000 (76.3750) alaph_mean 0.2663 (0.3238) alpha_min 0.0000 (0.0000) alpha_max 0.5050 (0.5250) lr 9.5173e-05 eta 0:02:51
epoch [45/50] batch [220/288] time 0.099 (0.112) data 0.000 (0.002) loss 1.5994 (1.5748) teacher_loss 0.7915 (0.8708) loss_zs_kd 0.2626 (0.1749) loss_oracle 0.6766 (0.6166) acc 84.3750 (76.7188) alaph_mean 0.2664 (0.3228) alpha_min 0.0000 (0.0000) alpha_max 0.5083 (0.5236) lr 9.5173e-05 eta 0:02:48
epoch [45/50] batch [240/288] time 0.110 (0.112) data 0.000 (0.002) loss 1.5257 (1.5692) teacher_loss 0.8042 (0.8617) loss_zs_kd 0.1464 (0.1742) loss_oracle 0.6484 (0.6203) acc 81.2500 (76.9661) alaph_mean 0.3168 (0.3217) alpha_min 0.0000 (0.0000) alpha_max 0.6426 (0.5238) lr 9.5173e-05 eta 0:02:46
epoch [45/50] batch [260/288] time 0.099 (0.112) data 0.000 (0.002) loss 1.6150 (1.5679) teacher_loss 1.0391 (0.8597) loss_zs_kd 0.2342 (0.1752) loss_oracle 0.4588 (0.6206) acc 75.0000 (77.0793) alaph_mean 0.4234 (0.3217) alpha_min 0.0000 (0.0000) alpha_max 0.5114 (0.5236) lr 9.5173e-05 eta 0:02:43
epoch [45/50] batch [280/288] time 0.109 (0.111) data 0.001 (0.002) loss 1.5874 (1.5793) teacher_loss 0.9727 (0.8700) loss_zs_kd 0.2010 (0.1761) loss_oracle 0.5143 (0.6213) acc 81.2500 (76.8750) alaph_mean 0.3330 (0.3210) alpha_min 0.0000 (0.0000) alpha_max 0.5062 (0.5242) lr 9.5173e-05 eta 0:02:40
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,426
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,032
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 80.2%
******* Domain a best val acc:      87.3%, epoch: 35 *******
******* Domain a best val test acc: 83.4%, epoch: 35 *******
******* Domain a best test acc:     84.1%, epoch: 40 *******
epoch [46/50] batch [20/288] time 0.099 (0.116) data 0.000 (0.014) loss 1.3115 (1.5905) teacher_loss 0.6396 (0.8740) loss_zs_kd 0.1551 (0.1710) loss_oracle 0.5944 (0.6310) acc 78.1250 (77.8125) alaph_mean 0.3616 (0.3098) alpha_min 0.0000 (0.0000) alpha_max 0.5083 (0.5174) lr 7.0224e-05 eta 0:02:44
