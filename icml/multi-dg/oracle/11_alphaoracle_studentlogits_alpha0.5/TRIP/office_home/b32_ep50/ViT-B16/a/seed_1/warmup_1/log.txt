Loading trainer: TRIP
Loading dataset: SPG_OfficeHome
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------------------------------
Dataset    SPG_OfficeHome
Source     ['clipart', 'product', 'real_world']
Target     ['art']
# classes  65
# train_x  9,222
# val      3,939
# test     2,427
---------  ------------------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,195
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/288] time 0.191 (0.246) data 0.000 (0.023) loss 1.1809 (1.3108) teacher_loss 1.1100 (1.2379) loss_zs_kd 0.0000 (0.0000) loss_oracle 0.0709 (0.0729) acc 71.8750 (68.4375) lr 1.0000e-05 eta 0:58:52
epoch [1/50] batch [40/288] time 0.198 (0.218) data 0.000 (0.011) loss 1.2974 (1.3301) teacher_loss 1.2160 (1.2592) loss_zs_kd 0.0001 (0.0000) loss_oracle 0.0814 (0.0709) acc 65.6250 (68.2812) lr 1.0000e-05 eta 0:52:15
epoch [1/50] batch [60/288] time 0.192 (0.210) data 0.001 (0.008) loss 1.4824 (1.3401) teacher_loss 1.4281 (1.2662) loss_zs_kd 0.0006 (0.0001) loss_oracle 0.0543 (0.0739) acc 59.3750 (67.6562) lr 1.0000e-05 eta 0:50:09
epoch [1/50] batch [80/288] time 0.192 (0.206) data 0.000 (0.006) loss 1.3253 (1.3477) teacher_loss 1.2564 (1.2702) loss_zs_kd 0.0009 (0.0002) loss_oracle 0.0689 (0.0775) acc 62.5000 (67.3828) lr 1.0000e-05 eta 0:49:10
epoch [1/50] batch [100/288] time 0.193 (0.204) data 0.000 (0.005) loss 1.1714 (1.3191) teacher_loss 1.0905 (1.2425) loss_zs_kd 0.0006 (0.0003) loss_oracle 0.0810 (0.0766) acc 62.5000 (68.0938) lr 1.0000e-05 eta 0:48:30
epoch [1/50] batch [120/288] time 0.190 (0.202) data 0.000 (0.004) loss 1.1163 (1.3137) teacher_loss 1.0323 (1.2363) loss_zs_kd 0.0008 (0.0004) loss_oracle 0.0840 (0.0775) acc 75.0000 (68.1771) lr 1.0000e-05 eta 0:48:04
epoch [1/50] batch [140/288] time 0.192 (0.201) data 0.000 (0.003) loss 1.5764 (1.3007) teacher_loss 1.4876 (1.2222) loss_zs_kd 0.0011 (0.0005) loss_oracle 0.0888 (0.0785) acc 56.2500 (68.3036) lr 1.0000e-05 eta 0:47:42
epoch [1/50] batch [160/288] time 0.200 (0.200) data 0.000 (0.003) loss 1.0883 (1.2975) teacher_loss 1.0572 (1.2197) loss_zs_kd 0.0021 (0.0006) loss_oracle 0.0311 (0.0778) acc 71.8750 (68.6328) lr 1.0000e-05 eta 0:47:27
epoch [1/50] batch [180/288] time 0.090 (0.206) data 0.000 (0.003) loss 1.0566 (1.3005) teacher_loss 0.9933 (1.2232) loss_zs_kd 0.0026 (0.0008) loss_oracle 0.0634 (0.0772) acc 71.8750 (68.4722) lr 1.0000e-05 eta 0:48:51
epoch [1/50] batch [200/288] time 0.218 (0.208) data 0.000 (0.002) loss 1.4316 (1.2970) teacher_loss 1.3382 (1.2200) loss_zs_kd 0.0022 (0.0009) loss_oracle 0.0934 (0.0770) acc 62.5000 (68.3594) lr 1.0000e-05 eta 0:49:14
epoch [1/50] batch [220/288] time 0.200 (0.207) data 0.000 (0.002) loss 1.4293 (1.2920) teacher_loss 1.4148 (1.2155) loss_zs_kd 0.0024 (0.0011) loss_oracle 0.0145 (0.0765) acc 65.6250 (68.6222) lr 1.0000e-05 eta 0:48:51
epoch [1/50] batch [240/288] time 0.195 (0.206) data 0.000 (0.002) loss 1.0532 (1.2891) teacher_loss 0.9846 (1.2131) loss_zs_kd 0.0012 (0.0012) loss_oracle 0.0687 (0.0761) acc 81.2500 (68.8802) lr 1.0000e-05 eta 0:48:33
epoch [1/50] batch [260/288] time 0.189 (0.205) data 0.000 (0.002) loss 1.4046 (1.2946) teacher_loss 1.2987 (1.2185) loss_zs_kd 0.0061 (0.0014) loss_oracle 0.1059 (0.0760) acc 71.8750 (68.7260) lr 1.0000e-05 eta 0:48:16
epoch [1/50] batch [280/288] time 0.196 (0.204) data 0.000 (0.002) loss 1.5624 (1.2943) teacher_loss 1.5170 (1.2183) loss_zs_kd 0.0060 (0.0016) loss_oracle 0.0454 (0.0760) acc 65.6250 (68.7946) lr 1.0000e-05 eta 0:48:02
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,268
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 82.0%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,966
* accuracy: 81.0%
* error: 19.0%
* macro_f1: 76.8%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      83.0%, epoch: 1 *******
******* Domain a best val test acc: 81.0%, epoch: 1 *******
******* Domain a best test acc:     81.0%, epoch: 1 *******
epoch [2/50] batch [20/288] time 0.177 (0.216) data 0.000 (0.018) loss 1.1039 (1.2535) teacher_loss 1.0348 (1.1786) loss_zs_kd 0.1492 (0.1303) loss_oracle 0.0691 (0.0749) acc 68.7500 (69.0625) lr 2.0000e-03 eta 0:50:49
epoch [2/50] batch [40/288] time 0.202 (0.206) data 0.000 (0.009) loss 0.7584 (1.1951) teacher_loss 0.6841 (1.1153) loss_zs_kd 0.1604 (0.1675) loss_oracle 0.0743 (0.0799) acc 81.2500 (70.8594) lr 2.0000e-03 eta 0:48:15
epoch [2/50] batch [60/288] time 0.194 (0.203) data 0.001 (0.006) loss 1.5187 (1.1994) teacher_loss 1.4225 (1.1176) loss_zs_kd 0.2578 (0.1804) loss_oracle 0.0961 (0.0818) acc 71.8750 (71.3542) lr 2.0000e-03 eta 0:47:29
epoch [2/50] batch [80/288] time 0.159 (0.201) data 0.000 (0.005) loss 0.9409 (1.1834) teacher_loss 0.8341 (1.0978) loss_zs_kd 0.1866 (0.1862) loss_oracle 0.1069 (0.0856) acc 71.8750 (71.8750) lr 2.0000e-03 eta 0:46:56
epoch [2/50] batch [100/288] time 0.196 (0.199) data 0.000 (0.004) loss 1.4510 (1.1832) teacher_loss 1.3536 (1.0956) loss_zs_kd 0.2226 (0.1972) loss_oracle 0.0973 (0.0876) acc 71.8750 (71.9375) lr 2.0000e-03 eta 0:46:33
epoch [2/50] batch [120/288] time 0.192 (0.199) data 0.000 (0.003) loss 1.2466 (1.1779) teacher_loss 1.1375 (1.0890) loss_zs_kd 0.1670 (0.2039) loss_oracle 0.1092 (0.0889) acc 71.8750 (71.7708) lr 2.0000e-03 eta 0:46:26
epoch [2/50] batch [140/288] time 0.220 (0.199) data 0.000 (0.003) loss 1.2211 (1.1831) teacher_loss 1.1693 (1.0931) loss_zs_kd 0.3038 (0.2058) loss_oracle 0.0517 (0.0900) acc 75.0000 (71.7411) lr 2.0000e-03 eta 0:46:15
epoch [2/50] batch [160/288] time 0.505 (0.207) data 0.000 (0.002) loss 1.2188 (1.1897) teacher_loss 1.0951 (1.0957) loss_zs_kd 0.3421 (0.2059) loss_oracle 0.1238 (0.0941) acc 71.8750 (71.6211) lr 2.0000e-03 eta 0:48:14
epoch [2/50] batch [180/288] time 0.178 (0.208) data 0.000 (0.002) loss 1.1318 (1.1848) teacher_loss 1.0298 (1.0895) loss_zs_kd 0.2114 (0.2064) loss_oracle 0.1019 (0.0952) acc 78.1250 (71.7014) lr 2.0000e-03 eta 0:48:12
epoch [2/50] batch [200/288] time 0.196 (0.206) data 0.000 (0.002) loss 1.8745 (1.1870) teacher_loss 1.7410 (1.0913) loss_zs_kd 0.2719 (0.2100) loss_oracle 0.1336 (0.0956) acc 56.2500 (71.4844) lr 2.0000e-03 eta 0:47:52
epoch [2/50] batch [220/288] time 0.194 (0.205) data 0.000 (0.002) loss 1.2410 (1.1801) teacher_loss 1.1261 (1.0804) loss_zs_kd 0.2061 (0.2107) loss_oracle 0.1149 (0.0997) acc 75.0000 (71.6193) lr 2.0000e-03 eta 0:47:32
epoch [2/50] batch [240/288] time 0.200 (0.204) data 0.000 (0.002) loss 1.3840 (1.1781) teacher_loss 1.2642 (1.0763) loss_zs_kd 0.2206 (0.2117) loss_oracle 0.1198 (0.1018) acc 62.5000 (71.6536) lr 2.0000e-03 eta 0:47:14
epoch [2/50] batch [260/288] time 0.190 (0.204) data 0.000 (0.002) loss 1.2735 (1.1848) teacher_loss 0.9375 (1.0707) loss_zs_kd 0.3850 (0.2157) loss_oracle 0.3359 (0.1141) acc 81.2500 (71.6947) lr 2.0000e-03 eta 0:46:59
epoch [2/50] batch [280/288] time 0.187 (0.203) data 0.000 (0.001) loss 0.9815 (1.1892) teacher_loss 0.8018 (1.0670) loss_zs_kd 0.2864 (0.2179) loss_oracle 0.1797 (0.1222) acc 75.0000 (71.7634) lr 2.0000e-03 eta 0:46:44
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,394
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.4%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.9%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.2%, epoch: 2 *******
******* Domain a best val test acc: 83.3%, epoch: 2 *******
******* Domain a best test acc:     83.3%, epoch: 2 *******
epoch [3/50] batch [20/288] time 0.199 (0.213) data 0.000 (0.017) loss 1.5722 (1.1805) teacher_loss 1.4572 (1.0478) loss_zs_kd 0.2828 (0.2494) loss_oracle 0.1150 (0.1327) acc 50.0000 (71.7188) lr 1.9980e-03 eta 0:48:57
epoch [3/50] batch [40/288] time 0.199 (0.204) data 0.000 (0.008) loss 1.0249 (1.1302) teacher_loss 0.8197 (0.9820) loss_zs_kd 0.1882 (0.2388) loss_oracle 0.2051 (0.1482) acc 78.1250 (73.3594) lr 1.9980e-03 eta 0:46:48
epoch [3/50] batch [60/288] time 0.196 (0.200) data 0.000 (0.006) loss 1.1533 (1.1900) teacher_loss 1.0272 (1.0353) loss_zs_kd 0.1850 (0.2440) loss_oracle 0.1262 (0.1547) acc 75.0000 (72.2396) lr 1.9980e-03 eta 0:45:58
epoch [3/50] batch [80/288] time 0.194 (0.199) data 0.000 (0.004) loss 1.0985 (1.1862) teacher_loss 0.9726 (1.0363) loss_zs_kd 0.2374 (0.2435) loss_oracle 0.1260 (0.1499) acc 71.8750 (72.0703) lr 1.9980e-03 eta 0:45:30
epoch [3/50] batch [100/288] time 0.181 (0.198) data 0.000 (0.004) loss 1.0309 (1.1923) teacher_loss 0.8960 (1.0446) loss_zs_kd 0.2360 (0.2439) loss_oracle 0.1349 (0.1477) acc 75.0000 (71.8125) lr 1.9980e-03 eta 0:45:11
epoch [3/50] batch [120/288] time 0.194 (0.197) data 0.000 (0.003) loss 0.8904 (1.1832) teacher_loss 0.7250 (1.0335) loss_zs_kd 0.2749 (0.2503) loss_oracle 0.1655 (0.1498) acc 81.2500 (72.1875) lr 1.9980e-03 eta 0:45:00
epoch [3/50] batch [140/288] time 0.090 (0.195) data 0.000 (0.003) loss 1.1449 (1.1799) teacher_loss 1.0247 (1.0295) loss_zs_kd 0.1702 (0.2524) loss_oracle 0.1202 (0.1504) acc 75.0000 (72.1875) lr 1.9980e-03 eta 0:44:26
epoch [3/50] batch [160/288] time 0.486 (0.206) data 0.000 (0.002) loss 1.6929 (1.1841) teacher_loss 1.5539 (1.0341) loss_zs_kd 0.2726 (0.2560) loss_oracle 0.1391 (0.1500) acc 59.3750 (72.1094) lr 1.9980e-03 eta 0:46:54
epoch [3/50] batch [180/288] time 0.198 (0.207) data 0.000 (0.002) loss 1.3635 (1.1859) teacher_loss 1.1918 (1.0321) loss_zs_kd 0.2660 (0.2558) loss_oracle 0.1717 (0.1538) acc 68.7500 (72.2396) lr 1.9980e-03 eta 0:47:01
epoch [3/50] batch [200/288] time 0.195 (0.206) data 0.000 (0.002) loss 1.1434 (1.1843) teacher_loss 0.9495 (1.0263) loss_zs_kd 0.1775 (0.2582) loss_oracle 0.1939 (0.1580) acc 81.2500 (72.5781) lr 1.9980e-03 eta 0:46:39
epoch [3/50] batch [220/288] time 0.195 (0.204) data 0.000 (0.002) loss 1.0737 (1.1979) teacher_loss 0.8621 (1.0373) loss_zs_kd 0.2679 (0.2608) loss_oracle 0.2116 (0.1606) acc 75.0000 (72.4574) lr 1.9980e-03 eta 0:46:20
epoch [3/50] batch [240/288] time 0.188 (0.204) data 0.000 (0.002) loss 1.5348 (1.2044) teacher_loss 1.3550 (1.0422) loss_zs_kd 0.2246 (0.2604) loss_oracle 0.1798 (0.1622) acc 65.6250 (72.3438) lr 1.9980e-03 eta 0:46:05
epoch [3/50] batch [260/288] time 0.195 (0.203) data 0.000 (0.002) loss 0.7632 (1.1993) teacher_loss 0.6248 (1.0373) loss_zs_kd 0.2470 (0.2595) loss_oracle 0.1383 (0.1620) acc 87.5000 (72.4880) lr 1.9980e-03 eta 0:45:52
epoch [3/50] batch [280/288] time 0.190 (0.202) data 0.000 (0.001) loss 1.2252 (1.1946) teacher_loss 1.0354 (1.0323) loss_zs_kd 0.4760 (0.2637) loss_oracle 0.1898 (0.1623) acc 68.7500 (72.6562) lr 1.9980e-03 eta 0:45:38
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,016
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.7%
******* Domain a best val acc:      86.6%, epoch: 3 *******
******* Domain a best val test acc: 83.1%, epoch: 3 *******
******* Domain a best test acc:     83.3%, epoch: 2 *******
epoch [4/50] batch [20/288] time 0.224 (0.222) data 0.000 (0.016) loss 0.9483 (1.1300) teacher_loss 0.7278 (0.9411) loss_zs_kd 0.2692 (0.2842) loss_oracle 0.2205 (0.1888) acc 87.5000 (73.9062) lr 1.9921e-03 eta 0:49:58
epoch [4/50] batch [40/288] time 0.197 (0.208) data 0.000 (0.008) loss 1.2567 (1.1592) teacher_loss 1.0738 (0.9673) loss_zs_kd 0.4895 (0.2943) loss_oracle 0.1829 (0.1919) acc 71.8750 (73.9062) lr 1.9921e-03 eta 0:46:51
epoch [4/50] batch [60/288] time 0.193 (0.204) data 0.001 (0.006) loss 1.4347 (1.1655) teacher_loss 1.2364 (0.9730) loss_zs_kd 0.2253 (0.2829) loss_oracle 0.1983 (0.1925) acc 65.6250 (73.6458) lr 1.9921e-03 eta 0:45:43
epoch [4/50] batch [80/288] time 0.194 (0.201) data 0.000 (0.004) loss 1.7338 (1.1910) teacher_loss 1.4809 (0.9931) loss_zs_kd 0.2554 (0.2749) loss_oracle 0.2529 (0.1979) acc 65.6250 (73.3594) lr 1.9921e-03 eta 0:45:09
epoch [4/50] batch [100/288] time 0.196 (0.200) data 0.000 (0.003) loss 1.0978 (1.2127) teacher_loss 0.8883 (1.0094) loss_zs_kd 0.2043 (0.2664) loss_oracle 0.2095 (0.2033) acc 84.3750 (73.0312) lr 1.9921e-03 eta 0:44:46
epoch [4/50] batch [120/288] time 0.186 (0.199) data 0.000 (0.003) loss 1.1791 (1.2298) teacher_loss 0.9117 (1.0172) loss_zs_kd 0.3061 (0.2653) loss_oracle 0.2673 (0.2125) acc 78.1250 (72.9948) lr 1.9921e-03 eta 0:44:28
epoch [4/50] batch [140/288] time 0.083 (0.209) data 0.000 (0.003) loss 0.9750 (1.2350) teacher_loss 0.7736 (1.0232) loss_zs_kd 0.2394 (0.2656) loss_oracle 0.2014 (0.2118) acc 78.1250 (72.8348) lr 1.9921e-03 eta 0:46:34
epoch [4/50] batch [160/288] time 0.192 (0.209) data 0.000 (0.002) loss 1.4037 (1.2249) teacher_loss 1.2386 (1.0150) loss_zs_kd 0.2654 (0.2691) loss_oracle 0.1651 (0.2099) acc 68.7500 (72.8906) lr 1.9921e-03 eta 0:46:40
epoch [4/50] batch [180/288] time 0.195 (0.208) data 0.000 (0.002) loss 1.5459 (1.2227) teacher_loss 1.3272 (1.0127) loss_zs_kd 0.3394 (0.2703) loss_oracle 0.2187 (0.2101) acc 68.7500 (72.9861) lr 1.9921e-03 eta 0:46:14
epoch [4/50] batch [200/288] time 0.194 (0.206) data 0.000 (0.002) loss 1.3535 (1.2257) teacher_loss 1.0828 (1.0113) loss_zs_kd 0.2630 (0.2762) loss_oracle 0.2707 (0.2144) acc 71.8750 (73.1719) lr 1.9921e-03 eta 0:45:52
epoch [4/50] batch [220/288] time 0.192 (0.205) data 0.000 (0.002) loss 1.4703 (1.2322) teacher_loss 1.2449 (1.0105) loss_zs_kd 0.2760 (0.2815) loss_oracle 0.2254 (0.2216) acc 71.8750 (73.1818) lr 1.9921e-03 eta 0:45:32
epoch [4/50] batch [240/288] time 0.190 (0.204) data 0.000 (0.002) loss 1.4335 (1.2272) teacher_loss 1.2025 (1.0087) loss_zs_kd 0.2619 (0.2810) loss_oracle 0.2310 (0.2185) acc 71.8750 (73.3594) lr 1.9921e-03 eta 0:45:16
epoch [4/50] batch [260/288] time 0.198 (0.204) data 0.000 (0.001) loss 1.0099 (1.2292) teacher_loss 0.7605 (1.0128) loss_zs_kd 0.3223 (0.2819) loss_oracle 0.2493 (0.2164) acc 81.2500 (73.2332) lr 1.9921e-03 eta 0:45:02
epoch [4/50] batch [280/288] time 0.190 (0.203) data 0.000 (0.001) loss 0.9621 (1.2237) teacher_loss 0.7390 (1.0073) loss_zs_kd 0.2186 (0.2805) loss_oracle 0.2232 (0.2164) acc 81.2500 (73.5156) lr 1.9921e-03 eta 0:44:48
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,419
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.1%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,037
* accuracy: 83.9%
* error: 16.1%
* macro_f1: 80.3%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.8%, epoch: 4 *******
******* Domain a best val test acc: 83.9%, epoch: 4 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [5/50] batch [20/288] time 0.196 (0.212) data 0.000 (0.015) loss 1.6590 (1.4086) teacher_loss 1.4123 (1.1509) loss_zs_kd 0.2813 (0.3054) loss_oracle 0.2467 (0.2577) acc 68.7500 (70.0000) lr 1.9823e-03 eta 0:46:39
epoch [5/50] batch [40/288] time 0.192 (0.203) data 0.000 (0.008) loss 0.6751 (1.2807) teacher_loss 0.4640 (1.0371) loss_zs_kd 0.4312 (0.3058) loss_oracle 0.2110 (0.2436) acc 90.6250 (72.2656) lr 1.9823e-03 eta 0:44:36
epoch [5/50] batch [60/288] time 0.195 (0.200) data 0.000 (0.005) loss 1.4214 (1.2342) teacher_loss 1.2825 (1.0041) loss_zs_kd 0.2273 (0.3012) loss_oracle 0.1389 (0.2302) acc 65.6250 (73.3854) lr 1.9823e-03 eta 0:43:51
epoch [5/50] batch [80/288] time 0.198 (0.198) data 0.000 (0.004) loss 1.6845 (1.2081) teacher_loss 1.4592 (0.9872) loss_zs_kd 0.2557 (0.3103) loss_oracle 0.2253 (0.2210) acc 75.0000 (74.0234) lr 1.9823e-03 eta 0:43:29
epoch [5/50] batch [100/288] time 0.182 (0.197) data 0.000 (0.003) loss 0.8050 (1.2122) teacher_loss 0.5623 (0.9950) loss_zs_kd 0.3034 (0.3095) loss_oracle 0.2427 (0.2171) acc 84.3750 (73.8750) lr 1.9823e-03 eta 0:43:13
epoch [5/50] batch [120/288] time 0.087 (0.195) data 0.000 (0.003) loss 1.2592 (1.2167) teacher_loss 1.0331 (0.9913) loss_zs_kd 0.3517 (0.3050) loss_oracle 0.2261 (0.2253) acc 75.0000 (74.0625) lr 1.9823e-03 eta 0:42:37
epoch [5/50] batch [140/288] time 0.095 (0.201) data 0.000 (0.002) loss 1.2068 (1.2184) teacher_loss 1.0646 (0.9945) loss_zs_kd 0.2300 (0.2988) loss_oracle 0.1422 (0.2239) acc 68.7500 (74.0402) lr 1.9823e-03 eta 0:44:01
epoch [5/50] batch [160/288] time 0.198 (0.204) data 0.000 (0.002) loss 1.4752 (1.2142) teacher_loss 1.2847 (0.9969) loss_zs_kd 0.2737 (0.2969) loss_oracle 0.1906 (0.2173) acc 65.6250 (73.7891) lr 1.9823e-03 eta 0:44:34
epoch [5/50] batch [180/288] time 0.194 (0.203) data 0.000 (0.002) loss 1.0726 (1.2174) teacher_loss 0.8960 (1.0036) loss_zs_kd 0.2474 (0.2992) loss_oracle 0.1766 (0.2138) acc 71.8750 (73.4201) lr 1.9823e-03 eta 0:44:14
epoch [5/50] batch [200/288] time 0.192 (0.202) data 0.000 (0.002) loss 1.0825 (1.2155) teacher_loss 0.8766 (1.0055) loss_zs_kd 0.3224 (0.2977) loss_oracle 0.2060 (0.2101) acc 81.2500 (73.6094) lr 1.9823e-03 eta 0:43:58
epoch [5/50] batch [220/288] time 0.098 (0.197) data 0.000 (0.002) loss 1.4190 (1.2206) teacher_loss 1.1986 (1.0086) loss_zs_kd 0.2727 (0.2991) loss_oracle 0.2204 (0.2120) acc 71.8750 (73.3949) lr 1.9823e-03 eta 0:42:51
epoch [5/50] batch [240/288] time 0.094 (0.189) data 0.000 (0.001) loss 0.9288 (1.2195) teacher_loss 0.7368 (1.0069) loss_zs_kd 0.2994 (0.3014) loss_oracle 0.1919 (0.2126) acc 81.2500 (73.3333) lr 1.9823e-03 eta 0:41:01
epoch [5/50] batch [260/288] time 0.095 (0.182) data 0.001 (0.001) loss 1.2854 (1.2139) teacher_loss 1.0400 (1.0015) loss_zs_kd 0.1830 (0.3023) loss_oracle 0.2454 (0.2124) acc 68.7500 (73.4255) lr 1.9823e-03 eta 0:39:28
epoch [5/50] batch [280/288] time 0.088 (0.176) data 0.000 (0.001) loss 1.0145 (1.2080) teacher_loss 0.7125 (0.9920) loss_zs_kd 0.3163 (0.3017) loss_oracle 0.3020 (0.2159) acc 81.2500 (73.6049) lr 1.9823e-03 eta 0:38:04
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,420
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.1%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,010
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.5%
******* Domain a best val acc:      86.8%, epoch: 5 *******
******* Domain a best val test acc: 82.8%, epoch: 5 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [6/50] batch [20/288] time 0.113 (0.127) data 0.001 (0.017) loss 1.3614 (1.0744) teacher_loss 1.1639 (0.8907) loss_zs_kd 0.2043 (0.2775) loss_oracle 0.1974 (0.1837) acc 68.7500 (73.7500) lr 1.9686e-03 eta 0:27:17
epoch [6/50] batch [40/288] time 0.092 (0.116) data 0.000 (0.009) loss 1.1137 (1.1276) teacher_loss 0.9182 (0.9458) loss_zs_kd 0.2771 (0.2905) loss_oracle 0.1955 (0.1819) acc 75.0000 (73.2812) lr 1.9686e-03 eta 0:24:52
epoch [6/50] batch [60/288] time 0.108 (0.114) data 0.001 (0.006) loss 1.1778 (1.1079) teacher_loss 1.0072 (0.9263) loss_zs_kd 0.3651 (0.2990) loss_oracle 0.1707 (0.1816) acc 68.7500 (73.8542) lr 1.9686e-03 eta 0:24:33
epoch [6/50] batch [80/288] time 0.110 (0.111) data 0.000 (0.004) loss 1.1142 (1.1213) teacher_loss 0.9579 (0.9379) loss_zs_kd 0.3971 (0.3099) loss_oracle 0.1564 (0.1834) acc 78.1250 (73.9844) lr 1.9686e-03 eta 0:23:51
epoch [6/50] batch [100/288] time 0.098 (0.110) data 0.000 (0.004) loss 1.3721 (1.1380) teacher_loss 1.1562 (0.9521) loss_zs_kd 0.3931 (0.3135) loss_oracle 0.2158 (0.1859) acc 68.7500 (74.2188) lr 1.9686e-03 eta 0:23:29
epoch [6/50] batch [120/288] time 0.099 (0.108) data 0.000 (0.003) loss 1.2251 (1.1356) teacher_loss 1.0871 (0.9470) loss_zs_kd 0.4485 (0.3165) loss_oracle 0.1381 (0.1886) acc 71.8750 (74.4271) lr 1.9686e-03 eta 0:23:10
epoch [6/50] batch [140/288] time 0.113 (0.108) data 0.000 (0.003) loss 0.9615 (1.1329) teacher_loss 0.7767 (0.9478) loss_zs_kd 0.1791 (0.3142) loss_oracle 0.1848 (0.1852) acc 84.3750 (74.5982) lr 1.9686e-03 eta 0:22:59
epoch [6/50] batch [160/288] time 0.106 (0.107) data 0.000 (0.002) loss 0.8149 (1.1343) teacher_loss 0.6688 (0.9489) loss_zs_kd 0.4204 (0.3192) loss_oracle 0.1461 (0.1853) acc 78.1250 (74.3164) lr 1.9686e-03 eta 0:22:49
epoch [6/50] batch [180/288] time 0.099 (0.106) data 0.000 (0.002) loss 0.9794 (1.1392) teacher_loss 0.7299 (0.9513) loss_zs_kd 0.5127 (0.3247) loss_oracle 0.2495 (0.1879) acc 78.1250 (74.2708) lr 1.9686e-03 eta 0:22:39
epoch [6/50] batch [200/288] time 0.113 (0.106) data 0.000 (0.002) loss 1.1013 (1.1474) teacher_loss 0.9549 (0.9582) loss_zs_kd 0.2840 (0.3235) loss_oracle 0.1464 (0.1891) acc 68.7500 (74.2812) lr 1.9686e-03 eta 0:22:33
epoch [6/50] batch [220/288] time 0.097 (0.106) data 0.000 (0.002) loss 1.3608 (1.1532) teacher_loss 1.1688 (0.9618) loss_zs_kd 0.4160 (0.3234) loss_oracle 0.1920 (0.1914) acc 71.8750 (74.2756) lr 1.9686e-03 eta 0:22:28
epoch [6/50] batch [240/288] time 0.104 (0.106) data 0.000 (0.002) loss 1.0539 (1.1591) teacher_loss 0.9114 (0.9678) loss_zs_kd 0.1716 (0.3216) loss_oracle 0.1425 (0.1913) acc 78.1250 (74.0755) lr 1.9686e-03 eta 0:22:23
epoch [6/50] batch [260/288] time 0.099 (0.105) data 0.000 (0.002) loss 1.4312 (1.1629) teacher_loss 1.1927 (0.9695) loss_zs_kd 0.2679 (0.3218) loss_oracle 0.2385 (0.1935) acc 75.0000 (74.0505) lr 1.9686e-03 eta 0:22:16
epoch [6/50] batch [280/288] time 0.111 (0.105) data 0.000 (0.001) loss 1.4354 (1.1656) teacher_loss 1.2753 (0.9711) loss_zs_kd 0.5324 (0.3234) loss_oracle 0.1601 (0.1945) acc 71.8750 (74.0737) lr 1.9686e-03 eta 0:22:14
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,409
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,011
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.7%
******* Domain a best val acc:      86.8%, epoch: 5 *******
******* Domain a best val test acc: 82.8%, epoch: 5 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [7/50] batch [20/288] time 0.096 (0.128) data 0.000 (0.015) loss 1.3271 (1.1750) teacher_loss 1.1412 (0.9983) loss_zs_kd 0.2980 (0.3278) loss_oracle 0.1859 (0.1766) acc 68.7500 (73.9062) lr 1.9511e-03 eta 0:26:56
epoch [7/50] batch [40/288] time 0.114 (0.116) data 0.000 (0.008) loss 0.9109 (1.1295) teacher_loss 0.7037 (0.9530) loss_zs_kd 0.3059 (0.3215) loss_oracle 0.2072 (0.1765) acc 78.1250 (74.7656) lr 1.9511e-03 eta 0:24:24
epoch [7/50] batch [60/288] time 0.119 (0.113) data 0.000 (0.005) loss 1.4133 (1.1205) teacher_loss 1.2230 (0.9332) loss_zs_kd 0.3846 (0.3319) loss_oracle 0.1903 (0.1873) acc 68.7500 (74.8958) lr 1.9511e-03 eta 0:23:49
epoch [7/50] batch [80/288] time 0.100 (0.111) data 0.000 (0.004) loss 0.9957 (1.1257) teacher_loss 0.8213 (0.9358) loss_zs_kd 0.3416 (0.3350) loss_oracle 0.1745 (0.1899) acc 78.1250 (74.4922) lr 1.9511e-03 eta 0:23:14
epoch [7/50] batch [100/288] time 0.103 (0.109) data 0.000 (0.003) loss 0.7770 (1.1278) teacher_loss 0.5657 (0.9359) loss_zs_kd 0.2630 (0.3318) loss_oracle 0.2113 (0.1919) acc 84.3750 (74.2500) lr 1.9511e-03 eta 0:22:50
epoch [7/50] batch [120/288] time 0.096 (0.108) data 0.000 (0.003) loss 1.5975 (1.1287) teacher_loss 1.3970 (0.9344) loss_zs_kd 0.3305 (0.3318) loss_oracle 0.2005 (0.1943) acc 68.7500 (74.3229) lr 1.9511e-03 eta 0:22:33
epoch [7/50] batch [140/288] time 0.100 (0.107) data 0.000 (0.002) loss 1.2250 (1.1371) teacher_loss 1.0162 (0.9447) loss_zs_kd 0.2869 (0.3332) loss_oracle 0.2088 (0.1924) acc 71.8750 (74.2411) lr 1.9511e-03 eta 0:22:22
epoch [7/50] batch [160/288] time 0.098 (0.106) data 0.000 (0.002) loss 1.2018 (1.1512) teacher_loss 0.9840 (0.9604) loss_zs_kd 0.3437 (0.3315) loss_oracle 0.2178 (0.1908) acc 75.0000 (73.9453) lr 1.9511e-03 eta 0:22:10
epoch [7/50] batch [180/288] time 0.106 (0.106) data 0.000 (0.002) loss 1.1783 (1.1554) teacher_loss 1.0193 (0.9667) loss_zs_kd 0.3621 (0.3329) loss_oracle 0.1590 (0.1887) acc 75.0000 (73.9583) lr 1.9511e-03 eta 0:22:01
epoch [7/50] batch [200/288] time 0.102 (0.106) data 0.000 (0.002) loss 0.8070 (1.1455) teacher_loss 0.6111 (0.9561) loss_zs_kd 0.3999 (0.3348) loss_oracle 0.1959 (0.1894) acc 71.8750 (74.1094) lr 1.9511e-03 eta 0:21:56
epoch [7/50] batch [220/288] time 0.102 (0.105) data 0.000 (0.002) loss 1.4307 (1.1483) teacher_loss 1.2392 (0.9579) loss_zs_kd 0.3598 (0.3344) loss_oracle 0.1915 (0.1903) acc 68.7500 (74.0199) lr 1.9511e-03 eta 0:21:48
epoch [7/50] batch [240/288] time 0.096 (0.105) data 0.000 (0.002) loss 1.5756 (1.1560) teacher_loss 1.3934 (0.9648) loss_zs_kd 0.3732 (0.3344) loss_oracle 0.1822 (0.1911) acc 65.6250 (74.0104) lr 1.9511e-03 eta 0:21:42
epoch [7/50] batch [260/288] time 0.099 (0.105) data 0.000 (0.001) loss 1.1472 (1.1607) teacher_loss 0.9645 (0.9687) loss_zs_kd 0.2554 (0.3320) loss_oracle 0.1827 (0.1921) acc 75.0000 (73.9543) lr 1.9511e-03 eta 0:21:38
epoch [7/50] batch [280/288] time 0.110 (0.105) data 0.000 (0.001) loss 0.9669 (1.1567) teacher_loss 0.6969 (0.9641) loss_zs_kd 0.2523 (0.3280) loss_oracle 0.2700 (0.1926) acc 78.1250 (74.1853) lr 1.9511e-03 eta 0:21:35
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,416
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,004
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.1%
******* Domain a best val acc:      86.8%, epoch: 5 *******
******* Domain a best val test acc: 82.8%, epoch: 5 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [8/50] batch [20/288] time 0.113 (0.125) data 0.000 (0.017) loss 1.0429 (1.1144) teacher_loss 0.8245 (0.9205) loss_zs_kd 0.4867 (0.3551) loss_oracle 0.2184 (0.1938) acc 87.5000 (75.9375) lr 1.9298e-03 eta 0:25:50
epoch [8/50] batch [40/288] time 0.104 (0.114) data 0.000 (0.008) loss 1.0556 (1.1097) teacher_loss 0.8303 (0.9176) loss_zs_kd 0.3934 (0.3412) loss_oracle 0.2253 (0.1921) acc 78.1250 (75.3906) lr 1.9298e-03 eta 0:23:32
epoch [8/50] batch [60/288] time 0.097 (0.111) data 0.000 (0.006) loss 1.8242 (1.1041) teacher_loss 1.6054 (0.9079) loss_zs_kd 0.2005 (0.3393) loss_oracle 0.2188 (0.1963) acc 62.5000 (75.3646) lr 1.9298e-03 eta 0:22:53
epoch [8/50] batch [80/288] time 0.098 (0.109) data 0.000 (0.004) loss 1.3494 (1.0985) teacher_loss 1.1253 (0.9021) loss_zs_kd 0.2475 (0.3462) loss_oracle 0.2241 (0.1965) acc 68.7500 (75.5078) lr 1.9298e-03 eta 0:22:18
epoch [8/50] batch [100/288] time 0.100 (0.107) data 0.000 (0.004) loss 0.8635 (1.1052) teacher_loss 0.7091 (0.9109) loss_zs_kd 0.5186 (0.3529) loss_oracle 0.1543 (0.1943) acc 87.5000 (75.5312) lr 1.9298e-03 eta 0:21:57
epoch [8/50] batch [120/288] time 0.101 (0.106) data 0.000 (0.003) loss 0.9291 (1.0942) teacher_loss 0.7580 (0.9033) loss_zs_kd 0.4026 (0.3564) loss_oracle 0.1711 (0.1909) acc 81.2500 (75.7812) lr 1.9298e-03 eta 0:21:44
epoch [8/50] batch [140/288] time 0.104 (0.106) data 0.000 (0.003) loss 1.1857 (1.1098) teacher_loss 0.9522 (0.9182) loss_zs_kd 0.4233 (0.3566) loss_oracle 0.2335 (0.1916) acc 65.6250 (75.3348) lr 1.9298e-03 eta 0:21:34
epoch [8/50] batch [160/288] time 0.110 (0.105) data 0.000 (0.002) loss 1.3198 (1.1351) teacher_loss 1.1543 (0.9404) loss_zs_kd 0.3826 (0.3550) loss_oracle 0.1655 (0.1947) acc 68.7500 (74.7656) lr 1.9298e-03 eta 0:21:26
epoch [8/50] batch [180/288] time 0.110 (0.105) data 0.001 (0.002) loss 1.5657 (1.1384) teacher_loss 1.3641 (0.9423) loss_zs_kd 0.2792 (0.3505) loss_oracle 0.2016 (0.1961) acc 68.7500 (74.6701) lr 1.9298e-03 eta 0:21:18
epoch [8/50] batch [200/288] time 0.102 (0.105) data 0.000 (0.002) loss 1.4137 (1.1409) teacher_loss 1.1968 (0.9446) loss_zs_kd 0.3225 (0.3503) loss_oracle 0.2169 (0.1963) acc 71.8750 (74.5312) lr 1.9298e-03 eta 0:21:13
epoch [8/50] batch [220/288] time 0.100 (0.104) data 0.001 (0.002) loss 0.8979 (1.1369) teacher_loss 0.6712 (0.9399) loss_zs_kd 0.3349 (0.3494) loss_oracle 0.2267 (0.1970) acc 84.3750 (74.7301) lr 1.9298e-03 eta 0:21:10
epoch [8/50] batch [240/288] time 0.112 (0.104) data 0.000 (0.002) loss 1.1073 (1.1425) teacher_loss 0.8819 (0.9454) loss_zs_kd 0.4052 (0.3519) loss_oracle 0.2254 (0.1971) acc 75.0000 (74.5312) lr 1.9298e-03 eta 0:21:08
epoch [8/50] batch [260/288] time 0.103 (0.104) data 0.000 (0.002) loss 0.9754 (1.1429) teacher_loss 0.8065 (0.9457) loss_zs_kd 0.4775 (0.3504) loss_oracle 0.1689 (0.1972) acc 81.2500 (74.6995) lr 1.9298e-03 eta 0:21:05
epoch [8/50] batch [280/288] time 0.111 (0.104) data 0.000 (0.001) loss 1.5513 (1.1499) teacher_loss 1.3790 (0.9532) loss_zs_kd 0.3715 (0.3477) loss_oracle 0.1723 (0.1967) acc 59.3750 (74.4754) lr 1.9298e-03 eta 0:21:03
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,436
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.6%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,009
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.3%
******* Domain a best val acc:      87.2%, epoch: 8 *******
******* Domain a best val test acc: 82.8%, epoch: 8 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [9/50] batch [20/288] time 0.102 (0.122) data 0.000 (0.014) loss 1.2263 (1.1735) teacher_loss 1.0311 (0.9636) loss_zs_kd 0.2954 (0.3353) loss_oracle 0.1951 (0.2099) acc 71.8750 (73.5938) lr 1.9048e-03 eta 0:24:34
epoch [9/50] batch [40/288] time 0.102 (0.114) data 0.000 (0.007) loss 0.7765 (1.1255) teacher_loss 0.5936 (0.9131) loss_zs_kd 0.2011 (0.3264) loss_oracle 0.1829 (0.2125) acc 87.5000 (75.4688) lr 1.9048e-03 eta 0:22:49
epoch [9/50] batch [60/288] time 0.102 (0.110) data 0.001 (0.005) loss 1.3063 (1.1190) teacher_loss 1.1381 (0.9107) loss_zs_kd 0.3318 (0.3354) loss_oracle 0.1682 (0.2083) acc 62.5000 (75.8333) lr 1.9048e-03 eta 0:22:04
epoch [9/50] batch [80/288] time 0.104 (0.108) data 0.000 (0.004) loss 0.9593 (1.1457) teacher_loss 0.7691 (0.9436) loss_zs_kd 0.3619 (0.3505) loss_oracle 0.1902 (0.2021) acc 87.5000 (75.2344) lr 1.9048e-03 eta 0:21:42
epoch [9/50] batch [100/288] time 0.097 (0.107) data 0.000 (0.003) loss 0.8905 (1.1415) teacher_loss 0.6823 (0.9434) loss_zs_kd 0.2956 (0.3456) loss_oracle 0.2082 (0.1981) acc 78.1250 (75.4375) lr 1.9048e-03 eta 0:21:21
epoch [9/50] batch [120/288] time 0.097 (0.106) data 0.000 (0.003) loss 1.2421 (1.1514) teacher_loss 1.0295 (0.9545) loss_zs_kd 0.3894 (0.3453) loss_oracle 0.2125 (0.1969) acc 75.0000 (75.0000) lr 1.9048e-03 eta 0:21:07
epoch [9/50] batch [140/288] time 0.098 (0.105) data 0.000 (0.002) loss 1.2364 (1.1595) teacher_loss 0.9720 (0.9619) loss_zs_kd 0.1762 (0.3468) loss_oracle 0.2645 (0.1976) acc 78.1250 (75.0670) lr 1.9048e-03 eta 0:20:56
epoch [9/50] batch [160/288] time 0.102 (0.104) data 0.000 (0.002) loss 0.9430 (1.1544) teacher_loss 0.7200 (0.9565) loss_zs_kd 0.4239 (0.3551) loss_oracle 0.2230 (0.1979) acc 81.2500 (74.9414) lr 1.9048e-03 eta 0:20:44
epoch [9/50] batch [180/288] time 0.104 (0.104) data 0.000 (0.002) loss 1.8772 (1.1593) teacher_loss 1.6668 (0.9597) loss_zs_kd 0.3903 (0.3525) loss_oracle 0.2104 (0.1995) acc 68.7500 (74.9132) lr 1.9048e-03 eta 0:20:40
epoch [9/50] batch [200/288] time 0.110 (0.104) data 0.000 (0.002) loss 1.5463 (1.1698) teacher_loss 1.3494 (0.9695) loss_zs_kd 0.3732 (0.3524) loss_oracle 0.1970 (0.2003) acc 71.8750 (74.6250) lr 1.9048e-03 eta 0:20:38
epoch [9/50] batch [220/288] time 0.099 (0.104) data 0.000 (0.002) loss 0.8099 (1.1700) teacher_loss 0.6111 (0.9697) loss_zs_kd 0.3770 (0.3498) loss_oracle 0.1988 (0.2003) acc 78.1250 (74.5312) lr 1.9048e-03 eta 0:20:35
epoch [9/50] batch [240/288] time 0.104 (0.104) data 0.000 (0.001) loss 1.0701 (1.1684) teacher_loss 0.9116 (0.9667) loss_zs_kd 0.5800 (0.3502) loss_oracle 0.1585 (0.2017) acc 78.1250 (74.5833) lr 1.9048e-03 eta 0:20:32
epoch [9/50] batch [260/288] time 0.098 (0.104) data 0.000 (0.001) loss 1.1566 (1.1728) teacher_loss 0.9361 (0.9704) loss_zs_kd 0.2960 (0.3510) loss_oracle 0.2205 (0.2024) acc 71.8750 (74.4832) lr 1.9048e-03 eta 0:20:26
epoch [9/50] batch [280/288] time 0.112 (0.104) data 0.000 (0.001) loss 1.1265 (1.1665) teacher_loss 0.9128 (0.9640) loss_zs_kd 0.1968 (0.3520) loss_oracle 0.2136 (0.2025) acc 78.1250 (74.5759) lr 1.9048e-03 eta 0:20:32
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,432
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,009
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.3%
******* Domain a best val acc:      87.2%, epoch: 8 *******
******* Domain a best val test acc: 82.8%, epoch: 8 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [10/50] batch [20/288] time 0.091 (0.112) data 0.000 (0.012) loss 0.9575 (1.1830) teacher_loss 0.7529 (0.9966) loss_zs_kd 0.2450 (0.3417) loss_oracle 0.2046 (0.1864) acc 81.2500 (74.6875) lr 1.8763e-03 eta 0:22:00
epoch [10/50] batch [40/288] time 0.098 (0.105) data 0.001 (0.006) loss 1.4478 (1.1387) teacher_loss 1.2447 (0.9493) loss_zs_kd 0.4422 (0.3551) loss_oracle 0.2030 (0.1894) acc 68.7500 (75.7031) lr 1.8763e-03 eta 0:20:39
epoch [10/50] batch [60/288] time 0.090 (0.102) data 0.000 (0.004) loss 1.1059 (1.1084) teacher_loss 0.9102 (0.9174) loss_zs_kd 0.2725 (0.3459) loss_oracle 0.1957 (0.1910) acc 75.0000 (75.8854) lr 1.8763e-03 eta 0:19:58
epoch [10/50] batch [80/288] time 0.099 (0.101) data 0.000 (0.003) loss 1.3210 (1.1389) teacher_loss 1.1160 (0.9435) loss_zs_kd 0.2285 (0.3506) loss_oracle 0.2051 (0.1954) acc 81.2500 (75.1172) lr 1.8763e-03 eta 0:19:39
epoch [10/50] batch [100/288] time 0.095 (0.100) data 0.000 (0.003) loss 1.3651 (1.1276) teacher_loss 1.1826 (0.9332) loss_zs_kd 0.3353 (0.3415) loss_oracle 0.1826 (0.1944) acc 75.0000 (75.5000) lr 1.8763e-03 eta 0:19:27
epoch [10/50] batch [120/288] time 0.099 (0.099) data 0.000 (0.002) loss 1.1021 (1.1243) teacher_loss 0.8587 (0.9292) loss_zs_kd 0.3575 (0.3427) loss_oracle 0.2434 (0.1952) acc 81.2500 (75.7552) lr 1.8763e-03 eta 0:19:19
epoch [10/50] batch [140/288] time 0.095 (0.099) data 0.000 (0.002) loss 1.3662 (1.1395) teacher_loss 1.2027 (0.9433) loss_zs_kd 0.3626 (0.3495) loss_oracle 0.1636 (0.1962) acc 71.8750 (75.5357) lr 1.8763e-03 eta 0:19:13
epoch [10/50] batch [160/288] time 0.103 (0.099) data 0.000 (0.002) loss 0.8415 (1.1419) teacher_loss 0.6371 (0.9411) loss_zs_kd 0.2719 (0.3484) loss_oracle 0.2044 (0.2008) acc 90.6250 (75.5664) lr 1.8763e-03 eta 0:19:08
epoch [10/50] batch [180/288] time 0.099 (0.098) data 0.000 (0.001) loss 1.2748 (1.1554) teacher_loss 1.0646 (0.9524) loss_zs_kd 0.2554 (0.3469) loss_oracle 0.2103 (0.2030) acc 75.0000 (75.3299) lr 1.8763e-03 eta 0:19:01
epoch [10/50] batch [200/288] time 0.099 (0.098) data 0.000 (0.001) loss 1.5050 (1.1615) teacher_loss 1.2735 (0.9593) loss_zs_kd 0.2999 (0.3457) loss_oracle 0.2315 (0.2023) acc 68.7500 (75.2031) lr 1.8763e-03 eta 0:18:59
epoch [10/50] batch [220/288] time 0.095 (0.098) data 0.000 (0.001) loss 1.1140 (1.1613) teacher_loss 0.9755 (0.9599) loss_zs_kd 0.3865 (0.3451) loss_oracle 0.1385 (0.2013) acc 71.8750 (75.2699) lr 1.8763e-03 eta 0:18:58
epoch [10/50] batch [240/288] time 0.098 (0.098) data 0.000 (0.001) loss 0.9362 (1.1612) teacher_loss 0.7478 (0.9608) loss_zs_kd 0.4548 (0.3460) loss_oracle 0.1885 (0.2003) acc 84.3750 (75.2734) lr 1.8763e-03 eta 0:18:58
epoch [10/50] batch [260/288] time 0.112 (0.099) data 0.000 (0.001) loss 1.1809 (1.1595) teacher_loss 1.0588 (0.9604) loss_zs_kd 0.3432 (0.3488) loss_oracle 0.1221 (0.1991) acc 71.8750 (75.1683) lr 1.8763e-03 eta 0:19:06
epoch [10/50] batch [280/288] time 0.111 (0.100) data 0.000 (0.001) loss 1.6542 (1.1658) teacher_loss 1.4707 (0.9681) loss_zs_kd 0.4494 (0.3514) loss_oracle 0.1834 (0.1977) acc 65.6250 (74.9777) lr 1.8763e-03 eta 0:19:07
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,430
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.7%
******* Domain a best val acc:      87.2%, epoch: 8 *******
******* Domain a best val test acc: 82.8%, epoch: 8 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [11/50] batch [20/288] time 0.092 (0.115) data 0.000 (0.015) loss 1.6687 (1.1750) teacher_loss 1.4586 (0.9722) loss_zs_kd 0.3308 (0.3482) loss_oracle 0.2101 (0.2028) acc 62.5000 (74.2188) lr 1.8443e-03 eta 0:21:57
epoch [11/50] batch [40/288] time 0.098 (0.105) data 0.000 (0.007) loss 1.3737 (1.1718) teacher_loss 1.1445 (0.9707) loss_zs_kd 0.4284 (0.3614) loss_oracle 0.2293 (0.2011) acc 68.7500 (73.2812) lr 1.8443e-03 eta 0:20:03
epoch [11/50] batch [60/288] time 0.101 (0.102) data 0.001 (0.005) loss 0.8575 (1.1757) teacher_loss 0.7072 (0.9725) loss_zs_kd 0.4340 (0.3601) loss_oracle 0.1503 (0.2033) acc 84.3750 (73.1771) lr 1.8443e-03 eta 0:19:28
epoch [11/50] batch [80/288] time 0.105 (0.101) data 0.000 (0.004) loss 1.3320 (1.1572) teacher_loss 1.1325 (0.9503) loss_zs_kd 0.2664 (0.3537) loss_oracle 0.1995 (0.2069) acc 65.6250 (73.7891) lr 1.8443e-03 eta 0:19:13
epoch [11/50] batch [100/288] time 0.103 (0.100) data 0.000 (0.003) loss 1.4045 (1.1424) teacher_loss 1.1543 (0.9360) loss_zs_kd 0.4165 (0.3554) loss_oracle 0.2502 (0.2064) acc 75.0000 (74.3125) lr 1.8443e-03 eta 0:19:02
epoch [11/50] batch [120/288] time 0.100 (0.100) data 0.000 (0.003) loss 1.7923 (1.1512) teacher_loss 1.5068 (0.9375) loss_zs_kd 0.2926 (0.3561) loss_oracle 0.2855 (0.2136) acc 53.1250 (74.4271) lr 1.8443e-03 eta 0:19:01
epoch [11/50] batch [140/288] time 0.096 (0.100) data 0.000 (0.002) loss 1.0977 (1.1626) teacher_loss 0.8365 (0.9393) loss_zs_kd 0.3052 (0.3546) loss_oracle 0.2612 (0.2233) acc 81.2500 (74.5089) lr 1.8443e-03 eta 0:18:54
epoch [11/50] batch [160/288] time 0.104 (0.100) data 0.000 (0.002) loss 1.5473 (1.1750) teacher_loss 1.3346 (0.9519) loss_zs_kd 0.3033 (0.3549) loss_oracle 0.2127 (0.2231) acc 65.6250 (74.2969) lr 1.8443e-03 eta 0:18:55
epoch [11/50] batch [180/288] time 0.099 (0.101) data 0.000 (0.002) loss 1.1802 (1.1734) teacher_loss 0.9876 (0.9492) loss_zs_kd 0.4248 (0.3565) loss_oracle 0.1926 (0.2242) acc 78.1250 (74.3056) lr 1.8443e-03 eta 0:19:00
epoch [11/50] batch [200/288] time 0.094 (0.100) data 0.000 (0.002) loss 1.3164 (1.1701) teacher_loss 1.0682 (0.9458) loss_zs_kd 0.5440 (0.3554) loss_oracle 0.2481 (0.2243) acc 78.1250 (74.4531) lr 1.8443e-03 eta 0:18:52
epoch [11/50] batch [220/288] time 0.142 (0.100) data 0.000 (0.001) loss 1.2371 (1.1664) teacher_loss 1.0281 (0.9439) loss_zs_kd 0.3830 (0.3615) loss_oracle 0.2090 (0.2224) acc 84.3750 (74.5170) lr 1.8443e-03 eta 0:18:52
epoch [11/50] batch [240/288] time 0.093 (0.100) data 0.000 (0.001) loss 1.1339 (1.1593) teacher_loss 0.8852 (0.9389) loss_zs_kd 0.3388 (0.3640) loss_oracle 0.2486 (0.2204) acc 71.8750 (74.6224) lr 1.8443e-03 eta 0:18:47
epoch [11/50] batch [260/288] time 0.093 (0.100) data 0.000 (0.001) loss 1.0772 (1.1576) teacher_loss 0.8572 (0.9396) loss_zs_kd 0.3313 (0.3653) loss_oracle 0.2200 (0.2180) acc 75.0000 (74.6394) lr 1.8443e-03 eta 0:18:40
epoch [11/50] batch [280/288] time 0.089 (0.099) data 0.000 (0.001) loss 0.8864 (1.1475) teacher_loss 0.7285 (0.9321) loss_zs_kd 0.2337 (0.3643) loss_oracle 0.1579 (0.2154) acc 78.1250 (74.9219) lr 1.8443e-03 eta 0:18:34
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,441
* accuracy: 87.4%
* error: 12.6%
* macro_f1: 86.8%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,012
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.5%
******* Domain a best val acc:      87.4%, epoch: 11 *******
******* Domain a best val test acc: 82.9%, epoch: 11 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [12/50] batch [20/288] time 0.091 (0.118) data 0.000 (0.015) loss 1.1680 (1.1674) teacher_loss 1.0062 (0.9783) loss_zs_kd 0.2628 (0.3776) loss_oracle 0.1618 (0.1892) acc 84.3750 (74.2188) lr 1.8090e-03 eta 0:22:01
epoch [12/50] batch [40/288] time 0.104 (0.107) data 0.000 (0.008) loss 1.2181 (1.1242) teacher_loss 1.0480 (0.9340) loss_zs_kd 0.2643 (0.3719) loss_oracle 0.1702 (0.1903) acc 71.8750 (74.5312) lr 1.8090e-03 eta 0:19:58
epoch [12/50] batch [60/288] time 0.098 (0.103) data 0.001 (0.005) loss 1.3071 (1.1533) teacher_loss 1.1291 (0.9543) loss_zs_kd 0.3876 (0.3740) loss_oracle 0.1780 (0.1990) acc 75.0000 (74.1146) lr 1.8090e-03 eta 0:19:11
epoch [12/50] batch [80/288] time 0.097 (0.101) data 0.000 (0.004) loss 2.2038 (1.1666) teacher_loss 2.0187 (0.9663) loss_zs_kd 0.4112 (0.3716) loss_oracle 0.1850 (0.2003) acc 56.2500 (74.3359) lr 1.8090e-03 eta 0:18:49
epoch [12/50] batch [100/288] time 0.103 (0.100) data 0.000 (0.003) loss 1.1068 (1.1418) teacher_loss 0.9569 (0.9440) loss_zs_kd 0.3774 (0.3604) loss_oracle 0.1499 (0.1978) acc 65.6250 (74.8750) lr 1.8090e-03 eta 0:18:37
epoch [12/50] batch [120/288] time 0.092 (0.100) data 0.000 (0.003) loss 1.4015 (1.1388) teacher_loss 1.1930 (0.9378) loss_zs_kd 0.3773 (0.3578) loss_oracle 0.2085 (0.2010) acc 65.6250 (75.1823) lr 1.8090e-03 eta 0:18:27
epoch [12/50] batch [140/288] time 0.092 (0.099) data 0.000 (0.002) loss 0.6924 (1.1467) teacher_loss 0.4521 (0.9409) loss_zs_kd 0.2947 (0.3620) loss_oracle 0.2403 (0.2058) acc 87.5000 (75.0223) lr 1.8090e-03 eta 0:18:18
epoch [12/50] batch [160/288] time 0.092 (0.099) data 0.000 (0.002) loss 1.2560 (1.1482) teacher_loss 1.0124 (0.9394) loss_zs_kd 0.5022 (0.3656) loss_oracle 0.2437 (0.2089) acc 75.0000 (75.0586) lr 1.8090e-03 eta 0:18:14
epoch [12/50] batch [180/288] time 0.094 (0.099) data 0.000 (0.002) loss 1.0948 (1.1674) teacher_loss 0.8396 (0.9543) loss_zs_kd 0.4480 (0.3680) loss_oracle 0.2552 (0.2131) acc 75.0000 (74.5486) lr 1.8090e-03 eta 0:18:09
epoch [12/50] batch [200/288] time 0.112 (0.098) data 0.000 (0.002) loss 1.3642 (1.1730) teacher_loss 1.0735 (0.9560) loss_zs_kd 0.4209 (0.3685) loss_oracle 0.2907 (0.2170) acc 68.7500 (74.6406) lr 1.8090e-03 eta 0:18:04
epoch [12/50] batch [220/288] time 0.100 (0.099) data 0.000 (0.002) loss 0.8342 (1.1692) teacher_loss 0.6129 (0.9512) loss_zs_kd 0.5032 (0.3721) loss_oracle 0.2212 (0.2179) acc 78.1250 (74.8438) lr 1.8090e-03 eta 0:18:08
epoch [12/50] batch [240/288] time 0.094 (0.099) data 0.000 (0.001) loss 1.6036 (1.1741) teacher_loss 1.3408 (0.9541) loss_zs_kd 0.4870 (0.3728) loss_oracle 0.2628 (0.2200) acc 65.6250 (74.7917) lr 1.8090e-03 eta 0:18:04
epoch [12/50] batch [260/288] time 0.091 (0.098) data 0.000 (0.001) loss 0.9325 (1.1712) teacher_loss 0.6569 (0.9493) loss_zs_kd 0.5018 (0.3738) loss_oracle 0.2756 (0.2219) acc 81.2500 (74.8798) lr 1.8090e-03 eta 0:17:58
epoch [12/50] batch [280/288] time 0.087 (0.098) data 0.000 (0.001) loss 1.1421 (1.1711) teacher_loss 0.8661 (0.9481) loss_zs_kd 0.2841 (0.3749) loss_oracle 0.2760 (0.2230) acc 75.0000 (74.8549) lr 1.8090e-03 eta 0:17:53
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,440
* accuracy: 87.3%
* error: 12.7%
* macro_f1: 86.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,004
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.3%
******* Domain a best val acc:      87.4%, epoch: 11 *******
******* Domain a best val test acc: 82.9%, epoch: 11 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [13/50] batch [20/288] time 0.098 (0.115) data 0.000 (0.014) loss 1.0001 (1.1261) teacher_loss 0.8056 (0.8985) loss_zs_kd 0.3957 (0.3739) loss_oracle 0.1944 (0.2276) acc 75.0000 (75.3125) lr 1.7705e-03 eta 0:20:55
epoch [13/50] batch [40/288] time 0.093 (0.105) data 0.000 (0.007) loss 0.9082 (1.2034) teacher_loss 0.7174 (0.9804) loss_zs_kd 0.5046 (0.3848) loss_oracle 0.1908 (0.2230) acc 84.3750 (73.8281) lr 1.7705e-03 eta 0:19:08
epoch [13/50] batch [60/288] time 0.094 (0.102) data 0.000 (0.005) loss 1.7329 (1.2506) teacher_loss 1.4617 (1.0245) loss_zs_kd 0.4677 (0.3821) loss_oracle 0.2711 (0.2261) acc 65.6250 (73.1250) lr 1.7705e-03 eta 0:18:32
epoch [13/50] batch [80/288] time 0.093 (0.101) data 0.000 (0.004) loss 1.4261 (1.2307) teacher_loss 1.1951 (0.9978) loss_zs_kd 0.4648 (0.3861) loss_oracle 0.2310 (0.2329) acc 71.8750 (73.9844) lr 1.7705e-03 eta 0:18:14
epoch [13/50] batch [100/288] time 0.100 (0.100) data 0.000 (0.003) loss 0.8937 (1.1982) teacher_loss 0.6718 (0.9683) loss_zs_kd 0.1889 (0.3802) loss_oracle 0.2219 (0.2299) acc 84.3750 (74.8750) lr 1.7705e-03 eta 0:18:02
epoch [13/50] batch [120/288] time 0.094 (0.099) data 0.000 (0.003) loss 1.1776 (1.1870) teacher_loss 0.9256 (0.9611) loss_zs_kd 0.3552 (0.3787) loss_oracle 0.2520 (0.2259) acc 81.2500 (75.1823) lr 1.7705e-03 eta 0:17:54
epoch [13/50] batch [140/288] time 0.101 (0.099) data 0.001 (0.002) loss 1.0384 (1.1981) teacher_loss 0.8217 (0.9733) loss_zs_kd 0.3587 (0.3729) loss_oracle 0.2167 (0.2248) acc 87.5000 (74.9330) lr 1.7705e-03 eta 0:17:48
epoch [13/50] batch [160/288] time 0.092 (0.099) data 0.000 (0.002) loss 1.4377 (1.1853) teacher_loss 1.1774 (0.9609) loss_zs_kd 0.3213 (0.3690) loss_oracle 0.2603 (0.2244) acc 75.0000 (75.1953) lr 1.7705e-03 eta 0:17:43
epoch [13/50] batch [180/288] time 0.096 (0.098) data 0.000 (0.002) loss 0.7876 (1.1913) teacher_loss 0.6047 (0.9685) loss_zs_kd 0.2247 (0.3714) loss_oracle 0.1828 (0.2227) acc 84.3750 (74.9826) lr 1.7705e-03 eta 0:17:39
epoch [13/50] batch [200/288] time 0.093 (0.099) data 0.000 (0.002) loss 1.5444 (1.1859) teacher_loss 1.3700 (0.9643) loss_zs_kd 0.2946 (0.3756) loss_oracle 0.1743 (0.2216) acc 62.5000 (75.0156) lr 1.7705e-03 eta 0:17:41
epoch [13/50] batch [220/288] time 0.098 (0.099) data 0.000 (0.001) loss 1.0615 (1.1824) teacher_loss 0.7656 (0.9608) loss_zs_kd 0.5788 (0.3795) loss_oracle 0.2959 (0.2216) acc 84.3750 (75.0000) lr 1.7705e-03 eta 0:17:36
epoch [13/50] batch [240/288] time 0.099 (0.098) data 0.000 (0.001) loss 1.3198 (1.1782) teacher_loss 1.1448 (0.9565) loss_zs_kd 0.3910 (0.3774) loss_oracle 0.1750 (0.2218) acc 71.8750 (75.0260) lr 1.7705e-03 eta 0:17:32
epoch [13/50] batch [260/288] time 0.092 (0.098) data 0.000 (0.001) loss 1.1606 (1.1663) teacher_loss 0.9848 (0.9453) loss_zs_kd 0.3354 (0.3746) loss_oracle 0.1757 (0.2211) acc 75.0000 (75.2764) lr 1.7705e-03 eta 0:17:28
epoch [13/50] batch [280/288] time 0.088 (0.098) data 0.000 (0.001) loss 0.8595 (1.1666) teacher_loss 0.6597 (0.9457) loss_zs_kd 0.2926 (0.3730) loss_oracle 0.1998 (0.2209) acc 75.0000 (75.2009) lr 1.7705e-03 eta 0:17:23
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,421
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,999
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 78.9%
******* Domain a best val acc:      87.4%, epoch: 11 *******
******* Domain a best val test acc: 82.9%, epoch: 11 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [14/50] batch [20/288] time 0.099 (0.119) data 0.000 (0.017) loss 1.2595 (1.1782) teacher_loss 1.0530 (0.9715) loss_zs_kd 0.3438 (0.3423) loss_oracle 0.2065 (0.2067) acc 75.0000 (75.4688) lr 1.7290e-03 eta 0:21:06
epoch [14/50] batch [40/288] time 0.105 (0.109) data 0.000 (0.008) loss 0.9419 (1.1415) teacher_loss 0.6817 (0.9208) loss_zs_kd 0.5136 (0.3543) loss_oracle 0.2602 (0.2207) acc 81.2500 (76.0938) lr 1.7290e-03 eta 0:19:22
epoch [14/50] batch [60/288] time 0.104 (0.106) data 0.000 (0.006) loss 1.1501 (1.1230) teacher_loss 0.9153 (0.8939) loss_zs_kd 0.3690 (0.3768) loss_oracle 0.2348 (0.2291) acc 75.0000 (76.3021) lr 1.7290e-03 eta 0:18:46
epoch [14/50] batch [80/288] time 0.098 (0.105) data 0.000 (0.004) loss 1.2308 (1.1501) teacher_loss 1.0041 (0.9193) loss_zs_kd 0.3325 (0.3921) loss_oracle 0.2267 (0.2308) acc 75.0000 (75.7812) lr 1.7290e-03 eta 0:18:28
epoch [14/50] batch [100/288] time 0.099 (0.104) data 0.000 (0.004) loss 1.3968 (1.1476) teacher_loss 1.1688 (0.9157) loss_zs_kd 0.3986 (0.4013) loss_oracle 0.2280 (0.2319) acc 71.8750 (75.7500) lr 1.7290e-03 eta 0:18:16
epoch [14/50] batch [120/288] time 0.104 (0.104) data 0.000 (0.003) loss 0.9400 (1.1643) teacher_loss 0.7106 (0.9310) loss_zs_kd 0.3536 (0.3957) loss_oracle 0.2295 (0.2334) acc 84.3750 (75.4948) lr 1.7290e-03 eta 0:18:11
epoch [14/50] batch [140/288] time 0.113 (0.103) data 0.000 (0.003) loss 0.9220 (1.1783) teacher_loss 0.6863 (0.9443) loss_zs_kd 0.3596 (0.3920) loss_oracle 0.2357 (0.2340) acc 81.2500 (75.4464) lr 1.7290e-03 eta 0:18:06
epoch [14/50] batch [160/288] time 0.100 (0.103) data 0.000 (0.002) loss 0.8835 (1.1666) teacher_loss 0.6549 (0.9327) loss_zs_kd 0.3524 (0.3875) loss_oracle 0.2286 (0.2339) acc 78.1250 (75.8203) lr 1.7290e-03 eta 0:18:03
epoch [14/50] batch [180/288] time 0.095 (0.103) data 0.000 (0.002) loss 0.7951 (1.1544) teacher_loss 0.5813 (0.9229) loss_zs_kd 0.2437 (0.3839) loss_oracle 0.2137 (0.2315) acc 84.3750 (75.9896) lr 1.7290e-03 eta 0:18:02
epoch [14/50] batch [200/288] time 0.092 (0.103) data 0.000 (0.002) loss 1.3445 (1.1565) teacher_loss 1.0650 (0.9264) loss_zs_kd 0.4214 (0.3843) loss_oracle 0.2795 (0.2301) acc 65.6250 (75.8750) lr 1.7290e-03 eta 0:17:53
epoch [14/50] batch [220/288] time 0.093 (0.102) data 0.000 (0.002) loss 0.5918 (1.1486) teacher_loss 0.3876 (0.9199) loss_zs_kd 0.3095 (0.3852) loss_oracle 0.2042 (0.2287) acc 90.6250 (76.0085) lr 1.7290e-03 eta 0:17:46
epoch [14/50] batch [240/288] time 0.100 (0.102) data 0.000 (0.002) loss 1.0704 (1.1418) teacher_loss 0.8442 (0.9139) loss_zs_kd 0.4621 (0.3861) loss_oracle 0.2262 (0.2279) acc 81.2500 (76.1328) lr 1.7290e-03 eta 0:17:39
epoch [14/50] batch [260/288] time 0.104 (0.101) data 0.000 (0.001) loss 1.4590 (1.1421) teacher_loss 1.2227 (0.9155) loss_zs_kd 0.4314 (0.3872) loss_oracle 0.2363 (0.2266) acc 75.0000 (76.1298) lr 1.7290e-03 eta 0:17:33
epoch [14/50] batch [280/288] time 0.087 (0.101) data 0.000 (0.001) loss 0.7261 (1.1398) teacher_loss 0.5276 (0.9151) loss_zs_kd 0.3066 (0.3861) loss_oracle 0.1985 (0.2247) acc 81.2500 (76.0714) lr 1.7290e-03 eta 0:17:27
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,423
* accuracy: 86.9%
* error: 13.1%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,996
* accuracy: 82.2%
* error: 17.8%
* macro_f1: 78.9%
******* Domain a best val acc:      87.4%, epoch: 11 *******
******* Domain a best val test acc: 82.9%, epoch: 11 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [15/50] batch [20/288] time 0.103 (0.127) data 0.000 (0.016) loss 1.2565 (1.1254) teacher_loss 1.0616 (0.9271) loss_zs_kd 0.2526 (0.3737) loss_oracle 0.1949 (0.1983) acc 78.1250 (74.8438) lr 1.6845e-03 eta 0:21:51
epoch [15/50] batch [40/288] time 0.097 (0.116) data 0.000 (0.008) loss 1.2609 (1.1885) teacher_loss 1.0239 (0.9902) loss_zs_kd 0.3108 (0.3690) loss_oracle 0.2370 (0.1984) acc 81.2500 (74.3750) lr 1.6845e-03 eta 0:19:57
epoch [15/50] batch [60/288] time 0.101 (0.114) data 0.000 (0.006) loss 0.7860 (1.1468) teacher_loss 0.5875 (0.9452) loss_zs_kd 0.2815 (0.3652) loss_oracle 0.1985 (0.2016) acc 81.2500 (75.4167) lr 1.6845e-03 eta 0:19:32
epoch [15/50] batch [80/288] time 0.099 (0.112) data 0.000 (0.004) loss 1.1833 (1.1185) teacher_loss 0.9620 (0.9098) loss_zs_kd 0.3299 (0.3614) loss_oracle 0.2213 (0.2088) acc 81.2500 (76.0547) lr 1.6845e-03 eta 0:19:09
epoch [15/50] batch [100/288] time 0.101 (0.110) data 0.000 (0.003) loss 1.0395 (1.1104) teacher_loss 0.8109 (0.8986) loss_zs_kd 0.3002 (0.3702) loss_oracle 0.2285 (0.2119) acc 78.1250 (76.0312) lr 1.6845e-03 eta 0:18:49
epoch [15/50] batch [120/288] time 0.100 (0.109) data 0.000 (0.003) loss 1.4671 (1.1200) teacher_loss 1.1953 (0.9041) loss_zs_kd 0.4905 (0.3701) loss_oracle 0.2718 (0.2159) acc 78.1250 (75.9896) lr 1.6845e-03 eta 0:18:34
epoch [15/50] batch [140/288] time 0.094 (0.109) data 0.000 (0.003) loss 1.2809 (1.1381) teacher_loss 1.0391 (0.9150) loss_zs_kd 0.2944 (0.3708) loss_oracle 0.2418 (0.2230) acc 75.0000 (75.7366) lr 1.6845e-03 eta 0:18:30
epoch [15/50] batch [160/288] time 0.100 (0.108) data 0.000 (0.002) loss 1.3862 (1.1457) teacher_loss 1.1389 (0.9208) loss_zs_kd 0.2723 (0.3715) loss_oracle 0.2473 (0.2250) acc 75.0000 (75.5664) lr 1.6845e-03 eta 0:18:19
epoch [15/50] batch [180/288] time 0.104 (0.107) data 0.000 (0.002) loss 1.6848 (1.1539) teacher_loss 1.3753 (0.9286) loss_zs_kd 0.4009 (0.3711) loss_oracle 0.3096 (0.2253) acc 68.7500 (75.2951) lr 1.6845e-03 eta 0:18:11
epoch [15/50] batch [200/288] time 0.106 (0.107) data 0.000 (0.002) loss 0.8364 (1.1628) teacher_loss 0.6306 (0.9358) loss_zs_kd 0.4629 (0.3749) loss_oracle 0.2057 (0.2271) acc 81.2500 (75.2969) lr 1.6845e-03 eta 0:18:03
epoch [15/50] batch [220/288] time 0.100 (0.106) data 0.000 (0.002) loss 1.2993 (1.1622) teacher_loss 1.0863 (0.9353) loss_zs_kd 0.3841 (0.3755) loss_oracle 0.2130 (0.2269) acc 75.0000 (75.2841) lr 1.6845e-03 eta 0:17:57
epoch [15/50] batch [240/288] time 0.100 (0.106) data 0.000 (0.002) loss 0.9316 (1.1565) teacher_loss 0.7348 (0.9302) loss_zs_kd 0.7169 (0.3785) loss_oracle 0.1968 (0.2264) acc 78.1250 (75.4557) lr 1.6845e-03 eta 0:17:52
epoch [15/50] batch [260/288] time 0.114 (0.106) data 0.000 (0.001) loss 1.1691 (1.1580) teacher_loss 1.0252 (0.9330) loss_zs_kd 0.3059 (0.3808) loss_oracle 0.1439 (0.2250) acc 75.0000 (75.3846) lr 1.6845e-03 eta 0:17:50
epoch [15/50] batch [280/288] time 0.113 (0.106) data 0.000 (0.001) loss 1.1070 (1.1549) teacher_loss 0.8943 (0.9310) loss_zs_kd 0.3361 (0.3780) loss_oracle 0.2127 (0.2239) acc 71.8750 (75.3683) lr 1.6845e-03 eta 0:17:50
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,436
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,001
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.0%
******* Domain a best val acc:      87.4%, epoch: 11 *******
******* Domain a best val test acc: 82.9%, epoch: 11 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [16/50] batch [20/288] time 0.094 (0.114) data 0.000 (0.017) loss 1.4905 (1.1595) teacher_loss 1.2675 (0.9474) loss_zs_kd 0.4176 (0.3772) loss_oracle 0.2230 (0.2120) acc 68.7500 (73.9062) lr 1.6374e-03 eta 0:19:10
epoch [16/50] batch [40/288] time 0.093 (0.105) data 0.000 (0.008) loss 1.4062 (1.1399) teacher_loss 1.1337 (0.9165) loss_zs_kd 0.3543 (0.3850) loss_oracle 0.2725 (0.2234) acc 68.7500 (74.6875) lr 1.6374e-03 eta 0:17:29
epoch [16/50] batch [60/288] time 0.095 (0.102) data 0.000 (0.006) loss 1.3161 (1.1392) teacher_loss 1.0508 (0.9120) loss_zs_kd 0.5002 (0.3838) loss_oracle 0.2654 (0.2272) acc 75.0000 (75.2083) lr 1.6374e-03 eta 0:17:01
epoch [16/50] batch [80/288] time 0.093 (0.101) data 0.000 (0.004) loss 1.3992 (1.1340) teacher_loss 1.2133 (0.9086) loss_zs_kd 0.4225 (0.3841) loss_oracle 0.1858 (0.2254) acc 62.5000 (75.2344) lr 1.6374e-03 eta 0:16:46
epoch [16/50] batch [100/288] time 0.097 (0.100) data 0.000 (0.003) loss 1.2448 (1.1502) teacher_loss 1.0579 (0.9260) loss_zs_kd 0.4429 (0.3896) loss_oracle 0.1869 (0.2242) acc 65.6250 (74.4688) lr 1.6374e-03 eta 0:16:38
epoch [16/50] batch [120/288] time 0.101 (0.101) data 0.000 (0.003) loss 0.9421 (1.1431) teacher_loss 0.7244 (0.9211) loss_zs_kd 0.4902 (0.3840) loss_oracle 0.2177 (0.2220) acc 81.2500 (74.7135) lr 1.6374e-03 eta 0:16:46
epoch [16/50] batch [140/288] time 0.104 (0.101) data 0.000 (0.003) loss 1.2496 (1.1237) teacher_loss 1.0242 (0.9029) loss_zs_kd 0.3302 (0.3833) loss_oracle 0.2254 (0.2207) acc 78.1250 (75.0893) lr 1.6374e-03 eta 0:16:40
epoch [16/50] batch [160/288] time 0.100 (0.101) data 0.000 (0.002) loss 1.3410 (1.1220) teacher_loss 1.1165 (0.9007) loss_zs_kd 0.3541 (0.3882) loss_oracle 0.2245 (0.2212) acc 78.1250 (75.3320) lr 1.6374e-03 eta 0:16:37
epoch [16/50] batch [180/288] time 0.094 (0.100) data 0.000 (0.002) loss 0.9745 (1.1351) teacher_loss 0.7076 (0.9138) loss_zs_kd 0.2718 (0.3895) loss_oracle 0.2669 (0.2213) acc 78.1250 (75.4167) lr 1.6374e-03 eta 0:16:33
epoch [16/50] batch [200/288] time 0.093 (0.100) data 0.000 (0.002) loss 0.8599 (1.1441) teacher_loss 0.6306 (0.9192) loss_zs_kd 0.4145 (0.3863) loss_oracle 0.2293 (0.2249) acc 81.2500 (75.1562) lr 1.6374e-03 eta 0:16:30
epoch [16/50] batch [220/288] time 0.097 (0.100) data 0.000 (0.002) loss 0.8063 (1.1418) teacher_loss 0.6064 (0.9162) loss_zs_kd 0.3457 (0.3853) loss_oracle 0.1999 (0.2257) acc 87.5000 (75.2699) lr 1.6374e-03 eta 0:16:27
epoch [16/50] batch [240/288] time 0.099 (0.100) data 0.000 (0.002) loss 0.9178 (1.1326) teacher_loss 0.7293 (0.9084) loss_zs_kd 0.5380 (0.3888) loss_oracle 0.1885 (0.2242) acc 78.1250 (75.5078) lr 1.6374e-03 eta 0:16:22
epoch [16/50] batch [260/288] time 0.095 (0.100) data 0.000 (0.001) loss 1.1448 (1.1377) teacher_loss 0.8945 (0.9141) loss_zs_kd 0.3321 (0.3904) loss_oracle 0.2503 (0.2236) acc 78.1250 (75.3726) lr 1.6374e-03 eta 0:16:17
epoch [16/50] batch [280/288] time 0.088 (0.099) data 0.000 (0.001) loss 1.5923 (1.1457) teacher_loss 1.3771 (0.9224) loss_zs_kd 0.4777 (0.3918) loss_oracle 0.2152 (0.2232) acc 59.3750 (75.2009) lr 1.6374e-03 eta 0:16:10
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,442
* accuracy: 87.4%
* error: 12.6%
* macro_f1: 86.8%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,009
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.4%
******* Domain a best val acc:      87.4%, epoch: 16 *******
******* Domain a best val test acc: 82.8%, epoch: 16 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [17/50] batch [20/288] time 0.094 (0.117) data 0.000 (0.015) loss 1.6344 (1.0209) teacher_loss 1.3902 (0.7948) loss_zs_kd 0.3211 (0.3362) loss_oracle 0.2442 (0.2261) acc 75.0000 (79.6875) lr 1.5878e-03 eta 0:19:03
epoch [17/50] batch [40/288] time 0.105 (0.107) data 0.000 (0.008) loss 1.1077 (1.1042) teacher_loss 0.8800 (0.8831) loss_zs_kd 0.4768 (0.3563) loss_oracle 0.2277 (0.2211) acc 68.7500 (77.5000) lr 1.5878e-03 eta 0:17:19
epoch [17/50] batch [60/288] time 0.090 (0.103) data 0.000 (0.005) loss 1.6261 (1.1159) teacher_loss 1.3671 (0.8961) loss_zs_kd 0.3220 (0.3676) loss_oracle 0.2590 (0.2199) acc 68.7500 (77.1354) lr 1.5878e-03 eta 0:16:39
epoch [17/50] batch [80/288] time 0.093 (0.101) data 0.000 (0.004) loss 0.9383 (1.1125) teacher_loss 0.7334 (0.8915) loss_zs_kd 0.4346 (0.3731) loss_oracle 0.2049 (0.2210) acc 81.2500 (76.8359) lr 1.5878e-03 eta 0:16:23
epoch [17/50] batch [100/288] time 0.085 (0.102) data 0.000 (0.003) loss 1.2590 (1.1071) teacher_loss 1.0100 (0.8875) loss_zs_kd 0.3420 (0.3759) loss_oracle 0.2490 (0.2197) acc 65.6250 (76.7188) lr 1.5878e-03 eta 0:16:25
epoch [17/50] batch [120/288] time 0.196 (0.112) data 0.000 (0.003) loss 1.1484 (1.1141) teacher_loss 0.9944 (0.8962) loss_zs_kd 0.2619 (0.3779) loss_oracle 0.1540 (0.2178) acc 75.0000 (76.4323) lr 1.5878e-03 eta 0:17:59
epoch [17/50] batch [140/288] time 0.198 (0.124) data 0.000 (0.002) loss 1.3735 (1.1343) teacher_loss 1.1652 (0.9161) loss_zs_kd 0.4453 (0.3850) loss_oracle 0.2082 (0.2182) acc 65.6250 (76.0045) lr 1.5878e-03 eta 0:19:52
epoch [17/50] batch [160/288] time 0.194 (0.132) data 0.000 (0.002) loss 1.1831 (1.1343) teacher_loss 0.9777 (0.9162) loss_zs_kd 0.3712 (0.3873) loss_oracle 0.2054 (0.2181) acc 65.6250 (76.0156) lr 1.5878e-03 eta 0:21:15
epoch [17/50] batch [180/288] time 0.193 (0.139) data 0.000 (0.002) loss 0.8737 (1.1268) teacher_loss 0.6709 (0.9087) loss_zs_kd 0.3017 (0.3832) loss_oracle 0.2029 (0.2181) acc 78.1250 (76.0938) lr 1.5878e-03 eta 0:22:18
epoch [17/50] batch [200/288] time 0.196 (0.145) data 0.000 (0.002) loss 0.9935 (1.1239) teacher_loss 0.8015 (0.9056) loss_zs_kd 0.3031 (0.3841) loss_oracle 0.1920 (0.2183) acc 75.0000 (76.2188) lr 1.5878e-03 eta 0:23:08
epoch [17/50] batch [220/288] time 0.196 (0.149) data 0.000 (0.002) loss 1.0133 (1.1244) teacher_loss 0.7632 (0.9058) loss_zs_kd 0.2484 (0.3889) loss_oracle 0.2500 (0.2186) acc 84.3750 (76.3210) lr 1.5878e-03 eta 0:23:48
epoch [17/50] batch [240/288] time 0.195 (0.153) data 0.000 (0.001) loss 1.1512 (1.1227) teacher_loss 0.9165 (0.9033) loss_zs_kd 0.3211 (0.3888) loss_oracle 0.2348 (0.2194) acc 75.0000 (76.3151) lr 1.5878e-03 eta 0:24:22
epoch [17/50] batch [260/288] time 0.194 (0.156) data 0.000 (0.001) loss 1.4961 (1.1259) teacher_loss 1.2462 (0.9053) loss_zs_kd 0.3610 (0.3921) loss_oracle 0.2499 (0.2206) acc 68.7500 (76.2139) lr 1.5878e-03 eta 0:24:49
epoch [17/50] batch [280/288] time 0.190 (0.159) data 0.000 (0.001) loss 1.4160 (1.1245) teacher_loss 1.1879 (0.9039) loss_zs_kd 0.5332 (0.3945) loss_oracle 0.2281 (0.2207) acc 68.7500 (76.1942) lr 1.5878e-03 eta 0:25:10
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,429
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,010
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.3%
******* Domain a best val acc:      87.4%, epoch: 16 *******
******* Domain a best val test acc: 82.8%, epoch: 16 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [18/50] batch [20/288] time 0.192 (0.210) data 0.000 (0.013) loss 1.3965 (1.1121) teacher_loss 1.1724 (0.8919) loss_zs_kd 0.3821 (0.4293) loss_oracle 0.2241 (0.2202) acc 68.7500 (76.0938) lr 1.5358e-03 eta 0:33:08
epoch [18/50] batch [40/288] time 0.194 (0.202) data 0.000 (0.007) loss 1.5991 (1.1509) teacher_loss 1.4037 (0.9297) loss_zs_kd 0.6094 (0.4342) loss_oracle 0.1954 (0.2212) acc 65.6250 (75.0000) lr 1.5358e-03 eta 0:31:49
epoch [18/50] batch [60/288] time 0.198 (0.199) data 0.000 (0.005) loss 0.9820 (1.1615) teacher_loss 0.6834 (0.9334) loss_zs_kd 0.4993 (0.4208) loss_oracle 0.2986 (0.2281) acc 81.2500 (75.1042) lr 1.5358e-03 eta 0:31:20
epoch [18/50] batch [80/288] time 0.189 (0.198) data 0.000 (0.004) loss 2.1900 (1.1844) teacher_loss 1.9123 (0.9581) loss_zs_kd 0.5861 (0.4197) loss_oracle 0.2777 (0.2263) acc 50.0000 (74.8047) lr 1.5358e-03 eta 0:31:01
epoch [18/50] batch [100/288] time 0.300 (0.206) data 0.000 (0.003) loss 1.0947 (1.1746) teacher_loss 0.7972 (0.9488) loss_zs_kd 0.3021 (0.4181) loss_oracle 0.2975 (0.2258) acc 87.5000 (75.1250) lr 1.5358e-03 eta 0:32:20
epoch [18/50] batch [120/288] time 0.091 (0.203) data 0.000 (0.002) loss 1.3113 (1.1596) teacher_loss 1.1161 (0.9358) loss_zs_kd 0.3766 (0.4143) loss_oracle 0.1951 (0.2238) acc 75.0000 (75.5729) lr 1.5358e-03 eta 0:31:46
epoch [18/50] batch [140/288] time 0.209 (0.200) data 0.000 (0.002) loss 1.3433 (1.1688) teacher_loss 1.1018 (0.9444) loss_zs_kd 0.3467 (0.4136) loss_oracle 0.2415 (0.2244) acc 71.8750 (75.2679) lr 1.5358e-03 eta 0:31:09
epoch [18/50] batch [160/288] time 0.192 (0.198) data 0.000 (0.002) loss 1.2057 (1.1628) teacher_loss 0.9438 (0.9377) loss_zs_kd 0.6090 (0.4158) loss_oracle 0.2619 (0.2251) acc 75.0000 (75.3711) lr 1.5358e-03 eta 0:30:50
epoch [18/50] batch [180/288] time 0.192 (0.197) data 0.000 (0.002) loss 0.8879 (1.1546) teacher_loss 0.6274 (0.9294) loss_zs_kd 0.5434 (0.4177) loss_oracle 0.2605 (0.2253) acc 81.2500 (75.5729) lr 1.5358e-03 eta 0:30:33
epoch [18/50] batch [200/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.5410 (1.1474) teacher_loss 1.3516 (0.9204) loss_zs_kd 0.4460 (0.4158) loss_oracle 0.1894 (0.2270) acc 62.5000 (75.6875) lr 1.5358e-03 eta 0:30:27
epoch [18/50] batch [220/288] time 0.194 (0.196) data 0.000 (0.001) loss 1.6511 (1.1508) teacher_loss 1.4186 (0.9225) loss_zs_kd 0.3588 (0.4167) loss_oracle 0.2325 (0.2283) acc 68.7500 (75.6534) lr 1.5358e-03 eta 0:30:21
epoch [18/50] batch [240/288] time 0.199 (0.196) data 0.000 (0.001) loss 0.8898 (1.1543) teacher_loss 0.6910 (0.9249) loss_zs_kd 0.4745 (0.4187) loss_oracle 0.1988 (0.2295) acc 81.2500 (75.5599) lr 1.5358e-03 eta 0:30:11
epoch [18/50] batch [260/288] time 0.195 (0.195) data 0.000 (0.001) loss 1.2762 (1.1539) teacher_loss 1.0480 (0.9245) loss_zs_kd 0.2305 (0.4164) loss_oracle 0.2282 (0.2295) acc 81.2500 (75.7332) lr 1.5358e-03 eta 0:30:04
epoch [18/50] batch [280/288] time 0.190 (0.195) data 0.000 (0.001) loss 1.4593 (1.1578) teacher_loss 1.2028 (0.9293) loss_zs_kd 0.5074 (0.4158) loss_oracle 0.2565 (0.2285) acc 68.7500 (75.5804) lr 1.5358e-03 eta 0:29:59
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,458
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 87.3%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,011
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.5%
******* Domain a best val acc:      87.8%, epoch: 18 *******
******* Domain a best val test acc: 82.9%, epoch: 18 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [19/50] batch [20/288] time 0.184 (0.206) data 0.000 (0.014) loss 1.1709 (1.0922) teacher_loss 0.9389 (0.8691) loss_zs_kd 0.3341 (0.3984) loss_oracle 0.2320 (0.2232) acc 71.8750 (76.7188) lr 1.4818e-03 eta 0:31:32
epoch [19/50] batch [40/288] time 0.203 (0.198) data 0.000 (0.007) loss 1.1793 (1.1533) teacher_loss 0.9641 (0.9318) loss_zs_kd 0.4061 (0.4011) loss_oracle 0.2152 (0.2215) acc 75.0000 (75.4688) lr 1.4818e-03 eta 0:30:17
epoch [19/50] batch [60/288] time 0.191 (0.197) data 0.000 (0.005) loss 1.4897 (1.1998) teacher_loss 1.2407 (0.9769) loss_zs_kd 0.4894 (0.4067) loss_oracle 0.2490 (0.2230) acc 65.6250 (74.1146) lr 1.4818e-03 eta 0:30:01
epoch [19/50] batch [80/288] time 0.192 (0.196) data 0.000 (0.004) loss 1.1060 (1.1594) teacher_loss 0.8341 (0.9337) loss_zs_kd 0.3713 (0.4030) loss_oracle 0.2719 (0.2256) acc 68.7500 (74.7656) lr 1.4818e-03 eta 0:29:49
epoch [19/50] batch [100/288] time 0.398 (0.193) data 0.000 (0.003) loss 1.2911 (1.1560) teacher_loss 1.0638 (0.9315) loss_zs_kd 0.3173 (0.4025) loss_oracle 0.2273 (0.2245) acc 71.8750 (75.0938) lr 1.4818e-03 eta 0:29:18
epoch [19/50] batch [120/288] time 0.455 (0.211) data 0.000 (0.003) loss 1.6589 (1.1550) teacher_loss 1.4161 (0.9291) loss_zs_kd 0.3647 (0.4037) loss_oracle 0.2428 (0.2259) acc 65.6250 (75.1562) lr 1.4818e-03 eta 0:32:00
epoch [19/50] batch [140/288] time 0.196 (0.206) data 0.000 (0.002) loss 1.5222 (1.1553) teacher_loss 1.3070 (0.9284) loss_zs_kd 0.3003 (0.4028) loss_oracle 0.2151 (0.2269) acc 75.0000 (75.2455) lr 1.4818e-03 eta 0:31:11
epoch [19/50] batch [160/288] time 0.192 (0.205) data 0.000 (0.002) loss 1.0940 (1.1554) teacher_loss 0.8607 (0.9292) loss_zs_kd 0.3919 (0.3944) loss_oracle 0.2333 (0.2262) acc 71.8750 (75.1758) lr 1.4818e-03 eta 0:30:53
epoch [19/50] batch [180/288] time 0.194 (0.203) data 0.000 (0.002) loss 1.2514 (1.1530) teacher_loss 0.9901 (0.9287) loss_zs_kd 0.3587 (0.3945) loss_oracle 0.2614 (0.2243) acc 75.0000 (75.1215) lr 1.4818e-03 eta 0:30:38
epoch [19/50] batch [200/288] time 0.196 (0.202) data 0.000 (0.002) loss 0.8230 (1.1442) teacher_loss 0.5959 (0.9200) loss_zs_kd 0.4941 (0.3958) loss_oracle 0.2271 (0.2242) acc 81.2500 (75.2969) lr 1.4818e-03 eta 0:30:24
epoch [19/50] batch [220/288] time 0.197 (0.201) data 0.000 (0.001) loss 1.1216 (1.1515) teacher_loss 0.9356 (0.9268) loss_zs_kd 0.5795 (0.4039) loss_oracle 0.1860 (0.2247) acc 81.2500 (75.3409) lr 1.4818e-03 eta 0:30:12
epoch [19/50] batch [240/288] time 0.193 (0.201) data 0.000 (0.001) loss 0.8949 (1.1495) teacher_loss 0.6455 (0.9240) loss_zs_kd 0.6832 (0.4067) loss_oracle 0.2494 (0.2255) acc 84.3750 (75.3125) lr 1.4818e-03 eta 0:30:02
epoch [19/50] batch [260/288] time 0.168 (0.200) data 0.000 (0.001) loss 1.1874 (1.1449) teacher_loss 1.0027 (0.9193) loss_zs_kd 0.4477 (0.4083) loss_oracle 0.1847 (0.2256) acc 75.0000 (75.4688) lr 1.4818e-03 eta 0:29:52
epoch [19/50] batch [280/288] time 0.190 (0.200) data 0.000 (0.001) loss 1.5712 (1.1453) teacher_loss 1.3690 (0.9202) loss_zs_kd 0.5320 (0.4087) loss_oracle 0.2021 (0.2251) acc 65.6250 (75.5134) lr 1.4818e-03 eta 0:29:45
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,463
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 87.3%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,992
* accuracy: 82.1%
* error: 17.9%
* macro_f1: 78.7%
******* Domain a best val acc:      87.9%, epoch: 19 *******
******* Domain a best val test acc: 82.1%, epoch: 19 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [20/50] batch [20/288] time 0.166 (0.208) data 0.000 (0.013) loss 1.5857 (1.1131) teacher_loss 1.3373 (0.8768) loss_zs_kd 0.4243 (0.4237) loss_oracle 0.2484 (0.2363) acc 65.6250 (75.3125) lr 1.4258e-03 eta 0:30:53
epoch [20/50] batch [40/288] time 0.228 (0.202) data 0.000 (0.006) loss 1.0473 (1.1001) teacher_loss 0.8564 (0.8709) loss_zs_kd 0.6801 (0.4500) loss_oracle 0.1909 (0.2292) acc 81.2500 (76.3281) lr 1.4258e-03 eta 0:29:56
epoch [20/50] batch [60/288] time 0.196 (0.199) data 0.000 (0.004) loss 1.0252 (1.1000) teacher_loss 0.8479 (0.8714) loss_zs_kd 0.2492 (0.4473) loss_oracle 0.1773 (0.2286) acc 78.1250 (76.5104) lr 1.4258e-03 eta 0:29:23
epoch [20/50] batch [80/288] time 0.195 (0.198) data 0.000 (0.003) loss 0.9482 (1.1022) teacher_loss 0.7069 (0.8766) loss_zs_kd 0.3588 (0.4402) loss_oracle 0.2412 (0.2256) acc 81.2500 (76.5234) lr 1.4258e-03 eta 0:29:09
epoch [20/50] batch [100/288] time 0.498 (0.196) data 0.000 (0.003) loss 1.3159 (1.1014) teacher_loss 1.1311 (0.8759) loss_zs_kd 0.4178 (0.4319) loss_oracle 0.1848 (0.2255) acc 65.6250 (76.6562) lr 1.4258e-03 eta 0:28:52
epoch [20/50] batch [120/288] time 0.482 (0.210) data 0.000 (0.002) loss 0.6806 (1.1221) teacher_loss 0.4703 (0.8984) loss_zs_kd 0.2247 (0.4276) loss_oracle 0.2103 (0.2238) acc 90.6250 (76.0677) lr 1.4258e-03 eta 0:30:50
epoch [20/50] batch [140/288] time 0.194 (0.207) data 0.000 (0.002) loss 0.9788 (1.1290) teacher_loss 0.7350 (0.9050) loss_zs_kd 0.4434 (0.4293) loss_oracle 0.2438 (0.2240) acc 84.3750 (75.9821) lr 1.4258e-03 eta 0:30:19
epoch [20/50] batch [160/288] time 0.190 (0.206) data 0.000 (0.002) loss 1.1370 (1.1268) teacher_loss 0.9236 (0.9018) loss_zs_kd 0.3257 (0.4254) loss_oracle 0.2134 (0.2250) acc 78.1250 (76.1523) lr 1.4258e-03 eta 0:30:02
epoch [20/50] batch [180/288] time 0.192 (0.204) data 0.000 (0.002) loss 0.8816 (1.1189) teacher_loss 0.6707 (0.8955) loss_zs_kd 0.3617 (0.4230) loss_oracle 0.2109 (0.2234) acc 81.2500 (76.3194) lr 1.4258e-03 eta 0:29:44
epoch [20/50] batch [200/288] time 0.194 (0.203) data 0.000 (0.001) loss 1.7050 (1.1219) teacher_loss 1.4002 (0.8985) loss_zs_kd 0.6189 (0.4210) loss_oracle 0.3048 (0.2234) acc 59.3750 (76.1875) lr 1.4258e-03 eta 0:29:32
epoch [20/50] batch [220/288] time 0.192 (0.202) data 0.000 (0.001) loss 1.1106 (1.1110) teacher_loss 0.9067 (0.8876) loss_zs_kd 0.3621 (0.4217) loss_oracle 0.2039 (0.2234) acc 78.1250 (76.3636) lr 1.4258e-03 eta 0:29:21
epoch [20/50] batch [240/288] time 0.195 (0.202) data 0.000 (0.001) loss 1.3001 (1.1104) teacher_loss 1.0713 (0.8872) loss_zs_kd 0.3467 (0.4229) loss_oracle 0.2288 (0.2231) acc 71.8750 (76.2760) lr 1.4258e-03 eta 0:29:11
epoch [20/50] batch [260/288] time 0.191 (0.201) data 0.000 (0.001) loss 1.4999 (1.1068) teacher_loss 1.2745 (0.8832) loss_zs_kd 0.2773 (0.4223) loss_oracle 0.2254 (0.2237) acc 68.7500 (76.3822) lr 1.4258e-03 eta 0:29:02
epoch [20/50] batch [280/288] time 0.194 (0.200) data 0.000 (0.001) loss 0.8761 (1.1078) teacher_loss 0.6730 (0.8819) loss_zs_kd 0.3846 (0.4197) loss_oracle 0.2031 (0.2259) acc 90.6250 (76.4509) lr 1.4258e-03 eta 0:28:53
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,455
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 87.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,005
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.3%
******* Domain a best val acc:      87.9%, epoch: 19 *******
******* Domain a best val test acc: 82.1%, epoch: 19 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [21/50] batch [20/288] time 0.188 (0.212) data 0.000 (0.018) loss 0.5554 (1.0521) teacher_loss 0.3448 (0.8218) loss_zs_kd 0.4165 (0.4462) loss_oracle 0.2106 (0.2303) acc 93.7500 (78.2812) lr 1.3681e-03 eta 0:30:24
epoch [21/50] batch [40/288] time 0.196 (0.203) data 0.000 (0.009) loss 0.7140 (1.0602) teacher_loss 0.4841 (0.8318) loss_zs_kd 0.4651 (0.4404) loss_oracle 0.2298 (0.2284) acc 84.3750 (76.6406) lr 1.3681e-03 eta 0:29:05
epoch [21/50] batch [60/288] time 0.193 (0.200) data 0.000 (0.006) loss 0.9223 (1.0848) teacher_loss 0.6986 (0.8563) loss_zs_kd 0.4312 (0.4258) loss_oracle 0.2237 (0.2286) acc 81.2500 (76.4062) lr 1.3681e-03 eta 0:28:35
epoch [21/50] batch [80/288] time 0.192 (0.198) data 0.000 (0.005) loss 0.7502 (1.1160) teacher_loss 0.5333 (0.8888) loss_zs_kd 0.2540 (0.4180) loss_oracle 0.2169 (0.2272) acc 87.5000 (75.6641) lr 1.3681e-03 eta 0:28:18
epoch [21/50] batch [100/288] time 0.115 (0.192) data 0.000 (0.004) loss 1.2132 (1.1050) teacher_loss 0.9825 (0.8759) loss_zs_kd 0.2856 (0.4165) loss_oracle 0.2306 (0.2291) acc 71.8750 (76.0312) lr 1.3681e-03 eta 0:27:23
epoch [21/50] batch [120/288] time 0.466 (0.208) data 0.000 (0.003) loss 0.9290 (1.1119) teacher_loss 0.6880 (0.8788) loss_zs_kd 0.3029 (0.4176) loss_oracle 0.2410 (0.2331) acc 84.3750 (76.3021) lr 1.3681e-03 eta 0:29:28
epoch [21/50] batch [140/288] time 0.199 (0.207) data 0.000 (0.003) loss 1.1407 (1.1253) teacher_loss 0.8666 (0.8914) loss_zs_kd 0.4774 (0.4144) loss_oracle 0.2741 (0.2340) acc 78.1250 (76.0491) lr 1.3681e-03 eta 0:29:17
epoch [21/50] batch [160/288] time 0.198 (0.205) data 0.000 (0.002) loss 1.0725 (1.1411) teacher_loss 0.8293 (0.9064) loss_zs_kd 0.3752 (0.4222) loss_oracle 0.2432 (0.2347) acc 81.2500 (75.4883) lr 1.3681e-03 eta 0:29:00
epoch [21/50] batch [180/288] time 0.193 (0.204) data 0.000 (0.002) loss 1.3460 (1.1457) teacher_loss 1.1596 (0.9127) loss_zs_kd 0.3046 (0.4227) loss_oracle 0.1864 (0.2330) acc 68.7500 (75.2778) lr 1.3681e-03 eta 0:28:43
epoch [21/50] batch [200/288] time 0.190 (0.203) data 0.000 (0.002) loss 1.3971 (1.1390) teacher_loss 1.1932 (0.9070) loss_zs_kd 0.3355 (0.4205) loss_oracle 0.2038 (0.2320) acc 71.8750 (75.5000) lr 1.3681e-03 eta 0:28:29
epoch [21/50] batch [220/288] time 0.197 (0.202) data 0.000 (0.002) loss 0.9411 (1.1385) teacher_loss 0.7247 (0.9068) loss_zs_kd 0.3369 (0.4194) loss_oracle 0.2164 (0.2316) acc 81.2500 (75.4688) lr 1.3681e-03 eta 0:28:19
epoch [21/50] batch [240/288] time 0.199 (0.201) data 0.000 (0.002) loss 1.1513 (1.1504) teacher_loss 0.9206 (0.9187) loss_zs_kd 0.4547 (0.4165) loss_oracle 0.2307 (0.2317) acc 84.3750 (75.3516) lr 1.3681e-03 eta 0:28:10
epoch [21/50] batch [260/288] time 0.197 (0.201) data 0.000 (0.002) loss 1.0958 (1.1471) teacher_loss 0.8520 (0.9149) loss_zs_kd 0.4201 (0.4169) loss_oracle 0.2438 (0.2322) acc 78.1250 (75.4087) lr 1.3681e-03 eta 0:28:02
epoch [21/50] batch [280/288] time 0.194 (0.200) data 0.000 (0.001) loss 1.1845 (1.1461) teacher_loss 0.9495 (0.9129) loss_zs_kd 0.4122 (0.4167) loss_oracle 0.2350 (0.2332) acc 81.2500 (75.5357) lr 1.3681e-03 eta 0:27:54
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,462
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 87.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,002
* accuracy: 82.5%
* error: 17.5%
* macro_f1: 79.1%
******* Domain a best val acc:      87.9%, epoch: 19 *******
******* Domain a best val test acc: 82.1%, epoch: 19 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [22/50] batch [20/288] time 0.192 (0.212) data 0.000 (0.015) loss 1.4814 (1.1199) teacher_loss 1.2473 (0.8933) loss_zs_kd 0.4801 (0.4552) loss_oracle 0.2341 (0.2266) acc 65.6250 (74.2188) lr 1.3090e-03 eta 0:29:30
epoch [22/50] batch [40/288] time 0.196 (0.204) data 0.000 (0.008) loss 1.1429 (1.1033) teacher_loss 0.9266 (0.8806) loss_zs_kd 0.3500 (0.4379) loss_oracle 0.2163 (0.2227) acc 81.2500 (75.7031) lr 1.3090e-03 eta 0:28:11
epoch [22/50] batch [60/288] time 0.193 (0.201) data 0.000 (0.005) loss 0.8280 (1.1211) teacher_loss 0.6043 (0.8980) loss_zs_kd 0.4951 (0.4420) loss_oracle 0.2237 (0.2232) acc 78.1250 (75.3646) lr 1.3090e-03 eta 0:27:44
epoch [22/50] batch [80/288] time 0.194 (0.199) data 0.000 (0.004) loss 1.5441 (1.1125) teacher_loss 1.2845 (0.8838) loss_zs_kd 0.4655 (0.4460) loss_oracle 0.2596 (0.2287) acc 68.7500 (76.0938) lr 1.3090e-03 eta 0:27:27
epoch [22/50] batch [100/288] time 0.090 (0.195) data 0.000 (0.003) loss 0.8100 (1.1160) teacher_loss 0.5933 (0.8862) loss_zs_kd 0.2613 (0.4345) loss_oracle 0.2167 (0.2298) acc 78.1250 (76.0625) lr 1.3090e-03 eta 0:26:47
epoch [22/50] batch [120/288] time 0.446 (0.206) data 0.000 (0.003) loss 1.1276 (1.1242) teacher_loss 0.8369 (0.8940) loss_zs_kd 0.5207 (0.4419) loss_oracle 0.2907 (0.2302) acc 75.0000 (76.1979) lr 1.3090e-03 eta 0:28:11
epoch [22/50] batch [140/288] time 0.198 (0.206) data 0.000 (0.002) loss 1.1102 (1.1180) teacher_loss 0.8917 (0.8864) loss_zs_kd 0.4927 (0.4362) loss_oracle 0.2186 (0.2316) acc 78.1250 (76.2946) lr 1.3090e-03 eta 0:28:12
epoch [22/50] batch [160/288] time 0.197 (0.205) data 0.000 (0.002) loss 0.8892 (1.1298) teacher_loss 0.6641 (0.8965) loss_zs_kd 0.3078 (0.4351) loss_oracle 0.2250 (0.2333) acc 81.2500 (76.0742) lr 1.3090e-03 eta 0:27:56
epoch [22/50] batch [180/288] time 0.196 (0.204) data 0.000 (0.002) loss 1.0580 (1.1237) teacher_loss 0.8507 (0.8912) loss_zs_kd 0.2278 (0.4338) loss_oracle 0.2072 (0.2325) acc 81.2500 (76.3889) lr 1.3090e-03 eta 0:27:43
epoch [22/50] batch [200/288] time 0.188 (0.202) data 0.000 (0.002) loss 1.0780 (1.1271) teacher_loss 0.8379 (0.8947) loss_zs_kd 0.3926 (0.4305) loss_oracle 0.2400 (0.2324) acc 78.1250 (76.3594) lr 1.3090e-03 eta 0:27:30
epoch [22/50] batch [220/288] time 0.223 (0.202) data 0.000 (0.002) loss 0.6867 (1.1296) teacher_loss 0.4295 (0.8960) loss_zs_kd 0.4191 (0.4308) loss_oracle 0.2571 (0.2335) acc 87.5000 (76.1364) lr 1.3090e-03 eta 0:27:21
epoch [22/50] batch [240/288] time 0.195 (0.201) data 0.000 (0.001) loss 1.2048 (1.1336) teacher_loss 0.9981 (0.8990) loss_zs_kd 0.4561 (0.4299) loss_oracle 0.2067 (0.2346) acc 71.8750 (75.9375) lr 1.3090e-03 eta 0:27:11
epoch [22/50] batch [260/288] time 0.192 (0.201) data 0.000 (0.001) loss 1.0374 (1.1358) teacher_loss 0.8068 (0.9009) loss_zs_kd 0.4368 (0.4272) loss_oracle 0.2307 (0.2349) acc 78.1250 (75.8774) lr 1.3090e-03 eta 0:27:03
epoch [22/50] batch [280/288] time 0.198 (0.200) data 0.000 (0.001) loss 0.8516 (1.1396) teacher_loss 0.6063 (0.9035) loss_zs_kd 0.4240 (0.4277) loss_oracle 0.2453 (0.2361) acc 81.2500 (75.7366) lr 1.3090e-03 eta 0:26:54
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,458
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 87.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,998
* accuracy: 82.3%
* error: 17.7%
* macro_f1: 79.2%
******* Domain a best val acc:      87.9%, epoch: 19 *******
******* Domain a best val test acc: 82.1%, epoch: 19 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [23/50] batch [20/288] time 0.197 (0.215) data 0.000 (0.016) loss 1.2668 (1.1211) teacher_loss 1.0536 (0.8731) loss_zs_kd 0.3742 (0.3935) loss_oracle 0.2131 (0.2481) acc 71.8750 (76.0938) lr 1.2487e-03 eta 0:28:52
epoch [23/50] batch [40/288] time 0.197 (0.205) data 0.000 (0.008) loss 1.1003 (1.1139) teacher_loss 0.8784 (0.8649) loss_zs_kd 0.3737 (0.4007) loss_oracle 0.2219 (0.2490) acc 78.1250 (76.9531) lr 1.2487e-03 eta 0:27:22
epoch [23/50] batch [60/288] time 0.177 (0.201) data 0.000 (0.006) loss 1.3903 (1.1339) teacher_loss 1.1751 (0.8890) loss_zs_kd 0.5544 (0.4197) loss_oracle 0.2152 (0.2449) acc 65.6250 (76.1458) lr 1.2487e-03 eta 0:26:50
epoch [23/50] batch [80/288] time 0.191 (0.199) data 0.000 (0.004) loss 1.4774 (1.1200) teacher_loss 1.1968 (0.8781) loss_zs_kd 0.4609 (0.4264) loss_oracle 0.2806 (0.2419) acc 68.7500 (76.7578) lr 1.2487e-03 eta 0:26:30
epoch [23/50] batch [100/288] time 0.098 (0.197) data 0.001 (0.003) loss 0.9726 (1.1163) teacher_loss 0.7168 (0.8715) loss_zs_kd 0.5218 (0.4340) loss_oracle 0.2558 (0.2448) acc 84.3750 (76.9688) lr 1.2487e-03 eta 0:26:07
epoch [23/50] batch [120/288] time 0.214 (0.205) data 0.000 (0.003) loss 0.7647 (1.1234) teacher_loss 0.5185 (0.8783) loss_zs_kd 0.4770 (0.4365) loss_oracle 0.2461 (0.2452) acc 87.5000 (76.7708) lr 1.2487e-03 eta 0:27:11
epoch [23/50] batch [140/288] time 0.225 (0.209) data 0.000 (0.003) loss 1.0268 (1.1218) teacher_loss 0.8399 (0.8753) loss_zs_kd 0.2985 (0.4304) loss_oracle 0.1869 (0.2465) acc 78.1250 (76.6741) lr 1.2487e-03 eta 0:27:37
epoch [23/50] batch [160/288] time 0.193 (0.207) data 0.000 (0.002) loss 1.1690 (1.1289) teacher_loss 0.9230 (0.8841) loss_zs_kd 0.4223 (0.4303) loss_oracle 0.2459 (0.2448) acc 75.0000 (76.3672) lr 1.2487e-03 eta 0:27:16
epoch [23/50] batch [180/288] time 0.199 (0.206) data 0.000 (0.002) loss 1.0312 (1.1275) teacher_loss 0.7944 (0.8818) loss_zs_kd 0.2480 (0.4285) loss_oracle 0.2368 (0.2458) acc 81.2500 (76.5625) lr 1.2487e-03 eta 0:27:00
epoch [23/50] batch [200/288] time 0.192 (0.204) data 0.000 (0.002) loss 0.9700 (1.1291) teacher_loss 0.7316 (0.8819) loss_zs_kd 0.4816 (0.4307) loss_oracle 0.2384 (0.2472) acc 75.0000 (76.7812) lr 1.2487e-03 eta 0:26:47
epoch [23/50] batch [220/288] time 0.192 (0.203) data 0.000 (0.002) loss 1.4354 (1.1324) teacher_loss 1.2137 (0.8846) loss_zs_kd 0.3880 (0.4317) loss_oracle 0.2217 (0.2478) acc 75.0000 (76.7188) lr 1.2487e-03 eta 0:26:35
epoch [23/50] batch [240/288] time 0.172 (0.203) data 0.000 (0.002) loss 1.0893 (1.1431) teacher_loss 0.8850 (0.8948) loss_zs_kd 0.4551 (0.4272) loss_oracle 0.2043 (0.2483) acc 71.8750 (76.3672) lr 1.2487e-03 eta 0:26:25
epoch [23/50] batch [260/288] time 0.193 (0.202) data 0.000 (0.001) loss 0.9406 (1.1504) teacher_loss 0.6674 (0.9028) loss_zs_kd 0.3820 (0.4279) loss_oracle 0.2731 (0.2476) acc 84.3750 (76.0697) lr 1.2487e-03 eta 0:26:15
epoch [23/50] batch [280/288] time 0.195 (0.201) data 0.000 (0.001) loss 1.1453 (1.1434) teacher_loss 0.8644 (0.8974) loss_zs_kd 0.4596 (0.4272) loss_oracle 0.2809 (0.2460) acc 68.7500 (76.1719) lr 1.2487e-03 eta 0:26:06
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,454
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 87.1%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,008
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 79.4%
******* Domain a best val acc:      87.9%, epoch: 19 *******
******* Domain a best val test acc: 82.1%, epoch: 19 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [24/50] batch [20/288] time 0.197 (0.209) data 0.000 (0.014) loss 1.3278 (1.1188) teacher_loss 1.1059 (0.8826) loss_zs_kd 0.3260 (0.3969) loss_oracle 0.2219 (0.2363) acc 75.0000 (77.1875) lr 1.1874e-03 eta 0:26:58
epoch [24/50] batch [40/288] time 0.196 (0.201) data 0.000 (0.007) loss 1.0407 (1.1458) teacher_loss 0.8742 (0.9173) loss_zs_kd 0.5230 (0.4028) loss_oracle 0.1665 (0.2285) acc 68.7500 (75.6250) lr 1.1874e-03 eta 0:25:56
epoch [24/50] batch [60/288] time 0.191 (0.199) data 0.000 (0.005) loss 1.0289 (1.1480) teacher_loss 0.7675 (0.9201) loss_zs_kd 0.3882 (0.4147) loss_oracle 0.2613 (0.2279) acc 78.1250 (75.7812) lr 1.1874e-03 eta 0:25:33
epoch [24/50] batch [80/288] time 0.195 (0.197) data 0.000 (0.004) loss 1.0267 (1.1240) teacher_loss 0.7780 (0.8943) loss_zs_kd 0.5691 (0.4152) loss_oracle 0.2487 (0.2298) acc 81.2500 (76.1719) lr 1.1874e-03 eta 0:25:18
epoch [24/50] batch [100/288] time 0.193 (0.196) data 0.000 (0.003) loss 0.9630 (1.1273) teacher_loss 0.7109 (0.8949) loss_zs_kd 0.5256 (0.4136) loss_oracle 0.2520 (0.2324) acc 84.3750 (76.3438) lr 1.1874e-03 eta 0:25:07
epoch [24/50] batch [120/288] time 0.089 (0.204) data 0.000 (0.003) loss 1.0963 (1.1147) teacher_loss 0.8716 (0.8822) loss_zs_kd 0.3098 (0.4148) loss_oracle 0.2247 (0.2325) acc 75.0000 (76.5625) lr 1.1874e-03 eta 0:26:04
epoch [24/50] batch [140/288] time 0.197 (0.206) data 0.000 (0.002) loss 1.1575 (1.1101) teacher_loss 0.8901 (0.8784) loss_zs_kd 0.3781 (0.4194) loss_oracle 0.2673 (0.2317) acc 78.1250 (76.7634) lr 1.1874e-03 eta 0:26:12
epoch [24/50] batch [160/288] time 0.193 (0.204) data 0.000 (0.002) loss 1.2830 (1.1174) teacher_loss 1.0011 (0.8844) loss_zs_kd 0.3832 (0.4291) loss_oracle 0.2820 (0.2330) acc 68.7500 (76.6992) lr 1.1874e-03 eta 0:25:55
epoch [24/50] batch [180/288] time 0.179 (0.203) data 0.000 (0.002) loss 1.0878 (1.1321) teacher_loss 0.8141 (0.8977) loss_zs_kd 0.3603 (0.4296) loss_oracle 0.2738 (0.2344) acc 75.0000 (76.1979) lr 1.1874e-03 eta 0:25:42
epoch [24/50] batch [200/288] time 0.194 (0.202) data 0.000 (0.002) loss 1.1042 (1.1355) teacher_loss 0.8800 (0.8992) loss_zs_kd 0.4084 (0.4303) loss_oracle 0.2241 (0.2362) acc 75.0000 (76.1406) lr 1.1874e-03 eta 0:25:32
epoch [24/50] batch [220/288] time 0.195 (0.202) data 0.000 (0.001) loss 0.9620 (1.1351) teacher_loss 0.7243 (0.8999) loss_zs_kd 0.4482 (0.4282) loss_oracle 0.2377 (0.2352) acc 81.2500 (76.1080) lr 1.1874e-03 eta 0:25:22
epoch [24/50] batch [240/288] time 0.187 (0.201) data 0.000 (0.001) loss 0.9661 (1.1374) teacher_loss 0.7198 (0.9016) loss_zs_kd 0.3991 (0.4267) loss_oracle 0.2463 (0.2357) acc 81.2500 (76.0547) lr 1.1874e-03 eta 0:25:13
epoch [24/50] batch [260/288] time 0.194 (0.200) data 0.000 (0.001) loss 0.7392 (1.1403) teacher_loss 0.5210 (0.9046) loss_zs_kd 0.4541 (0.4251) loss_oracle 0.2183 (0.2357) acc 81.2500 (75.9014) lr 1.1874e-03 eta 0:25:05
epoch [24/50] batch [280/288] time 0.196 (0.200) data 0.000 (0.001) loss 0.9900 (1.1431) teacher_loss 0.8315 (0.9081) loss_zs_kd 0.3463 (0.4259) loss_oracle 0.1586 (0.2349) acc 75.0000 (75.9040) lr 1.1874e-03 eta 0:24:58
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,461
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 87.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,005
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.3%
******* Domain a best val acc:      87.9%, epoch: 19 *******
******* Domain a best val test acc: 82.1%, epoch: 19 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [25/50] batch [20/288] time 0.193 (0.216) data 0.000 (0.016) loss 1.4884 (1.1509) teacher_loss 1.2213 (0.9093) loss_zs_kd 0.4606 (0.4371) loss_oracle 0.2670 (0.2416) acc 65.6250 (75.0000) lr 1.1253e-03 eta 0:26:55
epoch [25/50] batch [40/288] time 0.194 (0.206) data 0.000 (0.008) loss 0.8654 (1.1560) teacher_loss 0.6119 (0.9133) loss_zs_kd 0.3684 (0.4284) loss_oracle 0.2535 (0.2427) acc 84.3750 (75.3906) lr 1.1253e-03 eta 0:25:33
epoch [25/50] batch [60/288] time 0.196 (0.202) data 0.001 (0.006) loss 1.4978 (1.1674) teacher_loss 1.2644 (0.9275) loss_zs_kd 0.4070 (0.4201) loss_oracle 0.2334 (0.2399) acc 62.5000 (75.1562) lr 1.1253e-03 eta 0:25:00
epoch [25/50] batch [80/288] time 0.201 (0.200) data 0.000 (0.004) loss 1.1903 (1.1669) teacher_loss 0.9896 (0.9247) loss_zs_kd 0.5796 (0.4236) loss_oracle 0.2007 (0.2422) acc 78.1250 (75.2734) lr 1.1253e-03 eta 0:24:41
epoch [25/50] batch [100/288] time 0.088 (0.198) data 0.000 (0.003) loss 0.9928 (1.1637) teacher_loss 0.7587 (0.9201) loss_zs_kd 0.4700 (0.4349) loss_oracle 0.2341 (0.2437) acc 75.0000 (75.5625) lr 1.1253e-03 eta 0:24:21
epoch [25/50] batch [120/288] time 0.097 (0.205) data 0.000 (0.003) loss 1.3483 (1.1541) teacher_loss 1.0825 (0.9102) loss_zs_kd 0.3797 (0.4289) loss_oracle 0.2658 (0.2439) acc 71.8750 (75.8073) lr 1.1253e-03 eta 0:25:10
epoch [25/50] batch [140/288] time 0.192 (0.209) data 0.000 (0.002) loss 0.5220 (1.1504) teacher_loss 0.3193 (0.9088) loss_zs_kd 0.3489 (0.4293) loss_oracle 0.2027 (0.2415) acc 90.6250 (75.8929) lr 1.1253e-03 eta 0:25:38
epoch [25/50] batch [160/288] time 0.196 (0.207) data 0.000 (0.002) loss 1.0382 (1.1425) teacher_loss 0.7585 (0.8987) loss_zs_kd 0.3872 (0.4267) loss_oracle 0.2797 (0.2438) acc 84.3750 (76.2305) lr 1.1253e-03 eta 0:25:20
epoch [25/50] batch [180/288] time 0.172 (0.206) data 0.000 (0.002) loss 1.2847 (1.1490) teacher_loss 1.0665 (0.9052) loss_zs_kd 0.4418 (0.4272) loss_oracle 0.2182 (0.2438) acc 71.8750 (76.1111) lr 1.1253e-03 eta 0:25:06
epoch [25/50] batch [200/288] time 0.194 (0.205) data 0.000 (0.002) loss 0.7269 (1.1434) teacher_loss 0.5270 (0.9002) loss_zs_kd 0.5064 (0.4267) loss_oracle 0.1998 (0.2432) acc 87.5000 (76.1875) lr 1.1253e-03 eta 0:24:52
epoch [25/50] batch [220/288] time 0.189 (0.204) data 0.000 (0.002) loss 0.9644 (1.1365) teacher_loss 0.7210 (0.8942) loss_zs_kd 0.4647 (0.4261) loss_oracle 0.2434 (0.2423) acc 81.2500 (76.3494) lr 1.1253e-03 eta 0:24:41
epoch [25/50] batch [240/288] time 0.199 (0.203) data 0.000 (0.002) loss 1.6572 (1.1389) teacher_loss 1.4157 (0.8970) loss_zs_kd 0.4517 (0.4253) loss_oracle 0.2415 (0.2419) acc 62.5000 (76.3021) lr 1.1253e-03 eta 0:24:31
epoch [25/50] batch [260/288] time 0.199 (0.202) data 0.000 (0.001) loss 0.9916 (1.1372) teacher_loss 0.7539 (0.8947) loss_zs_kd 0.3947 (0.4271) loss_oracle 0.2376 (0.2425) acc 81.2500 (76.4543) lr 1.1253e-03 eta 0:24:22
epoch [25/50] batch [280/288] time 0.193 (0.201) data 0.000 (0.001) loss 1.2318 (1.1380) teacher_loss 0.9697 (0.8946) loss_zs_kd 0.3152 (0.4266) loss_oracle 0.2621 (0.2434) acc 68.7500 (76.5513) lr 1.1253e-03 eta 0:24:11
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,463
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 87.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,999
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.1%
******* Domain a best val acc:      87.9%, epoch: 19 *******
******* Domain a best val test acc: 82.1%, epoch: 19 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [26/50] batch [20/288] time 0.191 (0.210) data 0.000 (0.014) loss 0.9115 (1.0266) teacher_loss 0.6699 (0.7624) loss_zs_kd 0.2659 (0.4142) loss_oracle 0.2416 (0.2641) acc 78.1250 (80.3125) lr 1.0628e-03 eta 0:25:06
epoch [26/50] batch [40/288] time 0.194 (0.202) data 0.000 (0.007) loss 1.2322 (1.0917) teacher_loss 0.9703 (0.8298) loss_zs_kd 0.2428 (0.4162) loss_oracle 0.2619 (0.2619) acc 71.8750 (78.1250) lr 1.0628e-03 eta 0:24:05
epoch [26/50] batch [60/288] time 0.194 (0.199) data 0.000 (0.005) loss 1.0634 (1.0981) teacher_loss 0.8285 (0.8424) loss_zs_kd 0.4253 (0.4264) loss_oracle 0.2349 (0.2557) acc 65.6250 (77.8646) lr 1.0628e-03 eta 0:23:42
epoch [26/50] batch [80/288] time 0.198 (0.198) data 0.000 (0.004) loss 1.6881 (1.1051) teacher_loss 1.4748 (0.8536) loss_zs_kd 0.4719 (0.4295) loss_oracle 0.2132 (0.2515) acc 68.7500 (77.5391) lr 1.0628e-03 eta 0:23:30
epoch [26/50] batch [100/288] time 0.194 (0.197) data 0.000 (0.003) loss 0.8490 (1.1175) teacher_loss 0.6255 (0.8675) loss_zs_kd 0.3317 (0.4276) loss_oracle 0.2235 (0.2499) acc 90.6250 (76.8750) lr 1.0628e-03 eta 0:23:19
epoch [26/50] batch [120/288] time 0.290 (0.206) data 0.000 (0.003) loss 1.0201 (1.1216) teacher_loss 0.7452 (0.8711) loss_zs_kd 0.6494 (0.4270) loss_oracle 0.2749 (0.2505) acc 78.1250 (76.5885) lr 1.0628e-03 eta 0:24:16
epoch [26/50] batch [140/288] time 0.201 (0.208) data 0.000 (0.002) loss 1.2604 (1.1221) teacher_loss 0.9731 (0.8687) loss_zs_kd 0.3729 (0.4258) loss_oracle 0.2873 (0.2534) acc 78.1250 (76.6741) lr 1.0628e-03 eta 0:24:31
epoch [26/50] batch [160/288] time 0.191 (0.207) data 0.000 (0.002) loss 1.4259 (1.1219) teacher_loss 1.1873 (0.8703) loss_zs_kd 0.5055 (0.4260) loss_oracle 0.2386 (0.2516) acc 71.8750 (76.7773) lr 1.0628e-03 eta 0:24:14
epoch [26/50] batch [180/288] time 0.197 (0.205) data 0.000 (0.002) loss 0.9870 (1.1216) teacher_loss 0.7515 (0.8712) loss_zs_kd 0.3419 (0.4255) loss_oracle 0.2355 (0.2504) acc 81.2500 (76.6146) lr 1.0628e-03 eta 0:24:02
epoch [26/50] batch [200/288] time 0.196 (0.204) data 0.000 (0.002) loss 1.0484 (1.1217) teacher_loss 0.8246 (0.8737) loss_zs_kd 0.3859 (0.4266) loss_oracle 0.2239 (0.2480) acc 84.3750 (76.4688) lr 1.0628e-03 eta 0:23:50
epoch [26/50] batch [220/288] time 0.195 (0.204) data 0.000 (0.002) loss 0.5379 (1.1228) teacher_loss 0.2738 (0.8767) loss_zs_kd 0.3251 (0.4271) loss_oracle 0.2641 (0.2460) acc 93.7500 (76.4205) lr 1.0628e-03 eta 0:23:40
epoch [26/50] batch [240/288] time 0.193 (0.203) data 0.000 (0.001) loss 1.0025 (1.1265) teacher_loss 0.8169 (0.8814) loss_zs_kd 0.4877 (0.4291) loss_oracle 0.1856 (0.2451) acc 81.2500 (76.3281) lr 1.0628e-03 eta 0:23:31
epoch [26/50] batch [260/288] time 0.198 (0.202) data 0.000 (0.001) loss 1.1254 (1.1335) teacher_loss 0.8633 (0.8884) loss_zs_kd 0.5088 (0.4344) loss_oracle 0.2621 (0.2451) acc 71.8750 (76.1178) lr 1.0628e-03 eta 0:23:22
epoch [26/50] batch [280/288] time 0.188 (0.202) data 0.000 (0.001) loss 1.1830 (1.1355) teacher_loss 0.9418 (0.8902) loss_zs_kd 0.4923 (0.4370) loss_oracle 0.2412 (0.2453) acc 65.6250 (76.0491) lr 1.0628e-03 eta 0:23:14
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,463
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 87.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,990
* accuracy: 82.0%
* error: 18.0%
* macro_f1: 78.7%
******* Domain a best val acc:      87.9%, epoch: 19 *******
******* Domain a best val test acc: 82.1%, epoch: 19 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [27/50] batch [20/288] time 0.203 (0.212) data 0.000 (0.015) loss 0.8602 (1.0567) teacher_loss 0.6389 (0.8382) loss_zs_kd 0.4945 (0.4884) loss_oracle 0.2213 (0.2185) acc 78.1250 (78.2812) lr 1.0000e-03 eta 0:24:21
epoch [27/50] batch [40/288] time 0.192 (0.203) data 0.000 (0.008) loss 0.6978 (1.1271) teacher_loss 0.4615 (0.8986) loss_zs_kd 0.3392 (0.4575) loss_oracle 0.2363 (0.2285) acc 84.3750 (76.0938) lr 1.0000e-03 eta 0:23:18
epoch [27/50] batch [60/288] time 0.199 (0.201) data 0.000 (0.005) loss 1.5507 (1.1127) teacher_loss 1.2842 (0.8860) loss_zs_kd 0.7044 (0.4534) loss_oracle 0.2666 (0.2268) acc 68.7500 (76.4062) lr 1.0000e-03 eta 0:22:54
epoch [27/50] batch [80/288] time 0.194 (0.199) data 0.000 (0.004) loss 0.8942 (1.1115) teacher_loss 0.6195 (0.8804) loss_zs_kd 0.3739 (0.4373) loss_oracle 0.2747 (0.2310) acc 84.3750 (76.1719) lr 1.0000e-03 eta 0:22:40
epoch [27/50] batch [100/288] time 0.191 (0.198) data 0.000 (0.003) loss 0.9302 (1.1208) teacher_loss 0.6941 (0.8893) loss_zs_kd 0.5277 (0.4363) loss_oracle 0.2361 (0.2315) acc 81.2500 (75.9375) lr 1.0000e-03 eta 0:22:28
epoch [27/50] batch [120/288] time 0.106 (0.205) data 0.000 (0.003) loss 0.9180 (1.0973) teacher_loss 0.7121 (0.8662) loss_zs_kd 0.3911 (0.4317) loss_oracle 0.2059 (0.2311) acc 84.3750 (76.9531) lr 1.0000e-03 eta 0:23:13
epoch [27/50] batch [140/288] time 0.199 (0.209) data 0.000 (0.002) loss 1.0732 (1.1059) teacher_loss 0.8305 (0.8731) loss_zs_kd 0.6444 (0.4375) loss_oracle 0.2427 (0.2327) acc 75.0000 (76.9196) lr 1.0000e-03 eta 0:23:38
epoch [27/50] batch [160/288] time 0.191 (0.208) data 0.000 (0.002) loss 1.2146 (1.1036) teacher_loss 0.9328 (0.8693) loss_zs_kd 0.5328 (0.4398) loss_oracle 0.2818 (0.2343) acc 68.7500 (76.9727) lr 1.0000e-03 eta 0:23:21
epoch [27/50] batch [180/288] time 0.198 (0.206) data 0.000 (0.002) loss 1.0054 (1.1105) teacher_loss 0.7697 (0.8759) loss_zs_kd 0.4851 (0.4400) loss_oracle 0.2357 (0.2345) acc 68.7500 (76.7535) lr 1.0000e-03 eta 0:23:07
epoch [27/50] batch [200/288] time 0.195 (0.205) data 0.000 (0.002) loss 1.1375 (1.1046) teacher_loss 0.8683 (0.8688) loss_zs_kd 0.3267 (0.4386) loss_oracle 0.2692 (0.2358) acc 78.1250 (76.8438) lr 1.0000e-03 eta 0:22:55
epoch [27/50] batch [220/288] time 0.196 (0.204) data 0.000 (0.002) loss 1.0770 (1.1054) teacher_loss 0.8788 (0.8687) loss_zs_kd 0.7698 (0.4370) loss_oracle 0.1982 (0.2367) acc 78.1250 (76.8892) lr 1.0000e-03 eta 0:22:44
epoch [27/50] batch [240/288] time 0.197 (0.203) data 0.000 (0.001) loss 0.7794 (1.1191) teacher_loss 0.5364 (0.8822) loss_zs_kd 0.2857 (0.4368) loss_oracle 0.2431 (0.2369) acc 87.5000 (76.4714) lr 1.0000e-03 eta 0:22:35
epoch [27/50] batch [260/288] time 0.199 (0.203) data 0.000 (0.001) loss 0.8575 (1.1190) teacher_loss 0.6565 (0.8817) loss_zs_kd 0.3996 (0.4382) loss_oracle 0.2010 (0.2373) acc 84.3750 (76.5986) lr 1.0000e-03 eta 0:22:27
epoch [27/50] batch [280/288] time 0.207 (0.202) data 0.000 (0.001) loss 1.4956 (1.1217) teacher_loss 1.2565 (0.8845) loss_zs_kd 0.6683 (0.4399) loss_oracle 0.2391 (0.2372) acc 56.2500 (76.4509) lr 1.0000e-03 eta 0:22:18
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,469
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.5%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,983
* accuracy: 81.7%
* error: 18.3%
* macro_f1: 78.6%
******* Domain a best val acc:      88.1%, epoch: 27 *******
******* Domain a best val test acc: 81.7%, epoch: 27 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [28/50] batch [20/288] time 0.195 (0.211) data 0.000 (0.014) loss 0.9746 (1.1487) teacher_loss 0.7335 (0.8797) loss_zs_kd 0.3754 (0.4803) loss_oracle 0.2411 (0.2690) acc 81.2500 (76.0938) lr 9.3721e-04 eta 0:23:15
epoch [28/50] batch [40/288] time 0.198 (0.203) data 0.001 (0.007) loss 0.7666 (1.1475) teacher_loss 0.5099 (0.8938) loss_zs_kd 0.3565 (0.4361) loss_oracle 0.2567 (0.2537) acc 87.5000 (75.8594) lr 9.3721e-04 eta 0:22:13
epoch [28/50] batch [60/288] time 0.192 (0.199) data 0.000 (0.005) loss 0.9735 (1.1555) teacher_loss 0.7287 (0.9069) loss_zs_kd 0.5455 (0.4388) loss_oracle 0.2447 (0.2487) acc 75.0000 (75.3125) lr 9.3721e-04 eta 0:21:49
epoch [28/50] batch [80/288] time 0.197 (0.198) data 0.000 (0.004) loss 1.1171 (1.1639) teacher_loss 0.8959 (0.9181) loss_zs_kd 0.4076 (0.4444) loss_oracle 0.2212 (0.2458) acc 71.8750 (75.0781) lr 9.3721e-04 eta 0:21:36
epoch [28/50] batch [100/288] time 0.087 (0.194) data 0.000 (0.003) loss 1.1175 (1.1556) teacher_loss 0.7758 (0.9055) loss_zs_kd 0.3142 (0.4432) loss_oracle 0.3417 (0.2501) acc 87.5000 (75.5625) lr 9.3721e-04 eta 0:21:05
epoch [28/50] batch [120/288] time 0.105 (0.202) data 0.000 (0.003) loss 1.0015 (1.1431) teacher_loss 0.7890 (0.8913) loss_zs_kd 0.3770 (0.4365) loss_oracle 0.2125 (0.2517) acc 84.3750 (75.8854) lr 9.3721e-04 eta 0:21:54
epoch [28/50] batch [140/288] time 0.200 (0.207) data 0.001 (0.002) loss 0.6493 (1.1362) teacher_loss 0.4088 (0.8832) loss_zs_kd 0.3194 (0.4387) loss_oracle 0.2406 (0.2530) acc 93.7500 (76.1161) lr 9.3721e-04 eta 0:22:20
epoch [28/50] batch [160/288] time 0.196 (0.206) data 0.000 (0.002) loss 1.3592 (1.1231) teacher_loss 1.1462 (0.8719) loss_zs_kd 0.4079 (0.4346) loss_oracle 0.2130 (0.2512) acc 71.8750 (76.5625) lr 9.3721e-04 eta 0:22:08
epoch [28/50] batch [180/288] time 0.198 (0.204) data 0.000 (0.002) loss 0.9237 (1.1162) teacher_loss 0.7034 (0.8658) loss_zs_kd 0.6426 (0.4372) loss_oracle 0.2203 (0.2504) acc 75.0000 (76.7014) lr 9.3721e-04 eta 0:21:56
epoch [28/50] batch [200/288] time 0.188 (0.203) data 0.000 (0.002) loss 1.3665 (1.1219) teacher_loss 1.0904 (0.8717) loss_zs_kd 0.5709 (0.4406) loss_oracle 0.2761 (0.2502) acc 71.8750 (76.6562) lr 9.3721e-04 eta 0:21:44
epoch [28/50] batch [220/288] time 0.217 (0.202) data 0.000 (0.001) loss 1.1981 (1.1243) teacher_loss 0.9528 (0.8736) loss_zs_kd 0.3454 (0.4406) loss_oracle 0.2453 (0.2507) acc 81.2500 (76.6051) lr 9.3721e-04 eta 0:21:36
epoch [28/50] batch [240/288] time 0.195 (0.202) data 0.000 (0.001) loss 0.6111 (1.1251) teacher_loss 0.3461 (0.8745) loss_zs_kd 0.3682 (0.4420) loss_oracle 0.2650 (0.2506) acc 96.8750 (76.5495) lr 9.3721e-04 eta 0:21:27
epoch [28/50] batch [260/288] time 0.193 (0.201) data 0.000 (0.001) loss 0.9813 (1.1286) teacher_loss 0.6860 (0.8775) loss_zs_kd 0.3806 (0.4416) loss_oracle 0.2952 (0.2511) acc 84.3750 (76.5745) lr 9.3721e-04 eta 0:21:19
epoch [28/50] batch [280/288] time 0.192 (0.200) data 0.000 (0.001) loss 0.9621 (1.1263) teacher_loss 0.7289 (0.8768) loss_zs_kd 0.4476 (0.4432) loss_oracle 0.2331 (0.2495) acc 81.2500 (76.5848) lr 9.3721e-04 eta 0:21:11
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,466
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 87.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,999
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.0%
******* Domain a best val acc:      88.1%, epoch: 27 *******
******* Domain a best val test acc: 81.7%, epoch: 27 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [29/50] batch [20/288] time 0.194 (0.211) data 0.000 (0.015) loss 0.8372 (1.0307) teacher_loss 0.6017 (0.7875) loss_zs_kd 0.6232 (0.4513) loss_oracle 0.2355 (0.2432) acc 81.2500 (78.7500) lr 8.7467e-04 eta 0:22:09
epoch [29/50] batch [40/288] time 0.190 (0.202) data 0.000 (0.008) loss 1.0840 (1.1270) teacher_loss 0.8299 (0.8807) loss_zs_kd 0.3841 (0.4420) loss_oracle 0.2541 (0.2463) acc 78.1250 (76.1719) lr 8.7467e-04 eta 0:21:14
epoch [29/50] batch [60/288] time 0.194 (0.200) data 0.000 (0.005) loss 2.0683 (1.1686) teacher_loss 1.8198 (0.9205) loss_zs_kd 0.5627 (0.4343) loss_oracle 0.2485 (0.2481) acc 59.3750 (75.4167) lr 8.7467e-04 eta 0:20:55
epoch [29/50] batch [80/288] time 0.197 (0.199) data 0.000 (0.004) loss 0.8581 (1.1594) teacher_loss 0.6066 (0.9117) loss_zs_kd 0.5483 (0.4536) loss_oracle 0.2516 (0.2477) acc 87.5000 (75.5078) lr 8.7467e-04 eta 0:20:43
epoch [29/50] batch [100/288] time 0.085 (0.196) data 0.000 (0.003) loss 1.5630 (1.1667) teacher_loss 1.3435 (0.9208) loss_zs_kd 0.4500 (0.4575) loss_oracle 0.2195 (0.2458) acc 71.8750 (75.6875) lr 8.7467e-04 eta 0:20:19
epoch [29/50] batch [120/288] time 0.443 (0.209) data 0.000 (0.003) loss 1.0525 (1.1542) teacher_loss 0.7854 (0.9065) loss_zs_kd 0.2841 (0.4509) loss_oracle 0.2671 (0.2476) acc 78.1250 (76.2240) lr 8.7467e-04 eta 0:21:36
epoch [29/50] batch [140/288] time 0.190 (0.209) data 0.000 (0.002) loss 1.2703 (1.1426) teacher_loss 0.9731 (0.8953) loss_zs_kd 0.2735 (0.4455) loss_oracle 0.2972 (0.2473) acc 65.6250 (76.3616) lr 8.7467e-04 eta 0:21:36
epoch [29/50] batch [160/288] time 0.192 (0.207) data 0.000 (0.002) loss 0.8454 (1.1414) teacher_loss 0.6128 (0.8935) loss_zs_kd 0.5129 (0.4492) loss_oracle 0.2327 (0.2479) acc 81.2500 (76.4453) lr 8.7467e-04 eta 0:21:20
epoch [29/50] batch [180/288] time 0.194 (0.206) data 0.000 (0.002) loss 1.3759 (1.1502) teacher_loss 1.1309 (0.9007) loss_zs_kd 0.5043 (0.4504) loss_oracle 0.2450 (0.2495) acc 62.5000 (76.2500) lr 8.7467e-04 eta 0:21:08
epoch [29/50] batch [200/288] time 0.192 (0.205) data 0.000 (0.002) loss 1.3046 (1.1439) teacher_loss 1.0060 (0.8935) loss_zs_kd 0.4491 (0.4490) loss_oracle 0.2986 (0.2504) acc 78.1250 (76.4688) lr 8.7467e-04 eta 0:20:56
epoch [29/50] batch [220/288] time 0.187 (0.204) data 0.000 (0.002) loss 1.1641 (1.1496) teacher_loss 0.9213 (0.8983) loss_zs_kd 0.4082 (0.4460) loss_oracle 0.2427 (0.2513) acc 78.1250 (76.3494) lr 8.7467e-04 eta 0:20:46
epoch [29/50] batch [240/288] time 0.219 (0.203) data 0.000 (0.001) loss 1.0947 (1.1483) teacher_loss 0.8041 (0.8967) loss_zs_kd 0.3362 (0.4445) loss_oracle 0.2906 (0.2516) acc 87.5000 (76.3021) lr 8.7467e-04 eta 0:20:37
epoch [29/50] batch [260/288] time 0.194 (0.202) data 0.000 (0.001) loss 0.9651 (1.1485) teacher_loss 0.7110 (0.8975) loss_zs_kd 0.4910 (0.4472) loss_oracle 0.2542 (0.2510) acc 78.1250 (76.3101) lr 8.7467e-04 eta 0:20:29
epoch [29/50] batch [280/288] time 0.191 (0.202) data 0.000 (0.001) loss 1.6680 (1.1478) teacher_loss 1.4288 (0.8972) loss_zs_kd 0.4552 (0.4504) loss_oracle 0.2392 (0.2506) acc 56.2500 (76.2835) lr 8.7467e-04 eta 0:20:20
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,470
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.6%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,998
* accuracy: 82.3%
* error: 17.7%
* macro_f1: 78.7%
******* Domain a best val acc:      88.1%, epoch: 29 *******
******* Domain a best val test acc: 82.3%, epoch: 29 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [30/50] batch [20/288] time 0.191 (0.207) data 0.000 (0.014) loss 0.8023 (1.0909) teacher_loss 0.5904 (0.8527) loss_zs_kd 0.4972 (0.4539) loss_oracle 0.2118 (0.2382) acc 81.2500 (75.0000) lr 8.1262e-04 eta 0:20:48
epoch [30/50] batch [40/288] time 0.191 (0.201) data 0.000 (0.007) loss 1.0164 (1.0382) teacher_loss 0.8043 (0.7939) loss_zs_kd 0.3540 (0.4560) loss_oracle 0.2121 (0.2443) acc 68.7500 (76.9531) lr 8.1262e-04 eta 0:20:07
epoch [30/50] batch [60/288] time 0.200 (0.199) data 0.000 (0.005) loss 1.1830 (1.0748) teacher_loss 1.0026 (0.8298) loss_zs_kd 0.6910 (0.4654) loss_oracle 0.1805 (0.2450) acc 75.0000 (77.2917) lr 8.1262e-04 eta 0:19:49
epoch [30/50] batch [80/288] time 0.196 (0.197) data 0.000 (0.004) loss 1.1611 (1.1012) teacher_loss 0.8718 (0.8544) loss_zs_kd 0.4061 (0.4624) loss_oracle 0.2893 (0.2468) acc 71.8750 (76.9922) lr 8.1262e-04 eta 0:19:38
epoch [30/50] batch [100/288] time 0.424 (0.193) data 0.000 (0.003) loss 0.8043 (1.1255) teacher_loss 0.5560 (0.8785) loss_zs_kd 0.3866 (0.4552) loss_oracle 0.2483 (0.2470) acc 93.7500 (76.4062) lr 8.1262e-04 eta 0:19:07
epoch [30/50] batch [120/288] time 0.085 (0.213) data 0.000 (0.003) loss 1.1208 (1.1400) teacher_loss 0.9029 (0.8897) loss_zs_kd 0.3329 (0.4554) loss_oracle 0.2179 (0.2503) acc 78.1250 (76.0677) lr 8.1262e-04 eta 0:21:02
epoch [30/50] batch [140/288] time 0.193 (0.209) data 0.000 (0.002) loss 1.3104 (1.1532) teacher_loss 1.0839 (0.9046) loss_zs_kd 0.2713 (0.4584) loss_oracle 0.2265 (0.2486) acc 71.8750 (75.7812) lr 8.1262e-04 eta 0:20:33
epoch [30/50] batch [160/288] time 0.193 (0.207) data 0.000 (0.002) loss 0.8690 (1.1469) teacher_loss 0.6144 (0.8994) loss_zs_kd 0.4139 (0.4583) loss_oracle 0.2546 (0.2475) acc 84.3750 (76.0742) lr 8.1262e-04 eta 0:20:18
epoch [30/50] batch [180/288] time 0.195 (0.206) data 0.000 (0.002) loss 1.2385 (1.1557) teacher_loss 1.0293 (0.9098) loss_zs_kd 0.4216 (0.4546) loss_oracle 0.2092 (0.2459) acc 75.0000 (75.8854) lr 8.1262e-04 eta 0:20:06
epoch [30/50] batch [200/288] time 0.214 (0.204) data 0.000 (0.002) loss 1.3650 (1.1512) teacher_loss 1.1612 (0.9062) loss_zs_kd 0.4830 (0.4528) loss_oracle 0.2038 (0.2450) acc 68.7500 (75.8750) lr 8.1262e-04 eta 0:19:55
epoch [30/50] batch [220/288] time 0.191 (0.203) data 0.000 (0.001) loss 1.2969 (1.1517) teacher_loss 1.0180 (0.9065) loss_zs_kd 0.4476 (0.4526) loss_oracle 0.2788 (0.2451) acc 75.0000 (75.7812) lr 8.1262e-04 eta 0:19:45
epoch [30/50] batch [240/288] time 0.193 (0.203) data 0.000 (0.001) loss 0.8156 (1.1554) teacher_loss 0.5668 (0.9096) loss_zs_kd 0.3791 (0.4493) loss_oracle 0.2488 (0.2458) acc 84.3750 (75.7031) lr 8.1262e-04 eta 0:19:36
epoch [30/50] batch [260/288] time 0.193 (0.202) data 0.001 (0.001) loss 0.9672 (1.1634) teacher_loss 0.6777 (0.9166) loss_zs_kd 0.3764 (0.4487) loss_oracle 0.2895 (0.2469) acc 81.2500 (75.5409) lr 8.1262e-04 eta 0:19:28
epoch [30/50] batch [280/288] time 0.184 (0.201) data 0.000 (0.001) loss 0.8902 (1.1541) teacher_loss 0.6067 (0.9065) loss_zs_kd 0.4099 (0.4482) loss_oracle 0.2836 (0.2477) acc 81.2500 (75.9375) lr 8.1262e-04 eta 0:19:20
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,473
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 87.5%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,001
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.1%
******* Domain a best val acc:      88.2%, epoch: 30 *******
******* Domain a best val test acc: 82.4%, epoch: 30 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [31/50] batch [20/288] time 0.194 (0.215) data 0.000 (0.017) loss 0.6369 (1.0653) teacher_loss 0.4035 (0.8247) loss_zs_kd 0.3502 (0.4883) loss_oracle 0.2333 (0.2406) acc 87.5000 (77.1875) lr 7.5131e-04 eta 0:20:36
epoch [31/50] batch [40/288] time 0.227 (0.207) data 0.000 (0.008) loss 0.8410 (1.1286) teacher_loss 0.6010 (0.8819) loss_zs_kd 0.4560 (0.4606) loss_oracle 0.2400 (0.2467) acc 87.5000 (76.5625) lr 7.5131e-04 eta 0:19:44
epoch [31/50] batch [60/288] time 0.191 (0.203) data 0.000 (0.006) loss 1.1105 (1.1436) teacher_loss 0.8570 (0.8962) loss_zs_kd 0.4408 (0.4514) loss_oracle 0.2534 (0.2473) acc 75.0000 (76.1979) lr 7.5131e-04 eta 0:19:17
epoch [31/50] batch [80/288] time 0.228 (0.201) data 0.000 (0.004) loss 1.1184 (1.1269) teacher_loss 0.8390 (0.8787) loss_zs_kd 0.3515 (0.4580) loss_oracle 0.2794 (0.2482) acc 78.1250 (76.0938) lr 7.5131e-04 eta 0:19:03
epoch [31/50] batch [100/288] time 0.530 (0.215) data 0.001 (0.004) loss 1.2830 (1.1129) teacher_loss 0.9746 (0.8626) loss_zs_kd 0.6589 (0.4572) loss_oracle 0.3084 (0.2503) acc 75.0000 (76.3125) lr 7.5131e-04 eta 0:20:14
epoch [31/50] batch [120/288] time 0.194 (0.215) data 0.000 (0.003) loss 0.7146 (1.1189) teacher_loss 0.4527 (0.8675) loss_zs_kd 0.3659 (0.4644) loss_oracle 0.2619 (0.2514) acc 90.6250 (76.5365) lr 7.5131e-04 eta 0:20:15
epoch [31/50] batch [140/288] time 0.197 (0.213) data 0.000 (0.003) loss 0.7558 (1.1180) teacher_loss 0.5488 (0.8647) loss_zs_kd 0.5495 (0.4642) loss_oracle 0.2070 (0.2534) acc 84.3750 (76.8750) lr 7.5131e-04 eta 0:19:54
epoch [31/50] batch [160/288] time 0.190 (0.210) data 0.000 (0.002) loss 0.9911 (1.1282) teacher_loss 0.7323 (0.8733) loss_zs_kd 0.4834 (0.4653) loss_oracle 0.2588 (0.2549) acc 84.3750 (76.6406) lr 7.5131e-04 eta 0:19:37
epoch [31/50] batch [180/288] time 0.196 (0.208) data 0.000 (0.002) loss 1.3835 (1.1316) teacher_loss 1.1339 (0.8770) loss_zs_kd 0.4238 (0.4583) loss_oracle 0.2496 (0.2546) acc 75.0000 (76.5278) lr 7.5131e-04 eta 0:19:22
epoch [31/50] batch [200/288] time 0.208 (0.207) data 0.001 (0.002) loss 0.9936 (1.1317) teacher_loss 0.7723 (0.8780) loss_zs_kd 0.3818 (0.4604) loss_oracle 0.2213 (0.2537) acc 81.2500 (76.5781) lr 7.5131e-04 eta 0:19:10
epoch [31/50] batch [220/288] time 0.191 (0.206) data 0.000 (0.002) loss 1.2092 (1.1376) teacher_loss 0.9508 (0.8852) loss_zs_kd 0.5512 (0.4596) loss_oracle 0.2584 (0.2524) acc 71.8750 (76.4205) lr 7.5131e-04 eta 0:19:00
epoch [31/50] batch [240/288] time 0.194 (0.205) data 0.000 (0.002) loss 1.1741 (1.1329) teacher_loss 0.8784 (0.8818) loss_zs_kd 0.4223 (0.4588) loss_oracle 0.2957 (0.2512) acc 78.1250 (76.4062) lr 7.5131e-04 eta 0:18:50
epoch [31/50] batch [260/288] time 0.189 (0.204) data 0.000 (0.002) loss 0.8550 (1.1353) teacher_loss 0.5722 (0.8850) loss_zs_kd 0.2620 (0.4551) loss_oracle 0.2828 (0.2502) acc 87.5000 (76.3582) lr 7.5131e-04 eta 0:18:41
epoch [31/50] batch [280/288] time 0.187 (0.203) data 0.000 (0.001) loss 1.0082 (1.1284) teacher_loss 0.7480 (0.8784) loss_zs_kd 0.3101 (0.4516) loss_oracle 0.2602 (0.2499) acc 78.1250 (76.6741) lr 7.5131e-04 eta 0:18:32
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,471
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.1%
******* Domain a best val acc:      88.2%, epoch: 30 *******
******* Domain a best val test acc: 82.4%, epoch: 30 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [32/50] batch [20/288] time 0.196 (0.210) data 0.000 (0.015) loss 1.1828 (1.1330) teacher_loss 0.9297 (0.8888) loss_zs_kd 0.2781 (0.4137) loss_oracle 0.2532 (0.2442) acc 78.1250 (77.0312) lr 6.9098e-04 eta 0:19:03
epoch [32/50] batch [40/288] time 0.187 (0.202) data 0.000 (0.007) loss 0.6653 (1.1277) teacher_loss 0.4443 (0.8884) loss_zs_kd 0.2751 (0.4436) loss_oracle 0.2210 (0.2393) acc 90.6250 (76.8750) lr 6.9098e-04 eta 0:18:16
epoch [32/50] batch [60/288] time 0.199 (0.199) data 0.000 (0.005) loss 0.9345 (1.1409) teacher_loss 0.6820 (0.8977) loss_zs_kd 0.5393 (0.4584) loss_oracle 0.2525 (0.2432) acc 81.2500 (76.4583) lr 6.9098e-04 eta 0:17:54
epoch [32/50] batch [80/288] time 0.193 (0.198) data 0.000 (0.004) loss 1.5974 (1.1219) teacher_loss 1.3132 (0.8772) loss_zs_kd 0.3616 (0.4584) loss_oracle 0.2842 (0.2447) acc 65.6250 (77.1875) lr 6.9098e-04 eta 0:17:49
epoch [32/50] batch [100/288] time 0.083 (0.211) data 0.000 (0.003) loss 0.6536 (1.1253) teacher_loss 0.4170 (0.8806) loss_zs_kd 0.4498 (0.4534) loss_oracle 0.2366 (0.2447) acc 90.6250 (77.0000) lr 6.9098e-04 eta 0:18:56
epoch [32/50] batch [120/288] time 0.198 (0.210) data 0.000 (0.003) loss 0.8076 (1.1187) teacher_loss 0.5570 (0.8760) loss_zs_kd 0.3370 (0.4545) loss_oracle 0.2506 (0.2427) acc 81.2500 (76.9010) lr 6.9098e-04 eta 0:18:42
epoch [32/50] batch [140/288] time 0.197 (0.208) data 0.000 (0.002) loss 1.0092 (1.1215) teacher_loss 0.7776 (0.8780) loss_zs_kd 0.6426 (0.4552) loss_oracle 0.2316 (0.2435) acc 81.2500 (76.6964) lr 6.9098e-04 eta 0:18:27
epoch [32/50] batch [160/288] time 0.222 (0.206) data 0.000 (0.002) loss 0.8435 (1.1222) teacher_loss 0.6219 (0.8784) loss_zs_kd 0.4580 (0.4525) loss_oracle 0.2216 (0.2438) acc 87.5000 (76.4453) lr 6.9098e-04 eta 0:18:14
epoch [32/50] batch [180/288] time 0.193 (0.205) data 0.000 (0.002) loss 1.0170 (1.1352) teacher_loss 0.7742 (0.8909) loss_zs_kd 0.5078 (0.4524) loss_oracle 0.2428 (0.2443) acc 78.1250 (76.1111) lr 6.9098e-04 eta 0:18:02
epoch [32/50] batch [200/288] time 0.198 (0.204) data 0.000 (0.002) loss 1.1078 (1.1310) teacher_loss 0.8505 (0.8865) loss_zs_kd 0.4526 (0.4514) loss_oracle 0.2573 (0.2446) acc 78.1250 (76.2500) lr 6.9098e-04 eta 0:17:52
epoch [32/50] batch [220/288] time 0.193 (0.203) data 0.000 (0.002) loss 1.0432 (1.1345) teacher_loss 0.7463 (0.8906) loss_zs_kd 0.3364 (0.4493) loss_oracle 0.2969 (0.2439) acc 78.1250 (76.1932) lr 6.9098e-04 eta 0:17:44
epoch [32/50] batch [240/288] time 0.193 (0.202) data 0.000 (0.001) loss 1.1015 (1.1268) teacher_loss 0.9292 (0.8833) loss_zs_kd 0.3555 (0.4475) loss_oracle 0.1723 (0.2434) acc 75.0000 (76.4714) lr 6.9098e-04 eta 0:17:36
epoch [32/50] batch [260/288] time 0.195 (0.201) data 0.000 (0.001) loss 1.1597 (1.1327) teacher_loss 0.8945 (0.8888) loss_zs_kd 0.2753 (0.4479) loss_oracle 0.2653 (0.2440) acc 78.1250 (76.2740) lr 6.9098e-04 eta 0:17:29
epoch [32/50] batch [280/288] time 0.191 (0.201) data 0.000 (0.001) loss 0.8708 (1.1305) teacher_loss 0.6492 (0.8867) loss_zs_kd 0.4157 (0.4479) loss_oracle 0.2216 (0.2439) acc 78.1250 (76.3393) lr 6.9098e-04 eta 0:17:22
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,471
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,011
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.5%
******* Domain a best val acc:      88.2%, epoch: 30 *******
******* Domain a best val test acc: 82.4%, epoch: 30 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [33/50] batch [20/288] time 0.197 (0.216) data 0.000 (0.016) loss 1.2401 (1.1547) teacher_loss 0.9575 (0.9128) loss_zs_kd 0.4694 (0.4622) loss_oracle 0.2826 (0.2419) acc 78.1250 (76.5625) lr 6.3188e-04 eta 0:18:34
epoch [33/50] batch [40/288] time 0.194 (0.205) data 0.000 (0.008) loss 1.2534 (1.1674) teacher_loss 1.0435 (0.9234) loss_zs_kd 0.4166 (0.4591) loss_oracle 0.2099 (0.2440) acc 75.0000 (75.1562) lr 6.3188e-04 eta 0:17:33
epoch [33/50] batch [60/288] time 0.194 (0.201) data 0.000 (0.006) loss 1.1295 (1.1597) teacher_loss 0.9193 (0.9132) loss_zs_kd 0.5707 (0.4656) loss_oracle 0.2101 (0.2465) acc 68.7500 (75.6250) lr 6.3188e-04 eta 0:17:11
epoch [33/50] batch [80/288] time 0.207 (0.200) data 0.000 (0.004) loss 0.9794 (1.1587) teacher_loss 0.7609 (0.9117) loss_zs_kd 0.3603 (0.4587) loss_oracle 0.2184 (0.2471) acc 75.0000 (75.8594) lr 6.3188e-04 eta 0:17:00
epoch [33/50] batch [100/288] time 0.084 (0.213) data 0.000 (0.003) loss 1.0867 (1.1416) teacher_loss 0.8333 (0.8953) loss_zs_kd 0.4438 (0.4653) loss_oracle 0.2535 (0.2464) acc 71.8750 (76.1875) lr 6.3188e-04 eta 0:18:02
epoch [33/50] batch [120/288] time 0.226 (0.215) data 0.000 (0.003) loss 1.8320 (1.1423) teacher_loss 1.5590 (0.8965) loss_zs_kd 0.4265 (0.4652) loss_oracle 0.2731 (0.2458) acc 56.2500 (76.2240) lr 6.3188e-04 eta 0:18:10
epoch [33/50] batch [140/288] time 0.193 (0.212) data 0.000 (0.002) loss 1.1904 (1.1430) teacher_loss 0.9508 (0.8954) loss_zs_kd 0.5735 (0.4663) loss_oracle 0.2396 (0.2477) acc 71.8750 (76.0714) lr 6.3188e-04 eta 0:17:49
epoch [33/50] batch [160/288] time 0.196 (0.210) data 0.000 (0.002) loss 1.3022 (1.1383) teacher_loss 1.0696 (0.8906) loss_zs_kd 0.5171 (0.4614) loss_oracle 0.2327 (0.2478) acc 71.8750 (76.2695) lr 6.3188e-04 eta 0:17:34
epoch [33/50] batch [180/288] time 0.198 (0.208) data 0.000 (0.002) loss 1.3603 (1.1424) teacher_loss 1.1474 (0.8931) loss_zs_kd 0.4427 (0.4582) loss_oracle 0.2129 (0.2492) acc 71.8750 (76.2847) lr 6.3188e-04 eta 0:17:21
epoch [33/50] batch [200/288] time 0.196 (0.207) data 0.000 (0.002) loss 1.2416 (1.1382) teacher_loss 0.9496 (0.8899) loss_zs_kd 0.3292 (0.4553) loss_oracle 0.2920 (0.2483) acc 71.8750 (76.2656) lr 6.3188e-04 eta 0:17:09
epoch [33/50] batch [220/288] time 0.161 (0.206) data 0.000 (0.002) loss 1.2793 (1.1358) teacher_loss 1.0515 (0.8881) loss_zs_kd 0.1705 (0.4554) loss_oracle 0.2278 (0.2477) acc 71.8750 (76.2784) lr 6.3188e-04 eta 0:17:00
epoch [33/50] batch [240/288] time 0.198 (0.205) data 0.000 (0.002) loss 0.9293 (1.1252) teacher_loss 0.6346 (0.8782) loss_zs_kd 0.3870 (0.4552) loss_oracle 0.2947 (0.2470) acc 81.2500 (76.4844) lr 6.3188e-04 eta 0:16:51
epoch [33/50] batch [260/288] time 0.194 (0.204) data 0.000 (0.001) loss 1.2180 (1.1326) teacher_loss 0.9187 (0.8846) loss_zs_kd 0.2885 (0.4537) loss_oracle 0.2993 (0.2480) acc 81.2500 (76.4423) lr 6.3188e-04 eta 0:16:43
epoch [33/50] batch [280/288] time 0.192 (0.203) data 0.000 (0.001) loss 1.0745 (1.1344) teacher_loss 0.8674 (0.8856) loss_zs_kd 0.3464 (0.4551) loss_oracle 0.2071 (0.2487) acc 75.0000 (76.3728) lr 6.3188e-04 eta 0:16:35
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,478
* accuracy: 88.3%
* error: 11.7%
* macro_f1: 87.8%
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,002
* accuracy: 82.5%
* error: 17.5%
* macro_f1: 79.3%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [34/50] batch [20/288] time 0.196 (0.218) data 0.000 (0.016) loss 1.3993 (1.0764) teacher_loss 1.1273 (0.8193) loss_zs_kd 0.4675 (0.4480) loss_oracle 0.2720 (0.2570) acc 68.7500 (78.4375) lr 5.7422e-04 eta 0:17:41
epoch [34/50] batch [40/288] time 0.193 (0.207) data 0.000 (0.008) loss 1.0497 (1.1120) teacher_loss 0.7623 (0.8625) loss_zs_kd 0.3034 (0.4439) loss_oracle 0.2874 (0.2495) acc 84.3750 (77.4219) lr 5.7422e-04 eta 0:16:47
epoch [34/50] batch [60/288] time 0.191 (0.203) data 0.000 (0.006) loss 0.9011 (1.1177) teacher_loss 0.6839 (0.8667) loss_zs_kd 0.3139 (0.4497) loss_oracle 0.2172 (0.2510) acc 81.2500 (77.2917) lr 5.7422e-04 eta 0:16:21
epoch [34/50] batch [80/288] time 0.094 (0.195) data 0.001 (0.004) loss 1.0642 (1.1184) teacher_loss 0.8151 (0.8677) loss_zs_kd 0.5241 (0.4489) loss_oracle 0.2491 (0.2507) acc 81.2500 (77.4609) lr 5.7422e-04 eta 0:15:41
epoch [34/50] batch [100/288] time 0.345 (0.223) data 0.000 (0.003) loss 1.1892 (1.0948) teacher_loss 0.9226 (0.8455) loss_zs_kd 0.6144 (0.4505) loss_oracle 0.2666 (0.2493) acc 75.0000 (77.9375) lr 5.7422e-04 eta 0:17:47
epoch [34/50] batch [120/288] time 0.187 (0.215) data 0.000 (0.003) loss 1.2457 (1.1114) teacher_loss 0.9378 (0.8603) loss_zs_kd 0.4015 (0.4485) loss_oracle 0.3079 (0.2512) acc 71.8750 (77.5260) lr 5.7422e-04 eta 0:17:05
epoch [34/50] batch [140/288] time 0.188 (0.212) data 0.000 (0.003) loss 1.2459 (1.1103) teacher_loss 0.9513 (0.8557) loss_zs_kd 0.4541 (0.4504) loss_oracle 0.2945 (0.2545) acc 71.8750 (77.4107) lr 5.7422e-04 eta 0:16:47
epoch [34/50] batch [160/288] time 0.232 (0.211) data 0.000 (0.002) loss 0.8415 (1.1115) teacher_loss 0.5644 (0.8548) loss_zs_kd 0.4713 (0.4554) loss_oracle 0.2771 (0.2568) acc 84.3750 (77.3242) lr 5.7422e-04 eta 0:16:38
epoch [34/50] batch [180/288] time 0.193 (0.209) data 0.000 (0.002) loss 1.0165 (1.1182) teacher_loss 0.7581 (0.8619) loss_zs_kd 0.4543 (0.4617) loss_oracle 0.2584 (0.2563) acc 81.2500 (77.0660) lr 5.7422e-04 eta 0:16:24
epoch [34/50] batch [200/288] time 0.191 (0.207) data 0.000 (0.002) loss 1.6632 (1.1336) teacher_loss 1.4104 (0.8778) loss_zs_kd 0.5224 (0.4606) loss_oracle 0.2528 (0.2558) acc 65.6250 (76.6875) lr 5.7422e-04 eta 0:16:13
epoch [34/50] batch [220/288] time 0.192 (0.206) data 0.000 (0.002) loss 1.2723 (1.1351) teacher_loss 0.9880 (0.8794) loss_zs_kd 0.3717 (0.4621) loss_oracle 0.2843 (0.2557) acc 71.8750 (76.7045) lr 5.7422e-04 eta 0:16:03
epoch [34/50] batch [240/288] time 0.210 (0.205) data 0.000 (0.002) loss 1.1032 (1.1383) teacher_loss 0.8620 (0.8834) loss_zs_kd 0.4955 (0.4626) loss_oracle 0.2412 (0.2549) acc 78.1250 (76.6146) lr 5.7422e-04 eta 0:15:55
epoch [34/50] batch [260/288] time 0.196 (0.204) data 0.000 (0.001) loss 1.0015 (1.1354) teacher_loss 0.7614 (0.8814) loss_zs_kd 0.4635 (0.4614) loss_oracle 0.2400 (0.2540) acc 87.5000 (76.7067) lr 5.7422e-04 eta 0:15:47
epoch [34/50] batch [280/288] time 0.187 (0.203) data 0.000 (0.001) loss 1.1948 (1.1309) teacher_loss 0.9383 (0.8772) loss_zs_kd 0.5619 (0.4581) loss_oracle 0.2565 (0.2537) acc 71.8750 (76.7188) lr 5.7422e-04 eta 0:15:38
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,461
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 87.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,005
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.3%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [35/50] batch [20/288] time 0.199 (0.207) data 0.000 (0.014) loss 1.3880 (1.2006) teacher_loss 1.1693 (0.9289) loss_zs_kd 0.4562 (0.4713) loss_oracle 0.2187 (0.2717) acc 68.7500 (75.1562) lr 5.1825e-04 eta 0:15:51
epoch [35/50] batch [40/288] time 0.194 (0.202) data 0.000 (0.007) loss 1.3491 (1.2302) teacher_loss 1.1649 (0.9667) loss_zs_kd 0.3786 (0.4813) loss_oracle 0.1842 (0.2635) acc 75.0000 (74.5312) lr 5.1825e-04 eta 0:15:21
epoch [35/50] batch [60/288] time 0.193 (0.201) data 0.001 (0.005) loss 1.6500 (1.2074) teacher_loss 1.4090 (0.9412) loss_zs_kd 0.3510 (0.4626) loss_oracle 0.2411 (0.2662) acc 65.6250 (75.3646) lr 5.1825e-04 eta 0:15:14
epoch [35/50] batch [80/288] time 0.092 (0.191) data 0.000 (0.004) loss 1.1983 (1.2023) teacher_loss 0.8938 (0.9428) loss_zs_kd 0.5111 (0.4548) loss_oracle 0.3045 (0.2596) acc 81.2500 (75.3516) lr 5.1825e-04 eta 0:14:25
epoch [35/50] batch [100/288] time 0.300 (0.220) data 0.000 (0.003) loss 1.3763 (1.1940) teacher_loss 1.1189 (0.9386) loss_zs_kd 0.3616 (0.4526) loss_oracle 0.2573 (0.2555) acc 71.8750 (75.4062) lr 5.1825e-04 eta 0:16:31
epoch [35/50] batch [120/288] time 0.198 (0.214) data 0.000 (0.003) loss 0.9179 (1.1812) teacher_loss 0.6801 (0.9278) loss_zs_kd 0.5443 (0.4534) loss_oracle 0.2378 (0.2534) acc 84.3750 (75.7292) lr 5.1825e-04 eta 0:15:58
epoch [35/50] batch [140/288] time 0.193 (0.211) data 0.000 (0.002) loss 0.6928 (1.1761) teacher_loss 0.4621 (0.9223) loss_zs_kd 0.2190 (0.4496) loss_oracle 0.2307 (0.2537) acc 87.5000 (76.0268) lr 5.1825e-04 eta 0:15:41
epoch [35/50] batch [160/288] time 0.194 (0.209) data 0.000 (0.002) loss 0.9694 (1.1691) teacher_loss 0.7211 (0.9156) loss_zs_kd 0.4074 (0.4496) loss_oracle 0.2483 (0.2536) acc 78.1250 (76.1523) lr 5.1825e-04 eta 0:15:27
epoch [35/50] batch [180/288] time 0.195 (0.207) data 0.000 (0.002) loss 0.8604 (1.1529) teacher_loss 0.6486 (0.8987) loss_zs_kd 0.2731 (0.4455) loss_oracle 0.2118 (0.2542) acc 78.1250 (76.5104) lr 5.1825e-04 eta 0:15:16
epoch [35/50] batch [200/288] time 0.195 (0.206) data 0.000 (0.002) loss 0.7357 (1.1485) teacher_loss 0.4761 (0.8936) loss_zs_kd 0.4631 (0.4486) loss_oracle 0.2596 (0.2549) acc 84.3750 (76.5000) lr 5.1825e-04 eta 0:15:06
epoch [35/50] batch [220/288] time 0.195 (0.205) data 0.000 (0.001) loss 0.8927 (1.1437) teacher_loss 0.5726 (0.8878) loss_zs_kd 0.3066 (0.4491) loss_oracle 0.3201 (0.2559) acc 87.5000 (76.5625) lr 5.1825e-04 eta 0:14:57
epoch [35/50] batch [240/288] time 0.190 (0.204) data 0.000 (0.001) loss 0.7167 (1.1393) teacher_loss 0.3867 (0.8826) loss_zs_kd 0.5773 (0.4501) loss_oracle 0.3299 (0.2567) acc 90.6250 (76.4974) lr 5.1825e-04 eta 0:14:49
epoch [35/50] batch [260/288] time 0.193 (0.203) data 0.000 (0.001) loss 0.9117 (1.1274) teacher_loss 0.6025 (0.8702) loss_zs_kd 0.3654 (0.4504) loss_oracle 0.3092 (0.2572) acc 87.5000 (76.7548) lr 5.1825e-04 eta 0:14:42
epoch [35/50] batch [280/288] time 0.197 (0.202) data 0.000 (0.001) loss 1.1465 (1.1329) teacher_loss 0.8711 (0.8750) loss_zs_kd 0.4052 (0.4541) loss_oracle 0.2754 (0.2579) acc 75.0000 (76.6964) lr 5.1825e-04 eta 0:14:34
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,471
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,009
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.4%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [36/50] batch [20/288] time 0.197 (0.209) data 0.000 (0.013) loss 1.5218 (1.0838) teacher_loss 1.2790 (0.8306) loss_zs_kd 0.4106 (0.4627) loss_oracle 0.2429 (0.2531) acc 68.7500 (79.2188) lr 4.6417e-04 eta 0:15:00
epoch [36/50] batch [40/288] time 0.196 (0.202) data 0.000 (0.007) loss 1.1009 (1.1371) teacher_loss 0.8610 (0.8834) loss_zs_kd 0.3946 (0.4823) loss_oracle 0.2399 (0.2537) acc 75.0000 (76.9531) lr 4.6417e-04 eta 0:14:24
epoch [36/50] batch [60/288] time 0.226 (0.200) data 0.000 (0.005) loss 0.8023 (1.1387) teacher_loss 0.5765 (0.8849) loss_zs_kd 0.6355 (0.4817) loss_oracle 0.2258 (0.2537) acc 81.2500 (76.6146) lr 4.6417e-04 eta 0:14:13
epoch [36/50] batch [80/288] time 0.124 (0.189) data 0.000 (0.003) loss 1.1200 (1.1212) teacher_loss 0.8417 (0.8652) loss_zs_kd 0.4235 (0.4709) loss_oracle 0.2783 (0.2560) acc 75.0000 (77.0703) lr 4.6417e-04 eta 0:13:22
epoch [36/50] batch [100/288] time 0.489 (0.215) data 0.000 (0.003) loss 1.4635 (1.0977) teacher_loss 1.1973 (0.8418) loss_zs_kd 0.3605 (0.4725) loss_oracle 0.2662 (0.2559) acc 75.0000 (77.7188) lr 4.6417e-04 eta 0:15:08
epoch [36/50] batch [120/288] time 0.201 (0.209) data 0.000 (0.002) loss 0.8888 (1.1092) teacher_loss 0.6216 (0.8524) loss_zs_kd 0.4068 (0.4641) loss_oracle 0.2673 (0.2567) acc 90.6250 (77.4219) lr 4.6417e-04 eta 0:14:38
epoch [36/50] batch [140/288] time 0.196 (0.207) data 0.000 (0.002) loss 1.0216 (1.1148) teacher_loss 0.7970 (0.8592) loss_zs_kd 0.4354 (0.4615) loss_oracle 0.2246 (0.2556) acc 81.2500 (77.0982) lr 4.6417e-04 eta 0:14:26
epoch [36/50] batch [160/288] time 0.196 (0.206) data 0.000 (0.002) loss 1.1019 (1.1067) teacher_loss 0.8516 (0.8519) loss_zs_kd 0.3600 (0.4651) loss_oracle 0.2503 (0.2548) acc 75.0000 (77.3633) lr 4.6417e-04 eta 0:14:15
epoch [36/50] batch [180/288] time 0.164 (0.204) data 0.000 (0.002) loss 1.4556 (1.1100) teacher_loss 1.2104 (0.8558) loss_zs_kd 0.4211 (0.4638) loss_oracle 0.2453 (0.2542) acc 68.7500 (77.2396) lr 4.6417e-04 eta 0:14:06
epoch [36/50] batch [200/288] time 0.190 (0.203) data 0.000 (0.002) loss 1.1374 (1.1197) teacher_loss 0.9267 (0.8643) loss_zs_kd 0.4008 (0.4642) loss_oracle 0.2106 (0.2554) acc 71.8750 (77.0938) lr 4.6417e-04 eta 0:13:57
epoch [36/50] batch [220/288] time 0.196 (0.203) data 0.000 (0.001) loss 1.1114 (1.1172) teacher_loss 0.9167 (0.8621) loss_zs_kd 0.4866 (0.4626) loss_oracle 0.1947 (0.2550) acc 75.0000 (77.1307) lr 4.6417e-04 eta 0:13:50
epoch [36/50] batch [240/288] time 0.194 (0.202) data 0.000 (0.001) loss 0.9518 (1.1150) teacher_loss 0.6956 (0.8602) loss_zs_kd 0.4438 (0.4619) loss_oracle 0.2561 (0.2548) acc 81.2500 (77.1745) lr 4.6417e-04 eta 0:13:43
epoch [36/50] batch [260/288] time 0.193 (0.201) data 0.000 (0.001) loss 0.8470 (1.1102) teacher_loss 0.6092 (0.8559) loss_zs_kd 0.4648 (0.4599) loss_oracle 0.2378 (0.2543) acc 81.2500 (77.1995) lr 4.6417e-04 eta 0:13:36
epoch [36/50] batch [280/288] time 0.183 (0.201) data 0.000 (0.001) loss 1.1181 (1.1132) teacher_loss 0.8781 (0.8600) loss_zs_kd 0.4206 (0.4640) loss_oracle 0.2401 (0.2532) acc 81.2500 (77.0312) lr 4.6417e-04 eta 0:13:30
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,466
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 87.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,000
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.1%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [37/50] batch [20/288] time 0.223 (0.213) data 0.000 (0.014) loss 1.1251 (1.1359) teacher_loss 0.8741 (0.8799) loss_zs_kd 0.3692 (0.5041) loss_oracle 0.2510 (0.2561) acc 81.2500 (77.9688) lr 4.1221e-04 eta 0:14:16
epoch [37/50] batch [40/288] time 0.199 (0.204) data 0.000 (0.007) loss 1.1848 (1.1386) teacher_loss 0.8679 (0.8936) loss_zs_kd 0.5298 (0.4727) loss_oracle 0.3169 (0.2450) acc 75.0000 (77.1094) lr 4.1221e-04 eta 0:13:34
epoch [37/50] batch [60/288] time 0.189 (0.201) data 0.001 (0.005) loss 0.9929 (1.1344) teacher_loss 0.7135 (0.8841) loss_zs_kd 0.5361 (0.4528) loss_oracle 0.2795 (0.2503) acc 84.3750 (77.3438) lr 4.1221e-04 eta 0:13:20
epoch [37/50] batch [80/288] time 0.094 (0.194) data 0.000 (0.004) loss 0.8308 (1.1292) teacher_loss 0.5737 (0.8753) loss_zs_kd 0.3363 (0.4597) loss_oracle 0.2571 (0.2538) acc 84.3750 (77.4609) lr 4.1221e-04 eta 0:12:45
epoch [37/50] batch [100/288] time 0.490 (0.215) data 0.000 (0.003) loss 0.9268 (1.1113) teacher_loss 0.6777 (0.8561) loss_zs_kd 0.3858 (0.4618) loss_oracle 0.2492 (0.2552) acc 84.3750 (78.0312) lr 4.1221e-04 eta 0:14:05
epoch [37/50] batch [120/288] time 0.196 (0.212) data 0.000 (0.002) loss 1.5020 (1.1301) teacher_loss 1.2448 (0.8729) loss_zs_kd 0.2827 (0.4682) loss_oracle 0.2572 (0.2572) acc 71.8750 (77.5521) lr 4.1221e-04 eta 0:13:49
epoch [37/50] batch [140/288] time 0.190 (0.209) data 0.000 (0.002) loss 1.1868 (1.1324) teacher_loss 0.9096 (0.8744) loss_zs_kd 0.4039 (0.4729) loss_oracle 0.2772 (0.2580) acc 78.1250 (77.4107) lr 4.1221e-04 eta 0:13:34
epoch [37/50] batch [160/288] time 0.197 (0.207) data 0.000 (0.002) loss 1.1237 (1.1253) teacher_loss 0.8596 (0.8688) loss_zs_kd 0.3749 (0.4753) loss_oracle 0.2641 (0.2565) acc 78.1250 (77.5977) lr 4.1221e-04 eta 0:13:23
epoch [37/50] batch [180/288] time 0.191 (0.206) data 0.000 (0.002) loss 1.1066 (1.1139) teacher_loss 0.8350 (0.8583) loss_zs_kd 0.4964 (0.4738) loss_oracle 0.2716 (0.2557) acc 75.0000 (77.8125) lr 4.1221e-04 eta 0:13:13
epoch [37/50] batch [200/288] time 0.199 (0.205) data 0.000 (0.002) loss 0.9855 (1.1189) teacher_loss 0.6966 (0.8636) loss_zs_kd 0.4304 (0.4760) loss_oracle 0.2889 (0.2553) acc 78.1250 (77.7344) lr 4.1221e-04 eta 0:13:04
epoch [37/50] batch [220/288] time 0.192 (0.204) data 0.000 (0.001) loss 1.3004 (1.1211) teacher_loss 1.0601 (0.8668) loss_zs_kd 0.5379 (0.4737) loss_oracle 0.2403 (0.2543) acc 75.0000 (77.6136) lr 4.1221e-04 eta 0:12:56
epoch [37/50] batch [240/288] time 0.195 (0.203) data 0.000 (0.001) loss 0.8912 (1.1207) teacher_loss 0.6460 (0.8664) loss_zs_kd 0.5153 (0.4721) loss_oracle 0.2452 (0.2543) acc 84.3750 (77.4089) lr 4.1221e-04 eta 0:12:49
epoch [37/50] batch [260/288] time 0.195 (0.202) data 0.000 (0.001) loss 1.2660 (1.1247) teacher_loss 1.0321 (0.8698) loss_zs_kd 0.4987 (0.4723) loss_oracle 0.2339 (0.2549) acc 78.1250 (77.2236) lr 4.1221e-04 eta 0:12:42
epoch [37/50] batch [280/288] time 0.190 (0.202) data 0.000 (0.001) loss 0.8987 (1.1215) teacher_loss 0.5962 (0.8649) loss_zs_kd 0.5073 (0.4740) loss_oracle 0.3025 (0.2567) acc 84.3750 (77.3326) lr 4.1221e-04 eta 0:12:36
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,473
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 87.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,007
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 79.3%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [38/50] batch [20/288] time 0.196 (0.210) data 0.000 (0.014) loss 1.3994 (1.1956) teacher_loss 1.1206 (0.9257) loss_zs_kd 0.3149 (0.4877) loss_oracle 0.2788 (0.2700) acc 68.7500 (76.2500) lr 3.6258e-04 eta 0:13:00
epoch [38/50] batch [40/288] time 0.195 (0.202) data 0.000 (0.007) loss 0.5723 (1.1204) teacher_loss 0.2346 (0.8490) loss_zs_kd 0.7927 (0.4883) loss_oracle 0.3377 (0.2715) acc 96.8750 (77.9688) lr 3.6258e-04 eta 0:12:27
epoch [38/50] batch [60/288] time 0.191 (0.199) data 0.000 (0.005) loss 1.0089 (1.1257) teacher_loss 0.6485 (0.8530) loss_zs_kd 0.4972 (0.4869) loss_oracle 0.3603 (0.2727) acc 84.3750 (77.6042) lr 3.6258e-04 eta 0:12:14
epoch [38/50] batch [80/288] time 0.444 (0.195) data 0.000 (0.004) loss 1.3101 (1.1336) teacher_loss 1.0266 (0.8646) loss_zs_kd 0.3074 (0.4820) loss_oracle 0.2835 (0.2690) acc 78.1250 (77.4609) lr 3.6258e-04 eta 0:11:52
epoch [38/50] batch [100/288] time 0.083 (0.219) data 0.001 (0.003) loss 1.1210 (1.1458) teacher_loss 0.8723 (0.8802) loss_zs_kd 0.4414 (0.4775) loss_oracle 0.2487 (0.2656) acc 78.1250 (77.0312) lr 3.6258e-04 eta 0:13:17
epoch [38/50] batch [120/288] time 0.200 (0.213) data 0.000 (0.002) loss 1.5952 (1.1527) teacher_loss 1.3880 (0.8881) loss_zs_kd 0.4593 (0.4733) loss_oracle 0.2072 (0.2646) acc 65.6250 (76.5885) lr 3.6258e-04 eta 0:12:50
epoch [38/50] batch [140/288] time 0.195 (0.210) data 0.000 (0.002) loss 0.9335 (1.1610) teacher_loss 0.6787 (0.8959) loss_zs_kd 0.4562 (0.4765) loss_oracle 0.2548 (0.2651) acc 87.5000 (76.3839) lr 3.6258e-04 eta 0:12:36
epoch [38/50] batch [160/288] time 0.190 (0.208) data 0.000 (0.002) loss 0.8178 (1.1687) teacher_loss 0.5304 (0.9037) loss_zs_kd 0.3583 (0.4811) loss_oracle 0.2874 (0.2650) acc 87.5000 (76.0742) lr 3.6258e-04 eta 0:12:25
epoch [38/50] batch [180/288] time 0.188 (0.206) data 0.000 (0.002) loss 1.2180 (1.1752) teacher_loss 0.9542 (0.9117) loss_zs_kd 0.5983 (0.4866) loss_oracle 0.2638 (0.2635) acc 81.2500 (75.8681) lr 3.6258e-04 eta 0:12:15
epoch [38/50] batch [200/288] time 0.192 (0.205) data 0.000 (0.002) loss 0.9866 (1.1639) teacher_loss 0.7257 (0.9009) loss_zs_kd 0.3066 (0.4827) loss_oracle 0.2609 (0.2631) acc 71.8750 (76.0469) lr 3.6258e-04 eta 0:12:07
epoch [38/50] batch [220/288] time 0.189 (0.204) data 0.000 (0.001) loss 0.8334 (1.1612) teacher_loss 0.5531 (0.8983) loss_zs_kd 0.4842 (0.4810) loss_oracle 0.2803 (0.2629) acc 81.2500 (76.0369) lr 3.6258e-04 eta 0:11:59
epoch [38/50] batch [240/288] time 0.192 (0.204) data 0.000 (0.001) loss 0.6688 (1.1598) teacher_loss 0.4201 (0.8976) loss_zs_kd 0.4636 (0.4811) loss_oracle 0.2487 (0.2622) acc 87.5000 (76.1719) lr 3.6258e-04 eta 0:11:53
epoch [38/50] batch [260/288] time 0.187 (0.203) data 0.000 (0.001) loss 1.1852 (1.1581) teacher_loss 0.9392 (0.8957) loss_zs_kd 0.3416 (0.4782) loss_oracle 0.2460 (0.2624) acc 81.2500 (76.2139) lr 3.6258e-04 eta 0:11:46
epoch [38/50] batch [280/288] time 0.185 (0.202) data 0.000 (0.001) loss 0.8213 (1.1522) teacher_loss 0.5510 (0.8903) loss_zs_kd 0.3946 (0.4753) loss_oracle 0.2703 (0.2619) acc 87.5000 (76.2388) lr 3.6258e-04 eta 0:11:39
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,467
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 87.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,004
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.2%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [39/50] batch [20/288] time 0.194 (0.210) data 0.000 (0.014) loss 1.0807 (1.0910) teacher_loss 0.8054 (0.8390) loss_zs_kd 0.3520 (0.4332) loss_oracle 0.2753 (0.2520) acc 81.2500 (77.3438) lr 3.1545e-04 eta 0:11:59
epoch [39/50] batch [40/288] time 0.195 (0.202) data 0.000 (0.007) loss 1.3243 (1.1202) teacher_loss 1.1038 (0.8750) loss_zs_kd 0.5502 (0.4616) loss_oracle 0.2205 (0.2451) acc 62.5000 (76.7969) lr 3.1545e-04 eta 0:11:30
epoch [39/50] batch [60/288] time 0.194 (0.200) data 0.000 (0.005) loss 1.0164 (1.1218) teacher_loss 0.7400 (0.8721) loss_zs_kd 0.3385 (0.4573) loss_oracle 0.2764 (0.2497) acc 84.3750 (76.4062) lr 3.1545e-04 eta 0:11:18
epoch [39/50] batch [80/288] time 0.504 (0.202) data 0.000 (0.004) loss 1.4190 (1.1233) teacher_loss 1.1640 (0.8692) loss_zs_kd 0.3580 (0.4563) loss_oracle 0.2550 (0.2541) acc 68.7500 (76.4844) lr 3.1545e-04 eta 0:11:21
epoch [39/50] batch [100/288] time 0.095 (0.220) data 0.000 (0.003) loss 1.2016 (1.1359) teacher_loss 0.9219 (0.8774) loss_zs_kd 0.4272 (0.4589) loss_oracle 0.2797 (0.2585) acc 75.0000 (76.3125) lr 3.1545e-04 eta 0:12:17
epoch [39/50] batch [120/288] time 0.194 (0.216) data 0.000 (0.002) loss 0.8471 (1.1408) teacher_loss 0.6194 (0.8815) loss_zs_kd 0.3987 (0.4585) loss_oracle 0.2277 (0.2593) acc 90.6250 (76.1979) lr 3.1545e-04 eta 0:11:59
epoch [39/50] batch [140/288] time 0.195 (0.213) data 0.000 (0.002) loss 1.4120 (1.1387) teacher_loss 1.1656 (0.8792) loss_zs_kd 0.5355 (0.4699) loss_oracle 0.2464 (0.2595) acc 62.5000 (76.4286) lr 3.1545e-04 eta 0:11:46
epoch [39/50] batch [160/288] time 0.191 (0.211) data 0.000 (0.002) loss 1.2728 (1.1357) teacher_loss 0.9780 (0.8759) loss_zs_kd 0.5333 (0.4717) loss_oracle 0.2949 (0.2598) acc 75.0000 (76.6016) lr 3.1545e-04 eta 0:11:34
epoch [39/50] batch [180/288] time 0.194 (0.209) data 0.000 (0.002) loss 0.9419 (1.1326) teacher_loss 0.7157 (0.8716) loss_zs_kd 0.5508 (0.4689) loss_oracle 0.2263 (0.2610) acc 81.2500 (76.5625) lr 3.1545e-04 eta 0:11:25
epoch [39/50] batch [200/288] time 0.191 (0.207) data 0.000 (0.002) loss 0.9221 (1.1397) teacher_loss 0.6901 (0.8804) loss_zs_kd 0.4922 (0.4699) loss_oracle 0.2320 (0.2594) acc 81.2500 (76.3594) lr 3.1545e-04 eta 0:11:15
epoch [39/50] batch [220/288] time 0.192 (0.206) data 0.000 (0.001) loss 1.3231 (1.1390) teacher_loss 1.0519 (0.8794) loss_zs_kd 0.5164 (0.4679) loss_oracle 0.2712 (0.2596) acc 75.0000 (76.4631) lr 3.1545e-04 eta 0:11:07
epoch [39/50] batch [240/288] time 0.195 (0.205) data 0.000 (0.001) loss 0.9679 (1.1410) teacher_loss 0.7566 (0.8819) loss_zs_kd 0.6146 (0.4736) loss_oracle 0.2113 (0.2591) acc 75.0000 (76.3151) lr 3.1545e-04 eta 0:11:00
epoch [39/50] batch [260/288] time 0.191 (0.204) data 0.000 (0.001) loss 0.7607 (1.1400) teacher_loss 0.4793 (0.8804) loss_zs_kd 0.3366 (0.4702) loss_oracle 0.2814 (0.2596) acc 90.6250 (76.2019) lr 3.1545e-04 eta 0:10:53
epoch [39/50] batch [280/288] time 0.197 (0.204) data 0.000 (0.001) loss 1.0730 (1.1396) teacher_loss 0.8093 (0.8789) loss_zs_kd 0.3286 (0.4686) loss_oracle 0.2637 (0.2607) acc 78.1250 (76.2054) lr 3.1545e-04 eta 0:10:46
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,472
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,004
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.2%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [40/50] batch [20/288] time 0.195 (0.208) data 0.001 (0.015) loss 1.1619 (1.1264) teacher_loss 0.8160 (0.8587) loss_zs_kd 0.3826 (0.4487) loss_oracle 0.3460 (0.2677) acc 75.0000 (75.4688) lr 2.7103e-04 eta 0:10:55
epoch [40/50] batch [40/288] time 0.194 (0.201) data 0.000 (0.007) loss 1.4640 (1.1548) teacher_loss 1.1675 (0.8885) loss_zs_kd 0.6120 (0.4813) loss_oracle 0.2965 (0.2664) acc 71.8750 (74.7656) lr 2.7103e-04 eta 0:10:28
epoch [40/50] batch [60/288] time 0.193 (0.199) data 0.000 (0.005) loss 1.0420 (1.1337) teacher_loss 0.7898 (0.8687) loss_zs_kd 0.4897 (0.4731) loss_oracle 0.2522 (0.2650) acc 78.1250 (75.5729) lr 2.7103e-04 eta 0:10:17
epoch [40/50] batch [80/288] time 0.485 (0.208) data 0.000 (0.004) loss 1.2666 (1.1390) teacher_loss 1.0732 (0.8747) loss_zs_kd 0.5088 (0.4799) loss_oracle 0.1934 (0.2643) acc 68.7500 (75.6641) lr 2.7103e-04 eta 0:10:43
epoch [40/50] batch [100/288] time 0.198 (0.216) data 0.000 (0.003) loss 1.2003 (1.1519) teacher_loss 0.9144 (0.8852) loss_zs_kd 0.3812 (0.4740) loss_oracle 0.2859 (0.2667) acc 81.2500 (75.9062) lr 2.7103e-04 eta 0:11:03
epoch [40/50] batch [120/288] time 0.192 (0.212) data 0.000 (0.003) loss 1.0741 (1.1464) teacher_loss 0.8138 (0.8816) loss_zs_kd 0.5649 (0.4753) loss_oracle 0.2603 (0.2648) acc 78.1250 (75.7552) lr 2.7103e-04 eta 0:10:47
epoch [40/50] batch [140/288] time 0.193 (0.210) data 0.000 (0.002) loss 1.3034 (1.1479) teacher_loss 1.0298 (0.8828) loss_zs_kd 0.3546 (0.4727) loss_oracle 0.2737 (0.2650) acc 68.7500 (75.7812) lr 2.7103e-04 eta 0:10:36
epoch [40/50] batch [160/288] time 0.192 (0.208) data 0.000 (0.002) loss 1.4282 (1.1463) teacher_loss 1.1791 (0.8816) loss_zs_kd 0.6230 (0.4764) loss_oracle 0.2491 (0.2646) acc 78.1250 (76.0352) lr 2.7103e-04 eta 0:10:25
epoch [40/50] batch [180/288] time 0.192 (0.207) data 0.000 (0.002) loss 0.8760 (1.1546) teacher_loss 0.6308 (0.8904) loss_zs_kd 0.4661 (0.4762) loss_oracle 0.2452 (0.2641) acc 84.3750 (75.9896) lr 2.7103e-04 eta 0:10:17
epoch [40/50] batch [200/288] time 0.195 (0.205) data 0.000 (0.002) loss 1.1175 (1.1468) teacher_loss 0.8631 (0.8842) loss_zs_kd 0.5993 (0.4740) loss_oracle 0.2544 (0.2626) acc 68.7500 (76.2031) lr 2.7103e-04 eta 0:10:09
epoch [40/50] batch [220/288] time 0.194 (0.204) data 0.000 (0.002) loss 1.0340 (1.1462) teacher_loss 0.7962 (0.8848) loss_zs_kd 0.2671 (0.4749) loss_oracle 0.2379 (0.2614) acc 78.1250 (76.2926) lr 2.7103e-04 eta 0:10:02
epoch [40/50] batch [240/288] time 0.199 (0.204) data 0.000 (0.001) loss 0.7121 (1.1386) teacher_loss 0.4652 (0.8774) loss_zs_kd 0.5681 (0.4740) loss_oracle 0.2469 (0.2611) acc 84.3750 (76.5234) lr 2.7103e-04 eta 0:09:56
epoch [40/50] batch [260/288] time 0.196 (0.203) data 0.000 (0.001) loss 1.4152 (1.1330) teacher_loss 1.0638 (0.8714) loss_zs_kd 0.5904 (0.4772) loss_oracle 0.3514 (0.2616) acc 68.7500 (76.6707) lr 2.7103e-04 eta 0:09:49
epoch [40/50] batch [280/288] time 0.191 (0.202) data 0.000 (0.001) loss 1.3047 (1.1312) teacher_loss 1.0494 (0.8696) loss_zs_kd 0.5978 (0.4749) loss_oracle 0.2553 (0.2616) acc 78.1250 (76.7969) lr 2.7103e-04 eta 0:09:43
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,470
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,009
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.3%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [41/50] batch [20/288] time 0.195 (0.209) data 0.000 (0.014) loss 0.9507 (1.1192) teacher_loss 0.6781 (0.8695) loss_zs_kd 0.5328 (0.5333) loss_oracle 0.2726 (0.2497) acc 75.0000 (76.7188) lr 2.2949e-04 eta 0:09:58
epoch [41/50] batch [40/288] time 0.195 (0.202) data 0.000 (0.007) loss 0.9350 (1.1200) teacher_loss 0.7227 (0.8726) loss_zs_kd 0.5784 (0.4825) loss_oracle 0.2124 (0.2474) acc 81.2500 (76.4844) lr 2.2949e-04 eta 0:09:32
epoch [41/50] batch [60/288] time 0.198 (0.199) data 0.000 (0.005) loss 1.3677 (1.1390) teacher_loss 1.1510 (0.8889) loss_zs_kd 0.5001 (0.4633) loss_oracle 0.2167 (0.2500) acc 75.0000 (76.6667) lr 2.2949e-04 eta 0:09:21
epoch [41/50] batch [80/288] time 0.549 (0.213) data 0.001 (0.004) loss 0.6630 (1.1369) teacher_loss 0.4266 (0.8845) loss_zs_kd 0.3631 (0.4552) loss_oracle 0.2363 (0.2524) acc 84.3750 (76.7969) lr 2.2949e-04 eta 0:09:57
epoch [41/50] batch [100/288] time 0.099 (0.213) data 0.000 (0.003) loss 1.6370 (1.1381) teacher_loss 1.3406 (0.8867) loss_zs_kd 0.5648 (0.4660) loss_oracle 0.2963 (0.2514) acc 65.6250 (76.5625) lr 2.2949e-04 eta 0:09:51
epoch [41/50] batch [120/288] time 0.200 (0.209) data 0.000 (0.002) loss 1.5076 (1.1345) teacher_loss 1.2847 (0.8809) loss_zs_kd 0.6648 (0.4679) loss_oracle 0.2229 (0.2535) acc 68.7500 (76.7708) lr 2.2949e-04 eta 0:09:37
epoch [41/50] batch [140/288] time 0.200 (0.207) data 0.000 (0.002) loss 1.4795 (1.1298) teacher_loss 1.2151 (0.8758) loss_zs_kd 0.3743 (0.4693) loss_oracle 0.2644 (0.2540) acc 68.7500 (76.8527) lr 2.2949e-04 eta 0:09:27
epoch [41/50] batch [160/288] time 0.189 (0.205) data 0.000 (0.002) loss 0.8726 (1.1259) teacher_loss 0.6152 (0.8684) loss_zs_kd 0.2812 (0.4676) loss_oracle 0.2575 (0.2575) acc 90.6250 (76.8945) lr 2.2949e-04 eta 0:09:18
epoch [41/50] batch [180/288] time 0.199 (0.204) data 0.000 (0.002) loss 1.9178 (1.1275) teacher_loss 1.6541 (0.8685) loss_zs_kd 0.8436 (0.4712) loss_oracle 0.2636 (0.2590) acc 62.5000 (76.9792) lr 2.2949e-04 eta 0:09:11
epoch [41/50] batch [200/288] time 0.198 (0.203) data 0.000 (0.002) loss 1.1983 (1.1291) teacher_loss 0.9484 (0.8684) loss_zs_kd 0.4586 (0.4732) loss_oracle 0.2499 (0.2607) acc 62.5000 (76.9219) lr 2.2949e-04 eta 0:09:04
epoch [41/50] batch [220/288] time 0.198 (0.202) data 0.000 (0.001) loss 1.0786 (1.1285) teacher_loss 0.8782 (0.8670) loss_zs_kd 0.6101 (0.4768) loss_oracle 0.2004 (0.2616) acc 81.2500 (77.0170) lr 2.2949e-04 eta 0:08:58
epoch [41/50] batch [240/288] time 0.201 (0.202) data 0.001 (0.001) loss 1.1294 (1.1289) teacher_loss 0.8818 (0.8670) loss_zs_kd 0.6267 (0.4732) loss_oracle 0.2477 (0.2619) acc 81.2500 (77.0443) lr 2.2949e-04 eta 0:08:52
epoch [41/50] batch [260/288] time 0.197 (0.201) data 0.000 (0.001) loss 1.7590 (1.1321) teacher_loss 1.4737 (0.8694) loss_zs_kd 0.5579 (0.4735) loss_oracle 0.2852 (0.2627) acc 62.5000 (76.9591) lr 2.2949e-04 eta 0:08:47
epoch [41/50] batch [280/288] time 0.188 (0.201) data 0.000 (0.001) loss 0.9847 (1.1324) teacher_loss 0.7876 (0.8702) loss_zs_kd 0.4482 (0.4726) loss_oracle 0.1970 (0.2622) acc 81.2500 (77.0089) lr 2.2949e-04 eta 0:08:42
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,469
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,003
* accuracy: 82.5%
* error: 17.5%
* macro_f1: 79.2%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [42/50] batch [20/288] time 0.197 (0.210) data 0.000 (0.014) loss 0.9609 (1.0806) teacher_loss 0.7058 (0.8318) loss_zs_kd 0.3695 (0.4357) loss_oracle 0.2551 (0.2488) acc 78.1250 (77.3438) lr 1.9098e-04 eta 0:09:00
epoch [42/50] batch [40/288] time 0.192 (0.202) data 0.000 (0.007) loss 1.1208 (1.1122) teacher_loss 0.8980 (0.8638) loss_zs_kd 0.4218 (0.4518) loss_oracle 0.2228 (0.2484) acc 75.0000 (76.9531) lr 1.9098e-04 eta 0:08:35
epoch [42/50] batch [60/288] time 0.190 (0.200) data 0.000 (0.005) loss 1.4365 (1.1303) teacher_loss 1.2487 (0.8814) loss_zs_kd 0.4122 (0.4733) loss_oracle 0.1878 (0.2488) acc 71.8750 (76.9792) lr 1.9098e-04 eta 0:08:25
epoch [42/50] batch [80/288] time 0.472 (0.206) data 0.000 (0.004) loss 0.9017 (1.1211) teacher_loss 0.6376 (0.8707) loss_zs_kd 0.5702 (0.4758) loss_oracle 0.2642 (0.2504) acc 81.2500 (76.7969) lr 1.9098e-04 eta 0:08:38
epoch [42/50] batch [100/288] time 0.160 (0.216) data 0.000 (0.003) loss 1.0995 (1.1380) teacher_loss 0.8395 (0.8834) loss_zs_kd 0.5277 (0.4792) loss_oracle 0.2600 (0.2546) acc 78.1250 (76.7500) lr 1.9098e-04 eta 0:08:58
epoch [42/50] batch [120/288] time 0.195 (0.212) data 0.000 (0.003) loss 0.8411 (1.1145) teacher_loss 0.5834 (0.8596) loss_zs_kd 0.5198 (0.4696) loss_oracle 0.2577 (0.2549) acc 78.1250 (77.1094) lr 1.9098e-04 eta 0:08:45
epoch [42/50] batch [140/288] time 0.188 (0.210) data 0.000 (0.002) loss 1.3949 (1.1099) teacher_loss 1.1114 (0.8530) loss_zs_kd 0.4274 (0.4765) loss_oracle 0.2834 (0.2569) acc 62.5000 (77.1429) lr 1.9098e-04 eta 0:08:34
epoch [42/50] batch [160/288] time 0.227 (0.208) data 0.000 (0.002) loss 1.2882 (1.1156) teacher_loss 1.0278 (0.8563) loss_zs_kd 0.5798 (0.4798) loss_oracle 0.2604 (0.2594) acc 71.8750 (76.9531) lr 1.9098e-04 eta 0:08:26
epoch [42/50] batch [180/288] time 0.197 (0.206) data 0.000 (0.002) loss 1.1357 (1.1139) teacher_loss 0.8994 (0.8530) loss_zs_kd 0.6136 (0.4803) loss_oracle 0.2363 (0.2609) acc 81.2500 (77.1007) lr 1.9098e-04 eta 0:08:17
epoch [42/50] batch [200/288] time 0.193 (0.205) data 0.000 (0.002) loss 1.1323 (1.1148) teacher_loss 0.8485 (0.8538) loss_zs_kd 0.3983 (0.4791) loss_oracle 0.2838 (0.2610) acc 78.1250 (77.2500) lr 1.9098e-04 eta 0:08:10
epoch [42/50] batch [220/288] time 0.201 (0.204) data 0.000 (0.001) loss 1.0786 (1.1193) teacher_loss 0.8333 (0.8577) loss_zs_kd 0.4115 (0.4792) loss_oracle 0.2453 (0.2617) acc 81.2500 (77.1591) lr 1.9098e-04 eta 0:08:04
epoch [42/50] batch [240/288] time 0.192 (0.203) data 0.000 (0.001) loss 1.1301 (1.1178) teacher_loss 0.8417 (0.8565) loss_zs_kd 0.5056 (0.4767) loss_oracle 0.2884 (0.2614) acc 78.1250 (77.2135) lr 1.9098e-04 eta 0:07:57
epoch [42/50] batch [260/288] time 0.191 (0.203) data 0.000 (0.001) loss 1.4348 (1.1245) teacher_loss 1.1565 (0.8631) loss_zs_kd 0.6187 (0.4766) loss_oracle 0.2782 (0.2614) acc 75.0000 (77.0072) lr 1.9098e-04 eta 0:07:52
epoch [42/50] batch [280/288] time 0.197 (0.202) data 0.000 (0.001) loss 1.0314 (1.1207) teacher_loss 0.7916 (0.8583) loss_zs_kd 0.3367 (0.4739) loss_oracle 0.2398 (0.2624) acc 84.3750 (77.1205) lr 1.9098e-04 eta 0:07:46
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,463
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 87.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,004
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.2%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [43/50] batch [20/288] time 0.198 (0.209) data 0.000 (0.014) loss 1.5264 (1.1953) teacher_loss 1.2485 (0.9341) loss_zs_kd 0.4406 (0.4790) loss_oracle 0.2779 (0.2612) acc 68.7500 (76.2500) lr 1.5567e-04 eta 0:07:56
epoch [43/50] batch [40/288] time 0.192 (0.201) data 0.000 (0.007) loss 1.2562 (1.1652) teacher_loss 0.9995 (0.9065) loss_zs_kd 0.4582 (0.4789) loss_oracle 0.2567 (0.2587) acc 65.6250 (76.2500) lr 1.5567e-04 eta 0:07:35
epoch [43/50] batch [60/288] time 0.191 (0.199) data 0.000 (0.005) loss 1.9212 (1.1895) teacher_loss 1.6638 (0.9265) loss_zs_kd 0.5019 (0.4926) loss_oracle 0.2574 (0.2630) acc 59.3750 (75.4688) lr 1.5567e-04 eta 0:07:26
epoch [43/50] batch [80/288] time 0.469 (0.208) data 0.000 (0.004) loss 0.7984 (1.1389) teacher_loss 0.5197 (0.8752) loss_zs_kd 0.2343 (0.4681) loss_oracle 0.2787 (0.2637) acc 90.6250 (76.7969) lr 1.5567e-04 eta 0:07:41
epoch [43/50] batch [100/288] time 0.197 (0.214) data 0.000 (0.003) loss 1.4595 (1.1424) teacher_loss 1.2203 (0.8789) loss_zs_kd 0.5821 (0.4607) loss_oracle 0.2392 (0.2634) acc 71.8750 (76.4375) lr 1.5567e-04 eta 0:07:52
epoch [43/50] batch [120/288] time 0.200 (0.211) data 0.000 (0.003) loss 0.9873 (1.1392) teacher_loss 0.7379 (0.8759) loss_zs_kd 0.3905 (0.4678) loss_oracle 0.2493 (0.2634) acc 71.8750 (76.6927) lr 1.5567e-04 eta 0:07:41
epoch [43/50] batch [140/288] time 0.197 (0.209) data 0.000 (0.002) loss 0.8178 (1.1368) teacher_loss 0.5984 (0.8735) loss_zs_kd 0.5056 (0.4717) loss_oracle 0.2195 (0.2633) acc 81.2500 (76.8750) lr 1.5567e-04 eta 0:07:31
epoch [43/50] batch [160/288] time 0.194 (0.207) data 0.000 (0.002) loss 1.1197 (1.1305) teacher_loss 0.8148 (0.8680) loss_zs_kd 0.4141 (0.4741) loss_oracle 0.3048 (0.2625) acc 71.8750 (76.8164) lr 1.5567e-04 eta 0:07:23
epoch [43/50] batch [180/288] time 0.195 (0.205) data 0.000 (0.002) loss 0.9632 (1.1204) teacher_loss 0.7534 (0.8595) loss_zs_kd 0.3533 (0.4701) loss_oracle 0.2098 (0.2609) acc 84.3750 (77.0312) lr 1.5567e-04 eta 0:07:16
epoch [43/50] batch [200/288] time 0.195 (0.204) data 0.000 (0.002) loss 0.9683 (1.1219) teacher_loss 0.7760 (0.8624) loss_zs_kd 0.5544 (0.4691) loss_oracle 0.1922 (0.2594) acc 84.3750 (76.8750) lr 1.5567e-04 eta 0:07:09
epoch [43/50] batch [220/288] time 0.194 (0.203) data 0.000 (0.001) loss 1.1104 (1.1252) teacher_loss 0.8369 (0.8643) loss_zs_kd 0.5348 (0.4708) loss_oracle 0.2735 (0.2609) acc 75.0000 (77.0597) lr 1.5567e-04 eta 0:07:04
epoch [43/50] batch [240/288] time 0.192 (0.203) data 0.000 (0.001) loss 1.0539 (1.1211) teacher_loss 0.7741 (0.8588) loss_zs_kd 0.7224 (0.4708) loss_oracle 0.2798 (0.2624) acc 81.2500 (77.0573) lr 1.5567e-04 eta 0:06:58
epoch [43/50] batch [260/288] time 0.196 (0.202) data 0.000 (0.001) loss 1.1335 (1.1207) teacher_loss 0.8497 (0.8590) loss_zs_kd 0.5330 (0.4724) loss_oracle 0.2838 (0.2617) acc 75.0000 (77.0553) lr 1.5567e-04 eta 0:06:52
epoch [43/50] batch [280/288] time 0.193 (0.201) data 0.000 (0.001) loss 1.0402 (1.1238) teacher_loss 0.7839 (0.8611) loss_zs_kd 0.4952 (0.4728) loss_oracle 0.2563 (0.2627) acc 84.3750 (77.0424) lr 1.5567e-04 eta 0:06:47
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,464
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 87.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,013
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.5%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [44/50] batch [20/288] time 0.196 (0.209) data 0.000 (0.014) loss 1.2771 (1.0962) teacher_loss 1.0868 (0.8462) loss_zs_kd 0.4418 (0.4813) loss_oracle 0.1903 (0.2500) acc 68.7500 (76.4062) lr 1.2369e-04 eta 0:06:57
epoch [44/50] batch [40/288] time 0.194 (0.203) data 0.000 (0.007) loss 0.9506 (1.1463) teacher_loss 0.6666 (0.8966) loss_zs_kd 0.4758 (0.4795) loss_oracle 0.2840 (0.2496) acc 81.2500 (76.0156) lr 1.2369e-04 eta 0:06:42
epoch [44/50] batch [60/288] time 0.197 (0.200) data 0.001 (0.005) loss 0.9807 (1.1294) teacher_loss 0.6822 (0.8730) loss_zs_kd 0.3214 (0.4906) loss_oracle 0.2985 (0.2564) acc 81.2500 (76.7188) lr 1.2369e-04 eta 0:06:31
epoch [44/50] batch [80/288] time 0.469 (0.204) data 0.000 (0.004) loss 1.3818 (1.1321) teacher_loss 1.0828 (0.8750) loss_zs_kd 0.6438 (0.4867) loss_oracle 0.2990 (0.2570) acc 75.0000 (76.6797) lr 1.2369e-04 eta 0:06:34
epoch [44/50] batch [100/288] time 0.084 (0.215) data 0.000 (0.003) loss 1.2158 (1.1256) teacher_loss 0.9994 (0.8694) loss_zs_kd 0.4513 (0.4833) loss_oracle 0.2164 (0.2561) acc 75.0000 (76.8750) lr 1.2369e-04 eta 0:06:52
epoch [44/50] batch [120/288] time 0.185 (0.210) data 0.000 (0.003) loss 1.2407 (1.1234) teacher_loss 1.0085 (0.8654) loss_zs_kd 0.6019 (0.4804) loss_oracle 0.2322 (0.2580) acc 71.8750 (77.2396) lr 1.2369e-04 eta 0:06:38
epoch [44/50] batch [140/288] time 0.192 (0.208) data 0.000 (0.002) loss 1.1023 (1.1251) teacher_loss 0.8617 (0.8667) loss_zs_kd 0.5022 (0.4798) loss_oracle 0.2406 (0.2583) acc 84.3750 (77.2768) lr 1.2369e-04 eta 0:06:29
epoch [44/50] batch [160/288] time 0.191 (0.206) data 0.000 (0.002) loss 1.3325 (1.1284) teacher_loss 1.0710 (0.8711) loss_zs_kd 0.4940 (0.4825) loss_oracle 0.2614 (0.2572) acc 68.7500 (76.8945) lr 1.2369e-04 eta 0:06:22
epoch [44/50] batch [180/288] time 0.193 (0.205) data 0.000 (0.002) loss 0.9795 (1.1347) teacher_loss 0.6887 (0.8765) loss_zs_kd 0.4523 (0.4832) loss_oracle 0.2908 (0.2582) acc 78.1250 (76.7535) lr 1.2369e-04 eta 0:06:15
epoch [44/50] batch [200/288] time 0.192 (0.204) data 0.000 (0.002) loss 0.9544 (1.1353) teacher_loss 0.7176 (0.8765) loss_zs_kd 0.4624 (0.4880) loss_oracle 0.2367 (0.2588) acc 81.2500 (76.8438) lr 1.2369e-04 eta 0:06:09
epoch [44/50] batch [220/288] time 0.191 (0.203) data 0.000 (0.001) loss 1.0803 (1.1396) teacher_loss 0.8337 (0.8802) loss_zs_kd 0.5045 (0.4866) loss_oracle 0.2467 (0.2594) acc 78.1250 (76.7614) lr 1.2369e-04 eta 0:06:03
epoch [44/50] batch [240/288] time 0.193 (0.202) data 0.000 (0.001) loss 1.0932 (1.1533) teacher_loss 0.8634 (0.8932) loss_zs_kd 0.4556 (0.4859) loss_oracle 0.2298 (0.2601) acc 75.0000 (76.4453) lr 1.2369e-04 eta 0:05:58
epoch [44/50] batch [260/288] time 0.192 (0.201) data 0.000 (0.001) loss 1.6465 (1.1500) teacher_loss 1.3290 (0.8895) loss_zs_kd 0.4151 (0.4831) loss_oracle 0.3175 (0.2604) acc 68.7500 (76.4183) lr 1.2369e-04 eta 0:05:53
epoch [44/50] batch [280/288] time 0.196 (0.201) data 0.000 (0.001) loss 1.5525 (1.1480) teacher_loss 1.2144 (0.8874) loss_zs_kd 0.3906 (0.4845) loss_oracle 0.3380 (0.2605) acc 65.6250 (76.3393) lr 1.2369e-04 eta 0:05:49
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,471
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,006
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 79.3%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [45/50] batch [20/288] time 0.190 (0.208) data 0.000 (0.013) loss 1.4168 (1.1321) teacher_loss 1.1734 (0.8758) loss_zs_kd 0.3097 (0.4861) loss_oracle 0.2434 (0.2563) acc 62.5000 (78.4375) lr 9.5173e-05 eta 0:05:55
epoch [45/50] batch [40/288] time 0.194 (0.201) data 0.000 (0.007) loss 0.8897 (1.0761) teacher_loss 0.6716 (0.8173) loss_zs_kd 0.2514 (0.4786) loss_oracle 0.2180 (0.2588) acc 78.1250 (78.8281) lr 9.5173e-05 eta 0:05:39
epoch [45/50] batch [60/288] time 0.191 (0.198) data 0.000 (0.005) loss 0.7418 (1.0930) teacher_loss 0.4723 (0.8337) loss_zs_kd 0.5035 (0.4659) loss_oracle 0.2695 (0.2593) acc 87.5000 (78.2812) lr 9.5173e-05 eta 0:05:30
epoch [45/50] batch [80/288] time 0.471 (0.198) data 0.000 (0.004) loss 0.8300 (1.1039) teacher_loss 0.5939 (0.8439) loss_zs_kd 0.3819 (0.4598) loss_oracle 0.2361 (0.2600) acc 84.3750 (77.6953) lr 9.5173e-05 eta 0:05:26
epoch [45/50] batch [100/288] time 0.086 (0.215) data 0.000 (0.003) loss 1.6677 (1.0939) teacher_loss 1.4010 (0.8359) loss_zs_kd 0.5090 (0.4623) loss_oracle 0.2667 (0.2580) acc 56.2500 (77.9375) lr 9.5173e-05 eta 0:05:49
epoch [45/50] batch [120/288] time 0.168 (0.209) data 0.000 (0.002) loss 1.0402 (1.1003) teacher_loss 0.7907 (0.8438) loss_zs_kd 0.3845 (0.4657) loss_oracle 0.2494 (0.2565) acc 84.3750 (77.6302) lr 9.5173e-05 eta 0:05:36
epoch [45/50] batch [140/288] time 0.193 (0.207) data 0.000 (0.002) loss 0.4806 (1.1066) teacher_loss 0.2170 (0.8470) loss_zs_kd 0.4778 (0.4657) loss_oracle 0.2636 (0.2596) acc 93.7500 (77.3884) lr 9.5173e-05 eta 0:05:28
epoch [45/50] batch [160/288] time 0.199 (0.205) data 0.000 (0.002) loss 0.9967 (1.1087) teacher_loss 0.7684 (0.8496) loss_zs_kd 0.5549 (0.4709) loss_oracle 0.2283 (0.2591) acc 78.1250 (77.3438) lr 9.5173e-05 eta 0:05:22
epoch [45/50] batch [180/288] time 0.193 (0.204) data 0.000 (0.002) loss 1.0856 (1.1109) teacher_loss 0.8459 (0.8521) loss_zs_kd 0.6667 (0.4721) loss_oracle 0.2397 (0.2588) acc 78.1250 (77.1354) lr 9.5173e-05 eta 0:05:15
epoch [45/50] batch [200/288] time 0.190 (0.203) data 0.000 (0.002) loss 0.7070 (1.1150) teacher_loss 0.4312 (0.8565) loss_zs_kd 0.3376 (0.4724) loss_oracle 0.2758 (0.2585) acc 90.6250 (77.1875) lr 9.5173e-05 eta 0:05:10
epoch [45/50] batch [220/288] time 0.193 (0.202) data 0.000 (0.001) loss 0.9058 (1.1100) teacher_loss 0.6309 (0.8521) loss_zs_kd 0.6208 (0.4727) loss_oracle 0.2749 (0.2579) acc 84.3750 (77.4148) lr 9.5173e-05 eta 0:05:04
epoch [45/50] batch [240/288] time 0.190 (0.202) data 0.000 (0.001) loss 1.0058 (1.1015) teacher_loss 0.7649 (0.8430) loss_zs_kd 0.4422 (0.4710) loss_oracle 0.2409 (0.2585) acc 78.1250 (77.7083) lr 9.5173e-05 eta 0:04:59
epoch [45/50] batch [260/288] time 0.196 (0.201) data 0.000 (0.001) loss 1.3403 (1.0997) teacher_loss 1.0940 (0.8414) loss_zs_kd 0.6239 (0.4735) loss_oracle 0.2463 (0.2584) acc 75.0000 (77.7404) lr 9.5173e-05 eta 0:04:55
epoch [45/50] batch [280/288] time 0.197 (0.200) data 0.000 (0.001) loss 1.2123 (1.1125) teacher_loss 0.8965 (0.8530) loss_zs_kd 0.5303 (0.4758) loss_oracle 0.3158 (0.2595) acc 75.0000 (77.4219) lr 9.5173e-05 eta 0:04:50
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,471
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,002
* accuracy: 82.5%
* error: 17.5%
* macro_f1: 79.2%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [46/50] batch [20/288] time 0.194 (0.212) data 0.000 (0.016) loss 1.0512 (1.1258) teacher_loss 0.7535 (0.8568) loss_zs_kd 0.4342 (0.4697) loss_oracle 0.2977 (0.2690) acc 78.1250 (78.1250) lr 7.0224e-05 eta 0:05:01
epoch [46/50] batch [40/288] time 0.192 (0.203) data 0.000 (0.008) loss 1.4338 (1.1583) teacher_loss 1.1117 (0.8909) loss_zs_kd 0.4620 (0.4854) loss_oracle 0.3221 (0.2674) acc 75.0000 (78.5938) lr 7.0224e-05 eta 0:04:44
epoch [46/50] batch [60/288] time 0.194 (0.201) data 0.001 (0.005) loss 1.0366 (1.1556) teacher_loss 0.7911 (0.8892) loss_zs_kd 0.4903 (0.5033) loss_oracle 0.2455 (0.2665) acc 84.3750 (77.3958) lr 7.0224e-05 eta 0:04:37
epoch [46/50] batch [80/288] time 0.469 (0.196) data 0.000 (0.004) loss 0.9599 (1.1351) teacher_loss 0.6999 (0.8679) loss_zs_kd 0.3906 (0.4904) loss_oracle 0.2600 (0.2672) acc 84.3750 (77.5391) lr 7.0224e-05 eta 0:04:26
epoch [46/50] batch [100/288] time 0.091 (0.217) data 0.000 (0.003) loss 0.8488 (1.1274) teacher_loss 0.5935 (0.8617) loss_zs_kd 0.5908 (0.4886) loss_oracle 0.2553 (0.2658) acc 81.2500 (77.5938) lr 7.0224e-05 eta 0:04:50
epoch [46/50] batch [120/288] time 0.198 (0.213) data 0.000 (0.003) loss 0.7524 (1.1311) teacher_loss 0.4769 (0.8645) loss_zs_kd 0.4538 (0.4885) loss_oracle 0.2755 (0.2666) acc 84.3750 (77.3698) lr 7.0224e-05 eta 0:04:41
epoch [46/50] batch [140/288] time 0.194 (0.210) data 0.000 (0.002) loss 1.0303 (1.1307) teacher_loss 0.7706 (0.8651) loss_zs_kd 0.4363 (0.4834) loss_oracle 0.2597 (0.2657) acc 75.0000 (77.1205) lr 7.0224e-05 eta 0:04:33
epoch [46/50] batch [160/288] time 0.198 (0.208) data 0.000 (0.002) loss 1.0365 (1.1242) teacher_loss 0.7871 (0.8582) loss_zs_kd 0.5814 (0.4817) loss_oracle 0.2494 (0.2659) acc 81.2500 (77.1875) lr 7.0224e-05 eta 0:04:26
epoch [46/50] batch [180/288] time 0.208 (0.207) data 0.000 (0.002) loss 1.1728 (1.1301) teacher_loss 0.9454 (0.8639) loss_zs_kd 0.5913 (0.4827) loss_oracle 0.2274 (0.2662) acc 78.1250 (77.1007) lr 7.0224e-05 eta 0:04:20
epoch [46/50] batch [200/288] time 0.193 (0.205) data 0.000 (0.002) loss 1.4045 (1.1393) teacher_loss 1.1563 (0.8735) loss_zs_kd 0.3201 (0.4812) loss_oracle 0.2482 (0.2658) acc 71.8750 (76.8750) lr 7.0224e-05 eta 0:04:14
epoch [46/50] batch [220/288] time 0.194 (0.204) data 0.000 (0.002) loss 1.0932 (1.1413) teacher_loss 0.8638 (0.8764) loss_zs_kd 0.3868 (0.4784) loss_oracle 0.2293 (0.2649) acc 75.0000 (76.7472) lr 7.0224e-05 eta 0:04:09
epoch [46/50] batch [240/288] time 0.191 (0.204) data 0.000 (0.002) loss 1.1636 (1.1429) teacher_loss 0.8625 (0.8776) loss_zs_kd 0.4632 (0.4808) loss_oracle 0.3011 (0.2653) acc 71.8750 (76.6927) lr 7.0224e-05 eta 0:04:04
epoch [46/50] batch [260/288] time 0.195 (0.203) data 0.000 (0.001) loss 1.0090 (1.1380) teacher_loss 0.7689 (0.8723) loss_zs_kd 0.3617 (0.4817) loss_oracle 0.2400 (0.2657) acc 87.5000 (76.7909) lr 7.0224e-05 eta 0:03:59
epoch [46/50] batch [280/288] time 0.170 (0.202) data 0.000 (0.001) loss 1.5340 (1.1405) teacher_loss 1.2369 (0.8749) loss_zs_kd 0.4704 (0.4825) loss_oracle 0.2972 (0.2655) acc 65.6250 (76.8080) lr 7.0224e-05 eta 0:03:54
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,472
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,001
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.1%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [47/50] batch [20/288] time 0.196 (0.212) data 0.000 (0.015) loss 1.3651 (1.1537) teacher_loss 1.1102 (0.8884) loss_zs_kd 0.4113 (0.4880) loss_oracle 0.2549 (0.2653) acc 62.5000 (76.2500) lr 4.8943e-05 eta 0:04:00
epoch [47/50] batch [40/288] time 0.195 (0.204) data 0.000 (0.007) loss 1.3126 (1.1568) teacher_loss 1.0699 (0.8911) loss_zs_kd 0.3756 (0.4576) loss_oracle 0.2427 (0.2657) acc 75.0000 (76.0938) lr 4.8943e-05 eta 0:03:46
epoch [47/50] batch [60/288] time 0.191 (0.200) data 0.000 (0.005) loss 1.8780 (1.1606) teacher_loss 1.5947 (0.8940) loss_zs_kd 0.4211 (0.4578) loss_oracle 0.2833 (0.2666) acc 65.6250 (76.4062) lr 4.8943e-05 eta 0:03:38
epoch [47/50] batch [80/288] time 0.440 (0.193) data 0.000 (0.004) loss 1.1810 (1.1532) teacher_loss 0.9178 (0.8887) loss_zs_kd 0.3831 (0.4665) loss_oracle 0.2632 (0.2644) acc 78.1250 (76.6797) lr 4.8943e-05 eta 0:03:26
epoch [47/50] batch [100/288] time 0.089 (0.217) data 0.000 (0.003) loss 0.9434 (1.1348) teacher_loss 0.6518 (0.8721) loss_zs_kd 0.3324 (0.4714) loss_oracle 0.2915 (0.2627) acc 78.1250 (76.5625) lr 4.8943e-05 eta 0:03:48
epoch [47/50] batch [120/288] time 0.191 (0.213) data 0.000 (0.003) loss 1.5179 (1.1226) teacher_loss 1.2489 (0.8588) loss_zs_kd 0.3792 (0.4755) loss_oracle 0.2690 (0.2638) acc 68.7500 (76.7969) lr 4.8943e-05 eta 0:03:40
epoch [47/50] batch [140/288] time 0.199 (0.210) data 0.000 (0.002) loss 0.9089 (1.1239) teacher_loss 0.6922 (0.8610) loss_zs_kd 0.6719 (0.4801) loss_oracle 0.2167 (0.2629) acc 75.0000 (76.8304) lr 4.8943e-05 eta 0:03:32
epoch [47/50] batch [160/288] time 0.188 (0.208) data 0.000 (0.002) loss 1.0266 (1.1296) teacher_loss 0.7214 (0.8671) loss_zs_kd 0.8768 (0.4867) loss_oracle 0.3052 (0.2625) acc 87.5000 (76.7383) lr 4.8943e-05 eta 0:03:26
epoch [47/50] batch [180/288] time 0.192 (0.207) data 0.000 (0.002) loss 1.0889 (1.1295) teacher_loss 0.8546 (0.8661) loss_zs_kd 0.4893 (0.4881) loss_oracle 0.2343 (0.2634) acc 71.8750 (76.5625) lr 4.8943e-05 eta 0:03:20
epoch [47/50] batch [200/288] time 0.194 (0.205) data 0.000 (0.002) loss 0.5718 (1.1298) teacher_loss 0.3295 (0.8662) loss_zs_kd 0.3199 (0.4826) loss_oracle 0.2423 (0.2636) acc 87.5000 (76.6250) lr 4.8943e-05 eta 0:03:15
epoch [47/50] batch [220/288] time 0.190 (0.204) data 0.000 (0.002) loss 0.9405 (1.1323) teacher_loss 0.6988 (0.8685) loss_zs_kd 0.4846 (0.4835) loss_oracle 0.2417 (0.2638) acc 75.0000 (76.6051) lr 4.8943e-05 eta 0:03:10
epoch [47/50] batch [240/288] time 0.198 (0.204) data 0.000 (0.001) loss 0.7415 (1.1307) teacher_loss 0.4969 (0.8665) loss_zs_kd 0.4086 (0.4834) loss_oracle 0.2446 (0.2642) acc 87.5000 (76.7969) lr 4.8943e-05 eta 0:03:05
epoch [47/50] batch [260/288] time 0.191 (0.203) data 0.000 (0.001) loss 1.5756 (1.1314) teacher_loss 1.2913 (0.8679) loss_zs_kd 0.6764 (0.4844) loss_oracle 0.2843 (0.2635) acc 71.8750 (76.7067) lr 4.8943e-05 eta 0:03:01
epoch [47/50] batch [280/288] time 0.194 (0.202) data 0.000 (0.001) loss 0.7142 (1.1344) teacher_loss 0.4114 (0.8711) loss_zs_kd 0.4713 (0.4853) loss_oracle 0.3028 (0.2633) acc 93.7500 (76.6518) lr 4.8943e-05 eta 0:02:56
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,470
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,003
* accuracy: 82.5%
* error: 17.5%
* macro_f1: 79.2%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [48/50] batch [20/288] time 0.192 (0.209) data 0.000 (0.014) loss 1.2384 (1.1866) teacher_loss 0.9956 (0.9264) loss_zs_kd 0.4339 (0.5031) loss_oracle 0.2429 (0.2602) acc 71.8750 (75.4688) lr 3.1417e-05 eta 0:02:56
epoch [48/50] batch [40/288] time 0.197 (0.201) data 0.000 (0.007) loss 1.4241 (1.1702) teacher_loss 1.1226 (0.9057) loss_zs_kd 0.5986 (0.4914) loss_oracle 0.3014 (0.2646) acc 75.0000 (76.6406) lr 3.1417e-05 eta 0:02:45
epoch [48/50] batch [60/288] time 0.194 (0.199) data 0.001 (0.005) loss 1.2174 (1.1353) teacher_loss 0.9202 (0.8725) loss_zs_kd 0.4959 (0.5152) loss_oracle 0.2972 (0.2628) acc 75.0000 (77.3958) lr 3.1417e-05 eta 0:02:39
epoch [48/50] batch [80/288] time 0.467 (0.196) data 0.000 (0.004) loss 1.3413 (1.1373) teacher_loss 1.0502 (0.8759) loss_zs_kd 0.4646 (0.5018) loss_oracle 0.2910 (0.2614) acc 78.1250 (77.1484) lr 3.1417e-05 eta 0:02:33
epoch [48/50] batch [100/288] time 0.097 (0.216) data 0.000 (0.003) loss 0.9768 (1.1288) teacher_loss 0.6809 (0.8665) loss_zs_kd 0.3533 (0.4944) loss_oracle 0.2959 (0.2624) acc 84.3750 (77.3438) lr 3.1417e-05 eta 0:02:45
epoch [48/50] batch [120/288] time 0.198 (0.212) data 0.000 (0.002) loss 1.1247 (1.1341) teacher_loss 0.8941 (0.8681) loss_zs_kd 0.6530 (0.4894) loss_oracle 0.2305 (0.2660) acc 75.0000 (77.3698) lr 3.1417e-05 eta 0:02:37
epoch [48/50] batch [140/288] time 0.192 (0.209) data 0.000 (0.002) loss 1.1112 (1.1367) teacher_loss 0.8260 (0.8699) loss_zs_kd 0.5307 (0.4885) loss_oracle 0.2852 (0.2668) acc 81.2500 (77.1875) lr 3.1417e-05 eta 0:02:31
epoch [48/50] batch [160/288] time 0.194 (0.207) data 0.000 (0.002) loss 1.0892 (1.1470) teacher_loss 0.8419 (0.8805) loss_zs_kd 0.3769 (0.4891) loss_oracle 0.2473 (0.2665) acc 75.0000 (76.7188) lr 3.1417e-05 eta 0:02:25
epoch [48/50] batch [180/288] time 0.195 (0.206) data 0.000 (0.002) loss 0.9668 (1.1368) teacher_loss 0.7176 (0.8707) loss_zs_kd 0.4593 (0.4917) loss_oracle 0.2492 (0.2660) acc 84.3750 (76.9618) lr 3.1417e-05 eta 0:02:20
epoch [48/50] batch [200/288] time 0.186 (0.204) data 0.000 (0.002) loss 0.8753 (1.1333) teacher_loss 0.6186 (0.8677) loss_zs_kd 0.5057 (0.4908) loss_oracle 0.2567 (0.2657) acc 84.3750 (77.0156) lr 3.1417e-05 eta 0:02:15
epoch [48/50] batch [220/288] time 0.208 (0.203) data 0.000 (0.001) loss 0.9771 (1.1303) teacher_loss 0.7017 (0.8656) loss_zs_kd 0.5456 (0.4910) loss_oracle 0.2753 (0.2647) acc 81.2500 (77.1875) lr 3.1417e-05 eta 0:02:11
epoch [48/50] batch [240/288] time 0.208 (0.203) data 0.000 (0.001) loss 1.2015 (1.1296) teacher_loss 0.9245 (0.8650) loss_zs_kd 0.4410 (0.4922) loss_oracle 0.2770 (0.2646) acc 81.2500 (77.1875) lr 3.1417e-05 eta 0:02:06
epoch [48/50] batch [260/288] time 0.190 (0.202) data 0.000 (0.001) loss 1.1639 (1.1272) teacher_loss 0.9229 (0.8640) loss_zs_kd 0.4311 (0.4899) loss_oracle 0.2410 (0.2632) acc 78.1250 (77.1875) lr 3.1417e-05 eta 0:02:01
epoch [48/50] batch [280/288] time 0.195 (0.201) data 0.000 (0.001) loss 1.0113 (1.1244) teacher_loss 0.7709 (0.8617) loss_zs_kd 0.4875 (0.4896) loss_oracle 0.2404 (0.2627) acc 81.2500 (77.1205) lr 3.1417e-05 eta 0:01:57
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,470
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,004
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.3%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [49/50] batch [20/288] time 0.196 (0.211) data 0.000 (0.016) loss 1.1470 (1.0796) teacher_loss 0.8642 (0.8160) loss_zs_kd 0.4408 (0.4836) loss_oracle 0.2828 (0.2636) acc 75.0000 (77.5000) lr 1.7713e-05 eta 0:01:57
epoch [49/50] batch [40/288] time 0.192 (0.203) data 0.000 (0.008) loss 1.5730 (1.1060) teacher_loss 1.2767 (0.8420) loss_zs_kd 0.4925 (0.4766) loss_oracle 0.2964 (0.2640) acc 62.5000 (77.4219) lr 1.7713e-05 eta 0:01:48
epoch [49/50] batch [60/288] time 0.197 (0.200) data 0.000 (0.005) loss 0.7122 (1.1000) teacher_loss 0.5011 (0.8390) loss_zs_kd 0.4034 (0.4846) loss_oracle 0.2111 (0.2610) acc 84.3750 (77.6562) lr 1.7713e-05 eta 0:01:43
epoch [49/50] batch [80/288] time 0.096 (0.190) data 0.000 (0.004) loss 1.2787 (1.0933) teacher_loss 0.9819 (0.8316) loss_zs_kd 0.4523 (0.4855) loss_oracle 0.2968 (0.2617) acc 75.0000 (77.9688) lr 1.7713e-05 eta 0:01:34
epoch [49/50] batch [100/288] time 0.377 (0.219) data 0.000 (0.003) loss 0.7697 (1.1050) teacher_loss 0.5361 (0.8457) loss_zs_kd 0.4241 (0.4918) loss_oracle 0.2335 (0.2593) acc 90.6250 (77.5625) lr 1.7713e-05 eta 0:01:44
epoch [49/50] batch [120/288] time 0.193 (0.212) data 0.000 (0.003) loss 1.3294 (1.1114) teacher_loss 1.0998 (0.8513) loss_zs_kd 0.4680 (0.4879) loss_oracle 0.2297 (0.2601) acc 78.1250 (77.3698) lr 1.7713e-05 eta 0:01:36
epoch [49/50] batch [140/288] time 0.201 (0.209) data 0.000 (0.002) loss 1.2939 (1.1096) teacher_loss 1.0930 (0.8498) loss_zs_kd 0.5892 (0.4908) loss_oracle 0.2009 (0.2598) acc 68.7500 (77.3884) lr 1.7713e-05 eta 0:01:31
epoch [49/50] batch [160/288] time 0.194 (0.207) data 0.000 (0.002) loss 0.9140 (1.1048) teacher_loss 0.7078 (0.8445) loss_zs_kd 0.4971 (0.4872) loss_oracle 0.2061 (0.2603) acc 84.3750 (77.6172) lr 1.7713e-05 eta 0:01:26
epoch [49/50] batch [180/288] time 0.200 (0.206) data 0.000 (0.002) loss 1.1765 (1.0991) teacher_loss 0.9111 (0.8397) loss_zs_kd 0.7011 (0.4851) loss_oracle 0.2654 (0.2594) acc 75.0000 (77.7951) lr 1.7713e-05 eta 0:01:21
epoch [49/50] batch [200/288] time 0.194 (0.205) data 0.000 (0.002) loss 1.7066 (1.1115) teacher_loss 1.4349 (0.8520) loss_zs_kd 0.4515 (0.4845) loss_oracle 0.2718 (0.2595) acc 62.5000 (77.4688) lr 1.7713e-05 eta 0:01:16
epoch [49/50] batch [220/288] time 0.191 (0.204) data 0.000 (0.002) loss 1.3563 (1.1194) teacher_loss 1.0837 (0.8587) loss_zs_kd 0.5420 (0.4856) loss_oracle 0.2726 (0.2607) acc 68.7500 (77.3438) lr 1.7713e-05 eta 0:01:12
epoch [49/50] batch [240/288] time 0.166 (0.203) data 0.000 (0.002) loss 0.9352 (1.1194) teacher_loss 0.6369 (0.8575) loss_zs_kd 0.4517 (0.4847) loss_oracle 0.2983 (0.2618) acc 84.3750 (77.3698) lr 1.7713e-05 eta 0:01:08
epoch [49/50] batch [260/288] time 0.199 (0.202) data 0.000 (0.001) loss 1.2502 (1.1136) teacher_loss 0.9543 (0.8512) loss_zs_kd 0.5738 (0.4821) loss_oracle 0.2959 (0.2625) acc 81.2500 (77.6562) lr 1.7713e-05 eta 0:01:03
epoch [49/50] batch [280/288] time 0.194 (0.202) data 0.000 (0.001) loss 0.7839 (1.1077) teacher_loss 0.5132 (0.8452) loss_zs_kd 0.3502 (0.4795) loss_oracle 0.2707 (0.2625) acc 84.3750 (77.6786) lr 1.7713e-05 eta 0:00:59
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,470
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,003
* accuracy: 82.5%
* error: 17.5%
* macro_f1: 79.3%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
epoch [50/50] batch [20/288] time 0.201 (0.210) data 0.000 (0.013) loss 1.4323 (1.0419) teacher_loss 1.1322 (0.7734) loss_zs_kd 0.3709 (0.4929) loss_oracle 0.3001 (0.2685) acc 71.8750 (77.8125) lr 7.8853e-06 eta 0:00:56
epoch [50/50] batch [40/288] time 0.198 (0.202) data 0.000 (0.006) loss 1.1553 (1.1167) teacher_loss 0.9070 (0.8499) loss_zs_kd 0.6191 (0.4962) loss_oracle 0.2482 (0.2668) acc 78.1250 (76.9531) lr 7.8853e-06 eta 0:00:50
epoch [50/50] batch [60/288] time 0.228 (0.200) data 0.000 (0.004) loss 1.0529 (1.1147) teacher_loss 0.8005 (0.8484) loss_zs_kd 0.4197 (0.4762) loss_oracle 0.2524 (0.2664) acc 81.2500 (77.7604) lr 7.8853e-06 eta 0:00:45
epoch [50/50] batch [80/288] time 0.479 (0.204) data 0.000 (0.003) loss 0.8743 (1.1237) teacher_loss 0.6038 (0.8592) loss_zs_kd 0.4299 (0.4779) loss_oracle 0.2706 (0.2645) acc 84.3750 (77.2266) lr 7.8853e-06 eta 0:00:42
epoch [50/50] batch [100/288] time 0.213 (0.217) data 0.000 (0.003) loss 1.3008 (1.1472) teacher_loss 1.0727 (0.8808) loss_zs_kd 0.5388 (0.4819) loss_oracle 0.2281 (0.2664) acc 78.1250 (76.5938) lr 7.8853e-06 eta 0:00:40
epoch [50/50] batch [120/288] time 0.194 (0.213) data 0.001 (0.002) loss 1.0498 (1.1451) teacher_loss 0.7768 (0.8808) loss_zs_kd 0.4295 (0.4786) loss_oracle 0.2730 (0.2643) acc 78.1250 (76.4844) lr 7.8853e-06 eta 0:00:35
epoch [50/50] batch [140/288] time 0.195 (0.210) data 0.000 (0.002) loss 1.3030 (1.1358) teacher_loss 1.0346 (0.8710) loss_zs_kd 0.5142 (0.4893) loss_oracle 0.2683 (0.2647) acc 68.7500 (76.6964) lr 7.8853e-06 eta 0:00:31
epoch [50/50] batch [160/288] time 0.189 (0.208) data 0.000 (0.002) loss 1.0310 (1.1233) teacher_loss 0.7389 (0.8593) loss_zs_kd 0.3618 (0.4844) loss_oracle 0.2921 (0.2640) acc 84.3750 (77.1289) lr 7.8853e-06 eta 0:00:26
epoch [50/50] batch [180/288] time 0.190 (0.207) data 0.000 (0.002) loss 1.2439 (1.1373) teacher_loss 0.9404 (0.8742) loss_zs_kd 0.5373 (0.4869) loss_oracle 0.3035 (0.2631) acc 81.2500 (76.8750) lr 7.8853e-06 eta 0:00:22
epoch [50/50] batch [200/288] time 0.193 (0.205) data 0.000 (0.001) loss 1.2826 (1.1427) teacher_loss 1.0300 (0.8800) loss_zs_kd 0.3474 (0.4861) loss_oracle 0.2526 (0.2627) acc 62.5000 (76.8750) lr 7.8853e-06 eta 0:00:18
epoch [50/50] batch [220/288] time 0.196 (0.204) data 0.000 (0.001) loss 0.8611 (1.1387) teacher_loss 0.6423 (0.8770) loss_zs_kd 0.3673 (0.4879) loss_oracle 0.2188 (0.2617) acc 75.0000 (76.8324) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [240/288] time 0.192 (0.203) data 0.000 (0.001) loss 1.0437 (1.1467) teacher_loss 0.7799 (0.8847) loss_zs_kd 0.3324 (0.4867) loss_oracle 0.2638 (0.2620) acc 75.0000 (76.7318) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [260/288] time 0.194 (0.203) data 0.000 (0.001) loss 0.8314 (1.1504) teacher_loss 0.5822 (0.8891) loss_zs_kd 0.4455 (0.4887) loss_oracle 0.2491 (0.2613) acc 78.1250 (76.5024) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [280/288] time 0.190 (0.202) data 0.000 (0.001) loss 1.4998 (1.1454) teacher_loss 1.1870 (0.8836) loss_zs_kd 0.4953 (0.4882) loss_oracle 0.3128 (0.2618) acc 59.3750 (76.6295) lr 7.8853e-06 eta 0:00:01
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,471
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,001
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.1%
******* Domain a best val acc:      88.3%, epoch: 33 *******
******* Domain a best val test acc: 82.5%, epoch: 33 *******
******* Domain a best test acc:     83.9%, epoch: 4 *******
Checkpoint saved to icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model.pth.tar-50
Finish the whole training
Elapsed: 0:48:40
[Info] Hyperparameters saved to: icml/multi-dg/oracle/11_alphaoracle_studentlogits_alpha0.5/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/hyperparameters.json
