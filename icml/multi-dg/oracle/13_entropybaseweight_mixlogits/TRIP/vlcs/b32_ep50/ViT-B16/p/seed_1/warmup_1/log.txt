Loading trainer: TRIP
Loading dataset: SPG_VLCS
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -----------------------------
Dataset    SPG_VLCS
Source     ['caltech', 'labelme', 'sun']
Target     ['pascal']
# classes  5
# train_x  5,147
# val      2,206
# test     3,376
---------  -----------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,195
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/160] time 0.079 (0.107) data 0.000 (0.015) loss 1.7287 (1.5274) teacher_loss 0.8027 (0.6801) loss_zs_kd 0.0003 (0.0001) loss_oracle 0.9258 (0.8472) acc 71.8750 (75.6250) alaph_mean 0.0630 (0.0916) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.4832) lr 1.0000e-05 eta 0:14:16
epoch [1/50] batch [40/160] time 0.078 (0.094) data 0.000 (0.008) loss 1.6244 (1.5221) teacher_loss 0.6807 (0.6692) loss_zs_kd 0.0012 (0.0005) loss_oracle 0.9431 (0.8527) acc 75.0000 (75.5469) alaph_mean 0.0594 (0.0886) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.4709) lr 1.0000e-05 eta 0:12:31
epoch [1/50] batch [60/160] time 0.088 (0.090) data 0.000 (0.005) loss 1.3846 (1.5445) teacher_loss 0.5396 (0.6970) loss_zs_kd 0.0023 (0.0009) loss_oracle 0.8439 (0.8471) acc 78.1250 (75.0521) alaph_mean 0.0939 (0.0907) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.4703) lr 1.0000e-05 eta 0:11:53
epoch [1/50] batch [80/160] time 0.081 (0.087) data 0.000 (0.004) loss 1.7939 (1.5572) teacher_loss 0.9204 (0.7074) loss_zs_kd 0.0032 (0.0015) loss_oracle 0.8719 (0.8491) acc 75.0000 (74.2188) alaph_mean 0.0849 (0.0901) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.4656) lr 1.0000e-05 eta 0:11:32
epoch [1/50] batch [100/160] time 0.078 (0.086) data 0.000 (0.003) loss 1.4248 (1.5562) teacher_loss 0.6270 (0.7042) loss_zs_kd 0.0043 (0.0020) loss_oracle 0.7957 (0.8510) acc 81.2500 (74.4062) alaph_mean 0.1179 (0.0895) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.4664) lr 1.0000e-05 eta 0:11:23
epoch [1/50] batch [120/160] time 0.083 (0.085) data 0.000 (0.003) loss 1.5212 (1.5464) teacher_loss 0.7153 (0.6945) loss_zs_kd 0.0064 (0.0026) loss_oracle 0.8026 (0.8506) acc 71.8750 (74.7396) alaph_mean 0.1077 (0.0896) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.4661) lr 1.0000e-05 eta 0:11:12
epoch [1/50] batch [140/160] time 0.093 (0.085) data 0.000 (0.002) loss 1.4086 (1.5501) teacher_loss 0.5103 (0.6910) loss_zs_kd 0.0065 (0.0033) loss_oracle 0.8951 (0.8575) acc 87.5000 (74.7991) alaph_mean 0.0677 (0.0869) alpha_min 0.0000 (0.0000) alpha_max 0.3851 (0.4629) lr 1.0000e-05 eta 0:11:05
epoch [1/50] batch [160/160] time 0.068 (0.083) data 0.000 (0.002) loss 1.7903 (1.5544) teacher_loss 0.9785 (0.6947) loss_zs_kd 0.0102 (0.0038) loss_oracle 0.8066 (0.8577) acc 65.6250 (74.7070) alaph_mean 0.1070 (0.0868) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.4632) lr 2.0000e-03 eta 0:10:51
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,727
* accuracy: 78.3%
* error: 21.7%
* macro_f1: 80.2%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,923
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 87.3%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      78.3%, epoch: 1 *******
******* Domain p best val test acc: 86.6%, epoch: 1 *******
******* Domain p best test acc:     86.6%, epoch: 1 *******
epoch [2/50] batch [20/160] time 0.089 (0.094) data 0.000 (0.015) loss 1.7362 (1.5871) teacher_loss 0.6943 (0.6279) loss_zs_kd 0.0835 (0.0721) loss_oracle 1.0001 (0.9231) acc 65.6250 (77.1875) alaph_mean 0.0387 (0.0744) alpha_min 0.0000 (0.0000) alpha_max 0.4483 (0.4990) lr 2.0000e-03 eta 0:12:16
epoch [2/50] batch [40/160] time 0.089 (0.088) data 0.000 (0.008) loss 1.6315 (1.5635) teacher_loss 0.6348 (0.5990) loss_zs_kd 0.1665 (0.0787) loss_oracle 0.9135 (0.9251) acc 78.1250 (77.4219) alaph_mean 0.0954 (0.0746) alpha_min 0.0000 (0.0000) alpha_max 0.5408 (0.5097) lr 2.0000e-03 eta 0:11:26
epoch [2/50] batch [60/160] time 0.079 (0.087) data 0.000 (0.005) loss 1.7107 (1.6040) teacher_loss 0.6006 (0.6134) loss_zs_kd 0.1348 (0.0921) loss_oracle 1.0428 (0.9445) acc 84.3750 (77.5521) alaph_mean 0.0196 (0.0737) alpha_min 0.0000 (0.0000) alpha_max 0.2556 (0.5284) lr 2.0000e-03 eta 0:11:17
epoch [2/50] batch [80/160] time 0.083 (0.086) data 0.000 (0.004) loss 1.4807 (1.6155) teacher_loss 0.4143 (0.6133) loss_zs_kd 0.0704 (0.0849) loss_oracle 1.0311 (0.9597) acc 84.3750 (77.1484) alaph_mean 0.0369 (0.0679) alpha_min 0.0000 (0.0000) alpha_max 0.5013 (0.5218) lr 2.0000e-03 eta 0:11:03
epoch [2/50] batch [100/160] time 0.086 (0.085) data 0.000 (0.003) loss 1.2942 (1.6008) teacher_loss 0.3396 (0.6039) loss_zs_kd 0.1282 (0.0840) loss_oracle 0.8905 (0.9548) acc 96.8750 (77.2188) alaph_mean 0.0919 (0.0690) alpha_min 0.0000 (0.0000) alpha_max 0.6851 (0.5191) lr 2.0000e-03 eta 0:10:58
epoch [2/50] batch [120/160] time 0.075 (0.085) data 0.000 (0.003) loss 1.5407 (1.5937) teacher_loss 0.5249 (0.5975) loss_zs_kd 0.1004 (0.0876) loss_oracle 0.9657 (0.9524) acc 81.2500 (77.3438) alaph_mean 0.0547 (0.0691) alpha_min 0.0000 (0.0000) alpha_max 0.4993 (0.5216) lr 2.0000e-03 eta 0:10:53
epoch [2/50] batch [140/160] time 0.073 (0.084) data 0.000 (0.002) loss 1.2863 (1.5732) teacher_loss 0.2554 (0.5792) loss_zs_kd 0.0870 (0.0884) loss_oracle 0.9875 (0.9498) acc 90.6250 (77.9464) alaph_mean 0.0401 (0.0685) alpha_min 0.0000 (0.0000) alpha_max 0.5004 (0.5151) lr 2.0000e-03 eta 0:10:43
epoch [2/50] batch [160/160] time 0.103 (0.085) data 0.001 (0.002) loss 1.4071 (1.5592) teacher_loss 0.4897 (0.5701) loss_zs_kd 0.0738 (0.0877) loss_oracle 0.8805 (0.9453) acc 81.2500 (78.3594) alaph_mean 0.0832 (0.0695) alpha_min -0.0000 (0.0000) alpha_max 0.5159 (0.5135) lr 1.9980e-03 eta 0:10:52
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,814
* accuracy: 82.2%
* error: 17.8%
* macro_f1: 84.2%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,943
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 88.1%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      82.2%, epoch: 2 *******
******* Domain p best val test acc: 87.2%, epoch: 2 *******
******* Domain p best test acc:     87.2%, epoch: 2 *******
epoch [3/50] batch [20/160] time 0.063 (0.085) data 0.000 (0.014) loss 1.3318 (1.4672) teacher_loss 0.3960 (0.5211) loss_zs_kd 0.1244 (0.0915) loss_oracle 0.8736 (0.9003) acc 87.5000 (81.0938) alaph_mean 0.1009 (0.0894) alpha_min 0.0000 (0.0000) alpha_max 0.5048 (0.5606) lr 1.9980e-03 eta 0:10:50
epoch [3/50] batch [40/160] time 0.076 (0.079) data 0.000 (0.007) loss 1.3275 (1.4928) teacher_loss 0.3853 (0.5231) loss_zs_kd 0.0645 (0.0979) loss_oracle 0.9100 (0.9208) acc 84.3750 (81.5625) alaph_mean 0.1192 (0.0866) alpha_min 0.0000 (0.0000) alpha_max 0.8417 (0.5871) lr 1.9980e-03 eta 0:10:03
epoch [3/50] batch [60/160] time 0.089 (0.078) data 0.001 (0.005) loss 1.6858 (1.5206) teacher_loss 0.6807 (0.5322) loss_zs_kd 0.0842 (0.0970) loss_oracle 0.9631 (0.9399) acc 78.1250 (81.4062) alaph_mean 0.0606 (0.0791) alpha_min -0.0000 (0.0000) alpha_max 0.5016 (0.5906) lr 1.9980e-03 eta 0:09:51
epoch [3/50] batch [80/160] time 0.078 (0.079) data 0.000 (0.004) loss 1.4810 (1.5246) teacher_loss 0.4949 (0.5332) loss_zs_kd 0.1156 (0.0953) loss_oracle 0.9283 (0.9438) acc 87.5000 (81.0547) alaph_mean 0.0647 (0.0772) alpha_min 0.0000 (0.0000) alpha_max 0.4374 (0.5943) lr 1.9980e-03 eta 0:09:59
epoch [3/50] batch [100/160] time 0.086 (0.080) data 0.000 (0.003) loss 1.4382 (1.5048) teacher_loss 0.4258 (0.5106) loss_zs_kd 0.0653 (0.0967) loss_oracle 0.9798 (0.9458) acc 87.5000 (81.8438) alaph_mean 0.0512 (0.0761) alpha_min 0.0000 (0.0000) alpha_max 0.5053 (0.5949) lr 1.9980e-03 eta 0:10:06
epoch [3/50] batch [120/160] time 0.097 (0.081) data 0.001 (0.003) loss 1.3287 (1.4986) teacher_loss 0.3533 (0.5042) loss_zs_kd 0.1353 (0.0980) loss_oracle 0.9078 (0.9454) acc 93.7500 (81.9271) alaph_mean 0.0909 (0.0750) alpha_min 0.0000 (0.0000) alpha_max 0.5012 (0.5829) lr 1.9980e-03 eta 0:10:08
epoch [3/50] batch [140/160] time 0.090 (0.082) data 0.000 (0.002) loss 1.3405 (1.4926) teacher_loss 0.4524 (0.5023) loss_zs_kd 0.1032 (0.1002) loss_oracle 0.8365 (0.9402) acc 81.2500 (81.8080) alaph_mean 0.1231 (0.0779) alpha_min 0.0000 (0.0000) alpha_max 0.5019 (0.5875) lr 1.9980e-03 eta 0:10:16
epoch [3/50] batch [160/160] time 0.078 (0.081) data 0.000 (0.002) loss 2.1777 (1.4966) teacher_loss 1.1729 (0.5094) loss_zs_kd 0.1034 (0.1007) loss_oracle 0.9531 (0.9369) acc 62.5000 (81.5820) alaph_mean 0.0729 (0.0786) alpha_min 0.0000 (0.0000) alpha_max 0.6483 (0.5814) lr 1.9921e-03 eta 0:10:12
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,829
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 84.4%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,961
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 88.6%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      82.9%, epoch: 3 *******
******* Domain p best val test acc: 87.7%, epoch: 3 *******
******* Domain p best test acc:     87.7%, epoch: 3 *******
epoch [4/50] batch [20/160] time 0.083 (0.101) data 0.000 (0.020) loss 1.3451 (1.5155) teacher_loss 0.4780 (0.5433) loss_zs_kd 0.0725 (0.0902) loss_oracle 0.8308 (0.9271) acc 84.3750 (79.2188) alaph_mean 0.1248 (0.0793) alpha_min 0.0000 (0.0000) alpha_max 0.5289 (0.5604) lr 1.9921e-03 eta 0:12:37
epoch [4/50] batch [40/160] time 0.093 (0.094) data 0.000 (0.010) loss 1.3656 (1.4542) teacher_loss 0.3933 (0.4820) loss_zs_kd 0.1228 (0.0957) loss_oracle 0.9109 (0.9243) acc 84.3750 (82.1875) alaph_mean 0.0864 (0.0810) alpha_min 0.0000 (0.0000) alpha_max 0.5117 (0.5532) lr 1.9921e-03 eta 0:11:39
epoch [4/50] batch [60/160] time 0.087 (0.090) data 0.000 (0.007) loss 1.4660 (1.4555) teacher_loss 0.4639 (0.4825) loss_zs_kd 0.1328 (0.0997) loss_oracle 0.9357 (0.9232) acc 90.6250 (83.0729) alaph_mean 0.0812 (0.0812) alpha_min 0.0000 (0.0000) alpha_max 0.6784 (0.5507) lr 1.9921e-03 eta 0:11:10
epoch [4/50] batch [80/160] time 0.088 (0.089) data 0.000 (0.005) loss 1.5201 (1.4694) teacher_loss 0.5444 (0.4896) loss_zs_kd 0.1041 (0.1082) loss_oracle 0.9237 (0.9257) acc 81.2500 (83.0469) alaph_mean 0.0714 (0.0803) alpha_min 0.0000 (0.0000) alpha_max 0.4990 (0.5600) lr 1.9921e-03 eta 0:11:03
epoch [4/50] batch [100/160] time 0.078 (0.092) data 0.000 (0.004) loss 1.4856 (1.4826) teacher_loss 0.5649 (0.5021) loss_zs_kd 0.0790 (0.1093) loss_oracle 0.8811 (0.9258) acc 81.2500 (82.3438) alaph_mean 0.1027 (0.0796) alpha_min 0.0000 (0.0000) alpha_max 0.6168 (0.5630) lr 1.9921e-03 eta 0:11:22
epoch [4/50] batch [120/160] time 0.089 (0.091) data 0.001 (0.004) loss 1.7212 (1.4878) teacher_loss 0.7266 (0.5088) loss_zs_kd 0.0920 (0.1061) loss_oracle 0.9486 (0.9260) acc 71.8750 (82.0052) alaph_mean 0.0587 (0.0796) alpha_min 0.0000 (0.0000) alpha_max 0.5339 (0.5661) lr 1.9921e-03 eta 0:11:11
epoch [4/50] batch [140/160] time 0.078 (0.090) data 0.000 (0.003) loss 1.5209 (1.4915) teacher_loss 0.5254 (0.5092) loss_zs_kd 0.0562 (0.1053) loss_oracle 0.9674 (0.9296) acc 81.2500 (81.8080) alaph_mean 0.0661 (0.0785) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.5684) lr 1.9921e-03 eta 0:11:02
epoch [4/50] batch [160/160] time 0.079 (0.088) data 0.000 (0.003) loss 1.3709 (1.4877) teacher_loss 0.3169 (0.5072) loss_zs_kd 0.0827 (0.1043) loss_oracle 1.0127 (0.9283) acc 93.7500 (81.7969) alaph_mean 0.0328 (0.0794) alpha_min 0.0000 (0.0000) alpha_max 0.5007 (0.5753) lr 1.9823e-03 eta 0:10:50
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,830
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 84.6%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,959
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 88.6%
******* Domain p best val acc:      83.0%, epoch: 4 *******
******* Domain p best val test acc: 87.6%, epoch: 4 *******
******* Domain p best test acc:     87.7%, epoch: 3 *******
epoch [5/50] batch [20/160] time 0.078 (0.097) data 0.000 (0.013) loss 1.2234 (1.5029) teacher_loss 0.2751 (0.5175) loss_zs_kd 0.0951 (0.0976) loss_oracle 0.9007 (0.9366) acc 90.6250 (80.7812) alaph_mean 0.1129 (0.0812) alpha_min 0.0000 (0.0000) alpha_max 0.7504 (0.5861) lr 1.9823e-03 eta 0:11:50
epoch [5/50] batch [40/160] time 0.079 (0.089) data 0.000 (0.007) loss 1.2559 (1.4879) teacher_loss 0.3413 (0.5138) loss_zs_kd 0.0816 (0.0955) loss_oracle 0.8738 (0.9263) acc 90.6250 (81.0156) alaph_mean 0.1055 (0.0858) alpha_min 0.0000 (0.0000) alpha_max 0.6533 (0.6116) lr 1.9823e-03 eta 0:10:50
epoch [5/50] batch [60/160] time 0.083 (0.087) data 0.001 (0.004) loss 1.6862 (1.4910) teacher_loss 0.7485 (0.5091) loss_zs_kd 0.0990 (0.1003) loss_oracle 0.8882 (0.9318) acc 81.2500 (81.0938) alaph_mean 0.1156 (0.0843) alpha_min 0.0000 (0.0000) alpha_max 0.5193 (0.6048) lr 1.9823e-03 eta 0:10:33
epoch [5/50] batch [80/160] time 0.066 (0.086) data 0.000 (0.003) loss 1.4268 (1.4879) teacher_loss 0.5283 (0.5130) loss_zs_kd 0.0917 (0.1002) loss_oracle 0.8526 (0.9247) acc 81.2500 (81.6797) alaph_mean 0.1042 (0.0856) alpha_min 0.0000 (0.0000) alpha_max 0.5761 (0.5889) lr 1.9823e-03 eta 0:10:24
epoch [5/50] batch [100/160] time 0.085 (0.085) data 0.000 (0.003) loss 1.4120 (1.4761) teacher_loss 0.4636 (0.5002) loss_zs_kd 0.0768 (0.0976) loss_oracle 0.9100 (0.9271) acc 81.2500 (81.8438) alaph_mean 0.0694 (0.0842) alpha_min 0.0000 (0.0000) alpha_max 0.3755 (0.5846) lr 1.9823e-03 eta 0:10:15
epoch [5/50] batch [120/160] time 0.085 (0.085) data 0.000 (0.002) loss 1.5905 (1.4746) teacher_loss 0.5693 (0.5010) loss_zs_kd 0.0999 (0.1009) loss_oracle 0.9712 (0.9232) acc 78.1250 (81.8490) alaph_mean 0.0457 (0.0850) alpha_min 0.0000 (0.0000) alpha_max 0.4802 (0.5847) lr 1.9823e-03 eta 0:10:13
epoch [5/50] batch [140/160] time 0.091 (0.084) data 0.000 (0.002) loss 1.9093 (1.4762) teacher_loss 0.8882 (0.5005) loss_zs_kd 0.1201 (0.1035) loss_oracle 0.9611 (0.9239) acc 75.0000 (81.7634) alaph_mean 0.0761 (0.0848) alpha_min 0.0000 (0.0000) alpha_max 0.5662 (0.5782) lr 1.9823e-03 eta 0:10:09
epoch [5/50] batch [160/160] time 0.065 (0.083) data 0.000 (0.002) loss 1.6491 (1.4697) teacher_loss 0.8037 (0.4966) loss_zs_kd 0.0741 (0.1045) loss_oracle 0.8084 (0.9209) acc 75.0000 (81.9531) alaph_mean 0.1551 (0.0866) alpha_min 0.0000 (0.0000) alpha_max 0.5907 (0.5793) lr 1.9686e-03 eta 0:10:00
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,831
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 84.5%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,977
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 89.0%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      83.0%, epoch: 5 *******
******* Domain p best val test acc: 88.2%, epoch: 5 *******
******* Domain p best test acc:     88.2%, epoch: 5 *******
epoch [6/50] batch [20/160] time 0.078 (0.099) data 0.000 (0.019) loss 1.4657 (1.4619) teacher_loss 0.3979 (0.4768) loss_zs_kd 0.1258 (0.1100) loss_oracle 1.0048 (0.9301) acc 84.3750 (82.6562) alaph_mean 0.0484 (0.0824) alpha_min 0.0000 (0.0000) alpha_max 0.5709 (0.5328) lr 1.9686e-03 eta 0:11:49
epoch [6/50] batch [40/160] time 0.075 (0.099) data 0.000 (0.010) loss 1.2928 (1.4411) teacher_loss 0.3003 (0.4644) loss_zs_kd 0.1094 (0.1203) loss_oracle 0.9378 (0.9165) acc 93.7500 (83.1250) alaph_mean 0.0666 (0.0890) alpha_min 0.0000 (0.0000) alpha_max 0.4991 (0.5621) lr 1.9686e-03 eta 0:11:50
epoch [6/50] batch [60/160] time 0.090 (0.094) data 0.001 (0.007) loss 1.2815 (1.4378) teacher_loss 0.3328 (0.4626) loss_zs_kd 0.1581 (0.1208) loss_oracle 0.8697 (0.9148) acc 84.3750 (83.1771) alaph_mean 0.1146 (0.0888) alpha_min 0.0000 (0.0000) alpha_max 0.4990 (0.5635) lr 1.9686e-03 eta 0:11:10
epoch [6/50] batch [80/160] time 0.082 (0.091) data 0.000 (0.005) loss 1.3905 (1.4531) teacher_loss 0.4490 (0.4764) loss_zs_kd 0.1189 (0.1211) loss_oracle 0.8820 (0.9161) acc 78.1250 (82.8125) alaph_mean 0.0997 (0.0874) alpha_min 0.0000 (0.0000) alpha_max 0.4719 (0.5538) lr 1.9686e-03 eta 0:10:49
epoch [6/50] batch [100/160] time 0.081 (0.090) data 0.000 (0.004) loss 1.3510 (1.4452) teacher_loss 0.4724 (0.4717) loss_zs_kd 0.1065 (0.1180) loss_oracle 0.8254 (0.9145) acc 87.5000 (83.0625) alaph_mean 0.1261 (0.0874) alpha_min 0.0000 (0.0000) alpha_max 0.5447 (0.5537) lr 1.9686e-03 eta 0:10:40
epoch [6/50] batch [120/160] time 0.082 (0.089) data 0.000 (0.003) loss 1.6348 (1.4507) teacher_loss 0.6309 (0.4738) loss_zs_kd 0.1592 (0.1216) loss_oracle 0.9243 (0.9161) acc 78.1250 (83.1771) alaph_mean 0.0800 (0.0863) alpha_min 0.0000 (0.0000) alpha_max 0.5544 (0.5530) lr 1.9686e-03 eta 0:10:32
epoch [6/50] batch [140/160] time 0.092 (0.089) data 0.001 (0.003) loss 1.5393 (1.4468) teacher_loss 0.5171 (0.4694) loss_zs_kd 0.0860 (0.1192) loss_oracle 0.9792 (0.9178) acc 71.8750 (83.3036) alaph_mean 0.0617 (0.0855) alpha_min 0.0000 (0.0000) alpha_max 0.4645 (0.5490) lr 1.9686e-03 eta 0:10:26
epoch [6/50] batch [160/160] time 0.076 (0.087) data 0.000 (0.003) loss 1.6185 (1.4561) teacher_loss 0.6655 (0.4791) loss_zs_kd 0.1207 (0.1189) loss_oracle 0.8926 (0.9175) acc 84.3750 (83.0469) alaph_mean 0.0765 (0.0849) alpha_min 0.0000 (0.0000) alpha_max 0.4972 (0.5448) lr 1.9511e-03 eta 0:10:15
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,837
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 84.8%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,978
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 89.0%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      83.3%, epoch: 6 *******
******* Domain p best val test acc: 88.2%, epoch: 6 *******
******* Domain p best test acc:     88.2%, epoch: 6 *******
epoch [7/50] batch [20/160] time 0.079 (0.100) data 0.000 (0.014) loss 1.5447 (1.4226) teacher_loss 0.6060 (0.4529) loss_zs_kd 0.1177 (0.1008) loss_oracle 0.8799 (0.9193) acc 71.8750 (83.4375) alaph_mean 0.1007 (0.0770) alpha_min 0.0000 (0.0000) alpha_max 0.5574 (0.5180) lr 1.9511e-03 eta 0:11:41
epoch [7/50] batch [40/160] time 0.084 (0.091) data 0.000 (0.007) loss 1.5796 (1.4419) teacher_loss 0.5547 (0.4719) loss_zs_kd 0.1103 (0.1131) loss_oracle 0.9697 (0.9135) acc 87.5000 (82.8125) alaph_mean 0.0584 (0.0785) alpha_min 0.0000 (0.0000) alpha_max 0.4414 (0.5159) lr 1.9511e-03 eta 0:10:38
epoch [7/50] batch [60/160] time 0.086 (0.089) data 0.001 (0.005) loss 1.6011 (1.4443) teacher_loss 0.5776 (0.4722) loss_zs_kd 0.1113 (0.1135) loss_oracle 0.9679 (0.9154) acc 78.1250 (82.7604) alaph_mean 0.0674 (0.0789) alpha_min 0.0000 (0.0000) alpha_max 0.7445 (0.5193) lr 1.9511e-03 eta 0:10:21
epoch [7/50] batch [80/160] time 0.085 (0.088) data 0.000 (0.004) loss 1.4687 (1.4547) teacher_loss 0.5703 (0.4827) loss_zs_kd 0.1601 (0.1164) loss_oracle 0.8183 (0.9139) acc 87.5000 (82.2266) alaph_mean 0.1187 (0.0794) alpha_min 0.0000 (0.0000) alpha_max 0.5131 (0.5184) lr 1.9511e-03 eta 0:10:13
epoch [7/50] batch [100/160] time 0.089 (0.088) data 0.000 (0.003) loss 1.3769 (1.4641) teacher_loss 0.4219 (0.4883) loss_zs_kd 0.0984 (0.1171) loss_oracle 0.9058 (0.9172) acc 75.0000 (81.9688) alaph_mean 0.0928 (0.0774) alpha_min 0.0000 (0.0000) alpha_max 0.5210 (0.5179) lr 1.9511e-03 eta 0:10:09
epoch [7/50] batch [120/160] time 0.078 (0.087) data 0.000 (0.003) loss 1.3719 (1.4708) teacher_loss 0.3186 (0.4940) loss_zs_kd 0.0688 (0.1170) loss_oracle 1.0189 (0.9183) acc 87.5000 (81.6146) alaph_mean 0.0324 (0.0765) alpha_min 0.0000 (0.0000) alpha_max 0.5573 (0.5199) lr 1.9511e-03 eta 0:10:01
epoch [7/50] batch [140/160] time 0.080 (0.087) data 0.000 (0.002) loss 1.3639 (1.4617) teacher_loss 0.4099 (0.4853) loss_zs_kd 0.0909 (0.1137) loss_oracle 0.9085 (0.9196) acc 90.6250 (81.8304) alaph_mean 0.0758 (0.0754) alpha_min 0.0000 (0.0000) alpha_max 0.5161 (0.5197) lr 1.9511e-03 eta 0:09:56
epoch [7/50] batch [160/160] time 0.076 (0.086) data 0.000 (0.002) loss 1.5735 (1.4659) teacher_loss 0.6528 (0.4900) loss_zs_kd 0.1418 (0.1130) loss_oracle 0.8498 (0.9194) acc 71.8750 (81.7773) alaph_mean 0.1078 (0.0748) alpha_min 0.0000 (0.0000) alpha_max 0.5172 (0.5197) lr 1.9298e-03 eta 0:09:48
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,838
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 85.0%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,987
* accuracy: 88.5%
* error: 11.5%
* macro_f1: 89.3%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      83.3%, epoch: 7 *******
******* Domain p best val test acc: 88.5%, epoch: 7 *******
******* Domain p best test acc:     88.5%, epoch: 7 *******
epoch [8/50] batch [20/160] time 0.084 (0.105) data 0.000 (0.018) loss 1.4827 (1.4764) teacher_loss 0.5811 (0.5019) loss_zs_kd 0.0972 (0.1005) loss_oracle 0.8530 (0.9242) acc 81.2500 (80.4688) alaph_mean 0.0922 (0.0608) alpha_min 0.0000 (0.0000) alpha_max 0.5031 (0.4623) lr 1.9298e-03 eta 0:11:59
epoch [8/50] batch [40/160] time 0.083 (0.094) data 0.000 (0.009) loss 1.5198 (1.4721) teacher_loss 0.5063 (0.4932) loss_zs_kd 0.1141 (0.1024) loss_oracle 0.9564 (0.9276) acc 68.7500 (80.7812) alaph_mean 0.0454 (0.0605) alpha_min 0.0000 (0.0000) alpha_max 0.5001 (0.4712) lr 1.9298e-03 eta 0:10:40
epoch [8/50] batch [60/160] time 0.083 (0.090) data 0.001 (0.006) loss 1.3582 (1.4493) teacher_loss 0.4980 (0.4747) loss_zs_kd 0.1265 (0.1104) loss_oracle 0.7969 (0.9193) acc 84.3750 (81.2500) alaph_mean 0.1177 (0.0653) alpha_min 0.0000 (0.0000) alpha_max 0.4999 (0.4896) lr 1.9298e-03 eta 0:10:15
epoch [8/50] batch [80/160] time 0.090 (0.089) data 0.001 (0.005) loss 1.1607 (1.4300) teacher_loss 0.2473 (0.4641) loss_zs_kd 0.1140 (0.1107) loss_oracle 0.8563 (0.9106) acc 96.8750 (82.1484) alaph_mean 0.0831 (0.0687) alpha_min 0.0000 (0.0000) alpha_max 0.4975 (0.4912) lr 1.9298e-03 eta 0:10:06
epoch [8/50] batch [100/160] time 0.082 (0.089) data 0.000 (0.004) loss 1.3986 (1.4293) teacher_loss 0.4329 (0.4654) loss_zs_kd 0.1179 (0.1115) loss_oracle 0.9067 (0.9081) acc 87.5000 (82.3438) alaph_mean 0.0682 (0.0699) alpha_min 0.0000 (0.0000) alpha_max 0.4971 (0.4972) lr 1.9298e-03 eta 0:10:00
epoch [8/50] batch [120/160] time 0.088 (0.088) data 0.001 (0.003) loss 1.2392 (1.4275) teacher_loss 0.3718 (0.4675) loss_zs_kd 0.1408 (0.1120) loss_oracle 0.7970 (0.9040) acc 90.6250 (82.4740) alaph_mean 0.1166 (0.0720) alpha_min 0.0000 (0.0000) alpha_max 0.5210 (0.5047) lr 1.9298e-03 eta 0:09:57
epoch [8/50] batch [140/160] time 0.083 (0.088) data 0.000 (0.003) loss 1.2209 (1.4274) teacher_loss 0.3333 (0.4684) loss_zs_kd 0.0776 (0.1116) loss_oracle 0.8488 (0.9032) acc 84.3750 (82.3884) alaph_mean 0.1183 (0.0729) alpha_min 0.0000 (0.0000) alpha_max 0.6647 (0.5086) lr 1.9298e-03 eta 0:09:54
epoch [8/50] batch [160/160] time 0.076 (0.087) data 0.000 (0.003) loss 1.2840 (1.4238) teacher_loss 0.3416 (0.4658) loss_zs_kd 0.0990 (0.1130) loss_oracle 0.8929 (0.9016) acc 81.2500 (82.3828) alaph_mean 0.0998 (0.0739) alpha_min 0.0000 (0.0000) alpha_max 0.5565 (0.5097) lr 1.9048e-03 eta 0:09:42
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,846
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 85.0%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,967
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 89.0%
******* Domain p best val acc:      83.7%, epoch: 8 *******
******* Domain p best val test acc: 87.9%, epoch: 8 *******
******* Domain p best test acc:     88.5%, epoch: 7 *******
epoch [9/50] batch [20/160] time 0.078 (0.098) data 0.000 (0.012) loss 1.3748 (1.3919) teacher_loss 0.3240 (0.4296) loss_zs_kd 0.1095 (0.1251) loss_oracle 0.9961 (0.8997) acc 90.6250 (84.0625) alaph_mean 0.0392 (0.0863) alpha_min 0.0000 (0.0000) alpha_max 0.4439 (0.5489) lr 1.9048e-03 eta 0:10:55
epoch [9/50] batch [40/160] time 0.081 (0.089) data 0.000 (0.006) loss 1.4329 (1.3994) teacher_loss 0.4878 (0.4431) loss_zs_kd 0.1191 (0.1216) loss_oracle 0.8856 (0.8955) acc 75.0000 (84.4531) alaph_mean 0.0876 (0.0899) alpha_min 0.0000 (0.0000) alpha_max 0.5009 (0.5421) lr 1.9048e-03 eta 0:09:54
epoch [9/50] batch [60/160] time 0.090 (0.087) data 0.001 (0.004) loss 1.3923 (1.4332) teacher_loss 0.4558 (0.4768) loss_zs_kd 0.1019 (0.1181) loss_oracle 0.8855 (0.8973) acc 78.1250 (82.9167) alaph_mean 0.0937 (0.0872) alpha_min 0.0000 (0.0000) alpha_max 0.5822 (0.5486) lr 1.9048e-03 eta 0:09:40
epoch [9/50] batch [80/160] time 0.087 (0.087) data 0.000 (0.003) loss 1.3225 (1.4334) teacher_loss 0.3481 (0.4687) loss_zs_kd 0.1174 (0.1218) loss_oracle 0.9157 (0.9038) acc 90.6250 (83.5938) alaph_mean 0.0760 (0.0830) alpha_min 0.0000 (0.0000) alpha_max 0.4254 (0.5448) lr 1.9048e-03 eta 0:09:35
epoch [9/50] batch [100/160] time 0.081 (0.085) data 0.000 (0.003) loss 1.2612 (1.4242) teacher_loss 0.3354 (0.4597) loss_zs_kd 0.1023 (0.1207) loss_oracle 0.8746 (0.9042) acc 93.7500 (83.8438) alaph_mean 0.0760 (0.0803) alpha_min 0.0000 (0.0000) alpha_max 0.4978 (0.5439) lr 1.9048e-03 eta 0:09:24
epoch [9/50] batch [120/160] time 0.106 (0.088) data 0.001 (0.002) loss 1.3947 (1.4285) teacher_loss 0.5068 (0.4615) loss_zs_kd 0.1337 (0.1220) loss_oracle 0.8210 (0.9060) acc 75.0000 (83.8281) alaph_mean 0.1130 (0.0785) alpha_min 0.0000 (0.0000) alpha_max 0.5022 (0.5326) lr 1.9048e-03 eta 0:09:38
epoch [9/50] batch [140/160] time 0.076 (0.087) data 0.000 (0.002) loss 1.4955 (1.4270) teacher_loss 0.4980 (0.4574) loss_zs_kd 0.1532 (0.1229) loss_oracle 0.9208 (0.9082) acc 84.3750 (84.0402) alaph_mean 0.0755 (0.0777) alpha_min 0.0000 (0.0000) alpha_max 0.4403 (0.5298) lr 1.9048e-03 eta 0:09:34
epoch [9/50] batch [160/160] time 0.080 (0.086) data 0.000 (0.002) loss 1.3758 (1.4245) teacher_loss 0.4402 (0.4550) loss_zs_kd 0.1072 (0.1230) loss_oracle 0.8820 (0.9080) acc 81.2500 (83.9258) alaph_mean 0.0849 (0.0782) alpha_min 0.0000 (0.0000) alpha_max 0.4925 (0.5296) lr 1.8763e-03 eta 0:09:27
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,846
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,985
* accuracy: 88.4%
* error: 11.6%
* macro_f1: 89.2%
******* Domain p best val acc:      83.7%, epoch: 8 *******
******* Domain p best val test acc: 87.9%, epoch: 8 *******
******* Domain p best test acc:     88.5%, epoch: 7 *******
epoch [10/50] batch [20/160] time 0.085 (0.104) data 0.000 (0.020) loss 1.3866 (1.4439) teacher_loss 0.3555 (0.4851) loss_zs_kd 0.0762 (0.1042) loss_oracle 0.9931 (0.9067) acc 87.5000 (83.9062) alaph_mean 0.0390 (0.0763) alpha_min 0.0000 (0.0000) alpha_max 0.3990 (0.5301) lr 1.8763e-03 eta 0:11:20
epoch [10/50] batch [40/160] time 0.091 (0.094) data 0.000 (0.010) loss 1.4874 (1.4220) teacher_loss 0.4417 (0.4784) loss_zs_kd 0.0910 (0.1002) loss_oracle 1.0003 (0.8935) acc 81.2500 (82.7344) alaph_mean 0.0244 (0.0832) alpha_min 0.0000 (0.0000) alpha_max 0.5006 (0.5267) lr 1.8763e-03 eta 0:10:10
epoch [10/50] batch [60/160] time 0.084 (0.090) data 0.001 (0.007) loss 1.4480 (1.4441) teacher_loss 0.4062 (0.4920) loss_zs_kd 0.1184 (0.1016) loss_oracle 0.9825 (0.9013) acc 84.3750 (81.9271) alaph_mean 0.0314 (0.0787) alpha_min -0.0000 (0.0000) alpha_max 0.4107 (0.5210) lr 1.8763e-03 eta 0:09:47
epoch [10/50] batch [80/160] time 0.088 (0.089) data 0.000 (0.005) loss 1.6216 (1.4491) teacher_loss 0.6494 (0.4929) loss_zs_kd 0.1164 (0.1065) loss_oracle 0.9140 (0.9030) acc 75.0000 (81.7969) alaph_mean 0.0742 (0.0773) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5197) lr 1.8763e-03 eta 0:09:37
epoch [10/50] batch [100/160] time 0.088 (0.088) data 0.000 (0.004) loss 1.5509 (1.4483) teacher_loss 0.4753 (0.4906) loss_zs_kd 0.1527 (0.1080) loss_oracle 0.9992 (0.9037) acc 81.2500 (81.9375) alaph_mean 0.0205 (0.0779) alpha_min 0.0000 (0.0000) alpha_max 0.4350 (0.5127) lr 1.8763e-03 eta 0:09:27
epoch [10/50] batch [120/160] time 0.086 (0.087) data 0.000 (0.004) loss 1.6491 (1.4527) teacher_loss 0.6338 (0.4913) loss_zs_kd 0.1888 (0.1098) loss_oracle 0.9209 (0.9065) acc 75.0000 (81.9010) alaph_mean 0.0833 (0.0786) alpha_min 0.0000 (0.0000) alpha_max 0.5004 (0.5108) lr 1.8763e-03 eta 0:09:23
epoch [10/50] batch [140/160] time 0.091 (0.087) data 0.000 (0.003) loss 1.4177 (1.4493) teacher_loss 0.4431 (0.4853) loss_zs_kd 0.1096 (0.1129) loss_oracle 0.9198 (0.9075) acc 84.3750 (82.1205) alaph_mean 0.0806 (0.0802) alpha_min -0.0000 (0.0000) alpha_max 0.5061 (0.5235) lr 1.8763e-03 eta 0:09:19
epoch [10/50] batch [160/160] time 0.077 (0.086) data 0.000 (0.003) loss 1.5120 (1.4578) teacher_loss 0.5903 (0.4918) loss_zs_kd 0.1133 (0.1151) loss_oracle 0.8650 (0.9084) acc 75.0000 (81.8164) alaph_mean 0.0821 (0.0792) alpha_min 0.0000 (0.0000) alpha_max 0.5273 (0.5244) lr 1.8443e-03 eta 0:09:10
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,854
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 85.2%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,928
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 88.1%
******* Domain p best val acc:      84.0%, epoch: 10 *******
******* Domain p best val test acc: 86.7%, epoch: 10 *******
******* Domain p best test acc:     88.5%, epoch: 7 *******
epoch [11/50] batch [20/160] time 0.083 (0.096) data 0.000 (0.014) loss 1.2530 (1.4338) teacher_loss 0.3101 (0.4627) loss_zs_kd 0.1029 (0.1060) loss_oracle 0.8915 (0.9181) acc 90.6250 (83.1250) alaph_mean 0.0806 (0.0634) alpha_min 0.0000 (0.0000) alpha_max 0.5160 (0.5147) lr 1.8443e-03 eta 0:10:14
epoch [11/50] batch [40/160] time 0.096 (0.090) data 0.000 (0.007) loss 1.2645 (1.4277) teacher_loss 0.2522 (0.4574) loss_zs_kd 0.1228 (0.1126) loss_oracle 0.9509 (0.9140) acc 96.8750 (82.6562) alaph_mean 0.0488 (0.0645) alpha_min 0.0000 (0.0000) alpha_max 0.5534 (0.5130) lr 1.8443e-03 eta 0:09:33
epoch [11/50] batch [60/160] time 0.075 (0.095) data 0.001 (0.005) loss 1.3930 (1.4351) teacher_loss 0.3440 (0.4662) loss_zs_kd 0.1111 (0.1159) loss_oracle 0.9934 (0.9110) acc 93.7500 (83.1250) alaph_mean 0.0128 (0.0653) alpha_min 0.0000 (0.0000) alpha_max 0.2088 (0.5097) lr 1.8443e-03 eta 0:10:04
epoch [11/50] batch [80/160] time 0.080 (0.092) data 0.000 (0.004) loss 1.3910 (1.4376) teacher_loss 0.4299 (0.4718) loss_zs_kd 0.1242 (0.1169) loss_oracle 0.8990 (0.9074) acc 87.5000 (82.9688) alaph_mean 0.0635 (0.0674) alpha_min 0.0000 (0.0000) alpha_max 0.4489 (0.5116) lr 1.8443e-03 eta 0:09:39
epoch [11/50] batch [100/160] time 0.079 (0.090) data 0.000 (0.003) loss 1.2999 (1.4237) teacher_loss 0.3491 (0.4601) loss_zs_kd 0.1307 (0.1162) loss_oracle 0.8854 (0.9055) acc 87.5000 (83.6250) alaph_mean 0.0695 (0.0695) alpha_min 0.0000 (0.0000) alpha_max 0.4185 (0.5130) lr 1.8443e-03 eta 0:09:27
epoch [11/50] batch [120/160] time 0.082 (0.089) data 0.000 (0.003) loss 1.4110 (1.4208) teacher_loss 0.4038 (0.4612) loss_zs_kd 0.1016 (0.1155) loss_oracle 0.9564 (0.9019) acc 87.5000 (83.4375) alaph_mean 0.0545 (0.0711) alpha_min 0.0000 (0.0000) alpha_max 0.5001 (0.5121) lr 1.8443e-03 eta 0:09:20
epoch [11/50] batch [140/160] time 0.083 (0.088) data 0.000 (0.002) loss 1.3968 (1.4186) teacher_loss 0.4280 (0.4596) loss_zs_kd 0.1165 (0.1146) loss_oracle 0.9106 (0.9017) acc 81.2500 (83.2812) alaph_mean 0.0561 (0.0719) alpha_min 0.0000 (0.0000) alpha_max 0.4920 (0.5175) lr 1.8443e-03 eta 0:09:12
epoch [11/50] batch [160/160] time 0.075 (0.087) data 0.000 (0.002) loss 1.3939 (1.4156) teacher_loss 0.3289 (0.4553) loss_zs_kd 0.1437 (0.1156) loss_oracle 0.9932 (0.9025) acc 87.5000 (83.5547) alaph_mean 0.0332 (0.0722) alpha_min 0.0000 (0.0000) alpha_max 0.3855 (0.5195) lr 1.8090e-03 eta 0:09:03
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,859
* accuracy: 84.3%
* error: 15.7%
* macro_f1: 85.7%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,948
* accuracy: 87.3%
* error: 12.7%
* macro_f1: 88.5%
******* Domain p best val acc:      84.3%, epoch: 11 *******
******* Domain p best val test acc: 87.3%, epoch: 11 *******
******* Domain p best test acc:     88.5%, epoch: 7 *******
epoch [12/50] batch [20/160] time 0.079 (0.095) data 0.000 (0.014) loss 1.3953 (1.3757) teacher_loss 0.3533 (0.4259) loss_zs_kd 0.1196 (0.1207) loss_oracle 0.9822 (0.8894) acc 90.6250 (83.7500) alaph_mean 0.0418 (0.0779) alpha_min 0.0000 (0.0000) alpha_max 0.5001 (0.5093) lr 1.8090e-03 eta 0:09:51
epoch [12/50] batch [40/160] time 0.081 (0.087) data 0.000 (0.007) loss 1.4336 (1.3691) teacher_loss 0.4810 (0.4057) loss_zs_kd 0.1277 (0.1206) loss_oracle 0.8888 (0.9031) acc 84.3750 (85.3125) alaph_mean 0.0762 (0.0741) alpha_min 0.0000 (0.0000) alpha_max 0.5375 (0.5071) lr 1.8090e-03 eta 0:08:57
epoch [12/50] batch [60/160] time 0.086 (0.083) data 0.001 (0.005) loss 1.4327 (1.3997) teacher_loss 0.3953 (0.4333) loss_zs_kd 0.1296 (0.1246) loss_oracle 0.9726 (0.9041) acc 93.7500 (84.4271) alaph_mean 0.0544 (0.0741) alpha_min 0.0000 (0.0000) alpha_max 0.4992 (0.5092) lr 1.8090e-03 eta 0:08:35
epoch [12/50] batch [80/160] time 0.085 (0.083) data 0.000 (0.004) loss 1.3976 (1.4085) teacher_loss 0.3428 (0.4464) loss_zs_kd 0.1057 (0.1180) loss_oracle 1.0020 (0.9031) acc 81.2500 (83.6328) alaph_mean 0.0305 (0.0756) alpha_min 0.0000 (0.0000) alpha_max 0.5280 (0.5143) lr 1.8090e-03 eta 0:08:28
epoch [12/50] batch [100/160] time 0.080 (0.083) data 0.000 (0.003) loss 1.3470 (1.4114) teacher_loss 0.3711 (0.4508) loss_zs_kd 0.1272 (0.1190) loss_oracle 0.9124 (0.9011) acc 84.3750 (83.5938) alaph_mean 0.0763 (0.0757) alpha_min 0.0000 (0.0000) alpha_max 0.5319 (0.5130) lr 1.8090e-03 eta 0:08:27
epoch [12/50] batch [120/160] time 0.092 (0.082) data 0.000 (0.003) loss 1.3422 (1.4174) teacher_loss 0.4507 (0.4541) loss_zs_kd 0.0758 (0.1180) loss_oracle 0.8536 (0.9043) acc 87.5000 (83.2812) alaph_mean 0.0907 (0.0755) alpha_min 0.0000 (0.0000) alpha_max 0.5109 (0.5194) lr 1.8090e-03 eta 0:08:22
epoch [12/50] batch [140/160] time 0.098 (0.083) data 0.000 (0.002) loss 1.1779 (1.4186) teacher_loss 0.3193 (0.4540) loss_zs_kd 0.0913 (0.1169) loss_oracle 0.8129 (0.9061) acc 93.7500 (83.3929) alaph_mean 0.1158 (0.0744) alpha_min 0.0000 (0.0000) alpha_max 0.5276 (0.5192) lr 1.8090e-03 eta 0:08:24
epoch [12/50] batch [160/160] time 0.069 (0.082) data 0.000 (0.002) loss 1.4858 (1.4184) teacher_loss 0.5679 (0.4543) loss_zs_kd 0.1032 (0.1185) loss_oracle 0.8663 (0.9049) acc 84.3750 (83.3984) alaph_mean 0.0817 (0.0747) alpha_min 0.0000 (0.0000) alpha_max 0.4162 (0.5214) lr 1.7705e-03 eta 0:08:18
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,846
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 85.1%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,964
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 89.0%
******* Domain p best val acc:      84.3%, epoch: 11 *******
******* Domain p best val test acc: 87.3%, epoch: 11 *******
******* Domain p best test acc:     88.5%, epoch: 7 *******
epoch [13/50] batch [20/160] time 0.080 (0.116) data 0.000 (0.015) loss 1.2593 (1.4297) teacher_loss 0.3513 (0.4735) loss_zs_kd 0.0837 (0.1291) loss_oracle 0.8662 (0.8917) acc 93.7500 (83.2812) alaph_mean 0.0835 (0.0778) alpha_min 0.0000 (0.0000) alpha_max 0.4999 (0.5618) lr 1.7705e-03 eta 0:11:44
epoch [13/50] batch [40/160] time 0.077 (0.097) data 0.000 (0.007) loss 1.3206 (1.4124) teacher_loss 0.3728 (0.4628) loss_zs_kd 0.0894 (0.1236) loss_oracle 0.9031 (0.8878) acc 90.6250 (83.5156) alaph_mean 0.0763 (0.0823) alpha_min 0.0000 (0.0000) alpha_max 0.5557 (0.5529) lr 1.7705e-03 eta 0:09:44
epoch [13/50] batch [60/160] time 0.078 (0.091) data 0.000 (0.005) loss 1.5797 (1.4085) teacher_loss 0.6348 (0.4646) loss_zs_kd 0.0799 (0.1189) loss_oracle 0.9050 (0.8844) acc 75.0000 (83.8542) alaph_mean 0.0580 (0.0853) alpha_min 0.0000 (0.0000) alpha_max 0.4996 (0.5433) lr 1.7705e-03 eta 0:09:09
epoch [13/50] batch [80/160] time 0.077 (0.089) data 0.000 (0.004) loss 1.3723 (1.4253) teacher_loss 0.3059 (0.4694) loss_zs_kd 0.1426 (0.1191) loss_oracle 0.9951 (0.8963) acc 90.6250 (83.4766) alaph_mean 0.0503 (0.0834) alpha_min 0.0000 (0.0000) alpha_max 0.4996 (0.5408) lr 1.7705e-03 eta 0:08:54
epoch [13/50] batch [100/160] time 0.082 (0.088) data 0.000 (0.003) loss 1.2467 (1.4302) teacher_loss 0.3137 (0.4709) loss_zs_kd 0.0785 (0.1204) loss_oracle 0.8937 (0.8991) acc 90.6250 (83.3750) alaph_mean 0.1073 (0.0825) alpha_min 0.0000 (0.0000) alpha_max 0.6793 (0.5337) lr 1.7705e-03 eta 0:08:45
epoch [13/50] batch [120/160] time 0.082 (0.087) data 0.001 (0.003) loss 1.4495 (1.4302) teacher_loss 0.4255 (0.4704) loss_zs_kd 0.0832 (0.1175) loss_oracle 0.9823 (0.9010) acc 84.3750 (83.3594) alaph_mean 0.0401 (0.0817) alpha_min 0.0000 (0.0000) alpha_max 0.4974 (0.5301) lr 1.7705e-03 eta 0:08:39
epoch [13/50] batch [140/160] time 0.087 (0.086) data 0.000 (0.002) loss 1.4052 (1.4286) teacher_loss 0.4565 (0.4668) loss_zs_kd 0.0953 (0.1173) loss_oracle 0.9011 (0.9031) acc 81.2500 (83.4598) alaph_mean 0.0832 (0.0816) alpha_min 0.0000 (0.0000) alpha_max 0.5581 (0.5290) lr 1.7705e-03 eta 0:08:33
epoch [13/50] batch [160/160] time 0.069 (0.085) data 0.000 (0.002) loss 1.6067 (1.4300) teacher_loss 0.6050 (0.4659) loss_zs_kd 0.1137 (0.1166) loss_oracle 0.9448 (0.9059) acc 75.0000 (83.4570) alaph_mean 0.0633 (0.0817) alpha_min 0.0000 (0.0000) alpha_max 0.4762 (0.5306) lr 1.7290e-03 eta 0:08:21
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,855
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,995
* accuracy: 88.7%
* error: 11.3%
* macro_f1: 89.6%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      84.3%, epoch: 11 *******
******* Domain p best val test acc: 87.3%, epoch: 11 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [14/50] batch [20/160] time 0.086 (0.098) data 0.000 (0.013) loss 1.2518 (1.4101) teacher_loss 0.3137 (0.4540) loss_zs_kd 0.1289 (0.1144) loss_oracle 0.8737 (0.8989) acc 87.5000 (83.4375) alaph_mean 0.0875 (0.0793) alpha_min 0.0000 (0.0000) alpha_max 0.5514 (0.4851) lr 1.7290e-03 eta 0:09:35
epoch [14/50] batch [40/160] time 0.093 (0.090) data 0.000 (0.007) loss 1.5043 (1.4181) teacher_loss 0.5391 (0.4662) loss_zs_kd 0.1430 (0.1141) loss_oracle 0.8938 (0.8949) acc 81.2500 (82.7344) alaph_mean 0.0677 (0.0781) alpha_min 0.0000 (0.0000) alpha_max 0.4639 (0.4864) lr 1.7290e-03 eta 0:08:49
epoch [14/50] batch [60/160] time 0.086 (0.087) data 0.001 (0.005) loss 1.2220 (1.4062) teacher_loss 0.2991 (0.4477) loss_zs_kd 0.1125 (0.1178) loss_oracle 0.8667 (0.8996) acc 87.5000 (83.3333) alaph_mean 0.0952 (0.0757) alpha_min 0.0000 (0.0000) alpha_max 0.4928 (0.4835) lr 1.7290e-03 eta 0:08:30
epoch [14/50] batch [80/160] time 0.086 (0.086) data 0.000 (0.004) loss 1.5036 (1.4148) teacher_loss 0.4856 (0.4510) loss_zs_kd 0.1436 (0.1253) loss_oracle 0.9462 (0.9011) acc 84.3750 (83.0859) alaph_mean 0.0370 (0.0752) alpha_min 0.0000 (0.0000) alpha_max 0.3548 (0.5019) lr 1.7290e-03 eta 0:08:24
epoch [14/50] batch [100/160] time 0.089 (0.086) data 0.000 (0.003) loss 1.3365 (1.4023) teacher_loss 0.4756 (0.4398) loss_zs_kd 0.1233 (0.1320) loss_oracle 0.7993 (0.8965) acc 84.3750 (83.6562) alaph_mean 0.1095 (0.0769) alpha_min 0.0000 (0.0000) alpha_max 0.4996 (0.5228) lr 1.7290e-03 eta 0:08:20
epoch [14/50] batch [120/160] time 0.088 (0.086) data 0.000 (0.002) loss 1.5419 (1.4050) teacher_loss 0.5200 (0.4435) loss_zs_kd 0.1278 (0.1303) loss_oracle 0.9580 (0.8964) acc 81.2500 (83.7500) alaph_mean 0.0330 (0.0756) alpha_min 0.0000 (0.0000) alpha_max 0.4169 (0.5191) lr 1.7290e-03 eta 0:08:19
epoch [14/50] batch [140/160] time 0.083 (0.086) data 0.000 (0.002) loss 1.2634 (1.4035) teacher_loss 0.4133 (0.4429) loss_zs_kd 0.1083 (0.1286) loss_oracle 0.7959 (0.8963) acc 81.2500 (83.8170) alaph_mean 0.1192 (0.0749) alpha_min 0.0000 (0.0000) alpha_max 0.5543 (0.5165) lr 1.7290e-03 eta 0:08:16
epoch [14/50] batch [160/160] time 0.077 (0.085) data 0.000 (0.002) loss 1.2767 (1.4028) teacher_loss 0.2986 (0.4447) loss_zs_kd 0.0931 (0.1279) loss_oracle 0.9315 (0.8941) acc 84.3750 (83.5547) alaph_mean 0.0739 (0.0752) alpha_min 0.0000 (0.0000) alpha_max 0.5005 (0.5167) lr 1.6845e-03 eta 0:08:09
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,865
* accuracy: 84.5%
* error: 15.5%
* macro_f1: 85.7%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,993
* accuracy: 88.7%
* error: 11.3%
* macro_f1: 89.5%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [15/50] batch [20/160] time 0.072 (0.099) data 0.000 (0.014) loss 1.2745 (1.3623) teacher_loss 0.3740 (0.4578) loss_zs_kd 0.1313 (0.1082) loss_oracle 0.8349 (0.8504) acc 75.0000 (81.7188) alaph_mean 0.0748 (0.0840) alpha_min 0.0000 (0.0000) alpha_max 0.5310 (0.5289) lr 1.6845e-03 eta 0:09:25
epoch [15/50] batch [40/160] time 0.088 (0.093) data 0.000 (0.007) loss 1.3146 (1.3884) teacher_loss 0.3689 (0.4659) loss_zs_kd 0.1580 (0.1112) loss_oracle 0.8667 (0.8669) acc 87.5000 (82.7344) alaph_mean 0.0618 (0.0737) alpha_min 0.0000 (0.0000) alpha_max 0.4583 (0.5044) lr 1.6845e-03 eta 0:08:49
epoch [15/50] batch [60/160] time 0.074 (0.089) data 0.001 (0.005) loss 1.4340 (1.3806) teacher_loss 0.4961 (0.4509) loss_zs_kd 0.1509 (0.1145) loss_oracle 0.8624 (0.8724) acc 87.5000 (83.0208) alaph_mean 0.0813 (0.0705) alpha_min 0.0000 (0.0000) alpha_max 0.5274 (0.5082) lr 1.6845e-03 eta 0:08:29
epoch [15/50] batch [80/160] time 0.086 (0.088) data 0.000 (0.004) loss 1.3777 (1.3841) teacher_loss 0.4817 (0.4549) loss_zs_kd 0.0968 (0.1147) loss_oracle 0.8476 (0.8718) acc 81.2500 (82.8125) alaph_mean 0.0860 (0.0706) alpha_min 0.0000 (0.0000) alpha_max 0.5011 (0.5022) lr 1.6845e-03 eta 0:08:19
epoch [15/50] batch [100/160] time 0.075 (0.087) data 0.000 (0.003) loss 1.3707 (1.3840) teacher_loss 0.3989 (0.4525) loss_zs_kd 0.1361 (0.1115) loss_oracle 0.9037 (0.8757) acc 84.3750 (83.1250) alaph_mean 0.0624 (0.0692) alpha_min 0.0000 (0.0000) alpha_max 0.5026 (0.4951) lr 1.6845e-03 eta 0:08:14
epoch [15/50] batch [120/160] time 0.082 (0.087) data 0.000 (0.003) loss 1.4488 (1.3878) teacher_loss 0.3970 (0.4506) loss_zs_kd 0.1394 (0.1143) loss_oracle 0.9821 (0.8800) acc 84.3750 (83.0208) alaph_mean 0.0305 (0.0665) alpha_min 0.0000 (0.0000) alpha_max 0.5017 (0.4929) lr 1.6845e-03 eta 0:08:11
epoch [15/50] batch [140/160] time 0.106 (0.087) data 0.000 (0.002) loss 1.4577 (1.3875) teacher_loss 0.5103 (0.4513) loss_zs_kd 0.1281 (0.1151) loss_oracle 0.8834 (0.8786) acc 81.2500 (83.1250) alaph_mean 0.0798 (0.0668) alpha_min 0.0000 (0.0000) alpha_max 0.5011 (0.4925) lr 1.6845e-03 eta 0:08:09
epoch [15/50] batch [160/160] time 0.079 (0.086) data 0.000 (0.002) loss 1.4139 (1.3969) teacher_loss 0.5225 (0.4623) loss_zs_kd 0.1187 (0.1148) loss_oracle 0.8320 (0.8772) acc 81.2500 (82.7539) alaph_mean 0.1097 (0.0673) alpha_min 0.0000 (0.0000) alpha_max 0.5091 (0.4922) lr 1.6374e-03 eta 0:08:02
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,854
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,979
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 89.2%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [16/50] batch [20/160] time 0.083 (0.097) data 0.000 (0.013) loss 1.2304 (1.3905) teacher_loss 0.4072 (0.4601) loss_zs_kd 0.0826 (0.1090) loss_oracle 0.7819 (0.8759) acc 81.2500 (82.9688) alaph_mean 0.1380 (0.0800) alpha_min 0.0000 (0.0000) alpha_max 0.5601 (0.5393) lr 1.6374e-03 eta 0:09:01
epoch [16/50] batch [40/160] time 0.080 (0.089) data 0.000 (0.007) loss 1.1104 (1.3956) teacher_loss 0.2668 (0.4565) loss_zs_kd 0.0467 (0.1030) loss_oracle 0.8202 (0.8876) acc 90.6250 (82.9688) alaph_mean 0.1054 (0.0708) alpha_min 0.0000 (0.0000) alpha_max 0.6623 (0.5192) lr 1.6374e-03 eta 0:08:12
epoch [16/50] batch [60/160] time 0.088 (0.087) data 0.001 (0.005) loss 1.1833 (1.3875) teacher_loss 0.2881 (0.4434) loss_zs_kd 0.0851 (0.1097) loss_oracle 0.8527 (0.8892) acc 93.7500 (83.4375) alaph_mean 0.0564 (0.0687) alpha_min 0.0000 (0.0000) alpha_max 0.3915 (0.5172) lr 1.6374e-03 eta 0:08:02
epoch [16/50] batch [80/160] time 0.085 (0.087) data 0.000 (0.003) loss 1.5210 (1.3895) teacher_loss 0.4988 (0.4430) loss_zs_kd 0.1208 (0.1134) loss_oracle 0.9618 (0.8898) acc 87.5000 (83.5547) alaph_mean 0.0500 (0.0678) alpha_min 0.0000 (0.0000) alpha_max 0.4296 (0.5131) lr 1.6374e-03 eta 0:07:58
epoch [16/50] batch [100/160] time 0.084 (0.087) data 0.000 (0.003) loss 1.5102 (1.3966) teacher_loss 0.4766 (0.4517) loss_zs_kd 0.1468 (0.1153) loss_oracle 0.9602 (0.8873) acc 75.0000 (83.1250) alaph_mean 0.0455 (0.0682) alpha_min 0.0000 (0.0000) alpha_max 0.5001 (0.5097) lr 1.6374e-03 eta 0:07:55
epoch [16/50] batch [120/160] time 0.091 (0.086) data 0.000 (0.002) loss 1.4509 (1.4028) teacher_loss 0.4619 (0.4580) loss_zs_kd 0.0996 (0.1138) loss_oracle 0.9392 (0.8879) acc 81.2500 (82.8385) alaph_mean 0.0533 (0.0684) alpha_min 0.0000 (0.0000) alpha_max 0.5880 (0.5082) lr 1.6374e-03 eta 0:07:53
epoch [16/50] batch [140/160] time 0.063 (0.086) data 0.000 (0.002) loss 1.8548 (1.4066) teacher_loss 0.8750 (0.4589) loss_zs_kd 0.1076 (0.1129) loss_oracle 0.9260 (0.8912) acc 59.3750 (82.9464) alaph_mean 0.0611 (0.0687) alpha_min 0.0000 (0.0000) alpha_max 0.4449 (0.5103) lr 1.6374e-03 eta 0:07:49
epoch [16/50] batch [160/160] time 0.077 (0.084) data 0.000 (0.002) loss 1.2367 (1.4056) teacher_loss 0.2686 (0.4573) loss_zs_kd 0.0921 (0.1149) loss_oracle 0.9221 (0.8908) acc 93.7500 (83.3398) alaph_mean 0.0764 (0.0697) alpha_min 0.0000 (0.0000) alpha_max 0.5719 (0.5104) lr 1.5878e-03 eta 0:07:39
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,860
* accuracy: 84.3%
* error: 15.7%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,970
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 89.1%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [17/50] batch [20/160] time 0.080 (0.101) data 0.001 (0.018) loss 1.2227 (1.3497) teacher_loss 0.4358 (0.4119) loss_zs_kd 0.1071 (0.1079) loss_oracle 0.7334 (0.8838) acc 87.5000 (84.5312) alaph_mean 0.1613 (0.0924) alpha_min 0.0000 (0.0000) alpha_max 0.5512 (0.5535) lr 1.5878e-03 eta 0:09:09
epoch [17/50] batch [40/160] time 0.084 (0.091) data 0.000 (0.009) loss 1.2593 (1.3580) teacher_loss 0.2886 (0.4006) loss_zs_kd 0.1248 (0.1205) loss_oracle 0.9084 (0.8971) acc 87.5000 (84.1406) alaph_mean 0.0567 (0.0803) alpha_min 0.0000 (0.0000) alpha_max 0.4994 (0.5491) lr 1.5878e-03 eta 0:08:09
epoch [17/50] batch [60/160] time 0.083 (0.089) data 0.001 (0.006) loss 1.3898 (1.3661) teacher_loss 0.4065 (0.4110) loss_zs_kd 0.1546 (0.1216) loss_oracle 0.9060 (0.8943) acc 81.2500 (83.7500) alaph_mean 0.0923 (0.0770) alpha_min -0.0000 (0.0000) alpha_max 0.4574 (0.5321) lr 1.5878e-03 eta 0:07:58
epoch [17/50] batch [80/160] time 0.085 (0.088) data 0.000 (0.005) loss 1.6539 (1.3731) teacher_loss 0.6968 (0.4285) loss_zs_kd 0.1077 (0.1219) loss_oracle 0.9033 (0.8836) acc 68.7500 (83.2422) alaph_mean 0.0734 (0.0780) alpha_min -0.0000 (0.0000) alpha_max 0.5028 (0.5241) lr 1.5878e-03 eta 0:07:53
epoch [17/50] batch [100/160] time 0.081 (0.087) data 0.000 (0.004) loss 1.4966 (1.3858) teacher_loss 0.5757 (0.4360) loss_zs_kd 0.1226 (0.1226) loss_oracle 0.8596 (0.8885) acc 81.2500 (82.9375) alaph_mean 0.1224 (0.0768) alpha_min 0.0000 (0.0000) alpha_max 0.5178 (0.5185) lr 1.5878e-03 eta 0:07:46
epoch [17/50] batch [120/160] time 0.082 (0.086) data 0.000 (0.003) loss 1.4869 (1.3899) teacher_loss 0.6343 (0.4410) loss_zs_kd 0.1333 (0.1218) loss_oracle 0.7859 (0.8880) acc 78.1250 (82.9688) alaph_mean 0.1326 (0.0773) alpha_min 0.0000 (0.0000) alpha_max 0.7084 (0.5200) lr 1.5878e-03 eta 0:07:38
epoch [17/50] batch [140/160] time 0.100 (0.087) data 0.001 (0.003) loss 1.3802 (1.3967) teacher_loss 0.4509 (0.4484) loss_zs_kd 0.1336 (0.1220) loss_oracle 0.8625 (0.8873) acc 78.1250 (82.8125) alaph_mean 0.0809 (0.0769) alpha_min 0.0000 (0.0000) alpha_max 0.5167 (0.5211) lr 1.5878e-03 eta 0:07:38
epoch [17/50] batch [160/160] time 0.079 (0.085) data 0.000 (0.003) loss 1.5180 (1.3970) teacher_loss 0.5625 (0.4473) loss_zs_kd 0.0909 (0.1218) loss_oracle 0.9101 (0.8888) acc 84.3750 (83.1836) alaph_mean 0.0740 (0.0781) alpha_min 0.0000 (0.0000) alpha_max 0.4059 (0.5273) lr 1.5358e-03 eta 0:07:30
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,858
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,973
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 89.1%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [18/50] batch [20/160] time 0.083 (0.097) data 0.000 (0.016) loss 1.4594 (1.4803) teacher_loss 0.5352 (0.5079) loss_zs_kd 0.1153 (0.1240) loss_oracle 0.8666 (0.9103) acc 68.7500 (81.0938) alaph_mean 0.0814 (0.0820) alpha_min 0.0000 (0.0000) alpha_max 0.5452 (0.5235) lr 1.5358e-03 eta 0:08:28
epoch [18/50] batch [40/160] time 0.079 (0.088) data 0.000 (0.008) loss 1.4433 (1.4283) teacher_loss 0.5254 (0.4661) loss_zs_kd 0.1288 (0.1308) loss_oracle 0.8535 (0.8967) acc 84.3750 (82.5000) alaph_mean 0.1048 (0.0858) alpha_min 0.0000 (0.0000) alpha_max 0.7185 (0.5241) lr 1.5358e-03 eta 0:07:39
epoch [18/50] batch [60/160] time 0.083 (0.085) data 0.000 (0.005) loss 1.5666 (1.4053) teacher_loss 0.5933 (0.4507) loss_zs_kd 0.1121 (0.1259) loss_oracle 0.9173 (0.8916) acc 78.1250 (83.4896) alaph_mean 0.0746 (0.0877) alpha_min 0.0000 (0.0000) alpha_max 0.4935 (0.5282) lr 1.5358e-03 eta 0:07:24
epoch [18/50] batch [80/160] time 0.079 (0.084) data 0.000 (0.004) loss 1.2676 (1.4035) teacher_loss 0.3354 (0.4488) loss_zs_kd 0.1694 (0.1236) loss_oracle 0.8475 (0.8929) acc 96.8750 (84.0234) alaph_mean 0.0999 (0.0855) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5261) lr 1.5358e-03 eta 0:07:15
epoch [18/50] batch [100/160] time 0.084 (0.087) data 0.000 (0.003) loss 1.8774 (1.4066) teacher_loss 0.8740 (0.4535) loss_zs_kd 0.1197 (0.1239) loss_oracle 0.9436 (0.8912) acc 78.1250 (83.5625) alaph_mean 0.0522 (0.0828) alpha_min 0.0000 (0.0000) alpha_max 0.4992 (0.5227) lr 1.5358e-03 eta 0:07:30
epoch [18/50] batch [120/160] time 0.081 (0.087) data 0.000 (0.003) loss 1.4719 (1.4095) teacher_loss 0.4517 (0.4586) loss_zs_kd 0.1656 (0.1233) loss_oracle 0.9375 (0.8892) acc 81.2500 (83.1771) alaph_mean 0.0346 (0.0799) alpha_min 0.0000 (0.0000) alpha_max 0.3429 (0.5203) lr 1.5358e-03 eta 0:07:26
epoch [18/50] batch [140/160] time 0.083 (0.086) data 0.000 (0.002) loss 1.6264 (1.4010) teacher_loss 0.6948 (0.4528) loss_zs_kd 0.1175 (0.1219) loss_oracle 0.8728 (0.8872) acc 68.7500 (83.2812) alaph_mean 0.1087 (0.0802) alpha_min 0.0000 (0.0000) alpha_max 0.7318 (0.5307) lr 1.5358e-03 eta 0:07:23
epoch [18/50] batch [160/160] time 0.077 (0.085) data 0.000 (0.002) loss 1.7827 (1.4034) teacher_loss 0.8701 (0.4547) loss_zs_kd 0.1208 (0.1206) loss_oracle 0.8522 (0.8883) acc 68.7500 (83.2031) alaph_mean 0.1167 (0.0796) alpha_min 0.0000 (0.0000) alpha_max 0.6157 (0.5363) lr 1.4818e-03 eta 0:07:17
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,857
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,958
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 88.8%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [19/50] batch [20/160] time 0.089 (0.098) data 0.000 (0.013) loss 1.4706 (1.3896) teacher_loss 0.4487 (0.4422) loss_zs_kd 0.1241 (0.1070) loss_oracle 0.9598 (0.8939) acc 87.5000 (84.2188) alaph_mean 0.0413 (0.0859) alpha_min -0.0000 (0.0000) alpha_max 0.4245 (0.5478) lr 1.4818e-03 eta 0:08:21
epoch [19/50] batch [40/160] time 0.090 (0.091) data 0.000 (0.007) loss 1.5412 (1.4135) teacher_loss 0.5786 (0.4651) loss_zs_kd 0.1468 (0.1167) loss_oracle 0.8892 (0.8901) acc 78.1250 (83.4375) alaph_mean 0.0802 (0.0847) alpha_min 0.0000 (0.0000) alpha_max 0.5040 (0.5498) lr 1.4818e-03 eta 0:07:42
epoch [19/50] batch [60/160] time 0.085 (0.088) data 0.001 (0.005) loss 1.3141 (1.4165) teacher_loss 0.3562 (0.4674) loss_zs_kd 0.1031 (0.1182) loss_oracle 0.9064 (0.8900) acc 90.6250 (83.1771) alaph_mean 0.0551 (0.0830) alpha_min 0.0000 (0.0000) alpha_max 0.3691 (0.5559) lr 1.4818e-03 eta 0:07:26
epoch [19/50] batch [80/160] time 0.077 (0.087) data 0.000 (0.004) loss 1.6662 (1.4083) teacher_loss 0.6978 (0.4598) loss_zs_kd 0.1137 (0.1198) loss_oracle 0.9117 (0.8885) acc 78.1250 (83.4375) alaph_mean 0.0655 (0.0824) alpha_min 0.0000 (0.0000) alpha_max 0.5013 (0.5468) lr 1.4818e-03 eta 0:07:20
epoch [19/50] batch [100/160] time 0.106 (0.087) data 0.000 (0.003) loss 1.4346 (1.4266) teacher_loss 0.3408 (0.4630) loss_zs_kd 0.1926 (0.1257) loss_oracle 0.9974 (0.9007) acc 93.7500 (83.3750) alaph_mean 0.0585 (0.0810) alpha_min 0.0000 (0.0000) alpha_max 0.5672 (0.5404) lr 1.4818e-03 eta 0:07:14
epoch [19/50] batch [120/160] time 0.074 (0.086) data 0.000 (0.002) loss 1.6171 (1.4287) teacher_loss 0.6421 (0.4572) loss_zs_kd 0.1900 (0.1276) loss_oracle 0.8800 (0.9076) acc 84.3750 (83.5938) alaph_mean 0.1096 (0.0814) alpha_min 0.0000 (0.0000) alpha_max 0.5040 (0.5410) lr 1.4818e-03 eta 0:07:09
epoch [19/50] batch [140/160] time 0.072 (0.085) data 0.000 (0.002) loss 1.3699 (1.4302) teacher_loss 0.3928 (0.4582) loss_zs_kd 0.0964 (0.1244) loss_oracle 0.9289 (0.9098) acc 81.2500 (83.6384) alaph_mean 0.0824 (0.0830) alpha_min 0.0000 (0.0000) alpha_max 0.4995 (0.5433) lr 1.4818e-03 eta 0:07:05
epoch [19/50] batch [160/160] time 0.078 (0.085) data 0.000 (0.002) loss 1.4565 (1.4302) teacher_loss 0.4236 (0.4554) loss_zs_kd 0.1354 (0.1260) loss_oracle 0.9652 (0.9118) acc 84.3750 (83.5156) alaph_mean 0.0694 (0.0831) alpha_min 0.0000 (0.0000) alpha_max 0.4962 (0.5458) lr 1.4258e-03 eta 0:06:59
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,858
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,970
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 89.0%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [20/50] batch [20/160] time 0.071 (0.097) data 0.000 (0.017) loss 1.4072 (1.4126) teacher_loss 0.4707 (0.4286) loss_zs_kd 0.1105 (0.1396) loss_oracle 0.8813 (0.9142) acc 78.1250 (84.0625) alaph_mean 0.0980 (0.0813) alpha_min 0.0000 (0.0000) alpha_max 0.6046 (0.5485) lr 1.4258e-03 eta 0:07:57
epoch [20/50] batch [40/160] time 0.075 (0.097) data 0.000 (0.008) loss 1.4672 (1.4301) teacher_loss 0.5200 (0.4427) loss_zs_kd 0.1111 (0.1281) loss_oracle 0.8917 (0.9233) acc 81.2500 (83.5156) alaph_mean 0.0636 (0.0826) alpha_min 0.0000 (0.0000) alpha_max 0.5013 (0.5569) lr 1.4258e-03 eta 0:07:55
epoch [20/50] batch [60/160] time 0.090 (0.092) data 0.000 (0.006) loss 1.2394 (1.4294) teacher_loss 0.3604 (0.4451) loss_zs_kd 0.1182 (0.1246) loss_oracle 0.8199 (0.9220) acc 87.5000 (83.7500) alaph_mean 0.1121 (0.0806) alpha_min 0.0000 (0.0000) alpha_max 0.6277 (0.5417) lr 1.4258e-03 eta 0:07:29
epoch [20/50] batch [80/160] time 0.079 (0.089) data 0.000 (0.004) loss 1.4098 (1.4369) teacher_loss 0.5024 (0.4567) loss_zs_kd 0.1085 (0.1252) loss_oracle 0.8531 (0.9176) acc 87.5000 (83.7109) alaph_mean 0.1793 (0.0837) alpha_min 0.0000 (0.0000) alpha_max 0.6552 (0.5517) lr 1.4258e-03 eta 0:07:13
epoch [20/50] batch [100/160] time 0.083 (0.087) data 0.000 (0.004) loss 1.3379 (1.4313) teacher_loss 0.4126 (0.4556) loss_zs_kd 0.1059 (0.1212) loss_oracle 0.8723 (0.9152) acc 87.5000 (83.4688) alaph_mean 0.0973 (0.0852) alpha_min 0.0000 (0.0000) alpha_max 0.5014 (0.5539) lr 1.4258e-03 eta 0:07:05
epoch [20/50] batch [120/160] time 0.072 (0.086) data 0.000 (0.003) loss 1.2282 (1.4310) teacher_loss 0.4033 (0.4622) loss_zs_kd 0.1103 (0.1198) loss_oracle 0.7697 (0.9089) acc 90.6250 (83.2812) alaph_mean 0.1389 (0.0882) alpha_min 0.0000 (0.0000) alpha_max 0.5575 (0.5604) lr 1.4258e-03 eta 0:06:58
epoch [20/50] batch [140/160] time 0.083 (0.086) data 0.000 (0.003) loss 1.1779 (1.4173) teacher_loss 0.2852 (0.4516) loss_zs_kd 0.1038 (0.1201) loss_oracle 0.8408 (0.9057) acc 87.5000 (83.7054) alaph_mean 0.0881 (0.0891) alpha_min 0.0000 (0.0000) alpha_max 0.4999 (0.5642) lr 1.4258e-03 eta 0:06:53
epoch [20/50] batch [160/160] time 0.076 (0.084) data 0.000 (0.002) loss 1.3917 (1.4191) teacher_loss 0.4607 (0.4520) loss_zs_kd 0.0830 (0.1229) loss_oracle 0.8895 (0.9057) acc 78.1250 (83.5547) alaph_mean 0.1006 (0.0895) alpha_min 0.0000 (0.0000) alpha_max 0.4967 (0.5646) lr 1.3681e-03 eta 0:06:45
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,864
* accuracy: 84.5%
* error: 15.5%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,961
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 88.9%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [21/50] batch [20/160] time 0.079 (0.099) data 0.000 (0.013) loss 1.3852 (1.4028) teacher_loss 0.4578 (0.4293) loss_zs_kd 0.1376 (0.1165) loss_oracle 0.8587 (0.9152) acc 78.1250 (84.6875) alaph_mean 0.1011 (0.0884) alpha_min 0.0000 (0.0000) alpha_max 0.4975 (0.5661) lr 1.3681e-03 eta 0:07:51
epoch [21/50] batch [40/160] time 0.085 (0.092) data 0.000 (0.006) loss 1.2560 (1.4069) teacher_loss 0.2727 (0.4420) loss_zs_kd 0.1039 (0.1141) loss_oracle 0.9314 (0.9079) acc 90.6250 (83.5938) alaph_mean 0.0527 (0.0829) alpha_min 0.0000 (0.0000) alpha_max 0.5040 (0.5452) lr 1.3681e-03 eta 0:07:17
epoch [21/50] batch [60/160] time 0.088 (0.089) data 0.001 (0.004) loss 1.4340 (1.4115) teacher_loss 0.3926 (0.4474) loss_zs_kd 0.1287 (0.1131) loss_oracle 0.9771 (0.9075) acc 87.5000 (83.7500) alaph_mean 0.0518 (0.0818) alpha_min 0.0000 (0.0000) alpha_max 0.4775 (0.5399) lr 1.3681e-03 eta 0:07:00
epoch [21/50] batch [80/160] time 0.081 (0.088) data 0.000 (0.003) loss 1.5851 (1.4138) teacher_loss 0.5200 (0.4512) loss_zs_kd 0.0863 (0.1122) loss_oracle 1.0220 (0.9065) acc 81.2500 (83.3594) alaph_mean 0.0459 (0.0840) alpha_min -0.0000 (0.0000) alpha_max 0.5060 (0.5369) lr 1.3681e-03 eta 0:06:53
epoch [21/50] batch [100/160] time 0.083 (0.087) data 0.000 (0.003) loss 1.4677 (1.4216) teacher_loss 0.4395 (0.4508) loss_zs_kd 0.1425 (0.1143) loss_oracle 0.9570 (0.9136) acc 87.5000 (83.5625) alaph_mean 0.0834 (0.0838) alpha_min 0.0000 (0.0000) alpha_max 0.4982 (0.5330) lr 1.3681e-03 eta 0:06:47
epoch [21/50] batch [120/160] time 0.081 (0.086) data 0.000 (0.002) loss 1.4015 (1.4270) teacher_loss 0.4246 (0.4492) loss_zs_kd 0.1369 (0.1179) loss_oracle 0.9085 (0.9188) acc 84.3750 (83.6719) alaph_mean 0.0721 (0.0837) alpha_min 0.0000 (0.0000) alpha_max 0.5123 (0.5337) lr 1.3681e-03 eta 0:06:42
epoch [21/50] batch [140/160] time 0.084 (0.086) data 0.000 (0.002) loss 1.6347 (1.4290) teacher_loss 0.6309 (0.4473) loss_zs_kd 0.1824 (0.1216) loss_oracle 0.9127 (0.9208) acc 81.2500 (83.7500) alaph_mean 0.0910 (0.0839) alpha_min -0.0000 (0.0000) alpha_max 0.6240 (0.5365) lr 1.3681e-03 eta 0:06:40
epoch [21/50] batch [160/160] time 0.079 (0.085) data 0.001 (0.002) loss 1.3067 (1.4304) teacher_loss 0.4629 (0.4500) loss_zs_kd 0.1250 (0.1230) loss_oracle 0.7813 (0.9189) acc 78.1250 (83.6328) alaph_mean 0.1170 (0.0840) alpha_min 0.0000 (0.0000) alpha_max 0.5266 (0.5352) lr 1.3090e-03 eta 0:06:32
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,846
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 85.0%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,980
* accuracy: 88.3%
* error: 11.7%
* macro_f1: 89.3%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [22/50] batch [20/160] time 0.064 (0.091) data 0.000 (0.014) loss 1.4134 (1.4197) teacher_loss 0.4277 (0.4579) loss_zs_kd 0.1478 (0.1211) loss_oracle 0.9118 (0.9012) acc 84.3750 (84.0625) alaph_mean 0.0596 (0.0836) alpha_min 0.0000 (0.0000) alpha_max 0.4745 (0.5489) lr 1.3090e-03 eta 0:07:00
epoch [22/50] batch [40/160] time 0.075 (0.081) data 0.000 (0.007) loss 1.3662 (1.4304) teacher_loss 0.3835 (0.4674) loss_zs_kd 0.0854 (0.1128) loss_oracle 0.9400 (0.9066) acc 87.5000 (83.5156) alaph_mean 0.0551 (0.0774) alpha_min 0.0000 (0.0000) alpha_max 0.5608 (0.5211) lr 1.3090e-03 eta 0:06:11
epoch [22/50] batch [60/160] time 0.082 (0.081) data 0.001 (0.005) loss 1.2342 (1.4251) teacher_loss 0.2800 (0.4657) loss_zs_kd 0.0682 (0.1134) loss_oracle 0.9200 (0.9027) acc 87.5000 (83.4375) alaph_mean 0.0760 (0.0777) alpha_min 0.0000 (0.0000) alpha_max 0.5305 (0.5263) lr 1.3090e-03 eta 0:06:11
epoch [22/50] batch [80/160] time 0.090 (0.082) data 0.001 (0.004) loss 1.2141 (1.4181) teacher_loss 0.3079 (0.4623) loss_zs_kd 0.1814 (0.1159) loss_oracle 0.8155 (0.8979) acc 84.3750 (83.1250) alaph_mean 0.1185 (0.0795) alpha_min 0.0000 (0.0000) alpha_max 0.6948 (0.5325) lr 1.3090e-03 eta 0:06:13
epoch [22/50] batch [100/160] time 0.075 (0.081) data 0.000 (0.003) loss 1.4501 (1.4071) teacher_loss 0.6055 (0.4563) loss_zs_kd 0.0981 (0.1167) loss_oracle 0.7956 (0.8924) acc 84.3750 (82.8438) alaph_mean 0.1636 (0.0829) alpha_min 0.0000 (0.0000) alpha_max 0.5738 (0.5319) lr 1.3090e-03 eta 0:06:08
epoch [22/50] batch [120/160] time 0.087 (0.082) data 0.000 (0.003) loss 1.4175 (1.4081) teacher_loss 0.4966 (0.4557) loss_zs_kd 0.1214 (0.1168) loss_oracle 0.8603 (0.8941) acc 87.5000 (83.4375) alaph_mean 0.0907 (0.0819) alpha_min 0.0000 (0.0000) alpha_max 0.5244 (0.5315) lr 1.3090e-03 eta 0:06:09
epoch [22/50] batch [140/160] time 0.074 (0.081) data 0.000 (0.002) loss 1.2585 (1.4022) teacher_loss 0.3298 (0.4516) loss_zs_kd 0.1397 (0.1158) loss_oracle 0.8588 (0.8927) acc 87.5000 (83.4821) alaph_mean 0.1010 (0.0832) alpha_min 0.0000 (0.0000) alpha_max 0.5699 (0.5345) lr 1.3090e-03 eta 0:06:06
epoch [22/50] batch [160/160] time 0.073 (0.081) data 0.000 (0.002) loss 1.3644 (1.4058) teacher_loss 0.4878 (0.4531) loss_zs_kd 0.1236 (0.1170) loss_oracle 0.8148 (0.8942) acc 75.0000 (83.1055) alaph_mean 0.1197 (0.0825) alpha_min 0.0000 (0.0000) alpha_max 0.5015 (0.5321) lr 1.2487e-03 eta 0:06:03
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,856
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 85.1%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,986
* accuracy: 88.4%
* error: 11.6%
* macro_f1: 89.4%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [23/50] batch [20/160] time 0.079 (0.096) data 0.000 (0.015) loss 1.5429 (1.3949) teacher_loss 0.4241 (0.4291) loss_zs_kd 0.1967 (0.1112) loss_oracle 1.0205 (0.9102) acc 87.5000 (83.2812) alaph_mean 0.0238 (0.0770) alpha_min 0.0000 (0.0000) alpha_max 0.3210 (0.5242) lr 1.2487e-03 eta 0:07:06
epoch [23/50] batch [40/160] time 0.077 (0.086) data 0.000 (0.008) loss 1.4506 (1.4060) teacher_loss 0.4133 (0.4373) loss_zs_kd 0.1505 (0.1140) loss_oracle 0.9620 (0.9117) acc 81.2500 (83.5156) alaph_mean 0.0590 (0.0743) alpha_min 0.0000 (0.0000) alpha_max 0.5017 (0.5202) lr 1.2487e-03 eta 0:06:21
epoch [23/50] batch [60/160] time 0.076 (0.083) data 0.001 (0.005) loss 1.6021 (1.4308) teacher_loss 0.6729 (0.4561) loss_zs_kd 0.0977 (0.1192) loss_oracle 0.8803 (0.9151) acc 71.8750 (83.1771) alaph_mean 0.0961 (0.0725) alpha_min 0.0000 (0.0000) alpha_max 0.5011 (0.5173) lr 1.2487e-03 eta 0:06:05
epoch [23/50] batch [80/160] time 0.082 (0.081) data 0.000 (0.004) loss 1.5501 (1.4128) teacher_loss 0.4956 (0.4396) loss_zs_kd 0.1340 (0.1172) loss_oracle 0.9875 (0.9146) acc 84.3750 (83.9844) alaph_mean 0.0340 (0.0738) alpha_min 0.0000 (0.0000) alpha_max 0.5406 (0.5235) lr 1.2487e-03 eta 0:05:57
epoch [23/50] batch [100/160] time 0.099 (0.081) data 0.000 (0.003) loss 1.2714 (1.4244) teacher_loss 0.3733 (0.4482) loss_zs_kd 0.1355 (0.1189) loss_oracle 0.8304 (0.9168) acc 90.6250 (83.4375) alaph_mean 0.1410 (0.0724) alpha_min 0.0000 (0.0000) alpha_max 0.6831 (0.5208) lr 1.2487e-03 eta 0:05:56
epoch [23/50] batch [120/160] time 0.083 (0.082) data 0.000 (0.003) loss 1.3306 (1.4265) teacher_loss 0.3250 (0.4515) loss_zs_kd 0.0772 (0.1210) loss_oracle 0.9671 (0.9146) acc 90.6250 (83.3333) alaph_mean 0.0777 (0.0723) alpha_min 0.0000 (0.0000) alpha_max 0.5386 (0.5165) lr 1.2487e-03 eta 0:05:56
epoch [23/50] batch [140/160] time 0.094 (0.081) data 0.000 (0.002) loss 1.4690 (1.4237) teacher_loss 0.4443 (0.4507) loss_zs_kd 0.1479 (0.1216) loss_oracle 0.9507 (0.9122) acc 78.1250 (83.4375) alaph_mean 0.0667 (0.0740) alpha_min 0.0000 (0.0000) alpha_max 0.5011 (0.5173) lr 1.2487e-03 eta 0:05:52
epoch [23/50] batch [160/160] time 0.076 (0.081) data 0.000 (0.002) loss 1.4093 (1.4194) teacher_loss 0.3884 (0.4486) loss_zs_kd 0.1156 (0.1220) loss_oracle 0.9630 (0.9098) acc 84.3750 (83.5352) alaph_mean 0.0459 (0.0748) alpha_min 0.0000 (0.0000) alpha_max 0.5020 (0.5164) lr 1.1874e-03 eta 0:05:49
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,858
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.1%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,987
* accuracy: 88.5%
* error: 11.5%
* macro_f1: 89.4%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [24/50] batch [20/160] time 0.091 (0.101) data 0.000 (0.014) loss 1.4928 (1.4224) teacher_loss 0.5664 (0.4658) loss_zs_kd 0.0846 (0.1104) loss_oracle 0.8840 (0.9014) acc 75.0000 (83.2812) alaph_mean 0.1009 (0.0819) alpha_min 0.0000 (0.0000) alpha_max 0.5764 (0.5254) lr 1.1874e-03 eta 0:07:14
epoch [24/50] batch [40/160] time 0.088 (0.094) data 0.000 (0.007) loss 1.4832 (1.4187) teacher_loss 0.5000 (0.4691) loss_zs_kd 0.1186 (0.1101) loss_oracle 0.9239 (0.8946) acc 78.1250 (83.1250) alaph_mean 0.0430 (0.0792) alpha_min 0.0000 (0.0000) alpha_max 0.5449 (0.5166) lr 1.1874e-03 eta 0:06:43
epoch [24/50] batch [60/160] time 0.086 (0.092) data 0.001 (0.005) loss 1.5247 (1.4149) teacher_loss 0.6035 (0.4644) loss_zs_kd 0.1114 (0.1082) loss_oracle 0.8655 (0.8964) acc 87.5000 (83.0729) alaph_mean 0.0806 (0.0759) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5093) lr 1.1874e-03 eta 0:06:30
epoch [24/50] batch [80/160] time 0.088 (0.090) data 0.003 (0.004) loss 1.6095 (1.4101) teacher_loss 0.6855 (0.4602) loss_zs_kd 0.1035 (0.1052) loss_oracle 0.8722 (0.8974) acc 71.8750 (83.0078) alaph_mean 0.0858 (0.0754) alpha_min 0.0000 (0.0000) alpha_max 0.4912 (0.5079) lr 1.1874e-03 eta 0:06:21
epoch [24/50] batch [100/160] time 0.076 (0.089) data 0.000 (0.003) loss 1.5727 (1.4127) teacher_loss 0.5493 (0.4606) loss_zs_kd 0.1515 (0.1065) loss_oracle 0.9476 (0.8989) acc 71.8750 (82.7812) alaph_mean 0.0620 (0.0757) alpha_min 0.0000 (0.0000) alpha_max 0.5030 (0.5103) lr 1.1874e-03 eta 0:06:13
epoch [24/50] batch [120/160] time 0.086 (0.088) data 0.000 (0.003) loss 1.4157 (1.4089) teacher_loss 0.5239 (0.4579) loss_zs_kd 0.1173 (0.1079) loss_oracle 0.8332 (0.8970) acc 90.6250 (82.7344) alaph_mean 0.1242 (0.0781) alpha_min 0.0000 (0.0000) alpha_max 0.6088 (0.5166) lr 1.1874e-03 eta 0:06:08
epoch [24/50] batch [140/160] time 0.082 (0.088) data 0.000 (0.002) loss 1.5957 (1.4144) teacher_loss 0.6489 (0.4606) loss_zs_kd 0.1133 (0.1106) loss_oracle 0.8901 (0.8985) acc 68.7500 (82.6562) alaph_mean 0.1068 (0.0786) alpha_min 0.0000 (0.0000) alpha_max 0.5291 (0.5194) lr 1.1874e-03 eta 0:06:06
epoch [24/50] batch [160/160] time 0.075 (0.086) data 0.000 (0.002) loss 1.4008 (1.4175) teacher_loss 0.4546 (0.4579) loss_zs_kd 0.1118 (0.1138) loss_oracle 0.8903 (0.9027) acc 78.1250 (82.7344) alaph_mean 0.0885 (0.0784) alpha_min -0.0000 (0.0000) alpha_max 0.5486 (0.5195) lr 1.1253e-03 eta 0:05:58
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,864
* accuracy: 84.5%
* error: 15.5%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,955
* accuracy: 87.5%
* error: 12.5%
* macro_f1: 88.7%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [25/50] batch [20/160] time 0.082 (0.088) data 0.000 (0.015) loss 1.4065 (1.3743) teacher_loss 0.4622 (0.4328) loss_zs_kd 0.0878 (0.1170) loss_oracle 0.9005 (0.8830) acc 87.5000 (84.3750) alaph_mean 0.0851 (0.0964) alpha_min 0.0000 (0.0000) alpha_max 0.5036 (0.5637) lr 1.1253e-03 eta 0:06:02
epoch [25/50] batch [40/160] time 0.068 (0.080) data 0.000 (0.008) loss 1.3217 (1.4003) teacher_loss 0.3533 (0.4372) loss_zs_kd 0.1157 (0.1157) loss_oracle 0.9106 (0.9052) acc 87.5000 (84.5312) alaph_mean 0.0694 (0.0839) alpha_min 0.0000 (0.0000) alpha_max 0.5433 (0.5399) lr 1.1253e-03 eta 0:05:30
epoch [25/50] batch [60/160] time 0.072 (0.076) data 0.001 (0.005) loss 1.1872 (1.3972) teacher_loss 0.2852 (0.4349) loss_zs_kd 0.1023 (0.1207) loss_oracle 0.8509 (0.9019) acc 90.6250 (84.6354) alaph_mean 0.1218 (0.0832) alpha_min 0.0000 (0.0000) alpha_max 0.5112 (0.5363) lr 1.1253e-03 eta 0:05:12
epoch [25/50] batch [80/160] time 0.066 (0.075) data 0.000 (0.004) loss 1.3020 (1.3834) teacher_loss 0.3132 (0.4213) loss_zs_kd 0.1388 (0.1214) loss_oracle 0.9193 (0.9015) acc 90.6250 (85.6250) alaph_mean 0.0685 (0.0823) alpha_min 0.0000 (0.0000) alpha_max 0.7082 (0.5358) lr 1.1253e-03 eta 0:05:05
epoch [25/50] batch [100/160] time 0.073 (0.074) data 0.000 (0.003) loss 1.5222 (1.4018) teacher_loss 0.4629 (0.4368) loss_zs_kd 0.1032 (0.1225) loss_oracle 1.0077 (0.9038) acc 81.2500 (84.7188) alaph_mean 0.0435 (0.0808) alpha_min 0.0000 (0.0000) alpha_max 0.5013 (0.5390) lr 1.1253e-03 eta 0:04:59
epoch [25/50] batch [120/160] time 0.096 (0.075) data 0.000 (0.003) loss 1.3984 (1.4121) teacher_loss 0.3860 (0.4408) loss_zs_kd 0.1493 (0.1259) loss_oracle 0.9378 (0.9083) acc 87.5000 (84.3750) alaph_mean 0.0522 (0.0774) alpha_min 0.0000 (0.0000) alpha_max 0.4979 (0.5278) lr 1.1253e-03 eta 0:05:04
epoch [25/50] batch [140/160] time 0.083 (0.079) data 0.000 (0.002) loss 1.2885 (1.4146) teacher_loss 0.3591 (0.4392) loss_zs_kd 0.0888 (0.1272) loss_oracle 0.8850 (0.9118) acc 84.3750 (84.2411) alaph_mean 0.0816 (0.0765) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5311) lr 1.1253e-03 eta 0:05:19
epoch [25/50] batch [160/160] time 0.076 (0.079) data 0.000 (0.002) loss 1.4737 (1.4177) teacher_loss 0.5293 (0.4434) loss_zs_kd 0.1097 (0.1268) loss_oracle 0.8896 (0.9109) acc 78.1250 (84.0039) alaph_mean 0.0919 (0.0772) alpha_min 0.0000 (0.0000) alpha_max 0.4977 (0.5311) lr 1.0628e-03 eta 0:05:17
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,865
* accuracy: 84.5%
* error: 15.5%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,973
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 89.1%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [26/50] batch [20/160] time 0.077 (0.105) data 0.000 (0.020) loss 1.2330 (1.4278) teacher_loss 0.4043 (0.4619) loss_zs_kd 0.1523 (0.1230) loss_oracle 0.7526 (0.9045) acc 81.2500 (80.4688) alaph_mean 0.1262 (0.0723) alpha_min 0.0000 (0.0000) alpha_max 0.6126 (0.5360) lr 1.0628e-03 eta 0:06:59
epoch [26/50] batch [40/160] time 0.074 (0.090) data 0.000 (0.010) loss 1.4685 (1.4216) teacher_loss 0.4995 (0.4610) loss_zs_kd 0.1444 (0.1257) loss_oracle 0.8968 (0.8978) acc 84.3750 (81.7969) alaph_mean 0.0667 (0.0778) alpha_min 0.0000 (0.0000) alpha_max 0.5332 (0.5561) lr 1.0628e-03 eta 0:05:57
epoch [26/50] batch [60/160] time 0.082 (0.084) data 0.000 (0.007) loss 1.3145 (1.4090) teacher_loss 0.3765 (0.4590) loss_zs_kd 0.1171 (0.1249) loss_oracle 0.8795 (0.8876) acc 84.3750 (82.3958) alaph_mean 0.1329 (0.0835) alpha_min 0.0000 (0.0000) alpha_max 0.8407 (0.5749) lr 1.0628e-03 eta 0:05:30
epoch [26/50] batch [80/160] time 0.069 (0.081) data 0.000 (0.005) loss 1.3317 (1.4008) teacher_loss 0.3652 (0.4492) loss_zs_kd 0.1878 (0.1264) loss_oracle 0.8726 (0.8884) acc 81.2500 (83.1641) alaph_mean 0.0651 (0.0817) alpha_min 0.0000 (0.0000) alpha_max 0.5851 (0.5690) lr 1.0628e-03 eta 0:05:16
epoch [26/50] batch [100/160] time 0.072 (0.079) data 0.000 (0.004) loss 1.2562 (1.3932) teacher_loss 0.3391 (0.4450) loss_zs_kd 0.0952 (0.1239) loss_oracle 0.8695 (0.8862) acc 87.5000 (83.5938) alaph_mean 0.0802 (0.0827) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.5711) lr 1.0628e-03 eta 0:05:09
epoch [26/50] batch [120/160] time 0.083 (0.079) data 0.000 (0.004) loss 1.5441 (1.3838) teacher_loss 0.4822 (0.4341) loss_zs_kd 0.1599 (0.1251) loss_oracle 0.9819 (0.8871) acc 84.3750 (84.2188) alaph_mean 0.0526 (0.0834) alpha_min 0.0000 (0.0000) alpha_max 0.4517 (0.5728) lr 1.0628e-03 eta 0:05:08
epoch [26/50] batch [140/160] time 0.068 (0.079) data 0.000 (0.003) loss 1.5203 (1.3858) teacher_loss 0.5508 (0.4347) loss_zs_kd 0.1375 (0.1275) loss_oracle 0.9008 (0.8874) acc 78.1250 (84.1071) alaph_mean 0.0613 (0.0823) alpha_min 0.0000 (0.0000) alpha_max 0.4686 (0.5611) lr 1.0628e-03 eta 0:05:03
epoch [26/50] batch [160/160] time 0.074 (0.078) data 0.000 (0.003) loss 1.4874 (1.3879) teacher_loss 0.5259 (0.4364) loss_zs_kd 0.1337 (0.1284) loss_oracle 0.8947 (0.8874) acc 81.2500 (84.0234) alaph_mean 0.0858 (0.0810) alpha_min 0.0000 (0.0000) alpha_max 0.5080 (0.5521) lr 1.0000e-03 eta 0:05:00
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,858
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,968
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 89.0%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [27/50] batch [20/160] time 0.084 (0.098) data 0.000 (0.017) loss 1.2874 (1.3897) teacher_loss 0.2825 (0.4209) loss_zs_kd 0.1679 (0.1224) loss_oracle 0.9210 (0.9077) acc 93.7500 (85.9375) alaph_mean 0.0522 (0.0692) alpha_min 0.0000 (0.0000) alpha_max 0.4247 (0.5304) lr 1.0000e-03 eta 0:06:16
epoch [27/50] batch [40/160] time 0.071 (0.087) data 0.000 (0.009) loss 1.5440 (1.3908) teacher_loss 0.5703 (0.4328) loss_zs_kd 0.1164 (0.1195) loss_oracle 0.9155 (0.8982) acc 81.2500 (84.7656) alaph_mean 0.0754 (0.0729) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.5430) lr 1.0000e-03 eta 0:05:32
epoch [27/50] batch [60/160] time 0.073 (0.083) data 0.001 (0.006) loss 1.2022 (1.3980) teacher_loss 0.2400 (0.4368) loss_zs_kd 0.1347 (0.1195) loss_oracle 0.8948 (0.9015) acc 87.5000 (83.9583) alaph_mean 0.0600 (0.0709) alpha_min 0.0000 (0.0000) alpha_max 0.5015 (0.5322) lr 1.0000e-03 eta 0:05:15
epoch [27/50] batch [80/160] time 0.083 (0.083) data 0.000 (0.005) loss 1.4315 (1.4062) teacher_loss 0.4126 (0.4374) loss_zs_kd 0.1085 (0.1191) loss_oracle 0.9646 (0.9092) acc 81.2500 (83.7891) alaph_mean 0.0207 (0.0664) alpha_min 0.0000 (0.0000) alpha_max 0.4845 (0.5266) lr 1.0000e-03 eta 0:05:12
epoch [27/50] batch [100/160] time 0.077 (0.086) data 0.000 (0.004) loss 1.3413 (1.4076) teacher_loss 0.3540 (0.4385) loss_zs_kd 0.1610 (0.1180) loss_oracle 0.9068 (0.9101) acc 84.3750 (83.6562) alaph_mean 0.0729 (0.0646) alpha_min 0.0000 (0.0000) alpha_max 0.6064 (0.5177) lr 1.0000e-03 eta 0:05:23
epoch [27/50] batch [120/160] time 0.085 (0.086) data 0.000 (0.003) loss 1.3547 (1.4079) teacher_loss 0.3286 (0.4372) loss_zs_kd 0.0926 (0.1194) loss_oracle 0.9798 (0.9111) acc 87.5000 (83.6719) alaph_mean 0.0419 (0.0641) alpha_min 0.0000 (0.0000) alpha_max 0.4998 (0.5108) lr 1.0000e-03 eta 0:05:18
epoch [27/50] batch [140/160] time 0.091 (0.086) data 0.000 (0.003) loss 1.4290 (1.4080) teacher_loss 0.4395 (0.4411) loss_zs_kd 0.1279 (0.1204) loss_oracle 0.9256 (0.9067) acc 84.3750 (83.5491) alaph_mean 0.0466 (0.0653) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.5059) lr 1.0000e-03 eta 0:05:18
epoch [27/50] batch [160/160] time 0.077 (0.085) data 0.000 (0.002) loss 1.7596 (1.4057) teacher_loss 0.7764 (0.4407) loss_zs_kd 0.1208 (0.1201) loss_oracle 0.9228 (0.9049) acc 65.6250 (83.7500) alaph_mean 0.0587 (0.0663) alpha_min 0.0000 (0.0000) alpha_max 0.5007 (0.5049) lr 9.3721e-04 eta 0:05:13
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,853
* accuracy: 84.0%
* error: 16.0%
* macro_f1: 85.0%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,972
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 89.0%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [28/50] batch [20/160] time 0.071 (0.099) data 0.000 (0.017) loss 1.3331 (1.3843) teacher_loss 0.3440 (0.4221) loss_zs_kd 0.1102 (0.1311) loss_oracle 0.9340 (0.8966) acc 90.6250 (85.1562) alaph_mean 0.0632 (0.0682) alpha_min 0.0000 (0.0000) alpha_max 0.6286 (0.5280) lr 9.3721e-04 eta 0:06:01
epoch [28/50] batch [40/160] time 0.078 (0.090) data 0.000 (0.008) loss 1.4957 (1.3684) teacher_loss 0.5474 (0.4379) loss_zs_kd 0.1151 (0.1253) loss_oracle 0.8908 (0.8679) acc 84.3750 (84.6094) alaph_mean 0.0718 (0.0812) alpha_min 0.0000 (0.0000) alpha_max 0.5661 (0.5335) lr 9.3721e-04 eta 0:05:28
epoch [28/50] batch [60/160] time 0.090 (0.088) data 0.001 (0.006) loss 1.1717 (1.3773) teacher_loss 0.2634 (0.4391) loss_zs_kd 0.1143 (0.1250) loss_oracle 0.8511 (0.8758) acc 90.6250 (84.3229) alaph_mean 0.0663 (0.0774) alpha_min 0.0000 (0.0000) alpha_max 0.4971 (0.5192) lr 9.3721e-04 eta 0:05:18
epoch [28/50] batch [80/160] time 0.083 (0.087) data 0.000 (0.004) loss 1.2506 (1.3733) teacher_loss 0.3113 (0.4326) loss_zs_kd 0.1337 (0.1247) loss_oracle 0.8724 (0.8784) acc 84.3750 (84.3359) alaph_mean 0.0997 (0.0770) alpha_min 0.0000 (0.0000) alpha_max 0.6731 (0.5142) lr 9.3721e-04 eta 0:05:11
epoch [28/50] batch [100/160] time 0.083 (0.086) data 0.000 (0.004) loss 1.2740 (1.3802) teacher_loss 0.3196 (0.4374) loss_zs_kd 0.1291 (0.1245) loss_oracle 0.8898 (0.8807) acc 84.3750 (84.3750) alaph_mean 0.0850 (0.0767) alpha_min 0.0000 (0.0000) alpha_max 0.4978 (0.5188) lr 9.3721e-04 eta 0:05:08
epoch [28/50] batch [120/160] time 0.083 (0.086) data 0.000 (0.003) loss 1.2738 (1.3787) teacher_loss 0.4578 (0.4379) loss_zs_kd 0.0885 (0.1244) loss_oracle 0.7717 (0.8785) acc 81.2500 (84.2188) alaph_mean 0.1478 (0.0777) alpha_min 0.0000 (0.0000) alpha_max 0.5035 (0.5178) lr 9.3721e-04 eta 0:05:05
epoch [28/50] batch [140/160] time 0.088 (0.086) data 0.000 (0.003) loss 1.4144 (1.3798) teacher_loss 0.3940 (0.4346) loss_zs_kd 0.1602 (0.1248) loss_oracle 0.9403 (0.8828) acc 84.3750 (84.3973) alaph_mean 0.0695 (0.0763) alpha_min 0.0000 (0.0000) alpha_max 0.5026 (0.5149) lr 9.3721e-04 eta 0:05:03
epoch [28/50] batch [160/160] time 0.074 (0.085) data 0.000 (0.002) loss 1.1982 (1.3844) teacher_loss 0.3491 (0.4372) loss_zs_kd 0.0944 (0.1253) loss_oracle 0.8019 (0.8845) acc 84.3750 (84.1992) alaph_mean 0.0792 (0.0758) alpha_min 0.0000 (0.0000) alpha_max 0.4348 (0.5190) lr 8.7467e-04 eta 0:04:57
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,860
* accuracy: 84.3%
* error: 15.7%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,967
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 89.0%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [29/50] batch [20/160] time 0.071 (0.091) data 0.000 (0.016) loss 1.5244 (1.3821) teacher_loss 0.4751 (0.4399) loss_zs_kd 0.2229 (0.1303) loss_oracle 0.9379 (0.8770) acc 81.2500 (84.5312) alaph_mean 0.0653 (0.0766) alpha_min 0.0000 (0.0000) alpha_max 0.5009 (0.5317) lr 8.7467e-04 eta 0:05:18
epoch [29/50] batch [40/160] time 0.127 (0.083) data 0.000 (0.008) loss 1.3514 (1.3578) teacher_loss 0.4072 (0.4151) loss_zs_kd 0.1093 (0.1327) loss_oracle 0.8895 (0.8763) acc 90.6250 (85.6250) alaph_mean 0.0709 (0.0763) alpha_min 0.0000 (0.0000) alpha_max 0.5582 (0.5526) lr 8.7467e-04 eta 0:04:49
epoch [29/50] batch [60/160] time 0.074 (0.087) data 0.001 (0.006) loss 1.3759 (1.3450) teacher_loss 0.4827 (0.4051) loss_zs_kd 0.1098 (0.1312) loss_oracle 0.8384 (0.8743) acc 75.0000 (85.1042) alaph_mean 0.0625 (0.0754) alpha_min 0.0000 (0.0000) alpha_max 0.5038 (0.5469) lr 8.7467e-04 eta 0:05:02
epoch [29/50] batch [80/160] time 0.079 (0.085) data 0.000 (0.004) loss 1.2555 (1.3520) teacher_loss 0.3181 (0.4123) loss_zs_kd 0.1047 (0.1287) loss_oracle 0.8850 (0.8754) acc 90.6250 (84.9609) alaph_mean 0.0614 (0.0770) alpha_min 0.0000 (0.0000) alpha_max 0.4070 (0.5538) lr 8.7467e-04 eta 0:04:50
epoch [29/50] batch [100/160] time 0.069 (0.083) data 0.000 (0.003) loss 1.4534 (1.3629) teacher_loss 0.4482 (0.4201) loss_zs_kd 0.1397 (0.1265) loss_oracle 0.9353 (0.8796) acc 84.3750 (84.6562) alaph_mean 0.0595 (0.0768) alpha_min 0.0000 (0.0000) alpha_max 0.4771 (0.5508) lr 8.7467e-04 eta 0:04:44
epoch [29/50] batch [120/160] time 0.078 (0.082) data 0.000 (0.003) loss 1.3440 (1.3731) teacher_loss 0.4341 (0.4293) loss_zs_kd 0.1537 (0.1267) loss_oracle 0.8330 (0.8804) acc 78.1250 (84.0625) alaph_mean 0.1222 (0.0769) alpha_min -0.0000 (0.0000) alpha_max 0.6800 (0.5491) lr 8.7467e-04 eta 0:04:38
epoch [29/50] batch [140/160] time 0.087 (0.082) data 0.000 (0.003) loss 1.5491 (1.3772) teacher_loss 0.6509 (0.4366) loss_zs_kd 0.0810 (0.1266) loss_oracle 0.8577 (0.8773) acc 78.1250 (83.9732) alaph_mean 0.0665 (0.0776) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5452) lr 8.7467e-04 eta 0:04:37
epoch [29/50] batch [160/160] time 0.075 (0.082) data 0.000 (0.002) loss 1.4257 (1.3796) teacher_loss 0.4224 (0.4393) loss_zs_kd 0.1149 (0.1257) loss_oracle 0.9459 (0.8774) acc 78.1250 (83.8672) alaph_mean 0.0343 (0.0782) alpha_min -0.0000 (0.0000) alpha_max 0.5475 (0.5538) lr 8.1262e-04 eta 0:04:34
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,857
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,948
* accuracy: 87.3%
* error: 12.7%
* macro_f1: 88.6%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [30/50] batch [20/160] time 0.081 (0.090) data 0.000 (0.013) loss 1.6902 (1.4326) teacher_loss 0.6802 (0.4856) loss_zs_kd 0.1503 (0.1139) loss_oracle 0.9349 (0.8901) acc 75.0000 (82.9688) alaph_mean 0.0520 (0.0687) alpha_min 0.0000 (0.0000) alpha_max 0.5089 (0.5294) lr 8.1262e-04 eta 0:04:59
epoch [30/50] batch [40/160] time 0.082 (0.090) data 0.000 (0.006) loss 1.2936 (1.3688) teacher_loss 0.4988 (0.4408) loss_zs_kd 0.0938 (0.1158) loss_oracle 0.7479 (0.8701) acc 75.0000 (84.5312) alaph_mean 0.1200 (0.0790) alpha_min 0.0000 (0.0000) alpha_max 0.5037 (0.5283) lr 8.1262e-04 eta 0:04:58
epoch [30/50] batch [60/160] time 0.086 (0.087) data 0.001 (0.004) loss 1.2472 (1.3738) teacher_loss 0.3159 (0.4455) loss_zs_kd 0.0866 (0.1150) loss_oracle 0.8880 (0.8708) acc 84.3750 (83.8021) alaph_mean 0.0855 (0.0829) alpha_min 0.0000 (0.0000) alpha_max 0.5010 (0.5359) lr 8.1262e-04 eta 0:04:48
epoch [30/50] batch [80/160] time 0.080 (0.087) data 0.000 (0.003) loss 1.5616 (1.3933) teacher_loss 0.6011 (0.4594) loss_zs_kd 0.1329 (0.1175) loss_oracle 0.8941 (0.8751) acc 75.0000 (83.5156) alaph_mean 0.0651 (0.0820) alpha_min 0.0000 (0.0000) alpha_max 0.5072 (0.5410) lr 8.1262e-04 eta 0:04:44
epoch [30/50] batch [100/160] time 0.089 (0.087) data 0.000 (0.003) loss 1.3048 (1.3843) teacher_loss 0.4121 (0.4522) loss_zs_kd 0.1347 (0.1183) loss_oracle 0.8254 (0.8730) acc 84.3750 (83.5000) alaph_mean 0.1076 (0.0833) alpha_min 0.0000 (0.0000) alpha_max 0.5034 (0.5529) lr 8.1262e-04 eta 0:04:43
epoch [30/50] batch [120/160] time 0.084 (0.087) data 0.000 (0.002) loss 1.2159 (1.3795) teacher_loss 0.2493 (0.4472) loss_zs_kd 0.0882 (0.1187) loss_oracle 0.9225 (0.8730) acc 93.7500 (83.8802) alaph_mean 0.0719 (0.0829) alpha_min 0.0000 (0.0000) alpha_max 0.6210 (0.5499) lr 8.1262e-04 eta 0:04:41
epoch [30/50] batch [140/160] time 0.085 (0.087) data 0.000 (0.002) loss 1.4278 (1.3797) teacher_loss 0.4753 (0.4443) loss_zs_kd 0.1250 (0.1191) loss_oracle 0.8899 (0.8759) acc 84.3750 (84.2188) alaph_mean 0.0788 (0.0806) alpha_min 0.0000 (0.0000) alpha_max 0.4991 (0.5458) lr 8.1262e-04 eta 0:04:41
epoch [30/50] batch [160/160] time 0.075 (0.086) data 0.000 (0.002) loss 1.6058 (1.3822) teacher_loss 0.6362 (0.4463) loss_zs_kd 0.1244 (0.1197) loss_oracle 0.9073 (0.8761) acc 71.8750 (84.0039) alaph_mean 0.0541 (0.0807) alpha_min 0.0000 (0.0000) alpha_max 0.4689 (0.5447) lr 7.5131e-04 eta 0:04:35
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,858
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,970
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 89.1%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [31/50] batch [20/160] time 0.080 (0.095) data 0.000 (0.017) loss 1.5841 (1.3794) teacher_loss 0.6050 (0.4191) loss_zs_kd 0.1579 (0.1227) loss_oracle 0.9002 (0.8990) acc 78.1250 (84.2188) alaph_mean 0.0475 (0.0660) alpha_min 0.0000 (0.0000) alpha_max 0.5003 (0.5234) lr 7.5131e-04 eta 0:05:02
epoch [31/50] batch [40/160] time 0.075 (0.085) data 0.000 (0.009) loss 1.3911 (1.4097) teacher_loss 0.3157 (0.4427) loss_zs_kd 0.1291 (0.1345) loss_oracle 1.0108 (0.8998) acc 87.5000 (83.6719) alaph_mean 0.0406 (0.0691) alpha_min 0.0000 (0.0000) alpha_max 0.4990 (0.5195) lr 7.5131e-04 eta 0:04:27
epoch [31/50] batch [60/160] time 0.086 (0.084) data 0.001 (0.006) loss 1.3693 (1.3940) teacher_loss 0.3962 (0.4197) loss_zs_kd 0.1658 (0.1353) loss_oracle 0.8901 (0.9066) acc 87.5000 (84.5833) alaph_mean 0.0772 (0.0701) alpha_min 0.0000 (0.0000) alpha_max 0.5538 (0.5280) lr 7.5131e-04 eta 0:04:23
epoch [31/50] batch [80/160] time 0.080 (0.084) data 0.000 (0.004) loss 1.2993 (1.3987) teacher_loss 0.2896 (0.4207) loss_zs_kd 0.1747 (0.1388) loss_oracle 0.9224 (0.9086) acc 90.6250 (84.2969) alaph_mean 0.0562 (0.0733) alpha_min 0.0000 (0.0000) alpha_max 0.4976 (0.5368) lr 7.5131e-04 eta 0:04:22
epoch [31/50] batch [100/160] time 0.084 (0.084) data 0.000 (0.004) loss 1.2691 (1.3972) teacher_loss 0.2079 (0.4166) loss_zs_kd 0.1131 (0.1390) loss_oracle 1.0046 (0.9111) acc 100.0000 (84.8750) alaph_mean 0.0748 (0.0749) alpha_min 0.0000 (0.0000) alpha_max 0.6668 (0.5467) lr 7.5131e-04 eta 0:04:21
epoch [31/50] batch [120/160] time 0.079 (0.085) data 0.000 (0.003) loss 1.5579 (1.4052) teacher_loss 0.6646 (0.4248) loss_zs_kd 0.1291 (0.1366) loss_oracle 0.8288 (0.9121) acc 71.8750 (84.6875) alaph_mean 0.1601 (0.0784) alpha_min 0.0000 (0.0000) alpha_max 0.5996 (0.5528) lr 7.5131e-04 eta 0:04:21
epoch [31/50] batch [140/160] time 0.080 (0.085) data 0.000 (0.003) loss 1.4050 (1.4149) teacher_loss 0.3491 (0.4264) loss_zs_kd 0.1073 (0.1349) loss_oracle 1.0022 (0.9210) acc 87.5000 (84.4196) alaph_mean 0.0984 (0.0786) alpha_min 0.0000 (0.0000) alpha_max 0.7617 (0.5565) lr 7.5131e-04 eta 0:04:18
epoch [31/50] batch [160/160] time 0.074 (0.083) data 0.000 (0.002) loss 1.3616 (1.4145) teacher_loss 0.2086 (0.4211) loss_zs_kd 0.2104 (0.1359) loss_oracle 1.0478 (0.9255) acc 96.8750 (84.8047) alaph_mean 0.0345 (0.0780) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5571) lr 6.9098e-04 eta 0:04:13
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,858
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,984
* accuracy: 88.4%
* error: 11.6%
* macro_f1: 89.4%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [32/50] batch [20/160] time 0.077 (0.098) data 0.000 (0.014) loss 1.3174 (1.4098) teacher_loss 0.3303 (0.4093) loss_zs_kd 0.1224 (0.1281) loss_oracle 0.9259 (0.9364) acc 90.6250 (86.0938) alaph_mean 0.0753 (0.0804) alpha_min 0.0000 (0.0000) alpha_max 0.7071 (0.5746) lr 6.9098e-04 eta 0:04:55
epoch [32/50] batch [40/160] time 0.086 (0.091) data 0.000 (0.007) loss 1.4818 (1.4292) teacher_loss 0.5151 (0.4479) loss_zs_kd 0.1459 (0.1294) loss_oracle 0.8937 (0.9166) acc 78.1250 (84.2969) alaph_mean 0.0814 (0.0808) alpha_min 0.0000 (0.0000) alpha_max 0.5025 (0.5670) lr 6.9098e-04 eta 0:04:32
epoch [32/50] batch [60/160] time 0.080 (0.090) data 0.000 (0.005) loss 1.2268 (1.4189) teacher_loss 0.3022 (0.4442) loss_zs_kd 0.1238 (0.1307) loss_oracle 0.8627 (0.9093) acc 87.5000 (84.0104) alaph_mean 0.0697 (0.0816) alpha_min 0.0000 (0.0000) alpha_max 0.6310 (0.5674) lr 6.9098e-04 eta 0:04:27
epoch [32/50] batch [80/160] time 0.082 (0.088) data 0.000 (0.004) loss 1.2884 (1.3875) teacher_loss 0.3706 (0.4167) loss_zs_kd 0.1405 (0.1319) loss_oracle 0.8476 (0.9049) acc 87.5000 (85.3906) alaph_mean 0.1141 (0.0801) alpha_min 0.0000 (0.0000) alpha_max 0.6830 (0.5590) lr 6.9098e-04 eta 0:04:21
epoch [32/50] batch [100/160] time 0.085 (0.088) data 0.000 (0.003) loss 1.2171 (1.3871) teacher_loss 0.2815 (0.4213) loss_zs_kd 0.1619 (0.1340) loss_oracle 0.8546 (0.8989) acc 93.7500 (85.2188) alaph_mean 0.0928 (0.0812) alpha_min 0.0000 (0.0000) alpha_max 0.5791 (0.5596) lr 6.9098e-04 eta 0:04:19
epoch [32/50] batch [120/160] time 0.085 (0.088) data 0.000 (0.003) loss 1.5267 (1.3877) teacher_loss 0.5137 (0.4222) loss_zs_kd 0.1246 (0.1324) loss_oracle 0.9507 (0.8993) acc 71.8750 (85.3125) alaph_mean 0.0455 (0.0786) alpha_min 0.0000 (0.0000) alpha_max 0.4992 (0.5532) lr 6.9098e-04 eta 0:04:16
epoch [32/50] batch [140/160] time 0.082 (0.088) data 0.000 (0.002) loss 1.3846 (1.3791) teacher_loss 0.3865 (0.4157) loss_zs_kd 0.2062 (0.1337) loss_oracle 0.8950 (0.8966) acc 90.6250 (85.6250) alaph_mean 0.1328 (0.0798) alpha_min 0.0000 (0.0000) alpha_max 0.8289 (0.5622) lr 6.9098e-04 eta 0:04:14
epoch [32/50] batch [160/160] time 0.121 (0.087) data 0.001 (0.002) loss 1.3455 (1.3789) teacher_loss 0.3967 (0.4177) loss_zs_kd 0.1709 (0.1349) loss_oracle 0.8633 (0.8937) acc 90.6250 (85.4688) alaph_mean 0.0922 (0.0803) alpha_min 0.0000 (0.0000) alpha_max 0.5007 (0.5637) lr 6.3188e-04 eta 0:04:10
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,860
* accuracy: 84.3%
* error: 15.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,968
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 89.1%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [33/50] batch [20/160] time 0.087 (0.099) data 0.000 (0.016) loss 1.3838 (1.3681) teacher_loss 0.4827 (0.4157) loss_zs_kd 0.1171 (0.1413) loss_oracle 0.8426 (0.8818) acc 81.2500 (85.3125) alaph_mean 0.1021 (0.0825) alpha_min 0.0000 (0.0000) alpha_max 0.5714 (0.5607) lr 6.3188e-04 eta 0:04:43
epoch [33/50] batch [40/160] time 0.079 (0.092) data 0.000 (0.008) loss 1.4664 (1.3715) teacher_loss 0.5088 (0.4269) loss_zs_kd 0.1260 (0.1339) loss_oracle 0.8947 (0.8776) acc 84.3750 (84.6875) alaph_mean 0.0449 (0.0876) alpha_min 0.0000 (0.0000) alpha_max 0.4163 (0.5764) lr 6.3188e-04 eta 0:04:20
epoch [33/50] batch [60/160] time 0.105 (0.091) data 0.001 (0.006) loss 1.6621 (1.3837) teacher_loss 0.6958 (0.4389) loss_zs_kd 0.1394 (0.1323) loss_oracle 0.8966 (0.8787) acc 78.1250 (84.6875) alaph_mean 0.0728 (0.0833) alpha_min 0.0000 (0.0000) alpha_max 0.4397 (0.5662) lr 6.3188e-04 eta 0:04:17
epoch [33/50] batch [80/160] time 0.081 (0.090) data 0.000 (0.004) loss 1.2369 (1.3700) teacher_loss 0.3896 (0.4306) loss_zs_kd 0.1193 (0.1288) loss_oracle 0.7876 (0.8750) acc 87.5000 (84.6875) alaph_mean 0.1200 (0.0837) alpha_min 0.0000 (0.0000) alpha_max 0.5755 (0.5651) lr 6.3188e-04 eta 0:04:12
epoch [33/50] batch [100/160] time 0.086 (0.089) data 0.000 (0.003) loss 1.2122 (1.3684) teacher_loss 0.3223 (0.4247) loss_zs_kd 0.0997 (0.1278) loss_oracle 0.8401 (0.8798) acc 93.7500 (84.9688) alaph_mean 0.0722 (0.0811) alpha_min 0.0000 (0.0000) alpha_max 0.4417 (0.5558) lr 6.3188e-04 eta 0:04:08
epoch [33/50] batch [120/160] time 0.082 (0.089) data 0.000 (0.003) loss 1.5111 (1.3691) teacher_loss 0.4849 (0.4263) loss_zs_kd 0.1819 (0.1293) loss_oracle 0.9353 (0.8781) acc 81.2500 (84.9219) alaph_mean 0.0645 (0.0829) alpha_min 0.0000 (0.0000) alpha_max 0.5172 (0.5586) lr 6.3188e-04 eta 0:04:05
epoch [33/50] batch [140/160] time 0.083 (0.088) data 0.000 (0.003) loss 1.5096 (1.3757) teacher_loss 0.5469 (0.4272) loss_zs_kd 0.1191 (0.1302) loss_oracle 0.9032 (0.8834) acc 87.5000 (84.7768) alaph_mean 0.0864 (0.0817) alpha_min -0.0000 (0.0000) alpha_max 0.5265 (0.5579) lr 6.3188e-04 eta 0:04:02
epoch [33/50] batch [160/160] time 0.075 (0.087) data 0.000 (0.002) loss 1.1794 (1.3731) teacher_loss 0.2220 (0.4238) loss_zs_kd 0.0870 (0.1303) loss_oracle 0.9139 (0.8842) acc 96.8750 (85.0391) alaph_mean 0.0750 (0.0813) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.5579) lr 5.7422e-04 eta 0:03:56
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,859
* accuracy: 84.3%
* error: 15.7%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,964
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 88.9%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [34/50] batch [20/160] time 0.079 (0.094) data 0.000 (0.018) loss 1.3327 (1.3583) teacher_loss 0.3601 (0.3987) loss_zs_kd 0.1140 (0.1132) loss_oracle 0.9156 (0.9029) acc 87.5000 (86.0938) alaph_mean 0.0482 (0.0827) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5369) lr 5.7422e-04 eta 0:04:13
epoch [34/50] batch [40/160] time 0.095 (0.087) data 0.000 (0.009) loss 1.2811 (1.4054) teacher_loss 0.3357 (0.4382) loss_zs_kd 0.1734 (0.1184) loss_oracle 0.8587 (0.9080) acc 90.6250 (83.9062) alaph_mean 0.0908 (0.0835) alpha_min 0.0000 (0.0000) alpha_max 0.6185 (0.5514) lr 5.7422e-04 eta 0:03:53
epoch [34/50] batch [60/160] time 0.074 (0.083) data 0.001 (0.006) loss 1.5808 (1.4253) teacher_loss 0.5439 (0.4526) loss_zs_kd 0.1386 (0.1192) loss_oracle 0.9675 (0.9131) acc 81.2500 (83.6458) alaph_mean 0.0673 (0.0819) alpha_min 0.0000 (0.0000) alpha_max 0.5143 (0.5499) lr 5.7422e-04 eta 0:03:41
epoch [34/50] batch [80/160] time 0.081 (0.082) data 0.000 (0.005) loss 1.4876 (1.4157) teacher_loss 0.5283 (0.4455) loss_zs_kd 0.1269 (0.1216) loss_oracle 0.8958 (0.9095) acc 71.8750 (83.7500) alaph_mean 0.1065 (0.0836) alpha_min 0.0000 (0.0000) alpha_max 0.5385 (0.5479) lr 5.7422e-04 eta 0:03:35
epoch [34/50] batch [100/160] time 0.076 (0.080) data 0.000 (0.004) loss 1.0586 (1.4143) teacher_loss 0.2151 (0.4415) loss_zs_kd 0.1234 (0.1252) loss_oracle 0.7818 (0.9102) acc 96.8750 (83.9062) alaph_mean 0.1384 (0.0850) alpha_min 0.0000 (0.0000) alpha_max 0.5062 (0.5489) lr 5.7422e-04 eta 0:03:28
epoch [34/50] batch [120/160] time 0.098 (0.083) data 0.000 (0.003) loss 1.4375 (1.4196) teacher_loss 0.4089 (0.4426) loss_zs_kd 0.1561 (0.1272) loss_oracle 0.9505 (0.9135) acc 84.3750 (83.6979) alaph_mean 0.1092 (0.0863) alpha_min -0.0000 (0.0000) alpha_max 0.6605 (0.5515) lr 5.7422e-04 eta 0:03:36
epoch [34/50] batch [140/160] time 0.075 (0.082) data 0.000 (0.003) loss 1.6590 (1.4150) teacher_loss 0.5854 (0.4310) loss_zs_kd 0.1767 (0.1269) loss_oracle 0.9852 (0.9206) acc 84.3750 (84.3080) alaph_mean 0.0666 (0.0857) alpha_min 0.0000 (0.0000) alpha_max 0.4989 (0.5490) lr 5.7422e-04 eta 0:03:32
epoch [34/50] batch [160/160] time 0.075 (0.082) data 0.000 (0.002) loss 1.4932 (1.4127) teacher_loss 0.5317 (0.4304) loss_zs_kd 0.1158 (0.1269) loss_oracle 0.9036 (0.9189) acc 84.3750 (84.3555) alaph_mean 0.0755 (0.0863) alpha_min 0.0000 (0.0000) alpha_max 0.5075 (0.5545) lr 5.1825e-04 eta 0:03:28
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,856
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,965
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 88.9%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [35/50] batch [20/160] time 0.071 (0.088) data 0.000 (0.012) loss 1.0849 (1.3830) teacher_loss 0.2458 (0.4543) loss_zs_kd 0.1134 (0.1232) loss_oracle 0.7823 (0.8671) acc 96.8750 (84.5312) alaph_mean 0.1173 (0.0888) alpha_min 0.0000 (0.0000) alpha_max 0.5074 (0.5325) lr 5.1825e-04 eta 0:03:42
epoch [35/50] batch [40/160] time 0.109 (0.087) data 0.000 (0.006) loss 1.2447 (1.3718) teacher_loss 0.2935 (0.4300) loss_zs_kd 0.0777 (0.1194) loss_oracle 0.9124 (0.8820) acc 90.6250 (84.8438) alaph_mean 0.0852 (0.0874) alpha_min 0.0000 (0.0000) alpha_max 0.6022 (0.5602) lr 5.1825e-04 eta 0:03:38
epoch [35/50] batch [60/160] time 0.081 (0.086) data 0.000 (0.004) loss 1.3663 (1.3501) teacher_loss 0.3772 (0.4034) loss_zs_kd 0.1073 (0.1231) loss_oracle 0.9355 (0.8852) acc 90.6250 (86.3542) alaph_mean 0.0702 (0.0869) alpha_min 0.0000 (0.0000) alpha_max 0.4086 (0.5542) lr 5.1825e-04 eta 0:03:35
epoch [35/50] batch [80/160] time 0.083 (0.087) data 0.000 (0.003) loss 1.5951 (1.3802) teacher_loss 0.5879 (0.4271) loss_zs_kd 0.1527 (0.1274) loss_oracle 0.9309 (0.8894) acc 81.2500 (85.1562) alaph_mean 0.0433 (0.0855) alpha_min 0.0000 (0.0000) alpha_max 0.5809 (0.5590) lr 5.1825e-04 eta 0:03:34
epoch [35/50] batch [100/160] time 0.086 (0.086) data 0.000 (0.003) loss 1.4119 (1.3800) teacher_loss 0.4707 (0.4249) loss_zs_kd 0.0818 (0.1268) loss_oracle 0.9003 (0.8917) acc 90.6250 (85.4688) alaph_mean 0.0402 (0.0841) alpha_min 0.0000 (0.0000) alpha_max 0.4522 (0.5633) lr 5.1825e-04 eta 0:03:31
epoch [35/50] batch [120/160] time 0.081 (0.086) data 0.000 (0.002) loss 1.3865 (1.3817) teacher_loss 0.4075 (0.4257) loss_zs_kd 0.1546 (0.1291) loss_oracle 0.9017 (0.8914) acc 84.3750 (85.2344) alaph_mean 0.0837 (0.0839) alpha_min 0.0000 (0.0000) alpha_max 0.9167 (0.5644) lr 5.1825e-04 eta 0:03:30
epoch [35/50] batch [140/160] time 0.087 (0.086) data 0.000 (0.002) loss 1.4767 (1.3808) teacher_loss 0.6177 (0.4280) loss_zs_kd 0.1399 (0.1273) loss_oracle 0.7891 (0.8891) acc 71.8750 (84.7991) alaph_mean 0.1113 (0.0841) alpha_min 0.0000 (0.0000) alpha_max 0.5387 (0.5634) lr 5.1825e-04 eta 0:03:28
epoch [35/50] batch [160/160] time 0.074 (0.085) data 0.000 (0.002) loss 1.4112 (1.3842) teacher_loss 0.4333 (0.4306) loss_zs_kd 0.1405 (0.1273) loss_oracle 0.9076 (0.8900) acc 84.3750 (84.5508) alaph_mean 0.0583 (0.0819) alpha_min 0.0000 (0.0000) alpha_max 0.5089 (0.5555) lr 4.6417e-04 eta 0:03:25
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,860
* accuracy: 84.3%
* error: 15.7%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,973
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 89.1%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [36/50] batch [20/160] time 0.084 (0.098) data 0.000 (0.015) loss 1.4289 (1.3644) teacher_loss 0.4509 (0.4278) loss_zs_kd 0.1222 (0.1263) loss_oracle 0.9168 (0.8735) acc 81.2500 (85.3125) alaph_mean 0.0450 (0.0773) alpha_min 0.0000 (0.0000) alpha_max 0.5034 (0.5489) lr 4.6417e-04 eta 0:03:53
epoch [36/50] batch [40/160] time 0.092 (0.092) data 0.001 (0.008) loss 1.2902 (1.3610) teacher_loss 0.4382 (0.4326) loss_zs_kd 0.0968 (0.1236) loss_oracle 0.8036 (0.8666) acc 90.6250 (84.6094) alaph_mean 0.1221 (0.0808) alpha_min 0.0000 (0.0000) alpha_max 0.6540 (0.5556) lr 4.6417e-04 eta 0:03:36
epoch [36/50] batch [60/160] time 0.095 (0.097) data 0.001 (0.005) loss 1.2686 (1.3668) teacher_loss 0.2947 (0.4356) loss_zs_kd 0.1019 (0.1208) loss_oracle 0.9230 (0.8707) acc 87.5000 (84.0625) alaph_mean 0.0645 (0.0777) alpha_min 0.0000 (0.0000) alpha_max 0.3934 (0.5446) lr 4.6417e-04 eta 0:03:45
epoch [36/50] batch [80/160] time 0.085 (0.094) data 0.000 (0.004) loss 1.2315 (1.3659) teacher_loss 0.3423 (0.4372) loss_zs_kd 0.1461 (0.1207) loss_oracle 0.8162 (0.8683) acc 87.5000 (83.9062) alaph_mean 0.1035 (0.0798) alpha_min 0.0000 (0.0000) alpha_max 0.5084 (0.5489) lr 4.6417e-04 eta 0:03:37
epoch [36/50] batch [100/160] time 0.084 (0.092) data 0.000 (0.003) loss 1.2815 (1.3706) teacher_loss 0.3750 (0.4370) loss_zs_kd 0.1076 (0.1229) loss_oracle 0.8527 (0.8722) acc 87.5000 (84.0312) alaph_mean 0.0898 (0.0779) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5477) lr 4.6417e-04 eta 0:03:32
epoch [36/50] batch [120/160] time 0.089 (0.091) data 0.000 (0.003) loss 1.1772 (1.3691) teacher_loss 0.2556 (0.4345) loss_zs_kd 0.1378 (0.1242) loss_oracle 0.8527 (0.8725) acc 93.7500 (84.2708) alaph_mean 0.0856 (0.0792) alpha_min 0.0000 (0.0000) alpha_max 0.5155 (0.5522) lr 4.6417e-04 eta 0:03:28
epoch [36/50] batch [140/160] time 0.081 (0.091) data 0.000 (0.002) loss 1.2579 (1.3693) teacher_loss 0.3347 (0.4323) loss_zs_kd 0.1563 (0.1233) loss_oracle 0.8450 (0.8754) acc 90.6250 (84.2188) alaph_mean 0.0870 (0.0786) alpha_min 0.0000 (0.0000) alpha_max 0.5654 (0.5478) lr 4.6417e-04 eta 0:03:24
epoch [36/50] batch [160/160] time 0.072 (0.089) data 0.000 (0.002) loss 1.4512 (1.3627) teacher_loss 0.4495 (0.4236) loss_zs_kd 0.1566 (0.1261) loss_oracle 0.9234 (0.8761) acc 84.3750 (84.8047) alaph_mean 0.0848 (0.0789) alpha_min 0.0000 (0.0000) alpha_max 0.6879 (0.5472) lr 4.1221e-04 eta 0:03:19
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,857
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,981
* accuracy: 88.3%
* error: 11.7%
* macro_f1: 89.3%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [37/50] batch [20/160] time 0.082 (0.107) data 0.000 (0.022) loss 1.3019 (1.3837) teacher_loss 0.3257 (0.4259) loss_zs_kd 0.1717 (0.1495) loss_oracle 0.8903 (0.8831) acc 87.5000 (84.0625) alaph_mean 0.0903 (0.0799) alpha_min 0.0000 (0.0000) alpha_max 0.7209 (0.5461) lr 4.1221e-04 eta 0:03:56
epoch [37/50] batch [40/160] time 0.089 (0.096) data 0.000 (0.011) loss 1.4235 (1.3456) teacher_loss 0.4641 (0.4002) loss_zs_kd 0.1174 (0.1353) loss_oracle 0.9007 (0.8777) acc 78.1250 (84.9219) alaph_mean 0.0813 (0.0842) alpha_min -0.0000 (0.0000) alpha_max 0.7907 (0.5639) lr 4.1221e-04 eta 0:03:30
epoch [37/50] batch [60/160] time 0.085 (0.091) data 0.000 (0.008) loss 1.5093 (1.3466) teacher_loss 0.5024 (0.4021) loss_zs_kd 0.1086 (0.1358) loss_oracle 0.9525 (0.8766) acc 84.3750 (85.3646) alaph_mean 0.0589 (0.0858) alpha_min 0.0000 (0.0000) alpha_max 0.4374 (0.5589) lr 4.1221e-04 eta 0:03:18
epoch [37/50] batch [80/160] time 0.090 (0.090) data 0.001 (0.006) loss 1.4095 (1.3569) teacher_loss 0.4861 (0.4092) loss_zs_kd 0.1120 (0.1346) loss_oracle 0.8674 (0.8804) acc 81.2500 (85.2734) alaph_mean 0.1075 (0.0848) alpha_min 0.0000 (0.0000) alpha_max 0.6643 (0.5638) lr 4.1221e-04 eta 0:03:14
epoch [37/50] batch [100/160] time 0.077 (0.087) data 0.000 (0.005) loss 1.7021 (1.3534) teacher_loss 0.7637 (0.4059) loss_zs_kd 0.1860 (0.1319) loss_oracle 0.8454 (0.8815) acc 71.8750 (85.3125) alaph_mean 0.0900 (0.0856) alpha_min 0.0000 (0.0000) alpha_max 0.6511 (0.5702) lr 4.1221e-04 eta 0:03:07
epoch [37/50] batch [120/160] time 0.080 (0.086) data 0.000 (0.004) loss 1.2474 (1.3707) teacher_loss 0.2937 (0.4214) loss_zs_kd 0.1469 (0.1336) loss_oracle 0.8802 (0.8825) acc 84.3750 (84.6354) alaph_mean 0.0893 (0.0862) alpha_min 0.0000 (0.0000) alpha_max 0.5944 (0.5713) lr 4.1221e-04 eta 0:03:02
epoch [37/50] batch [140/160] time 0.082 (0.085) data 0.000 (0.003) loss 1.3870 (1.3723) teacher_loss 0.4692 (0.4206) loss_zs_kd 0.1229 (0.1318) loss_oracle 0.8563 (0.8857) acc 87.5000 (84.8438) alaph_mean 0.0880 (0.0850) alpha_min 0.0000 (0.0000) alpha_max 0.4543 (0.5632) lr 4.1221e-04 eta 0:02:59
epoch [37/50] batch [160/160] time 0.076 (0.084) data 0.000 (0.003) loss 1.4768 (1.3817) teacher_loss 0.5391 (0.4298) loss_zs_kd 0.0828 (0.1299) loss_oracle 0.8963 (0.8870) acc 81.2500 (84.6875) alaph_mean 0.0730 (0.0847) alpha_min 0.0000 (0.0000) alpha_max 0.5008 (0.5659) lr 3.6258e-04 eta 0:02:55
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,857
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,979
* accuracy: 88.2%
* error: 11.8%
* macro_f1: 89.2%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [38/50] batch [20/160] time 0.080 (0.105) data 0.000 (0.015) loss 1.4708 (1.4072) teacher_loss 0.5215 (0.4589) loss_zs_kd 0.1325 (0.1169) loss_oracle 0.8830 (0.8898) acc 84.3750 (82.8125) alaph_mean 0.0899 (0.0852) alpha_min 0.0000 (0.0000) alpha_max 0.4692 (0.5395) lr 3.6258e-04 eta 0:03:35
epoch [38/50] batch [40/160] time 0.068 (0.093) data 0.000 (0.008) loss 1.5451 (1.4258) teacher_loss 0.4580 (0.4724) loss_zs_kd 0.1742 (0.1227) loss_oracle 1.0000 (0.8921) acc 81.2500 (83.2812) alaph_mean 0.0747 (0.0877) alpha_min 0.0000 (0.0000) alpha_max 0.5043 (0.5526) lr 3.6258e-04 eta 0:03:09
epoch [38/50] batch [60/160] time 0.079 (0.088) data 0.001 (0.005) loss 1.3218 (1.4194) teacher_loss 0.3721 (0.4548) loss_zs_kd 0.1221 (0.1242) loss_oracle 0.8887 (0.9025) acc 84.3750 (84.0104) alaph_mean 0.0781 (0.0872) alpha_min 0.0000 (0.0000) alpha_max 0.4913 (0.5589) lr 3.6258e-04 eta 0:02:57
epoch [38/50] batch [80/160] time 0.062 (0.087) data 0.000 (0.004) loss 1.5539 (1.4277) teacher_loss 0.5190 (0.4490) loss_zs_kd 0.1201 (0.1267) loss_oracle 0.9748 (0.9154) acc 81.2500 (84.0234) alaph_mean 0.0975 (0.0852) alpha_min 0.0000 (0.0000) alpha_max 0.5344 (0.5552) lr 3.6258e-04 eta 0:02:53
epoch [38/50] batch [100/160] time 0.086 (0.086) data 0.001 (0.003) loss 1.5721 (1.4284) teacher_loss 0.4275 (0.4432) loss_zs_kd 0.1071 (0.1259) loss_oracle 1.0911 (0.9223) acc 78.1250 (84.1562) alaph_mean 0.0593 (0.0838) alpha_min 0.0000 (0.0000) alpha_max 0.6245 (0.5466) lr 3.6258e-04 eta 0:02:49
epoch [38/50] batch [120/160] time 0.108 (0.086) data 0.000 (0.003) loss 1.4203 (1.4315) teacher_loss 0.4170 (0.4400) loss_zs_kd 0.1449 (0.1252) loss_oracle 0.9309 (0.9290) acc 84.3750 (84.1406) alaph_mean 0.0796 (0.0817) alpha_min 0.0000 (0.0000) alpha_max 0.5104 (0.5430) lr 3.6258e-04 eta 0:02:48
epoch [38/50] batch [140/160] time 0.086 (0.087) data 0.000 (0.002) loss 1.3787 (1.4294) teacher_loss 0.2595 (0.4392) loss_zs_kd 0.1257 (0.1256) loss_oracle 1.0564 (0.9274) acc 87.5000 (84.1071) alaph_mean 0.0157 (0.0824) alpha_min 0.0000 (0.0000) alpha_max 0.1867 (0.5423) lr 3.6258e-04 eta 0:02:48
epoch [38/50] batch [160/160] time 0.077 (0.086) data 0.000 (0.002) loss 1.3204 (1.4270) teacher_loss 0.3018 (0.4386) loss_zs_kd 0.1162 (0.1257) loss_oracle 0.9605 (0.9255) acc 90.6250 (84.1602) alaph_mean 0.0684 (0.0840) alpha_min 0.0000 (0.0000) alpha_max 0.5127 (0.5467) lr 3.1545e-04 eta 0:02:45
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,862
* accuracy: 84.4%
* error: 15.6%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,959
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 88.7%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [39/50] batch [20/160] time 0.071 (0.091) data 0.000 (0.017) loss 1.0674 (1.4085) teacher_loss 0.1934 (0.4302) loss_zs_kd 0.0707 (0.1276) loss_oracle 0.8387 (0.9145) acc 93.7500 (84.8438) alaph_mean 0.1109 (0.0890) alpha_min 0.0000 (0.0000) alpha_max 0.5428 (0.5383) lr 3.1545e-04 eta 0:02:53
epoch [39/50] batch [40/160] time 0.080 (0.085) data 0.000 (0.009) loss 1.3635 (1.4258) teacher_loss 0.4402 (0.4465) loss_zs_kd 0.1210 (0.1279) loss_oracle 0.8628 (0.9153) acc 87.5000 (84.1406) alaph_mean 0.0823 (0.0836) alpha_min 0.0000 (0.0000) alpha_max 0.4325 (0.5256) lr 3.1545e-04 eta 0:02:39
epoch [39/50] batch [60/160] time 0.075 (0.081) data 0.000 (0.006) loss 1.3376 (1.4174) teacher_loss 0.3884 (0.4381) loss_zs_kd 0.0892 (0.1305) loss_oracle 0.9046 (0.9140) acc 81.2500 (83.8021) alaph_mean 0.1029 (0.0848) alpha_min 0.0000 (0.0000) alpha_max 0.5409 (0.5382) lr 3.1545e-04 eta 0:02:29
epoch [39/50] batch [80/160] time 0.084 (0.082) data 0.000 (0.005) loss 1.4678 (1.4278) teacher_loss 0.4590 (0.4411) loss_zs_kd 0.1306 (0.1289) loss_oracle 0.9434 (0.9222) acc 81.2500 (83.5938) alaph_mean 0.0652 (0.0838) alpha_min 0.0000 (0.0000) alpha_max 0.5739 (0.5406) lr 3.1545e-04 eta 0:02:30
epoch [39/50] batch [100/160] time 0.097 (0.083) data 0.000 (0.004) loss 1.3029 (1.4346) teacher_loss 0.2773 (0.4420) loss_zs_kd 0.1238 (0.1303) loss_oracle 0.9636 (0.9274) acc 90.6250 (83.7500) alaph_mean 0.0888 (0.0834) alpha_min 0.0000 (0.0000) alpha_max 0.4898 (0.5420) lr 3.1545e-04 eta 0:02:30
epoch [39/50] batch [120/160] time 0.088 (0.084) data 0.001 (0.003) loss 1.5486 (1.4398) teacher_loss 0.5688 (0.4448) loss_zs_kd 0.1401 (0.1320) loss_oracle 0.9097 (0.9290) acc 81.2500 (83.5156) alaph_mean 0.1105 (0.0840) alpha_min 0.0000 (0.0000) alpha_max 0.4988 (0.5405) lr 3.1545e-04 eta 0:02:30
epoch [39/50] batch [140/160] time 0.088 (0.084) data 0.000 (0.003) loss 1.1476 (1.4405) teacher_loss 0.2150 (0.4411) loss_zs_kd 0.0759 (0.1304) loss_oracle 0.8947 (0.9342) acc 90.6250 (83.7054) alaph_mean 0.1126 (0.0830) alpha_min 0.0000 (0.0000) alpha_max 0.5017 (0.5328) lr 3.1545e-04 eta 0:02:29
epoch [39/50] batch [160/160] time 0.076 (0.083) data 0.000 (0.002) loss 1.3917 (1.4421) teacher_loss 0.3879 (0.4391) loss_zs_kd 0.1895 (0.1314) loss_oracle 0.9090 (0.9372) acc 90.6250 (83.9258) alaph_mean 0.0960 (0.0823) alpha_min 0.0000 (0.0000) alpha_max 0.6124 (0.5309) lr 2.7103e-04 eta 0:02:26
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,858
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,957
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 88.8%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [40/50] batch [20/160] time 0.076 (0.087) data 0.000 (0.013) loss 1.2973 (1.3821) teacher_loss 0.3259 (0.3975) loss_zs_kd 0.1312 (0.1303) loss_oracle 0.9058 (0.9195) acc 84.3750 (86.0938) alaph_mean 0.0610 (0.0883) alpha_min 0.0000 (0.0000) alpha_max 0.3187 (0.5294) lr 2.7103e-04 eta 0:02:30
epoch [40/50] batch [40/160] time 0.080 (0.082) data 0.000 (0.007) loss 1.5916 (1.3916) teacher_loss 0.5024 (0.4050) loss_zs_kd 0.1144 (0.1243) loss_oracle 1.0320 (0.9245) acc 75.0000 (85.3125) alaph_mean 0.0517 (0.0881) alpha_min 0.0000 (0.0000) alpha_max 0.4518 (0.5255) lr 2.7103e-04 eta 0:02:20
epoch [40/50] batch [60/160] time 0.060 (0.080) data 0.000 (0.005) loss 1.3012 (1.3957) teacher_loss 0.2198 (0.3993) loss_zs_kd 0.1111 (0.1273) loss_oracle 1.0258 (0.9328) acc 93.7500 (85.8333) alaph_mean 0.0548 (0.0860) alpha_min 0.0000 (0.0000) alpha_max 0.5026 (0.5313) lr 2.7103e-04 eta 0:02:15
epoch [40/50] batch [80/160] time 0.075 (0.078) data 0.000 (0.004) loss 1.2805 (1.4193) teacher_loss 0.3418 (0.4213) loss_zs_kd 0.1436 (0.1286) loss_oracle 0.8668 (0.9337) acc 87.5000 (84.9219) alaph_mean 0.0816 (0.0835) alpha_min 0.0000 (0.0000) alpha_max 0.6692 (0.5289) lr 2.7103e-04 eta 0:02:11
epoch [40/50] batch [100/160] time 0.060 (0.077) data 0.000 (0.003) loss 1.8072 (1.4305) teacher_loss 0.6636 (0.4273) loss_zs_kd 0.0939 (0.1275) loss_oracle 1.0967 (0.9394) acc 75.0000 (84.8125) alaph_mean 0.0249 (0.0807) alpha_min 0.0000 (0.0000) alpha_max 0.3337 (0.5209) lr 2.7103e-04 eta 0:02:06
epoch [40/50] batch [120/160] time 0.071 (0.076) data 0.000 (0.002) loss 1.4924 (1.4392) teacher_loss 0.4260 (0.4300) loss_zs_kd 0.1420 (0.1288) loss_oracle 0.9954 (0.9448) acc 84.3750 (84.8177) alaph_mean 0.0685 (0.0781) alpha_min 0.0000 (0.0000) alpha_max 0.5755 (0.5206) lr 2.7103e-04 eta 0:02:03
epoch [40/50] batch [140/160] time 0.067 (0.075) data 0.000 (0.002) loss 1.2667 (1.4429) teacher_loss 0.2578 (0.4334) loss_zs_kd 0.1626 (0.1289) loss_oracle 0.9276 (0.9450) acc 93.7500 (84.3527) alaph_mean 0.1020 (0.0775) alpha_min 0.0000 (0.0000) alpha_max 0.5418 (0.5207) lr 2.7103e-04 eta 0:02:01
epoch [40/50] batch [160/160] time 0.073 (0.075) data 0.000 (0.002) loss 1.3447 (1.4468) teacher_loss 0.3491 (0.4395) loss_zs_kd 0.1120 (0.1301) loss_oracle 0.9396 (0.9422) acc 84.3750 (84.0820) alaph_mean 0.0682 (0.0774) alpha_min 0.0000 (0.0000) alpha_max 0.4667 (0.5220) lr 2.2949e-04 eta 0:01:59
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,857
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,967
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 89.0%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [41/50] batch [20/160] time 0.073 (0.088) data 0.000 (0.011) loss 1.3723 (1.4517) teacher_loss 0.4482 (0.4355) loss_zs_kd 0.1076 (0.1231) loss_oracle 0.8703 (0.9547) acc 84.3750 (85.4688) alaph_mean 0.1177 (0.0641) alpha_min 0.0000 (0.0000) alpha_max 0.5389 (0.4844) lr 2.2949e-04 eta 0:02:18
epoch [41/50] batch [40/160] time 0.086 (0.081) data 0.000 (0.006) loss 1.4167 (1.4595) teacher_loss 0.4883 (0.4519) loss_zs_kd 0.1198 (0.1220) loss_oracle 0.8685 (0.9466) acc 78.1250 (84.2188) alaph_mean 0.0909 (0.0659) alpha_min 0.0000 (0.0000) alpha_max 0.5275 (0.4912) lr 2.2949e-04 eta 0:02:06
epoch [41/50] batch [60/160] time 0.071 (0.079) data 0.001 (0.004) loss 1.3585 (1.4480) teacher_loss 0.3479 (0.4425) loss_zs_kd 0.0925 (0.1217) loss_oracle 0.9643 (0.9447) acc 90.6250 (84.0625) alaph_mean 0.0534 (0.0691) alpha_min 0.0000 (0.0000) alpha_max 0.4014 (0.4963) lr 2.2949e-04 eta 0:02:01
epoch [41/50] batch [80/160] time 0.089 (0.078) data 0.000 (0.003) loss 1.1512 (1.4443) teacher_loss 0.2485 (0.4432) loss_zs_kd 0.1173 (0.1225) loss_oracle 0.8441 (0.9398) acc 93.7500 (84.1797) alaph_mean 0.1222 (0.0710) alpha_min 0.0000 (0.0000) alpha_max 0.6229 (0.4943) lr 2.2949e-04 eta 0:01:58
epoch [41/50] batch [100/160] time 0.067 (0.077) data 0.000 (0.003) loss 1.4139 (1.4374) teacher_loss 0.3918 (0.4394) loss_zs_kd 0.1469 (0.1244) loss_oracle 0.9486 (0.9358) acc 81.2500 (84.2500) alaph_mean 0.0488 (0.0716) alpha_min 0.0000 (0.0000) alpha_max 0.4413 (0.4978) lr 2.2949e-04 eta 0:01:55
epoch [41/50] batch [120/160] time 0.078 (0.078) data 0.000 (0.002) loss 1.4988 (1.4343) teacher_loss 0.5127 (0.4365) loss_zs_kd 0.1356 (0.1238) loss_oracle 0.9183 (0.9360) acc 81.2500 (84.2708) alaph_mean 0.0887 (0.0721) alpha_min 0.0000 (0.0000) alpha_max 0.5020 (0.5053) lr 2.2949e-04 eta 0:01:55
epoch [41/50] batch [140/160] time 0.089 (0.079) data 0.000 (0.002) loss 1.5569 (1.4407) teacher_loss 0.5527 (0.4431) loss_zs_kd 0.1034 (0.1243) loss_oracle 0.9525 (0.9355) acc 78.1250 (83.8616) alaph_mean 0.0749 (0.0734) alpha_min 0.0000 (0.0000) alpha_max 0.5246 (0.5115) lr 2.2949e-04 eta 0:01:54
epoch [41/50] batch [160/160] time 0.089 (0.081) data 0.001 (0.002) loss 1.5265 (1.4398) teacher_loss 0.4778 (0.4406) loss_zs_kd 0.1451 (0.1248) loss_oracle 0.9762 (0.9369) acc 75.0000 (83.8086) alaph_mean 0.0877 (0.0728) alpha_min 0.0000 (0.0000) alpha_max 0.6278 (0.5119) lr 1.9098e-04 eta 0:01:56
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,861
* accuracy: 84.4%
* error: 15.6%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,963
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 88.9%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [42/50] batch [20/160] time 0.074 (0.098) data 0.000 (0.015) loss 1.5702 (1.4401) teacher_loss 0.5439 (0.4383) loss_zs_kd 0.1460 (0.1366) loss_oracle 0.9533 (0.9335) acc 78.1250 (84.6875) alaph_mean 0.0901 (0.0802) alpha_min 0.0000 (0.0000) alpha_max 0.4979 (0.5381) lr 1.9098e-04 eta 0:02:18
epoch [42/50] batch [40/160] time 0.084 (0.089) data 0.000 (0.008) loss 1.3567 (1.4595) teacher_loss 0.3721 (0.4551) loss_zs_kd 0.1095 (0.1330) loss_oracle 0.9299 (0.9379) acc 87.5000 (83.9844) alaph_mean 0.0659 (0.0763) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5163) lr 1.9098e-04 eta 0:02:05
epoch [42/50] batch [60/160] time 0.087 (0.087) data 0.000 (0.005) loss 1.4159 (1.4408) teacher_loss 0.4880 (0.4433) loss_zs_kd 0.1138 (0.1271) loss_oracle 0.8710 (0.9339) acc 81.2500 (84.3750) alaph_mean 0.1006 (0.0771) alpha_min 0.0000 (0.0000) alpha_max 0.6290 (0.5172) lr 1.9098e-04 eta 0:01:59
epoch [42/50] batch [80/160] time 0.088 (0.085) data 0.000 (0.004) loss 1.3269 (1.4236) teacher_loss 0.3184 (0.4292) loss_zs_kd 0.1019 (0.1261) loss_oracle 0.9576 (0.9314) acc 87.5000 (84.8828) alaph_mean 0.0697 (0.0774) alpha_min 0.0000 (0.0000) alpha_max 0.5325 (0.5177) lr 1.9098e-04 eta 0:01:56
epoch [42/50] batch [100/160] time 0.099 (0.085) data 0.000 (0.003) loss 1.3950 (1.4223) teacher_loss 0.3735 (0.4295) loss_zs_kd 0.1049 (0.1239) loss_oracle 0.9690 (0.9309) acc 87.5000 (84.8438) alaph_mean 0.0341 (0.0767) alpha_min 0.0000 (0.0000) alpha_max 0.3099 (0.5164) lr 1.9098e-04 eta 0:01:53
epoch [42/50] batch [120/160] time 0.081 (0.085) data 0.000 (0.003) loss 1.3443 (1.4192) teacher_loss 0.3738 (0.4273) loss_zs_kd 0.1012 (0.1226) loss_oracle 0.9199 (0.9306) acc 81.2500 (84.8438) alaph_mean 0.0674 (0.0768) alpha_min 0.0000 (0.0000) alpha_max 0.4985 (0.5143) lr 1.9098e-04 eta 0:01:52
epoch [42/50] batch [140/160] time 0.089 (0.085) data 0.000 (0.002) loss 1.4909 (1.4262) teacher_loss 0.5527 (0.4375) loss_zs_kd 0.0922 (0.1228) loss_oracle 0.8920 (0.9273) acc 75.0000 (84.5536) alaph_mean 0.0766 (0.0778) alpha_min 0.0000 (0.0000) alpha_max 0.4963 (0.5164) lr 1.9098e-04 eta 0:01:50
epoch [42/50] batch [160/160] time 0.076 (0.084) data 0.000 (0.002) loss 1.3717 (1.4270) teacher_loss 0.3469 (0.4350) loss_zs_kd 0.1236 (0.1232) loss_oracle 0.9629 (0.9305) acc 87.5000 (84.4922) alaph_mean 0.0541 (0.0767) alpha_min 0.0000 (0.0000) alpha_max 0.5046 (0.5192) lr 1.5567e-04 eta 0:01:48
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,861
* accuracy: 84.4%
* error: 15.6%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,966
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 89.0%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [43/50] batch [20/160] time 0.078 (0.100) data 0.000 (0.017) loss 1.5940 (1.4159) teacher_loss 0.5576 (0.4224) loss_zs_kd 0.1554 (0.1261) loss_oracle 0.9587 (0.9305) acc 84.3750 (86.0938) alaph_mean 0.0479 (0.0746) alpha_min 0.0000 (0.0000) alpha_max 0.4548 (0.5019) lr 1.5567e-04 eta 0:02:05
epoch [43/50] batch [40/160] time 0.083 (0.093) data 0.000 (0.009) loss 1.3023 (1.4067) teacher_loss 0.4102 (0.4113) loss_zs_kd 0.1286 (0.1252) loss_oracle 0.8278 (0.9329) acc 90.6250 (86.4844) alaph_mean 0.1152 (0.0711) alpha_min 0.0000 (0.0000) alpha_max 0.5323 (0.4931) lr 1.5567e-04 eta 0:01:54
epoch [43/50] batch [60/160] time 0.080 (0.090) data 0.001 (0.006) loss 1.4100 (1.4008) teacher_loss 0.4749 (0.4155) loss_zs_kd 0.1791 (0.1274) loss_oracle 0.8456 (0.9215) acc 84.3750 (86.0938) alaph_mean 0.0767 (0.0732) alpha_min 0.0000 (0.0000) alpha_max 0.5297 (0.5016) lr 1.5567e-04 eta 0:01:50
epoch [43/50] batch [80/160] time 0.092 (0.089) data 0.001 (0.005) loss 1.3090 (1.4134) teacher_loss 0.3354 (0.4301) loss_zs_kd 0.1781 (0.1292) loss_oracle 0.8845 (0.9187) acc 90.6250 (85.4297) alaph_mean 0.0597 (0.0742) alpha_min 0.0000 (0.0000) alpha_max 0.5023 (0.5101) lr 1.5567e-04 eta 0:01:46
epoch [43/50] batch [100/160] time 0.137 (0.089) data 0.001 (0.004) loss 1.1938 (1.4021) teacher_loss 0.3125 (0.4239) loss_zs_kd 0.1429 (0.1266) loss_oracle 0.8099 (0.9149) acc 87.5000 (85.0000) alaph_mean 0.1288 (0.0754) alpha_min 0.0000 (0.0000) alpha_max 0.4969 (0.5110) lr 1.5567e-04 eta 0:01:45
epoch [43/50] batch [120/160] time 0.078 (0.090) data 0.000 (0.003) loss 1.1632 (1.4025) teacher_loss 0.1929 (0.4251) loss_zs_kd 0.1192 (0.1270) loss_oracle 0.9107 (0.9139) acc 93.7500 (84.9219) alaph_mean 0.0772 (0.0755) alpha_min 0.0000 (0.0000) alpha_max 0.5383 (0.5111) lr 1.5567e-04 eta 0:01:44
epoch [43/50] batch [140/160] time 0.071 (0.088) data 0.000 (0.003) loss 1.3410 (1.3964) teacher_loss 0.3384 (0.4202) loss_zs_kd 0.0974 (0.1267) loss_oracle 0.9539 (0.9129) acc 93.7500 (85.0223) alaph_mean 0.0523 (0.0764) alpha_min 0.0000 (0.0000) alpha_max 0.4344 (0.5126) lr 1.5567e-04 eta 0:01:39
epoch [43/50] batch [160/160] time 0.073 (0.086) data 0.000 (0.003) loss 1.3516 (1.4048) teacher_loss 0.4238 (0.4266) loss_zs_kd 0.1415 (0.1270) loss_oracle 0.8570 (0.9146) acc 84.3750 (84.5508) alaph_mean 0.1231 (0.0752) alpha_min 0.0000 (0.0000) alpha_max 0.5898 (0.5117) lr 1.2369e-04 eta 0:01:36
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,861
* accuracy: 84.4%
* error: 15.6%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,965
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 88.9%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [44/50] batch [20/160] time 0.076 (0.100) data 0.000 (0.015) loss 1.4385 (1.4013) teacher_loss 0.4426 (0.4114) loss_zs_kd 0.1153 (0.1279) loss_oracle 0.9382 (0.9260) acc 81.2500 (85.1562) alaph_mean 0.0733 (0.0722) alpha_min 0.0000 (0.0000) alpha_max 0.6068 (0.5190) lr 1.2369e-04 eta 0:01:49
epoch [44/50] batch [40/160] time 0.084 (0.092) data 0.000 (0.008) loss 1.5332 (1.4054) teacher_loss 0.4651 (0.4213) loss_zs_kd 0.1391 (0.1252) loss_oracle 0.9986 (0.9215) acc 90.6250 (85.3125) alaph_mean 0.0477 (0.0748) alpha_min 0.0000 (0.0000) alpha_max 0.5006 (0.5389) lr 1.2369e-04 eta 0:01:39
epoch [44/50] batch [60/160] time 0.084 (0.089) data 0.001 (0.005) loss 1.4517 (1.4074) teacher_loss 0.5195 (0.4257) loss_zs_kd 0.0954 (0.1253) loss_oracle 0.8845 (0.9191) acc 81.2500 (85.0521) alaph_mean 0.1031 (0.0719) alpha_min 0.0000 (0.0000) alpha_max 0.5946 (0.5219) lr 1.2369e-04 eta 0:01:34
epoch [44/50] batch [80/160] time 0.086 (0.088) data 0.000 (0.004) loss 1.3166 (1.4125) teacher_loss 0.3530 (0.4291) loss_zs_kd 0.1609 (0.1269) loss_oracle 0.8831 (0.9200) acc 87.5000 (84.2969) alaph_mean 0.0925 (0.0717) alpha_min 0.0000 (0.0000) alpha_max 0.5905 (0.5233) lr 1.2369e-04 eta 0:01:31
epoch [44/50] batch [100/160] time 0.081 (0.088) data 0.000 (0.003) loss 1.5467 (1.4112) teacher_loss 0.5278 (0.4275) loss_zs_kd 0.1241 (0.1280) loss_oracle 0.9568 (0.9197) acc 78.1250 (84.2812) alaph_mean 0.0591 (0.0720) alpha_min 0.0000 (0.0000) alpha_max 0.5747 (0.5220) lr 1.2369e-04 eta 0:01:29
epoch [44/50] batch [120/160] time 0.085 (0.088) data 0.000 (0.003) loss 1.3797 (1.4070) teacher_loss 0.3674 (0.4260) loss_zs_kd 0.0954 (0.1258) loss_oracle 0.9646 (0.9181) acc 84.3750 (84.0885) alaph_mean 0.0595 (0.0724) alpha_min 0.0000 (0.0000) alpha_max 0.5008 (0.5184) lr 1.2369e-04 eta 0:01:27
epoch [44/50] batch [140/160] time 0.081 (0.087) data 0.000 (0.002) loss 1.5601 (1.4027) teacher_loss 0.5562 (0.4257) loss_zs_kd 0.1795 (0.1265) loss_oracle 0.9142 (0.9137) acc 75.0000 (84.0848) alaph_mean 0.0668 (0.0733) alpha_min 0.0000 (0.0000) alpha_max 0.4643 (0.5176) lr 1.2369e-04 eta 0:01:25
epoch [44/50] batch [160/160] time 0.075 (0.086) data 0.000 (0.002) loss 1.5314 (1.4039) teacher_loss 0.4424 (0.4279) loss_zs_kd 0.1497 (0.1264) loss_oracle 1.0141 (0.9128) acc 87.5000 (84.0430) alaph_mean 0.0453 (0.0736) alpha_min 0.0000 (0.0000) alpha_max 0.5030 (0.5175) lr 9.5173e-05 eta 0:01:22
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,857
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,962
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 88.9%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [45/50] batch [20/160] time 0.067 (0.090) data 0.000 (0.017) loss 1.2448 (1.4059) teacher_loss 0.3601 (0.4410) loss_zs_kd 0.1077 (0.1209) loss_oracle 0.8308 (0.9045) acc 81.2500 (82.8125) alaph_mean 0.0932 (0.0742) alpha_min 0.0000 (0.0000) alpha_max 0.5924 (0.5214) lr 9.5173e-05 eta 0:01:24
epoch [45/50] batch [40/160] time 0.079 (0.081) data 0.000 (0.009) loss 1.4529 (1.3977) teacher_loss 0.4460 (0.4353) loss_zs_kd 0.0819 (0.1219) loss_oracle 0.9659 (0.9014) acc 90.6250 (83.9062) alaph_mean 0.0529 (0.0750) alpha_min 0.0000 (0.0000) alpha_max 0.4999 (0.5211) lr 9.5173e-05 eta 0:01:14
epoch [45/50] batch [60/160] time 0.074 (0.087) data 0.000 (0.006) loss 1.2829 (1.4146) teacher_loss 0.3159 (0.4452) loss_zs_kd 0.1251 (0.1257) loss_oracle 0.9044 (0.9066) acc 90.6250 (84.1146) alaph_mean 0.0698 (0.0729) alpha_min 0.0000 (0.0000) alpha_max 0.5042 (0.5116) lr 9.5173e-05 eta 0:01:18
epoch [45/50] batch [80/160] time 0.084 (0.084) data 0.000 (0.004) loss 1.3593 (1.4163) teacher_loss 0.3665 (0.4430) loss_zs_kd 0.1319 (0.1267) loss_oracle 0.9269 (0.9099) acc 90.6250 (83.9453) alaph_mean 0.0551 (0.0720) alpha_min 0.0000 (0.0000) alpha_max 0.4064 (0.5129) lr 9.5173e-05 eta 0:01:14
epoch [45/50] batch [100/160] time 0.069 (0.082) data 0.000 (0.004) loss 1.3356 (1.4083) teacher_loss 0.4075 (0.4371) loss_zs_kd 0.1273 (0.1271) loss_oracle 0.8645 (0.9077) acc 84.3750 (84.1250) alaph_mean 0.0631 (0.0720) alpha_min 0.0000 (0.0000) alpha_max 0.4578 (0.5138) lr 9.5173e-05 eta 0:01:10
epoch [45/50] batch [120/160] time 0.085 (0.081) data 0.000 (0.003) loss 1.3810 (1.4042) teacher_loss 0.3704 (0.4339) loss_zs_kd 0.1556 (0.1271) loss_oracle 0.9329 (0.9068) acc 90.6250 (84.0625) alaph_mean 0.0669 (0.0715) alpha_min 0.0000 (0.0000) alpha_max 0.4485 (0.5101) lr 9.5173e-05 eta 0:01:07
epoch [45/50] batch [140/160] time 0.065 (0.079) data 0.000 (0.003) loss 1.5065 (1.3974) teacher_loss 0.5054 (0.4269) loss_zs_kd 0.1275 (0.1259) loss_oracle 0.9374 (0.9076) acc 75.0000 (84.3304) alaph_mean 0.0513 (0.0710) alpha_min 0.0000 (0.0000) alpha_max 0.4046 (0.5065) lr 9.5173e-05 eta 0:01:05
epoch [45/50] batch [160/160] time 0.071 (0.079) data 0.000 (0.002) loss 1.4438 (1.3990) teacher_loss 0.4707 (0.4274) loss_zs_kd 0.1239 (0.1251) loss_oracle 0.9111 (0.9090) acc 81.2500 (84.1992) alaph_mean 0.0694 (0.0703) alpha_min 0.0000 (0.0000) alpha_max 0.6288 (0.5066) lr 7.0224e-05 eta 0:01:02
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,856
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,962
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 88.8%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [46/50] batch [20/160] time 0.076 (0.095) data 0.000 (0.016) loss 1.8025 (1.3895) teacher_loss 0.7896 (0.4191) loss_zs_kd 0.1277 (0.1346) loss_oracle 0.9491 (0.9030) acc 65.6250 (83.1250) alaph_mean 0.0587 (0.0713) alpha_min 0.0000 (0.0000) alpha_max 0.5020 (0.5035) lr 7.0224e-05 eta 0:01:14
epoch [46/50] batch [40/160] time 0.086 (0.086) data 0.000 (0.008) loss 1.3430 (1.4189) teacher_loss 0.3911 (0.4554) loss_zs_kd 0.1493 (0.1330) loss_oracle 0.8773 (0.8970) acc 84.3750 (82.7344) alaph_mean 0.0657 (0.0710) alpha_min 0.0000 (0.0000) alpha_max 0.5011 (0.4978) lr 7.0224e-05 eta 0:01:05
epoch [46/50] batch [60/160] time 0.078 (0.083) data 0.001 (0.005) loss 1.3349 (1.3951) teacher_loss 0.3694 (0.4374) loss_zs_kd 0.1141 (0.1303) loss_oracle 0.9085 (0.8925) acc 87.5000 (83.5938) alaph_mean 0.0967 (0.0735) alpha_min 0.0000 (0.0000) alpha_max 0.5113 (0.5049) lr 7.0224e-05 eta 0:01:01
epoch [46/50] batch [80/160] time 0.085 (0.080) data 0.000 (0.004) loss 1.2678 (1.3895) teacher_loss 0.3230 (0.4321) loss_zs_kd 0.1130 (0.1302) loss_oracle 0.8883 (0.8923) acc 90.6250 (83.5547) alaph_mean 0.0465 (0.0728) alpha_min 0.0000 (0.0000) alpha_max 0.4118 (0.5080) lr 7.0224e-05 eta 0:00:57
epoch [46/50] batch [100/160] time 0.072 (0.079) data 0.000 (0.003) loss 1.2351 (1.3871) teacher_loss 0.2803 (0.4296) loss_zs_kd 0.0924 (0.1293) loss_oracle 0.9087 (0.8928) acc 87.5000 (83.7812) alaph_mean 0.0957 (0.0735) alpha_min 0.0000 (0.0000) alpha_max 0.5883 (0.5071) lr 7.0224e-05 eta 0:00:55
epoch [46/50] batch [120/160] time 0.072 (0.078) data 0.000 (0.003) loss 1.4589 (1.3932) teacher_loss 0.4709 (0.4327) loss_zs_kd 0.1362 (0.1309) loss_oracle 0.9199 (0.8950) acc 81.2500 (83.6198) alaph_mean 0.0240 (0.0730) alpha_min 0.0000 (0.0000) alpha_max 0.4647 (0.5073) lr 7.0224e-05 eta 0:00:52
epoch [46/50] batch [140/160] time 0.066 (0.076) data 0.000 (0.002) loss 1.0932 (1.3957) teacher_loss 0.2313 (0.4359) loss_zs_kd 0.0962 (0.1294) loss_oracle 0.8138 (0.8950) acc 90.6250 (83.4598) alaph_mean 0.1101 (0.0731) alpha_min 0.0000 (0.0000) alpha_max 0.5082 (0.5087) lr 7.0224e-05 eta 0:00:50
epoch [46/50] batch [160/160] time 0.073 (0.076) data 0.000 (0.002) loss 1.2549 (1.3901) teacher_loss 0.2898 (0.4341) loss_zs_kd 0.1267 (0.1282) loss_oracle 0.9017 (0.8919) acc 90.6250 (83.4961) alaph_mean 0.0991 (0.0738) alpha_min 0.0000 (0.0000) alpha_max 0.5086 (0.5099) lr 4.8943e-05 eta 0:00:48
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,856
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,962
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 88.9%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [47/50] batch [20/160] time 0.156 (0.115) data 0.001 (0.016) loss 1.4185 (1.3689) teacher_loss 0.5151 (0.4212) loss_zs_kd 0.0889 (0.1208) loss_oracle 0.8589 (0.8872) acc 81.2500 (84.2188) alaph_mean 0.0658 (0.0798) alpha_min 0.0000 (0.0000) alpha_max 0.5015 (0.5061) lr 4.8943e-05 eta 0:01:11
epoch [47/50] batch [40/160] time 0.061 (0.103) data 0.000 (0.008) loss 1.5254 (1.3834) teacher_loss 0.5332 (0.4297) loss_zs_kd 0.1265 (0.1212) loss_oracle 0.9290 (0.8931) acc 75.0000 (83.3594) alaph_mean 0.0602 (0.0739) alpha_min 0.0000 (0.0000) alpha_max 0.4973 (0.5065) lr 4.8943e-05 eta 0:01:01
epoch [47/50] batch [60/160] time 0.087 (0.093) data 0.000 (0.005) loss 1.4013 (1.3745) teacher_loss 0.4482 (0.4261) loss_zs_kd 0.1123 (0.1213) loss_oracle 0.8969 (0.8878) acc 84.3750 (83.7500) alaph_mean 0.0803 (0.0767) alpha_min 0.0000 (0.0000) alpha_max 0.4966 (0.5054) lr 4.8943e-05 eta 0:00:53
epoch [47/50] batch [80/160] time 0.074 (0.088) data 0.000 (0.004) loss 1.2276 (1.3729) teacher_loss 0.3257 (0.4239) loss_zs_kd 0.0942 (0.1211) loss_oracle 0.8548 (0.8884) acc 84.3750 (84.1406) alaph_mean 0.0948 (0.0767) alpha_min 0.0000 (0.0000) alpha_max 0.5668 (0.5143) lr 4.8943e-05 eta 0:00:49
epoch [47/50] batch [100/160] time 0.069 (0.085) data 0.000 (0.003) loss 1.4429 (1.3806) teacher_loss 0.4768 (0.4307) loss_zs_kd 0.0883 (0.1237) loss_oracle 0.9219 (0.8881) acc 84.3750 (84.1562) alaph_mean 0.0438 (0.0754) alpha_min 0.0000 (0.0000) alpha_max 0.4988 (0.5150) lr 4.8943e-05 eta 0:00:45
epoch [47/50] batch [120/160] time 0.073 (0.082) data 0.000 (0.003) loss 1.5108 (1.3816) teacher_loss 0.5576 (0.4296) loss_zs_kd 0.1586 (0.1247) loss_oracle 0.8739 (0.8896) acc 78.1250 (84.3490) alaph_mean 0.0921 (0.0747) alpha_min 0.0000 (0.0000) alpha_max 0.4958 (0.5143) lr 4.8943e-05 eta 0:00:42
epoch [47/50] batch [140/160] time 0.074 (0.081) data 0.000 (0.002) loss 1.3688 (1.3820) teacher_loss 0.4390 (0.4286) loss_zs_kd 0.1700 (0.1261) loss_oracle 0.8448 (0.8903) acc 81.2500 (84.2857) alaph_mean 0.0920 (0.0741) alpha_min 0.0000 (0.0000) alpha_max 0.5268 (0.5121) lr 4.8943e-05 eta 0:00:40
epoch [47/50] batch [160/160] time 0.074 (0.080) data 0.000 (0.002) loss 1.6014 (1.3871) teacher_loss 0.6040 (0.4329) loss_zs_kd 0.1366 (0.1268) loss_oracle 0.9291 (0.8908) acc 81.2500 (84.1016) alaph_mean 0.0548 (0.0733) alpha_min 0.0000 (0.0000) alpha_max 0.5760 (0.5137) lr 3.1417e-05 eta 0:00:38
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,855
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,966
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 88.9%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [48/50] batch [20/160] time 0.068 (0.093) data 0.000 (0.014) loss 1.2520 (1.3970) teacher_loss 0.4097 (0.4461) loss_zs_kd 0.1108 (0.1276) loss_oracle 0.7869 (0.8871) acc 87.5000 (83.9062) alaph_mean 0.1334 (0.0726) alpha_min 0.0000 (0.0000) alpha_max 0.5947 (0.5179) lr 3.1417e-05 eta 0:00:42
epoch [48/50] batch [40/160] time 0.081 (0.085) data 0.000 (0.007) loss 1.3667 (1.3781) teacher_loss 0.3870 (0.4333) loss_zs_kd 0.1075 (0.1288) loss_oracle 0.9259 (0.8803) acc 81.2500 (84.4531) alaph_mean 0.0971 (0.0779) alpha_min 0.0000 (0.0000) alpha_max 0.5425 (0.5227) lr 3.1417e-05 eta 0:00:37
epoch [48/50] batch [60/160] time 0.075 (0.082) data 0.001 (0.005) loss 1.4233 (1.3886) teacher_loss 0.4084 (0.4358) loss_zs_kd 0.1409 (0.1283) loss_oracle 0.9444 (0.8887) acc 84.3750 (83.9583) alaph_mean 0.0800 (0.0756) alpha_min 0.0000 (0.0000) alpha_max 0.6293 (0.5283) lr 3.1417e-05 eta 0:00:34
epoch [48/50] batch [80/160] time 0.087 (0.081) data 0.000 (0.004) loss 1.5605 (1.3937) teacher_loss 0.5215 (0.4363) loss_zs_kd 0.1285 (0.1272) loss_oracle 0.9747 (0.8937) acc 71.8750 (83.6328) alaph_mean 0.0477 (0.0744) alpha_min 0.0000 (0.0000) alpha_max 0.5427 (0.5259) lr 3.1417e-05 eta 0:00:32
epoch [48/50] batch [100/160] time 0.083 (0.081) data 0.000 (0.003) loss 1.3489 (1.3799) teacher_loss 0.5063 (0.4255) loss_zs_kd 0.1129 (0.1270) loss_oracle 0.7861 (0.8909) acc 84.3750 (84.5000) alaph_mean 0.1037 (0.0745) alpha_min 0.0000 (0.0000) alpha_max 0.4953 (0.5278) lr 3.1417e-05 eta 0:00:30
epoch [48/50] batch [120/160] time 0.083 (0.082) data 0.000 (0.003) loss 1.2223 (1.3762) teacher_loss 0.3479 (0.4220) loss_zs_kd 0.0953 (0.1259) loss_oracle 0.8267 (0.8913) acc 81.2500 (84.5833) alaph_mean 0.0970 (0.0749) alpha_min 0.0000 (0.0000) alpha_max 0.5522 (0.5257) lr 3.1417e-05 eta 0:00:29
epoch [48/50] batch [140/160] time 0.081 (0.081) data 0.000 (0.002) loss 1.4060 (1.3764) teacher_loss 0.4150 (0.4195) loss_zs_kd 0.1630 (0.1260) loss_oracle 0.9095 (0.8939) acc 90.6250 (84.6429) alaph_mean 0.1154 (0.0738) alpha_min 0.0000 (0.0000) alpha_max 0.5819 (0.5259) lr 3.1417e-05 eta 0:00:27
epoch [48/50] batch [160/160] time 0.077 (0.081) data 0.000 (0.002) loss 1.6369 (1.3882) teacher_loss 0.7051 (0.4284) loss_zs_kd 0.1165 (0.1259) loss_oracle 0.8735 (0.8969) acc 78.1250 (84.2578) alaph_mean 0.0588 (0.0719) alpha_min 0.0000 (0.0000) alpha_max 0.4981 (0.5204) lr 1.7713e-05 eta 0:00:25
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,856
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,966
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 88.9%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [49/50] batch [20/160] time 0.062 (0.094) data 0.000 (0.018) loss 1.7333 (1.3718) teacher_loss 0.7354 (0.4180) loss_zs_kd 0.1536 (0.1235) loss_oracle 0.9211 (0.8920) acc 65.6250 (84.5312) alaph_mean 0.0737 (0.0708) alpha_min 0.0000 (0.0000) alpha_max 0.6064 (0.4833) lr 1.7713e-05 eta 0:00:28
epoch [49/50] batch [40/160] time 0.078 (0.083) data 0.000 (0.009) loss 1.3665 (1.3997) teacher_loss 0.4080 (0.4443) loss_zs_kd 0.1756 (0.1236) loss_oracle 0.8708 (0.8936) acc 87.5000 (83.8281) alaph_mean 0.0763 (0.0691) alpha_min 0.0000 (0.0000) alpha_max 0.4140 (0.4912) lr 1.7713e-05 eta 0:00:23
epoch [49/50] batch [60/160] time 0.059 (0.079) data 0.000 (0.006) loss 1.5476 (1.3786) teacher_loss 0.6475 (0.4222) loss_zs_kd 0.1341 (0.1246) loss_oracle 0.8331 (0.8941) acc 71.8750 (84.8438) alaph_mean 0.0855 (0.0715) alpha_min 0.0000 (0.0000) alpha_max 0.6158 (0.5100) lr 1.7713e-05 eta 0:00:20
epoch [49/50] batch [80/160] time 0.084 (0.078) data 0.001 (0.005) loss 1.2981 (1.3733) teacher_loss 0.3525 (0.4223) loss_zs_kd 0.1016 (0.1272) loss_oracle 0.8947 (0.8874) acc 87.5000 (84.7656) alaph_mean 0.0687 (0.0750) alpha_min -0.0000 (0.0000) alpha_max 0.4710 (0.5097) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [100/160] time 0.067 (0.077) data 0.000 (0.004) loss 1.4210 (1.3877) teacher_loss 0.4873 (0.4359) loss_zs_kd 0.1416 (0.1279) loss_oracle 0.8629 (0.8878) acc 78.1250 (84.0312) alaph_mean 0.0641 (0.0740) alpha_min 0.0000 (0.0000) alpha_max 0.5412 (0.5048) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [120/160] time 0.083 (0.077) data 0.000 (0.003) loss 1.5317 (1.3982) teacher_loss 0.5913 (0.4390) loss_zs_kd 0.1405 (0.1281) loss_oracle 0.8701 (0.8952) acc 81.2500 (83.8542) alaph_mean 0.0973 (0.0705) alpha_min 0.0000 (0.0000) alpha_max 0.4537 (0.4982) lr 1.7713e-05 eta 0:00:15
epoch [49/50] batch [140/160] time 0.095 (0.078) data 0.000 (0.003) loss 1.3961 (1.3943) teacher_loss 0.4998 (0.4389) loss_zs_kd 0.1128 (0.1261) loss_oracle 0.8400 (0.8924) acc 78.1250 (83.9062) alaph_mean 0.0919 (0.0718) alpha_min 0.0000 (0.0000) alpha_max 0.5754 (0.4991) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [160/160] time 0.078 (0.078) data 0.000 (0.003) loss 1.3413 (1.3915) teacher_loss 0.4402 (0.4357) loss_zs_kd 0.1693 (0.1254) loss_oracle 0.8164 (0.8931) acc 84.3750 (84.1016) alaph_mean 0.0984 (0.0717) alpha_min 0.0000 (0.0000) alpha_max 0.5422 (0.5017) lr 7.8853e-06 eta 0:00:12
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,858
* accuracy: 84.2%
* error: 15.8%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,965
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 88.9%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
epoch [50/50] batch [20/160] time 0.077 (0.101) data 0.000 (0.020) loss 1.5958 (1.4092) teacher_loss 0.5391 (0.4328) loss_zs_kd 0.1339 (0.1328) loss_oracle 0.9898 (0.9100) acc 87.5000 (84.6875) alaph_mean 0.0362 (0.0701) alpha_min 0.0000 (0.0000) alpha_max 0.5425 (0.5276) lr 7.8853e-06 eta 0:00:14
epoch [50/50] batch [40/160] time 0.080 (0.091) data 0.000 (0.010) loss 1.4414 (1.3906) teacher_loss 0.5015 (0.4219) loss_zs_kd 0.1461 (0.1313) loss_oracle 0.8669 (0.9031) acc 84.3750 (84.9219) alaph_mean 0.0648 (0.0707) alpha_min 0.0000 (0.0000) alpha_max 0.7498 (0.5263) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [60/160] time 0.076 (0.088) data 0.001 (0.007) loss 1.4936 (1.3908) teacher_loss 0.4951 (0.4156) loss_zs_kd 0.1171 (0.1275) loss_oracle 0.9399 (0.9114) acc 81.2500 (84.7396) alaph_mean 0.0509 (0.0681) alpha_min -0.0000 (0.0000) alpha_max 0.4740 (0.5170) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [80/160] time 0.088 (0.086) data 0.000 (0.005) loss 1.3445 (1.3901) teacher_loss 0.4373 (0.4196) loss_zs_kd 0.1636 (0.1273) loss_oracle 0.8255 (0.9068) acc 87.5000 (84.4531) alaph_mean 0.1134 (0.0707) alpha_min 0.0000 (0.0000) alpha_max 0.5617 (0.5296) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [100/160] time 0.077 (0.085) data 0.000 (0.004) loss 1.4496 (1.3864) teacher_loss 0.4502 (0.4197) loss_zs_kd 0.0855 (0.1261) loss_oracle 0.9566 (0.9037) acc 84.3750 (84.5938) alaph_mean 0.0349 (0.0708) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5276) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [120/160] time 0.084 (0.085) data 0.000 (0.004) loss 1.3651 (1.3825) teacher_loss 0.3508 (0.4178) loss_zs_kd 0.1109 (0.1265) loss_oracle 0.9588 (0.9014) acc 87.5000 (84.4271) alaph_mean 0.0344 (0.0711) alpha_min 0.0000 (0.0000) alpha_max 0.4212 (0.5277) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [140/160] time 0.107 (0.085) data 0.000 (0.003) loss 1.2432 (1.3793) teacher_loss 0.2708 (0.4184) loss_zs_kd 0.1323 (0.1271) loss_oracle 0.9063 (0.8974) acc 96.8750 (84.4866) alaph_mean 0.0657 (0.0724) alpha_min 0.0000 (0.0000) alpha_max 0.5859 (0.5286) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [160/160] time 0.074 (0.084) data 0.000 (0.003) loss 1.3287 (1.3776) teacher_loss 0.3447 (0.4192) loss_zs_kd 0.1259 (0.1260) loss_oracle 0.9210 (0.8953) acc 87.5000 (84.5898) alaph_mean 0.0689 (0.0735) alpha_min 0.0000 (0.0000) alpha_max 0.5536 (0.5269) lr 1.9733e-06 eta 0:00:00
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,856
* accuracy: 84.1%
* error: 15.9%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,966
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 88.9%
******* Domain p best val acc:      84.5%, epoch: 14 *******
******* Domain p best val test acc: 88.7%, epoch: 14 *******
******* Domain p best test acc:     88.7%, epoch: 13 *******
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model.pth.tar-50
Finish the whole training
Elapsed: 0:15:37
[Info] Hyperparameters saved to: icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/hyperparameters.json
