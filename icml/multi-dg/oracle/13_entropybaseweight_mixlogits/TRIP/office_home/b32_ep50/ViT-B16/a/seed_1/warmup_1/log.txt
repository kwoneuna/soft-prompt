Loading trainer: TRIP
Loading dataset: SPG_OfficeHome
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------------------------------
Dataset    SPG_OfficeHome
Source     ['clipart', 'product', 'real_world']
Target     ['art']
# classes  65
# train_x  9,222
# val      3,939
# test     2,427
---------  ------------------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,195
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/288] time 0.200 (0.247) data 0.000 (0.017) loss 1.4346 (1.5656) teacher_loss 1.1104 (1.2378) loss_zs_kd 0.0000 (0.0000) loss_oracle 0.3243 (0.3278) acc 71.8750 (68.5938) alaph_mean 0.3497 (0.3457) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.5000) lr 1.0000e-05 eta 0:59:08
epoch [1/50] batch [40/288] time 0.200 (0.223) data 0.000 (0.009) loss 1.5970 (1.5755) teacher_loss 1.2158 (1.2591) loss_zs_kd 0.0002 (0.0001) loss_oracle 0.3811 (0.3164) acc 65.6250 (68.3594) alaph_mean 0.3245 (0.3503) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.5000) lr 1.0000e-05 eta 0:53:26
epoch [1/50] batch [60/288] time 0.196 (0.214) data 0.000 (0.006) loss 1.6540 (1.5961) teacher_loss 1.4297 (1.2661) loss_zs_kd 0.0006 (0.0001) loss_oracle 0.2240 (0.3299) acc 59.3750 (67.6562) alaph_mean 0.3880 (0.3439) alpha_min -0.0000 (0.0000) alpha_max 0.5003 (0.5001) lr 1.0000e-05 eta 0:51:03
epoch [1/50] batch [80/288] time 0.199 (0.208) data 0.000 (0.004) loss 1.5732 (1.6167) teacher_loss 1.2568 (1.2701) loss_zs_kd 0.0009 (0.0002) loss_oracle 0.3159 (0.3465) acc 62.5000 (67.3828) alaph_mean 0.3520 (0.3362) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:49:44
epoch [1/50] batch [100/288] time 0.193 (0.205) data 0.000 (0.004) loss 1.4512 (1.5860) teacher_loss 1.0908 (1.2425) loss_zs_kd 0.0006 (0.0003) loss_oracle 0.3601 (0.3434) acc 62.5000 (68.0938) alaph_mean 0.3297 (0.3380) alpha_min 0.0000 (0.0000) alpha_max 0.5001 (0.5001) lr 1.0000e-05 eta 0:48:55
epoch [1/50] batch [120/288] time 0.190 (0.202) data 0.000 (0.003) loss 1.4011 (1.5843) teacher_loss 1.0332 (1.2363) loss_zs_kd 0.0007 (0.0004) loss_oracle 0.3675 (0.3478) acc 75.0000 (68.1771) alaph_mean 0.3240 (0.3361) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:48:10
epoch [1/50] batch [140/288] time 0.187 (0.201) data 0.000 (0.003) loss 1.8801 (1.5750) teacher_loss 1.4873 (1.2223) loss_zs_kd 0.0011 (0.0005) loss_oracle 0.3922 (0.3524) acc 56.2500 (68.3259) alaph_mean 0.3154 (0.3340) alpha_min -0.0000 (0.0000) alpha_max 0.5001 (0.5001) lr 1.0000e-05 eta 0:47:50
epoch [1/50] batch [160/288] time 0.197 (0.200) data 0.000 (0.002) loss 1.2016 (1.5695) teacher_loss 1.0596 (1.2199) loss_zs_kd 0.0018 (0.0006) loss_oracle 0.1411 (0.3492) acc 71.8750 (68.6523) alaph_mean 0.4339 (0.3353) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:47:33
epoch [1/50] batch [180/288] time 0.191 (0.200) data 0.000 (0.002) loss 1.2857 (1.5709) teacher_loss 0.9956 (1.2235) loss_zs_kd 0.0024 (0.0007) loss_oracle 0.2889 (0.3470) acc 71.8750 (68.4722) alaph_mean 0.3625 (0.3364) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:47:17
epoch [1/50] batch [200/288] time 0.186 (0.199) data 0.000 (0.002) loss 1.7624 (1.5668) teacher_loss 1.3379 (1.2203) loss_zs_kd 0.0020 (0.0008) loss_oracle 0.4235 (0.3461) acc 62.5000 (68.3594) alaph_mean 0.3014 (0.3369) alpha_min 0.0000 (0.0000) alpha_max 0.5000 (0.5001) lr 1.0000e-05 eta 0:47:05
epoch [1/50] batch [220/288] time 0.211 (0.199) data 0.000 (0.002) loss 1.4869 (1.5600) teacher_loss 1.4170 (1.2159) loss_zs_kd 0.0023 (0.0010) loss_oracle 0.0687 (0.3436) acc 65.6250 (68.6222) alaph_mean 0.4687 (0.3380) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:46:55
epoch [1/50] batch [240/288] time 0.194 (0.198) data 0.000 (0.002) loss 1.2778 (1.5560) teacher_loss 0.9844 (1.2135) loss_zs_kd 0.0010 (0.0011) loss_oracle 0.2929 (0.3419) acc 81.2500 (68.8802) alaph_mean 0.3564 (0.3389) alpha_min -0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:46:44
epoch [1/50] batch [260/288] time 0.176 (0.198) data 0.000 (0.001) loss 1.7923 (1.5616) teacher_loss 1.3018 (1.2190) loss_zs_kd 0.0054 (0.0013) loss_oracle 0.4878 (0.3419) acc 71.8750 (68.7260) alaph_mean 0.2738 (0.3389) alpha_min -0.0000 (0.0000) alpha_max 0.5000 (0.5001) lr 1.0000e-05 eta 0:46:34
epoch [1/50] batch [280/288] time 0.081 (0.201) data 0.000 (0.001) loss 1.7309 (1.5608) teacher_loss 1.5205 (1.2189) loss_zs_kd 0.0053 (0.0015) loss_oracle 0.2078 (0.3412) acc 62.5000 (68.7835) alaph_mean 0.4030 (0.3391) alpha_min 0.0000 (0.0000) alpha_max 0.5002 (0.5001) lr 1.0000e-05 eta 0:47:15
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,265
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 82.0%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,964
* accuracy: 80.9%
* error: 19.1%
* macro_f1: 76.7%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      82.9%, epoch: 1 *******
******* Domain a best val test acc: 80.9%, epoch: 1 *******
******* Domain a best test acc:     80.9%, epoch: 1 *******
epoch [2/50] batch [20/288] time 0.217 (0.207) data 0.000 (0.012) loss 1.5596 (1.7190) teacher_loss 0.9819 (1.1810) loss_zs_kd 0.1238 (0.1001) loss_oracle 0.5158 (0.4880) acc 71.8750 (69.0625) alaph_mean 0.3328 (0.3147) alpha_min 0.0000 (0.0000) alpha_max 0.5031 (0.5064) lr 2.0000e-03 eta 0:48:32
epoch [2/50] batch [40/288] time 0.196 (0.200) data 0.000 (0.006) loss 1.2985 (1.7058) teacher_loss 0.6577 (1.1144) loss_zs_kd 0.1437 (0.1364) loss_oracle 0.5689 (0.5232) acc 84.3750 (71.0156) alaph_mean 0.3442 (0.3181) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5235) lr 2.0000e-03 eta 0:46:51
epoch [2/50] batch [60/288] time 0.201 (0.199) data 0.000 (0.004) loss 1.9525 (1.7316) teacher_loss 1.3047 (1.1098) loss_zs_kd 0.1441 (0.1388) loss_oracle 0.5757 (0.5523) acc 68.7500 (71.5625) alaph_mean 0.3037 (0.3168) alpha_min -0.0000 (0.0082) alpha_max 0.5063 (0.5192) lr 2.0000e-03 eta 0:46:37
epoch [2/50] batch [80/288] time 0.195 (0.198) data 0.000 (0.003) loss 1.4771 (1.7226) teacher_loss 0.7700 (1.0904) loss_zs_kd 0.1205 (0.1385) loss_oracle 0.6468 (0.5629) acc 75.0000 (71.7969) alaph_mean 0.3088 (0.3145) alpha_min 0.0000 (0.0062) alpha_max 0.5073 (0.5217) lr 2.0000e-03 eta 0:46:11
epoch [2/50] batch [100/288] time 0.195 (0.196) data 0.000 (0.003) loss 2.0074 (1.7192) teacher_loss 1.4141 (1.0872) loss_zs_kd 0.1473 (0.1371) loss_oracle 0.5197 (0.5635) acc 65.6250 (72.0312) alaph_mean 0.3342 (0.3153) alpha_min 0.0000 (0.0049) alpha_max 0.5051 (0.5185) lr 2.0000e-03 eta 0:45:49
epoch [2/50] batch [120/288] time 0.199 (0.196) data 0.000 (0.002) loss 1.7283 (1.7199) teacher_loss 1.0615 (1.0789) loss_zs_kd 0.0994 (0.1374) loss_oracle 0.6170 (0.5723) acc 75.0000 (72.2135) alaph_mean 0.2801 (0.3155) alpha_min 0.0000 (0.0041) alpha_max 0.5046 (0.5182) lr 2.0000e-03 eta 0:45:44
epoch [2/50] batch [140/288] time 0.201 (0.196) data 0.000 (0.002) loss 1.6500 (1.7216) teacher_loss 1.1309 (1.0815) loss_zs_kd 0.1511 (0.1356) loss_oracle 0.4436 (0.5723) acc 75.0000 (72.0536) alaph_mean 0.4374 (0.3163) alpha_min -0.0000 (0.0035) alpha_max 0.5093 (0.5168) lr 2.0000e-03 eta 0:45:34
epoch [2/50] batch [160/288] time 0.195 (0.195) data 0.000 (0.002) loss 1.6702 (1.7246) teacher_loss 1.0879 (1.0826) loss_zs_kd 0.2022 (0.1335) loss_oracle 0.4812 (0.5753) acc 68.7500 (71.9727) alaph_mean 0.3690 (0.3144) alpha_min -0.0000 (0.0031) alpha_max 0.7983 (0.5174) lr 2.0000e-03 eta 0:45:26
epoch [2/50] batch [180/288] time 0.193 (0.195) data 0.000 (0.001) loss 1.5728 (1.7165) teacher_loss 0.9302 (1.0775) loss_zs_kd 0.1522 (0.1337) loss_oracle 0.5666 (0.5722) acc 78.1250 (71.9444) alaph_mean 0.2916 (0.3148) alpha_min -0.0000 (0.0027) alpha_max 0.5021 (0.5171) lr 2.0000e-03 eta 0:45:20
epoch [2/50] batch [200/288] time 0.194 (0.195) data 0.000 (0.001) loss 2.4827 (1.7160) teacher_loss 1.7549 (1.0791) loss_zs_kd 0.1651 (0.1346) loss_oracle 0.6453 (0.5695) acc 59.3750 (71.8281) alaph_mean 0.2612 (0.3156) alpha_min 0.0000 (0.0025) alpha_max 0.5024 (0.5183) lr 2.0000e-03 eta 0:45:14
epoch [2/50] batch [220/288] time 0.195 (0.195) data 0.000 (0.001) loss 1.7288 (1.7024) teacher_loss 1.1406 (1.0686) loss_zs_kd 0.1308 (0.1344) loss_oracle 0.5227 (0.5666) acc 71.8750 (72.0028) alaph_mean 0.3176 (0.3158) alpha_min 0.0000 (0.0022) alpha_max 0.5081 (0.5170) lr 2.0000e-03 eta 0:45:09
epoch [2/50] batch [240/288] time 0.198 (0.195) data 0.000 (0.001) loss 1.8154 (1.6922) teacher_loss 1.3506 (1.0629) loss_zs_kd 0.1302 (0.1338) loss_oracle 0.3997 (0.5623) acc 68.7500 (72.0573) alaph_mean 0.3897 (0.3162) alpha_min -0.0000 (0.0021) alpha_max 0.5091 (0.5161) lr 2.0000e-03 eta 0:45:03
epoch [2/50] batch [260/288] time 0.087 (0.194) data 0.000 (0.001) loss 1.6304 (1.6874) teacher_loss 0.9722 (1.0588) loss_zs_kd 0.1853 (0.1336) loss_oracle 0.5656 (0.5618) acc 71.8750 (72.0553) alaph_mean 0.3018 (0.3155) alpha_min 0.0000 (0.0019) alpha_max 0.5032 (0.5154) lr 2.0000e-03 eta 0:44:46
epoch [2/50] batch [280/288] time 0.080 (0.197) data 0.000 (0.001) loss 1.5261 (1.6817) teacher_loss 0.7705 (1.0547) loss_zs_kd 0.1486 (0.1330) loss_oracle 0.6813 (0.5605) acc 68.7500 (72.1094) alaph_mean 0.2542 (0.3151) alpha_min 0.0000 (0.0018) alpha_max 0.5237 (0.5154) lr 2.0000e-03 eta 0:45:23
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,371
* accuracy: 85.6%
* error: 14.4%
* macro_f1: 84.9%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.7%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      85.6%, epoch: 2 *******
******* Domain a best val test acc: 83.3%, epoch: 2 *******
******* Domain a best test acc:     83.3%, epoch: 2 *******
epoch [3/50] batch [20/288] time 0.200 (0.204) data 0.000 (0.012) loss 1.8663 (1.6325) teacher_loss 1.3008 (1.0304) loss_zs_kd 0.1718 (0.1322) loss_oracle 0.4796 (0.5360) acc 59.3750 (72.5000) alaph_mean 0.3855 (0.3178) alpha_min -0.0000 (0.0000) alpha_max 0.5133 (0.5088) lr 1.9980e-03 eta 0:46:56
epoch [3/50] batch [40/288] time 0.179 (0.199) data 0.000 (0.006) loss 1.4641 (1.5396) teacher_loss 0.8374 (0.9541) loss_zs_kd 0.1318 (0.1365) loss_oracle 0.5608 (0.5173) acc 81.2500 (74.8438) alaph_mean 0.2739 (0.3244) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5070) lr 1.9980e-03 eta 0:45:36
epoch [3/50] batch [60/288] time 0.222 (0.197) data 0.000 (0.004) loss 1.4169 (1.5988) teacher_loss 0.9375 (1.0059) loss_zs_kd 0.1050 (0.1383) loss_oracle 0.4269 (0.5238) acc 75.0000 (73.8021) alaph_mean 0.3595 (0.3187) alpha_min -0.0000 (0.0000) alpha_max 0.5044 (0.5068) lr 1.9980e-03 eta 0:45:17
epoch [3/50] batch [80/288] time 0.193 (0.197) data 0.000 (0.003) loss 1.5329 (1.6042) teacher_loss 0.9644 (1.0116) loss_zs_kd 0.1278 (0.1340) loss_oracle 0.5047 (0.5256) acc 71.8750 (73.4375) alaph_mean 0.3224 (0.3160) alpha_min 0.0000 (0.0000) alpha_max 0.5034 (0.5064) lr 1.9980e-03 eta 0:45:02
epoch [3/50] batch [100/288] time 0.194 (0.196) data 0.000 (0.003) loss 1.5095 (1.6223) teacher_loss 0.9893 (1.0257) loss_zs_kd 0.1174 (0.1344) loss_oracle 0.4616 (0.5294) acc 78.1250 (73.3125) alaph_mean 0.3302 (0.3131) alpha_min 0.0000 (0.0000) alpha_max 0.5016 (0.5061) lr 1.9980e-03 eta 0:44:47
epoch [3/50] batch [120/288] time 0.186 (0.196) data 0.000 (0.002) loss 1.3556 (1.6066) teacher_loss 0.6973 (1.0163) loss_zs_kd 0.1397 (0.1360) loss_oracle 0.5885 (0.5223) acc 78.1250 (73.5417) alaph_mean 0.2917 (0.3158) alpha_min 0.0000 (0.0000) alpha_max 0.5027 (0.5059) lr 1.9980e-03 eta 0:44:40
epoch [3/50] batch [140/288] time 0.179 (0.195) data 0.000 (0.002) loss 1.5334 (1.6122) teacher_loss 0.9712 (1.0130) loss_zs_kd 0.0933 (0.1352) loss_oracle 0.5156 (0.5316) acc 75.0000 (73.3705) alaph_mean 0.3291 (0.3155) alpha_min 0.0000 (0.0000) alpha_max 0.5025 (0.5082) lr 1.9980e-03 eta 0:44:32
epoch [3/50] batch [160/288] time 0.195 (0.195) data 0.000 (0.002) loss 2.0161 (1.6216) teacher_loss 1.4961 (1.0192) loss_zs_kd 0.1402 (0.1358) loss_oracle 0.4499 (0.5345) acc 59.3750 (73.1055) alaph_mean 0.3513 (0.3148) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5083) lr 1.9980e-03 eta 0:44:26
epoch [3/50] batch [180/288] time 0.193 (0.195) data 0.000 (0.002) loss 1.6524 (1.6218) teacher_loss 1.0762 (1.0185) loss_zs_kd 0.1519 (0.1362) loss_oracle 0.5003 (0.5352) acc 75.0000 (73.1944) alaph_mean 0.3283 (0.3140) alpha_min 0.0000 (0.0000) alpha_max 0.5022 (0.5084) lr 1.9980e-03 eta 0:44:19
epoch [3/50] batch [200/288] time 0.193 (0.195) data 0.000 (0.001) loss 1.5450 (1.6222) teacher_loss 0.9058 (1.0169) loss_zs_kd 0.1201 (0.1367) loss_oracle 0.5792 (0.5369) acc 78.1250 (73.2812) alaph_mean 0.3098 (0.3133) alpha_min 0.0000 (0.0000) alpha_max 0.5091 (0.5101) lr 1.9980e-03 eta 0:44:13
epoch [3/50] batch [220/288] time 0.192 (0.195) data 0.000 (0.001) loss 1.4445 (1.6312) teacher_loss 0.8140 (1.0261) loss_zs_kd 0.1566 (0.1379) loss_oracle 0.5522 (0.5362) acc 75.0000 (73.0540) alaph_mean 0.2877 (0.3135) alpha_min -0.0000 (0.0000) alpha_max 0.5046 (0.5096) lr 1.9980e-03 eta 0:44:06
epoch [3/50] batch [240/288] time 0.190 (0.195) data 0.000 (0.001) loss 2.0075 (1.6350) teacher_loss 1.3447 (1.0298) loss_zs_kd 0.0987 (0.1378) loss_oracle 0.6135 (0.5363) acc 62.5000 (72.9036) alaph_mean 0.2813 (0.3137) alpha_min -0.0000 (0.0000) alpha_max 0.5030 (0.5098) lr 1.9980e-03 eta 0:44:02
epoch [3/50] batch [260/288] time 0.091 (0.193) data 0.000 (0.001) loss 1.1448 (1.6266) teacher_loss 0.5947 (1.0243) loss_zs_kd 0.1463 (0.1375) loss_oracle 0.4769 (0.5336) acc 84.3750 (73.0529) alaph_mean 0.3509 (0.3146) alpha_min 0.0000 (0.0000) alpha_max 0.5041 (0.5104) lr 1.9980e-03 eta 0:43:40
epoch [3/50] batch [280/288] time 0.083 (0.196) data 0.000 (0.001) loss 1.7885 (1.6218) teacher_loss 1.1182 (1.0204) loss_zs_kd 0.2096 (0.1384) loss_oracle 0.5655 (0.5323) acc 62.5000 (73.0915) alaph_mean 0.3032 (0.3149) alpha_min 0.0000 (0.0000) alpha_max 0.5041 (0.5101) lr 1.9980e-03 eta 0:44:18
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,382
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 84.9%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,005
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 78.6%
******* Domain a best val acc:      85.9%, epoch: 3 *******
******* Domain a best val test acc: 82.6%, epoch: 3 *******
******* Domain a best test acc:     83.3%, epoch: 2 *******
epoch [4/50] batch [20/288] time 0.192 (0.209) data 0.000 (0.015) loss 1.3533 (1.5508) teacher_loss 0.8125 (0.9533) loss_zs_kd 0.1240 (0.1388) loss_oracle 0.4788 (0.5281) acc 81.2500 (73.9062) alaph_mean 0.3283 (0.3207) alpha_min 0.0000 (0.0000) alpha_max 0.5022 (0.5174) lr 1.9921e-03 eta 0:46:58
epoch [4/50] batch [40/288] time 0.196 (0.202) data 0.000 (0.008) loss 1.5870 (1.5647) teacher_loss 1.0811 (0.9673) loss_zs_kd 0.1867 (0.1432) loss_oracle 0.4126 (0.5258) acc 68.7500 (74.2969) alaph_mean 0.3749 (0.3211) alpha_min -0.0000 (0.0000) alpha_max 0.5037 (0.5222) lr 1.9921e-03 eta 0:45:22
epoch [4/50] batch [60/288] time 0.194 (0.199) data 0.000 (0.005) loss 1.8449 (1.5447) teacher_loss 1.1807 (0.9580) loss_zs_kd 0.1431 (0.1437) loss_oracle 0.5927 (0.5149) acc 71.8750 (74.8438) alaph_mean 0.2714 (0.3221) alpha_min 0.0000 (0.0000) alpha_max 0.5063 (0.5165) lr 1.9921e-03 eta 0:44:45
epoch [4/50] batch [80/288] time 0.203 (0.198) data 0.000 (0.004) loss 1.8891 (1.5622) teacher_loss 1.4512 (0.9775) loss_zs_kd 0.1205 (0.1418) loss_oracle 0.3777 (0.5138) acc 62.5000 (74.4531) alaph_mean 0.3816 (0.3216) alpha_min 0.0000 (0.0000) alpha_max 0.5038 (0.5137) lr 1.9921e-03 eta 0:44:26
epoch [4/50] batch [100/288] time 0.194 (0.198) data 0.000 (0.003) loss 1.4231 (1.5827) teacher_loss 0.8242 (0.9970) loss_zs_kd 0.1139 (0.1383) loss_oracle 0.5419 (0.5166) acc 84.3750 (74.0312) alaph_mean 0.2968 (0.3208) alpha_min 0.0000 (0.0000) alpha_max 0.5027 (0.5122) lr 1.9921e-03 eta 0:44:19
epoch [4/50] batch [120/288] time 0.187 (0.197) data 0.000 (0.003) loss 1.6667 (1.5921) teacher_loss 0.8965 (1.0048) loss_zs_kd 0.1236 (0.1373) loss_oracle 0.7084 (0.5187) acc 78.1250 (73.8281) alaph_mean 0.2144 (0.3193) alpha_min -0.0000 (0.0000) alpha_max 0.5065 (0.5125) lr 1.9921e-03 eta 0:44:07
epoch [4/50] batch [140/288] time 0.187 (0.197) data 0.000 (0.002) loss 1.4734 (1.6046) teacher_loss 0.7417 (1.0101) loss_zs_kd 0.1246 (0.1366) loss_oracle 0.6694 (0.5262) acc 81.2500 (73.5938) alaph_mean 0.2362 (0.3155) alpha_min -0.0000 (0.0000) alpha_max 0.5076 (0.5129) lr 1.9921e-03 eta 0:43:55
epoch [4/50] batch [160/288] time 0.202 (0.196) data 0.000 (0.002) loss 1.6847 (1.6007) teacher_loss 1.1621 (1.0058) loss_zs_kd 0.1300 (0.1365) loss_oracle 0.4576 (0.5267) acc 68.7500 (73.6133) alaph_mean 0.3734 (0.3165) alpha_min -0.0000 (0.0000) alpha_max 0.5065 (0.5149) lr 1.9921e-03 eta 0:43:47
epoch [4/50] batch [180/288] time 0.189 (0.196) data 0.000 (0.002) loss 1.9318 (1.5977) teacher_loss 1.3789 (1.0036) loss_zs_kd 0.1667 (0.1366) loss_oracle 0.4695 (0.5258) acc 62.5000 (73.5243) alaph_mean 0.3255 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5042 (0.5176) lr 1.9921e-03 eta 0:43:37
epoch [4/50] batch [200/288] time 0.188 (0.196) data 0.000 (0.002) loss 1.7757 (1.5949) teacher_loss 1.0508 (1.0016) loss_zs_kd 0.1171 (0.1381) loss_oracle 0.6664 (0.5242) acc 71.8750 (73.7031) alaph_mean 0.2382 (0.3174) alpha_min 0.0000 (0.0000) alpha_max 0.5024 (0.5164) lr 1.9921e-03 eta 0:43:30
epoch [4/50] batch [220/288] time 0.193 (0.196) data 0.000 (0.002) loss 1.7357 (1.5950) teacher_loss 1.1104 (1.0037) loss_zs_kd 0.1255 (0.1385) loss_oracle 0.5626 (0.5220) acc 68.7500 (73.5511) alaph_mean 0.2931 (0.3183) alpha_min 0.0000 (0.0000) alpha_max 0.5068 (0.5183) lr 1.9921e-03 eta 0:43:24
epoch [4/50] batch [240/288] time 0.182 (0.196) data 0.000 (0.001) loss 1.9831 (1.5930) teacher_loss 1.1924 (1.0012) loss_zs_kd 0.1319 (0.1381) loss_oracle 0.7248 (0.5227) acc 71.8750 (73.7240) alaph_mean 0.2261 (0.3184) alpha_min -0.0000 (0.0000) alpha_max 0.5082 (0.5181) lr 1.9921e-03 eta 0:43:19
epoch [4/50] batch [260/288] time 0.090 (0.192) data 0.000 (0.001) loss 1.5185 (1.6013) teacher_loss 0.8032 (1.0051) loss_zs_kd 0.1390 (0.1380) loss_oracle 0.6457 (0.5271) acc 71.8750 (73.5938) alaph_mean 0.2814 (0.3189) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5176) lr 1.9921e-03 eta 0:42:34
epoch [4/50] batch [280/288] time 0.476 (0.201) data 0.000 (0.001) loss 1.2721 (1.5965) teacher_loss 0.6377 (1.0002) loss_zs_kd 0.1313 (0.1376) loss_oracle 0.5688 (0.5276) acc 84.3750 (73.8951) alaph_mean 0.3037 (0.3195) alpha_min 0.0000 (0.0000) alpha_max 0.5063 (0.5171) lr 1.9921e-03 eta 0:44:27
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,390
* accuracy: 86.1%
* error: 13.9%
* macro_f1: 85.3%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,011
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.3%
******* Domain a best val acc:      86.1%, epoch: 4 *******
******* Domain a best val test acc: 82.9%, epoch: 4 *******
******* Domain a best test acc:     83.3%, epoch: 2 *******
epoch [5/50] batch [20/288] time 0.193 (0.207) data 0.000 (0.012) loss 2.0167 (1.7674) teacher_loss 1.3262 (1.1373) loss_zs_kd 0.1662 (0.1553) loss_oracle 0.6075 (0.5524) acc 71.8750 (70.0000) alaph_mean 0.2936 (0.3172) alpha_min 0.0000 (0.0000) alpha_max 0.5091 (0.5078) lr 1.9823e-03 eta 0:45:40
epoch [5/50] batch [40/288] time 0.183 (0.200) data 0.000 (0.006) loss 1.1240 (1.6599) teacher_loss 0.5234 (1.0359) loss_zs_kd 0.1774 (0.1514) loss_oracle 0.5119 (0.5483) acc 90.6250 (73.0469) alaph_mean 0.3137 (0.3208) alpha_min 0.0000 (0.0000) alpha_max 0.5034 (0.5088) lr 1.9823e-03 eta 0:44:02
epoch [5/50] batch [60/288] time 0.195 (0.198) data 0.000 (0.004) loss 1.5646 (1.6239) teacher_loss 1.1230 (1.0061) loss_zs_kd 0.1030 (0.1443) loss_oracle 0.3901 (0.5457) acc 65.6250 (74.1146) alaph_mean 0.4061 (0.3174) alpha_min 0.0000 (0.0000) alpha_max 0.5055 (0.5105) lr 1.9823e-03 eta 0:43:33
epoch [5/50] batch [80/288] time 0.197 (0.197) data 0.000 (0.003) loss 2.0251 (1.6033) teacher_loss 1.4209 (0.9838) loss_zs_kd 0.1330 (0.1451) loss_oracle 0.5377 (0.5470) acc 68.7500 (74.2578) alaph_mean 0.3225 (0.3185) alpha_min 0.0000 (0.0000) alpha_max 0.5089 (0.5123) lr 1.9823e-03 eta 0:43:13
epoch [5/50] batch [100/288] time 0.195 (0.196) data 0.000 (0.003) loss 1.2934 (1.6128) teacher_loss 0.6138 (0.9929) loss_zs_kd 0.1498 (0.1447) loss_oracle 0.6048 (0.5475) acc 81.2500 (74.1875) alaph_mean 0.3208 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.5105 (0.5144) lr 1.9823e-03 eta 0:43:02
epoch [5/50] batch [120/288] time 0.197 (0.196) data 0.000 (0.002) loss 1.6008 (1.5986) teacher_loss 0.8999 (0.9803) loss_zs_kd 0.2284 (0.1444) loss_oracle 0.5868 (0.5461) acc 75.0000 (74.4271) alaph_mean 0.2824 (0.3193) alpha_min 0.0000 (0.0000) alpha_max 0.5053 (0.5157) lr 1.9823e-03 eta 0:42:53
epoch [5/50] batch [140/288] time 0.198 (0.197) data 0.000 (0.002) loss 1.4575 (1.5999) teacher_loss 0.9897 (0.9828) loss_zs_kd 0.1394 (0.1451) loss_oracle 0.3981 (0.5445) acc 78.1250 (74.4420) alaph_mean 0.4082 (0.3191) alpha_min 0.0000 (0.0000) alpha_max 0.5029 (0.5144) lr 1.9823e-03 eta 0:42:56
epoch [5/50] batch [160/288] time 0.196 (0.197) data 0.000 (0.002) loss 1.9702 (1.5981) teacher_loss 1.3193 (0.9827) loss_zs_kd 0.1339 (0.1451) loss_oracle 0.5839 (0.5428) acc 62.5000 (74.2773) alaph_mean 0.2834 (0.3202) alpha_min 0.0000 (0.0000) alpha_max 0.5121 (0.5148) lr 1.9823e-03 eta 0:42:52
epoch [5/50] batch [180/288] time 0.182 (0.196) data 0.000 (0.002) loss 1.4689 (1.6057) teacher_loss 0.8921 (0.9872) loss_zs_kd 0.1417 (0.1460) loss_oracle 0.5059 (0.5456) acc 78.1250 (74.0799) alaph_mean 0.3563 (0.3187) alpha_min 0.0000 (0.0000) alpha_max 0.5076 (0.5150) lr 1.9823e-03 eta 0:42:42
epoch [5/50] batch [200/288] time 0.195 (0.195) data 0.000 (0.001) loss 1.6431 (1.6098) teacher_loss 0.9907 (0.9898) loss_zs_kd 0.1383 (0.1453) loss_oracle 0.5833 (0.5474) acc 75.0000 (74.1250) alaph_mean 0.3114 (0.3182) alpha_min 0.0000 (0.0000) alpha_max 0.5021 (0.5162) lr 1.9823e-03 eta 0:42:28
epoch [5/50] batch [220/288] time 0.159 (0.195) data 0.000 (0.001) loss 1.8133 (1.6133) teacher_loss 1.1562 (0.9932) loss_zs_kd 0.1225 (0.1451) loss_oracle 0.5957 (0.5475) acc 75.0000 (73.8778) alaph_mean 0.2545 (0.3177) alpha_min -0.0000 (0.0000) alpha_max 0.5012 (0.5156) lr 1.9823e-03 eta 0:42:21
epoch [5/50] batch [240/288] time 0.187 (0.195) data 0.000 (0.001) loss 1.4237 (1.6076) teacher_loss 0.8174 (0.9917) loss_zs_kd 0.1227 (0.1446) loss_oracle 0.5449 (0.5435) acc 81.2500 (73.8932) alaph_mean 0.3097 (0.3189) alpha_min 0.0000 (0.0000) alpha_max 0.5034 (0.5149) lr 1.9823e-03 eta 0:42:15
epoch [5/50] batch [260/288] time 0.084 (0.192) data 0.000 (0.001) loss 1.7283 (1.6006) teacher_loss 1.1064 (0.9878) loss_zs_kd 0.0920 (0.1442) loss_oracle 0.5759 (0.5407) acc 65.6250 (73.7861) alaph_mean 0.2629 (0.3192) alpha_min 0.0000 (0.0000) alpha_max 0.5027 (0.5144) lr 1.9823e-03 eta 0:41:37
epoch [5/50] batch [280/288] time 0.466 (0.199) data 0.000 (0.001) loss 1.2287 (1.5888) teacher_loss 0.6035 (0.9782) loss_zs_kd 0.1553 (0.1435) loss_oracle 0.5476 (0.5388) acc 87.5000 (74.0625) alaph_mean 0.3034 (0.3201) alpha_min 0.0000 (0.0000) alpha_max 0.5118 (0.5161) lr 1.9823e-03 eta 0:43:07
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,387
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,012
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.2%
******* Domain a best val acc:      86.1%, epoch: 4 *******
******* Domain a best val test acc: 82.9%, epoch: 4 *******
******* Domain a best test acc:     83.3%, epoch: 2 *******
epoch [6/50] batch [20/288] time 0.187 (0.209) data 0.000 (0.012) loss 1.7181 (1.4917) teacher_loss 1.0449 (0.8965) loss_zs_kd 0.1110 (0.1329) loss_oracle 0.6177 (0.5288) acc 71.8750 (74.5312) alaph_mean 0.2815 (0.3167) alpha_min -0.0000 (0.0000) alpha_max 0.5050 (0.5071) lr 1.9686e-03 eta 0:45:06
epoch [6/50] batch [40/288] time 0.188 (0.202) data 0.000 (0.006) loss 1.6116 (1.5417) teacher_loss 0.9136 (0.9404) loss_zs_kd 0.1517 (0.1428) loss_oracle 0.6222 (0.5300) acc 75.0000 (73.6719) alaph_mean 0.2605 (0.3176) alpha_min -0.0000 (0.0000) alpha_max 0.5043 (0.5098) lr 1.9686e-03 eta 0:43:24
epoch [6/50] batch [60/288] time 0.195 (0.199) data 0.000 (0.004) loss 1.6625 (1.5279) teacher_loss 1.0361 (0.9244) loss_zs_kd 0.1895 (0.1443) loss_oracle 0.5316 (0.5313) acc 68.7500 (74.1667) alaph_mean 0.3201 (0.3206) alpha_min 0.0000 (0.0000) alpha_max 0.5039 (0.5149) lr 1.9686e-03 eta 0:42:49
epoch [6/50] batch [80/288] time 0.196 (0.198) data 0.000 (0.003) loss 1.5182 (1.5365) teacher_loss 0.8613 (0.9250) loss_zs_kd 0.1619 (0.1485) loss_oracle 0.5759 (0.5373) acc 78.1250 (74.5703) alaph_mean 0.3437 (0.3206) alpha_min 0.0000 (0.0000) alpha_max 0.5095 (0.5128) lr 1.9686e-03 eta 0:42:31
epoch [6/50] batch [100/288] time 0.197 (0.197) data 0.000 (0.003) loss 1.8394 (1.5615) teacher_loss 1.1406 (0.9454) loss_zs_kd 0.1888 (0.1492) loss_oracle 0.6044 (0.5415) acc 68.7500 (74.5312) alaph_mean 0.3002 (0.3188) alpha_min 0.0000 (0.0000) alpha_max 0.5059 (0.5145) lr 1.9686e-03 eta 0:42:15
epoch [6/50] batch [120/288] time 0.198 (0.197) data 0.000 (0.002) loss 1.6566 (1.5548) teacher_loss 1.1006 (0.9392) loss_zs_kd 0.1839 (0.1493) loss_oracle 0.4641 (0.5409) acc 71.8750 (74.8698) alaph_mean 0.3714 (0.3197) alpha_min 0.0000 (0.0000) alpha_max 0.5077 (0.5130) lr 1.9686e-03 eta 0:42:07
epoch [6/50] batch [140/288] time 0.197 (0.196) data 0.000 (0.002) loss 1.3358 (1.5503) teacher_loss 0.7622 (0.9395) loss_zs_kd 0.1131 (0.1489) loss_oracle 0.5170 (0.5363) acc 81.2500 (74.8438) alaph_mean 0.3436 (0.3234) alpha_min 0.0000 (0.0000) alpha_max 0.5052 (0.5135) lr 1.9686e-03 eta 0:41:55
epoch [6/50] batch [160/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.2671 (1.5611) teacher_loss 0.6460 (0.9425) loss_zs_kd 0.1570 (0.1506) loss_oracle 0.5426 (0.5433) acc 75.0000 (74.7266) alaph_mean 0.3041 (0.3198) alpha_min 0.0000 (0.0000) alpha_max 0.5041 (0.5149) lr 1.9686e-03 eta 0:41:47
epoch [6/50] batch [180/288] time 0.190 (0.196) data 0.000 (0.002) loss 1.4247 (1.5642) teacher_loss 0.6895 (0.9444) loss_zs_kd 0.2174 (0.1523) loss_oracle 0.6266 (0.5437) acc 84.3750 (74.6701) alaph_mean 0.3101 (0.3201) alpha_min 0.0000 (0.0000) alpha_max 0.5112 (0.5146) lr 1.9686e-03 eta 0:41:40
epoch [6/50] batch [200/288] time 0.199 (0.196) data 0.000 (0.001) loss 1.3993 (1.5673) teacher_loss 0.9644 (0.9493) loss_zs_kd 0.1340 (0.1519) loss_oracle 0.3680 (0.5421) acc 68.7500 (74.5938) alaph_mean 0.4268 (0.3211) alpha_min 0.0000 (0.0000) alpha_max 0.5050 (0.5141) lr 1.9686e-03 eta 0:41:34
epoch [6/50] batch [220/288] time 0.209 (0.195) data 0.000 (0.001) loss 1.7675 (1.5689) teacher_loss 1.1592 (0.9524) loss_zs_kd 0.1515 (0.1524) loss_oracle 0.5325 (0.5403) acc 75.0000 (74.6449) alaph_mean 0.3214 (0.3216) alpha_min -0.0000 (0.0000) alpha_max 0.5068 (0.5137) lr 1.9686e-03 eta 0:41:28
epoch [6/50] batch [240/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.3620 (1.5704) teacher_loss 0.9546 (0.9575) loss_zs_kd 0.0966 (0.1513) loss_oracle 0.3590 (0.5373) acc 75.0000 (74.5182) alaph_mean 0.4188 (0.3226) alpha_min -0.0000 (0.0000) alpha_max 0.5064 (0.5141) lr 1.9686e-03 eta 0:41:22
epoch [6/50] batch [260/288] time 0.197 (0.195) data 0.000 (0.001) loss 1.9208 (1.5766) teacher_loss 1.2285 (0.9612) loss_zs_kd 0.1223 (0.1514) loss_oracle 0.6312 (0.5397) acc 71.8750 (74.4471) alaph_mean 0.2620 (0.3201) alpha_min -0.0000 (0.0000) alpha_max 0.5009 (0.5141) lr 1.9686e-03 eta 0:41:17
epoch [6/50] batch [280/288] time 0.469 (0.196) data 0.000 (0.001) loss 1.7651 (1.5760) teacher_loss 1.1992 (0.9611) loss_zs_kd 0.2052 (0.1515) loss_oracle 0.4633 (0.5391) acc 75.0000 (74.5089) alaph_mean 0.3376 (0.3199) alpha_min 0.0000 (0.0000) alpha_max 0.5015 (0.5140) lr 1.9686e-03 eta 0:41:21
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,383
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.6%
******* Domain a best val acc:      86.1%, epoch: 4 *******
******* Domain a best val test acc: 82.9%, epoch: 4 *******
******* Domain a best test acc:     83.3%, epoch: 2 *******
epoch [7/50] batch [20/288] time 0.191 (0.207) data 0.000 (0.014) loss 1.8393 (1.6341) teacher_loss 1.1992 (1.0190) loss_zs_kd 0.1246 (0.1448) loss_oracle 0.5777 (0.5426) acc 65.6250 (74.0625) alaph_mean 0.2893 (0.3120) alpha_min 0.0000 (0.0000) alpha_max 0.5031 (0.5105) lr 1.9511e-03 eta 0:43:35
epoch [7/50] batch [40/288] time 0.197 (0.200) data 0.000 (0.007) loss 1.5226 (1.5949) teacher_loss 0.8027 (0.9588) loss_zs_kd 0.1395 (0.1462) loss_oracle 0.6502 (0.5629) acc 84.3750 (75.6250) alaph_mean 0.2661 (0.3074) alpha_min -0.0000 (0.0000) alpha_max 0.5014 (0.5173) lr 1.9511e-03 eta 0:42:08
epoch [7/50] batch [60/288] time 0.200 (0.198) data 0.000 (0.005) loss 1.7033 (1.5742) teacher_loss 1.1729 (0.9434) loss_zs_kd 0.1419 (0.1463) loss_oracle 0.4595 (0.5577) acc 68.7500 (75.9896) alaph_mean 0.3574 (0.3109) alpha_min 0.0000 (0.0000) alpha_max 0.5050 (0.5155) lr 1.9511e-03 eta 0:41:41
epoch [7/50] batch [80/288] time 0.194 (0.198) data 0.000 (0.004) loss 1.3565 (1.5644) teacher_loss 0.8184 (0.9411) loss_zs_kd 0.1624 (0.1489) loss_oracle 0.4569 (0.5489) acc 78.1250 (75.8203) alaph_mean 0.3681 (0.3153) alpha_min -0.0000 (0.0000) alpha_max 0.5087 (0.5128) lr 1.9511e-03 eta 0:41:27
epoch [7/50] batch [100/288] time 0.202 (0.197) data 0.000 (0.003) loss 1.2213 (1.5593) teacher_loss 0.5688 (0.9367) loss_zs_kd 0.1165 (0.1501) loss_oracle 0.5942 (0.5476) acc 87.5000 (75.6875) alaph_mean 0.2829 (0.3145) alpha_min 0.0000 (0.0000) alpha_max 0.5057 (0.5147) lr 1.9511e-03 eta 0:41:16
epoch [7/50] batch [120/288] time 0.202 (0.196) data 0.000 (0.003) loss 1.9233 (1.5622) teacher_loss 1.2168 (0.9355) loss_zs_kd 0.1710 (0.1504) loss_oracle 0.6210 (0.5515) acc 68.7500 (75.5729) alaph_mean 0.2681 (0.3110) alpha_min -0.0000 (0.0000) alpha_max 0.5052 (0.5131) lr 1.9511e-03 eta 0:41:05
epoch [7/50] batch [140/288] time 0.202 (0.195) data 0.000 (0.002) loss 1.7338 (1.5725) teacher_loss 1.0225 (0.9465) loss_zs_kd 0.1564 (0.1524) loss_oracle 0.6331 (0.5498) acc 78.1250 (75.1786) alaph_mean 0.2499 (0.3116) alpha_min 0.0000 (0.0000) alpha_max 0.5031 (0.5123) lr 1.9511e-03 eta 0:40:44
epoch [7/50] batch [160/288] time 0.185 (0.195) data 0.000 (0.002) loss 1.7405 (1.5898) teacher_loss 1.0293 (0.9599) loss_zs_kd 0.1577 (0.1523) loss_oracle 0.6324 (0.5538) acc 78.1250 (74.9023) alaph_mean 0.2421 (0.3099) alpha_min 0.0000 (0.0000) alpha_max 0.5040 (0.5116) lr 1.9511e-03 eta 0:40:34
epoch [7/50] batch [180/288] time 0.198 (0.194) data 0.000 (0.002) loss 1.5848 (1.5896) teacher_loss 1.0205 (0.9627) loss_zs_kd 0.1418 (0.1524) loss_oracle 0.4933 (0.5508) acc 71.8750 (74.7743) alaph_mean 0.3474 (0.3116) alpha_min 0.0000 (0.0000) alpha_max 0.5049 (0.5111) lr 1.9511e-03 eta 0:40:27
epoch [7/50] batch [200/288] time 0.210 (0.194) data 0.001 (0.002) loss 1.2653 (1.5814) teacher_loss 0.7026 (0.9555) loss_zs_kd 0.1752 (0.1526) loss_oracle 0.4751 (0.5496) acc 71.8750 (74.8438) alaph_mean 0.3443 (0.3120) alpha_min 0.0000 (0.0000) alpha_max 0.5052 (0.5112) lr 1.9511e-03 eta 0:40:23
epoch [7/50] batch [220/288] time 0.196 (0.194) data 0.000 (0.001) loss 1.9126 (1.5807) teacher_loss 1.2666 (0.9561) loss_zs_kd 0.1686 (0.1521) loss_oracle 0.5617 (0.5486) acc 65.6250 (74.6449) alaph_mean 0.2966 (0.3120) alpha_min 0.0000 (0.0000) alpha_max 0.5041 (0.5113) lr 1.9511e-03 eta 0:40:20
epoch [7/50] batch [240/288] time 0.183 (0.194) data 0.000 (0.001) loss 2.0954 (1.5865) teacher_loss 1.4346 (0.9627) loss_zs_kd 0.1388 (0.1526) loss_oracle 0.5914 (0.5475) acc 56.2500 (74.5052) alaph_mean 0.2762 (0.3122) alpha_min 0.0000 (0.0000) alpha_max 0.5039 (0.5109) lr 1.9511e-03 eta 0:40:15
epoch [7/50] batch [260/288] time 0.211 (0.194) data 0.000 (0.001) loss 1.5304 (1.5887) teacher_loss 0.9663 (0.9654) loss_zs_kd 0.1145 (0.1513) loss_oracle 0.5068 (0.5476) acc 75.0000 (74.3990) alaph_mean 0.3284 (0.3117) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5115) lr 1.9511e-03 eta 0:40:11
epoch [7/50] batch [280/288] time 0.085 (0.192) data 0.000 (0.001) loss 1.2373 (1.5829) teacher_loss 0.6099 (0.9616) loss_zs_kd 0.1272 (0.1496) loss_oracle 0.5638 (0.5465) acc 84.3750 (74.6540) alaph_mean 0.3158 (0.3118) alpha_min -0.0000 (0.0000) alpha_max 0.5014 (0.5112) lr 1.9511e-03 eta 0:39:42
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,394
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.4%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.2%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.2%, epoch: 7 *******
******* Domain a best val test acc: 83.4%, epoch: 7 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [8/50] batch [20/288] time 0.205 (0.208) data 0.000 (0.011) loss 1.4879 (1.5582) teacher_loss 0.7993 (0.9425) loss_zs_kd 0.2247 (0.1551) loss_oracle 0.5762 (0.5382) acc 87.5000 (75.7812) alaph_mean 0.2966 (0.3101) alpha_min 0.0000 (0.0000) alpha_max 0.5081 (0.5173) lr 1.9298e-03 eta 0:42:56
epoch [8/50] batch [40/288] time 0.190 (0.202) data 0.000 (0.006) loss 1.4074 (1.5258) teacher_loss 0.7632 (0.9167) loss_zs_kd 0.2158 (0.1540) loss_oracle 0.5363 (0.5321) acc 78.1250 (75.3125) alaph_mean 0.3524 (0.3192) alpha_min 0.0000 (0.0000) alpha_max 0.5067 (0.5129) lr 1.9298e-03 eta 0:41:32
epoch [8/50] batch [60/288] time 0.167 (0.199) data 0.001 (0.004) loss 2.2604 (1.5235) teacher_loss 1.4980 (0.9075) loss_zs_kd 0.1063 (0.1550) loss_oracle 0.7092 (0.5385) acc 59.3750 (75.3646) alaph_mean 0.2157 (0.3161) alpha_min 0.0000 (0.0000) alpha_max 0.5056 (0.5160) lr 1.9298e-03 eta 0:40:49
epoch [8/50] batch [80/288] time 0.189 (0.198) data 0.000 (0.003) loss 1.7521 (1.5084) teacher_loss 1.0586 (0.8978) loss_zs_kd 0.1096 (0.1561) loss_oracle 0.6387 (0.5326) acc 71.8750 (75.8203) alaph_mean 0.2556 (0.3204) alpha_min 0.0000 (0.0000) alpha_max 0.5035 (0.5144) lr 1.9298e-03 eta 0:40:36
epoch [8/50] batch [100/288] time 0.206 (0.197) data 0.000 (0.002) loss 1.3551 (1.5144) teacher_loss 0.7495 (0.9061) loss_zs_kd 0.2028 (0.1563) loss_oracle 0.5042 (0.5301) acc 81.2500 (76.0938) alaph_mean 0.3445 (0.3224) alpha_min 0.0000 (0.0000) alpha_max 0.5106 (0.5143) lr 1.9298e-03 eta 0:40:24
epoch [8/50] batch [120/288] time 0.214 (0.197) data 0.000 (0.002) loss 1.6573 (1.5090) teacher_loss 0.8491 (0.8992) loss_zs_kd 0.1772 (0.1580) loss_oracle 0.7196 (0.5308) acc 78.1250 (76.0677) alaph_mean 0.2392 (0.3219) alpha_min 0.0000 (0.0000) alpha_max 0.5028 (0.5130) lr 1.9298e-03 eta 0:40:13
epoch [8/50] batch [140/288] time 0.197 (0.196) data 0.000 (0.002) loss 1.3242 (1.5252) teacher_loss 0.8545 (0.9119) loss_zs_kd 0.2112 (0.1589) loss_oracle 0.3642 (0.5339) acc 71.8750 (75.6250) alaph_mean 0.4128 (0.3210) alpha_min 0.0000 (0.0000) alpha_max 0.5059 (0.5156) lr 1.9298e-03 eta 0:40:05
epoch [8/50] batch [160/288] time 0.198 (0.196) data 0.000 (0.002) loss 1.5884 (1.5444) teacher_loss 1.0762 (0.9284) loss_zs_kd 0.1762 (0.1597) loss_oracle 0.4241 (0.5361) acc 68.7500 (75.2539) alaph_mean 0.3597 (0.3201) alpha_min 0.0000 (0.0000) alpha_max 0.5048 (0.5145) lr 1.9298e-03 eta 0:39:56
epoch [8/50] batch [180/288] time 0.162 (0.195) data 0.000 (0.001) loss 1.8202 (1.5474) teacher_loss 1.2402 (0.9308) loss_zs_kd 0.1374 (0.1584) loss_oracle 0.5113 (0.5374) acc 71.8750 (75.1736) alaph_mean 0.3567 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.5112 (0.5137) lr 1.9298e-03 eta 0:39:42
epoch [8/50] batch [200/288] time 0.184 (0.194) data 0.000 (0.001) loss 1.6648 (1.5495) teacher_loss 1.0469 (0.9333) loss_zs_kd 0.1636 (0.1584) loss_oracle 0.5362 (0.5370) acc 75.0000 (75.0469) alaph_mean 0.3014 (0.3187) alpha_min 0.0000 (0.0000) alpha_max 0.5039 (0.5130) lr 1.9298e-03 eta 0:39:28
epoch [8/50] batch [220/288] time 0.199 (0.194) data 0.001 (0.001) loss 1.1695 (1.5450) teacher_loss 0.6245 (0.9298) loss_zs_kd 0.1450 (0.1578) loss_oracle 0.4724 (0.5363) acc 87.5000 (75.1989) alaph_mean 0.3688 (0.3183) alpha_min 0.0000 (0.0000) alpha_max 0.5063 (0.5129) lr 1.9298e-03 eta 0:39:24
epoch [8/50] batch [240/288] time 0.196 (0.194) data 0.000 (0.001) loss 1.4739 (1.5466) teacher_loss 0.8950 (0.9329) loss_zs_kd 0.1952 (0.1594) loss_oracle 0.4812 (0.5340) acc 75.0000 (75.0521) alaph_mean 0.3614 (0.3199) alpha_min 0.0000 (0.0000) alpha_max 0.5058 (0.5123) lr 1.9298e-03 eta 0:39:21
epoch [8/50] batch [260/288] time 0.196 (0.194) data 0.000 (0.001) loss 1.3864 (1.5483) teacher_loss 0.8511 (0.9331) loss_zs_kd 0.2002 (0.1593) loss_oracle 0.4352 (0.5355) acc 81.2500 (75.1322) alaph_mean 0.3492 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.5049 (0.5132) lr 1.9298e-03 eta 0:39:16
epoch [8/50] batch [280/288] time 0.093 (0.193) data 0.000 (0.001) loss 1.9657 (1.5539) teacher_loss 1.2871 (0.9384) loss_zs_kd 0.1871 (0.1592) loss_oracle 0.5851 (0.5359) acc 59.3750 (74.9777) alaph_mean 0.3346 (0.3193) alpha_min -0.0000 (0.0000) alpha_max 0.5097 (0.5142) lr 1.9298e-03 eta 0:38:54
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,407
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.9%
******* Domain a best val acc:      86.5%, epoch: 8 *******
******* Domain a best val test acc: 83.4%, epoch: 8 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [9/50] batch [20/288] time 0.191 (0.205) data 0.000 (0.014) loss 1.6118 (1.5666) teacher_loss 1.0645 (0.9467) loss_zs_kd 0.1657 (0.1595) loss_oracle 0.4645 (0.5401) acc 71.8750 (74.2188) alaph_mean 0.3782 (0.3305) alpha_min -0.0000 (0.0000) alpha_max 0.5072 (0.5241) lr 1.9048e-03 eta 0:41:17
epoch [9/50] batch [40/288] time 0.196 (0.196) data 0.000 (0.007) loss 1.0860 (1.5230) teacher_loss 0.6294 (0.9018) loss_zs_kd 0.0930 (0.1585) loss_oracle 0.4101 (0.5420) acc 78.1250 (75.7031) alaph_mean 0.4101 (0.3251) alpha_min -0.0000 (0.0000) alpha_max 0.7501 (0.5263) lr 1.9048e-03 eta 0:39:19
epoch [9/50] batch [60/288] time 0.192 (0.195) data 0.001 (0.005) loss 1.6293 (1.5310) teacher_loss 1.0273 (0.9102) loss_zs_kd 0.1531 (0.1596) loss_oracle 0.5254 (0.5410) acc 68.7500 (75.5208) alaph_mean 0.3253 (0.3213) alpha_min -0.0000 (0.0000) alpha_max 0.5039 (0.5195) lr 1.9048e-03 eta 0:39:04
epoch [9/50] batch [80/288] time 0.208 (0.195) data 0.000 (0.004) loss 1.3809 (1.5559) teacher_loss 0.7676 (0.9369) loss_zs_kd 0.1606 (0.1639) loss_oracle 0.5331 (0.5370) acc 84.3750 (75.0000) alaph_mean 0.3121 (0.3233) alpha_min 0.0000 (0.0000) alpha_max 0.5027 (0.5163) lr 1.9048e-03 eta 0:39:02
epoch [9/50] batch [100/288] time 0.198 (0.195) data 0.000 (0.003) loss 1.4214 (1.5533) teacher_loss 0.7666 (0.9386) loss_zs_kd 0.1562 (0.1625) loss_oracle 0.5767 (0.5335) acc 65.6250 (74.7500) alaph_mean 0.2965 (0.3238) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5143) lr 1.9048e-03 eta 0:38:56
epoch [9/50] batch [120/288] time 0.206 (0.195) data 0.000 (0.003) loss 1.5333 (1.5604) teacher_loss 0.9507 (0.9512) loss_zs_kd 0.1502 (0.1623) loss_oracle 0.5075 (0.5280) acc 71.8750 (74.2969) alaph_mean 0.3389 (0.3256) alpha_min 0.0000 (0.0000) alpha_max 0.5040 (0.5129) lr 1.9048e-03 eta 0:38:53
epoch [9/50] batch [140/288] time 0.200 (0.195) data 0.000 (0.002) loss 1.4094 (1.5681) teacher_loss 0.9062 (0.9580) loss_zs_kd 0.0828 (0.1624) loss_oracle 0.4617 (0.5290) acc 78.1250 (74.3527) alaph_mean 0.3440 (0.3248) alpha_min 0.0000 (0.0000) alpha_max 0.5053 (0.5121) lr 1.9048e-03 eta 0:38:47
epoch [9/50] batch [160/288] time 0.189 (0.195) data 0.000 (0.002) loss 1.4497 (1.5641) teacher_loss 0.7090 (0.9519) loss_zs_kd 0.1667 (0.1647) loss_oracle 0.6574 (0.5299) acc 81.2500 (74.4531) alaph_mean 0.2311 (0.3237) alpha_min 0.0000 (0.0000) alpha_max 0.5074 (0.5116) lr 1.9048e-03 eta 0:38:42
epoch [9/50] batch [180/288] time 0.197 (0.195) data 0.001 (0.002) loss 2.1194 (1.5649) teacher_loss 1.5166 (0.9541) loss_zs_kd 0.1872 (0.1637) loss_oracle 0.5092 (0.5290) acc 65.6250 (74.4271) alaph_mean 0.3750 (0.3245) alpha_min 0.0000 (0.0000) alpha_max 0.5070 (0.5123) lr 1.9048e-03 eta 0:38:37
epoch [9/50] batch [200/288] time 0.205 (0.195) data 0.000 (0.002) loss 1.6690 (1.5719) teacher_loss 1.1309 (0.9587) loss_zs_kd 0.1601 (0.1643) loss_oracle 0.4581 (0.5311) acc 68.7500 (74.3906) alaph_mean 0.3790 (0.3250) alpha_min 0.0000 (0.0000) alpha_max 0.5118 (0.5117) lr 1.9048e-03 eta 0:38:33
epoch [9/50] batch [220/288] time 0.190 (0.194) data 0.000 (0.001) loss 1.2966 (1.5744) teacher_loss 0.6230 (0.9585) loss_zs_kd 0.1728 (0.1639) loss_oracle 0.5872 (0.5339) acc 81.2500 (74.3466) alaph_mean 0.3061 (0.3241) alpha_min 0.0000 (0.0000) alpha_max 0.9146 (0.5145) lr 1.9048e-03 eta 0:38:28
epoch [9/50] batch [240/288] time 0.193 (0.194) data 0.000 (0.001) loss 1.4518 (1.5729) teacher_loss 0.9180 (0.9552) loss_zs_kd 0.2508 (0.1635) loss_oracle 0.4085 (0.5360) acc 78.1250 (74.4531) alaph_mean 0.4208 (0.3232) alpha_min 0.0000 (0.0000) alpha_max 0.5062 (0.5140) lr 1.9048e-03 eta 0:38:24
epoch [9/50] batch [260/288] time 0.196 (0.194) data 0.000 (0.001) loss 1.3893 (1.5756) teacher_loss 0.8550 (0.9584) loss_zs_kd 0.1303 (0.1630) loss_oracle 0.4692 (0.5357) acc 71.8750 (74.3990) alaph_mean 0.3312 (0.3236) alpha_min 0.0000 (0.0000) alpha_max 0.5046 (0.5149) lr 1.9048e-03 eta 0:38:20
epoch [9/50] batch [280/288] time 0.088 (0.194) data 0.000 (0.001) loss 1.4304 (1.5685) teacher_loss 0.9048 (0.9522) loss_zs_kd 0.0926 (0.1622) loss_oracle 0.4793 (0.5352) acc 78.1250 (74.5201) alaph_mean 0.3671 (0.3239) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5159) lr 1.9048e-03 eta 0:38:12
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,401
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,013
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.5%
******* Domain a best val acc:      86.5%, epoch: 8 *******
******* Domain a best val test acc: 83.4%, epoch: 8 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [10/50] batch [20/288] time 0.188 (0.208) data 0.000 (0.014) loss 1.3691 (1.5766) teacher_loss 0.7217 (0.9745) loss_zs_kd 0.1330 (0.1604) loss_oracle 0.5809 (0.5219) acc 81.2500 (75.7812) alaph_mean 0.3107 (0.3241) alpha_min 0.0000 (0.0000) alpha_max 0.5068 (0.5216) lr 1.8763e-03 eta 0:40:54
epoch [10/50] batch [40/288] time 0.198 (0.199) data 0.000 (0.007) loss 1.7218 (1.5502) teacher_loss 1.0508 (0.9332) loss_zs_kd 0.2198 (0.1641) loss_oracle 0.5611 (0.5350) acc 62.5000 (76.2500) alaph_mean 0.3042 (0.3246) alpha_min 0.0000 (0.0000) alpha_max 0.5028 (0.5145) lr 1.8763e-03 eta 0:39:01
epoch [10/50] batch [60/288] time 0.196 (0.197) data 0.000 (0.005) loss 1.4458 (1.5198) teacher_loss 0.8750 (0.8951) loss_zs_kd 0.1528 (0.1632) loss_oracle 0.4944 (0.5431) acc 78.1250 (76.8229) alaph_mean 0.3352 (0.3196) alpha_min 0.0000 (0.0000) alpha_max 0.6128 (0.5137) lr 1.8763e-03 eta 0:38:37
epoch [10/50] batch [80/288] time 0.192 (0.196) data 0.000 (0.004) loss 1.7002 (1.5374) teacher_loss 1.0977 (0.9125) loss_zs_kd 0.1394 (0.1662) loss_oracle 0.5328 (0.5418) acc 81.2500 (76.5234) alaph_mean 0.3291 (0.3203) alpha_min 0.0000 (0.0000) alpha_max 0.5069 (0.5209) lr 1.8763e-03 eta 0:38:24
epoch [10/50] batch [100/288] time 0.193 (0.196) data 0.000 (0.003) loss 1.8878 (1.5228) teacher_loss 1.2383 (0.9033) loss_zs_kd 0.1698 (0.1636) loss_oracle 0.5646 (0.5378) acc 68.7500 (76.5625) alaph_mean 0.3138 (0.3238) alpha_min 0.0000 (0.0000) alpha_max 0.5161 (0.5233) lr 1.8763e-03 eta 0:38:13
epoch [10/50] batch [120/288] time 0.195 (0.196) data 0.000 (0.003) loss 1.6573 (1.5220) teacher_loss 0.8877 (0.9004) loss_zs_kd 0.1528 (0.1632) loss_oracle 0.6932 (0.5400) acc 81.2500 (76.5625) alaph_mean 0.2639 (0.3234) alpha_min 0.0000 (0.0000) alpha_max 0.5115 (0.5209) lr 1.8763e-03 eta 0:38:09
epoch [10/50] batch [140/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.7064 (1.5399) teacher_loss 1.1992 (0.9178) loss_zs_kd 0.1557 (0.1638) loss_oracle 0.4294 (0.5402) acc 71.8750 (76.1384) alaph_mean 0.3704 (0.3242) alpha_min 0.0000 (0.0000) alpha_max 0.5039 (0.5188) lr 1.8763e-03 eta 0:38:05
epoch [10/50] batch [160/288] time 0.196 (0.195) data 0.000 (0.002) loss 1.1176 (1.5381) teacher_loss 0.6475 (0.9153) loss_zs_kd 0.1358 (0.1625) loss_oracle 0.4022 (0.5416) acc 84.3750 (75.8984) alaph_mean 0.4083 (0.3234) alpha_min 0.0000 (0.0000) alpha_max 0.5033 (0.5172) lr 1.8763e-03 eta 0:37:47
epoch [10/50] batch [180/288] time 0.191 (0.195) data 0.000 (0.002) loss 1.6118 (1.5516) teacher_loss 1.0664 (0.9277) loss_zs_kd 0.1356 (0.1624) loss_oracle 0.4776 (0.5427) acc 75.0000 (75.5382) alaph_mean 0.3314 (0.3221) alpha_min 0.0000 (0.0000) alpha_max 0.5091 (0.5162) lr 1.8763e-03 eta 0:37:42
epoch [10/50] batch [200/288] time 0.179 (0.194) data 0.000 (0.002) loss 1.8756 (1.5611) teacher_loss 1.1162 (0.9362) loss_zs_kd 0.1324 (0.1616) loss_oracle 0.6931 (0.5440) acc 65.6250 (75.2500) alaph_mean 0.2545 (0.3211) alpha_min 0.0000 (0.0000) alpha_max 0.5404 (0.5166) lr 1.8763e-03 eta 0:37:37
epoch [10/50] batch [220/288] time 0.215 (0.194) data 0.000 (0.001) loss 1.5270 (1.5573) teacher_loss 1.0029 (0.9349) loss_zs_kd 0.1574 (0.1611) loss_oracle 0.4454 (0.5418) acc 71.8750 (75.3409) alaph_mean 0.3815 (0.3217) alpha_min 0.0000 (0.0000) alpha_max 0.5165 (0.5163) lr 1.8763e-03 eta 0:37:33
epoch [10/50] batch [240/288] time 0.194 (0.194) data 0.000 (0.001) loss 1.3424 (1.5616) teacher_loss 0.8047 (0.9351) loss_zs_kd 0.1863 (0.1618) loss_oracle 0.4445 (0.5455) acc 81.2500 (75.2344) alaph_mean 0.3567 (0.3201) alpha_min -0.0000 (0.0000) alpha_max 0.5069 (0.5156) lr 1.8763e-03 eta 0:37:28
epoch [10/50] batch [260/288] time 0.195 (0.194) data 0.000 (0.001) loss 1.3951 (1.5616) teacher_loss 0.9873 (0.9354) loss_zs_kd 0.1974 (0.1637) loss_oracle 0.3091 (0.5443) acc 78.1250 (75.1202) alaph_mean 0.4701 (0.3208) alpha_min 0.0000 (0.0000) alpha_max 0.5081 (0.5153) lr 1.8763e-03 eta 0:37:24
epoch [10/50] batch [280/288] time 0.181 (0.194) data 0.000 (0.001) loss 2.2960 (1.5710) teacher_loss 1.4785 (0.9436) loss_zs_kd 0.2362 (0.1650) loss_oracle 0.6994 (0.5449) acc 65.6250 (74.9777) alaph_mean 0.2634 (0.3204) alpha_min 0.0000 (0.0000) alpha_max 0.5049 (0.5146) lr 1.8763e-03 eta 0:37:19
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 80.1%
******* Domain a best val acc:      86.6%, epoch: 10 *******
******* Domain a best val test acc: 83.3%, epoch: 10 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [11/50] batch [20/288] time 0.199 (0.210) data 0.000 (0.015) loss 2.1509 (1.5479) teacher_loss 1.4814 (0.9467) loss_zs_kd 0.1629 (0.1668) loss_oracle 0.5880 (0.5179) acc 65.6250 (74.3750) alaph_mean 0.3023 (0.3315) alpha_min 0.0000 (0.0000) alpha_max 0.5034 (0.5127) lr 1.8443e-03 eta 0:40:15
epoch [11/50] batch [40/288] time 0.197 (0.201) data 0.000 (0.007) loss 1.8201 (1.5946) teacher_loss 1.1436 (0.9588) loss_zs_kd 0.1912 (0.1700) loss_oracle 0.5810 (0.5507) acc 71.8750 (73.6719) alaph_mean 0.2892 (0.3156) alpha_min 0.0000 (0.0000) alpha_max 0.5040 (0.5095) lr 1.8443e-03 eta 0:38:27
epoch [11/50] batch [60/288] time 0.200 (0.199) data 0.000 (0.005) loss 1.2646 (1.5850) teacher_loss 0.7393 (0.9487) loss_zs_kd 0.1920 (0.1701) loss_oracle 0.4294 (0.5512) acc 87.5000 (74.4271) alaph_mean 0.4079 (0.3164) alpha_min -0.0000 (0.0000) alpha_max 0.5121 (0.5093) lr 1.8443e-03 eta 0:37:56
epoch [11/50] batch [80/288] time 0.194 (0.197) data 0.000 (0.004) loss 1.7029 (1.5586) teacher_loss 1.1074 (0.9248) loss_zs_kd 0.1185 (0.1676) loss_oracle 0.5363 (0.5500) acc 68.7500 (75.3906) alaph_mean 0.3071 (0.3169) alpha_min 0.0000 (0.0000) alpha_max 0.5044 (0.5088) lr 1.8443e-03 eta 0:37:38
epoch [11/50] batch [100/288] time 0.205 (0.197) data 0.000 (0.003) loss 1.5879 (1.5350) teacher_loss 1.0068 (0.9066) loss_zs_kd 0.2029 (0.1679) loss_oracle 0.4797 (0.5445) acc 81.2500 (75.7812) alaph_mean 0.3566 (0.3196) alpha_min 0.0000 (0.0000) alpha_max 0.5028 (0.5096) lr 1.8443e-03 eta 0:37:26
epoch [11/50] batch [120/288] time 0.217 (0.196) data 0.000 (0.003) loss 2.0061 (1.5418) teacher_loss 1.4092 (0.9133) loss_zs_kd 0.1466 (0.1669) loss_oracle 0.5236 (0.5451) acc 59.3750 (75.6771) alaph_mean 0.3220 (0.3198) alpha_min 0.0000 (0.0000) alpha_max 0.5350 (0.5130) lr 1.8443e-03 eta 0:37:18
epoch [11/50] batch [140/288] time 0.205 (0.196) data 0.000 (0.002) loss 1.3762 (1.5388) teacher_loss 0.7612 (0.9157) loss_zs_kd 0.1235 (0.1659) loss_oracle 0.5532 (0.5401) acc 84.3750 (75.5134) alaph_mean 0.2973 (0.3221) alpha_min -0.0000 (0.0000) alpha_max 0.5068 (0.5163) lr 1.8443e-03 eta 0:37:11
epoch [11/50] batch [160/288] time 0.184 (0.196) data 0.000 (0.002) loss 1.8542 (1.5550) teacher_loss 1.1865 (0.9299) loss_zs_kd 0.1632 (0.1657) loss_oracle 0.5861 (0.5422) acc 62.5000 (74.9414) alaph_mean 0.3116 (0.3216) alpha_min 0.0000 (0.0000) alpha_max 0.5035 (0.5174) lr 1.8443e-03 eta 0:37:04
epoch [11/50] batch [180/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.6207 (1.5537) teacher_loss 0.9961 (0.9287) loss_zs_kd 0.1730 (0.1661) loss_oracle 0.5381 (0.5419) acc 78.1250 (74.8958) alaph_mean 0.3178 (0.3215) alpha_min 0.0000 (0.0000) alpha_max 0.5044 (0.5162) lr 1.8443e-03 eta 0:36:59
epoch [11/50] batch [200/288] time 0.190 (0.195) data 0.000 (0.002) loss 1.7065 (1.5550) teacher_loss 1.0029 (0.9287) loss_zs_kd 0.2369 (0.1661) loss_oracle 0.5851 (0.5432) acc 78.1250 (75.0312) alaph_mean 0.3335 (0.3213) alpha_min 0.0000 (0.0000) alpha_max 0.5068 (0.5153) lr 1.8443e-03 eta 0:36:52
epoch [11/50] batch [220/288] time 0.192 (0.195) data 0.000 (0.001) loss 1.5456 (1.5643) teacher_loss 0.8877 (0.9305) loss_zs_kd 0.1771 (0.1682) loss_oracle 0.5694 (0.5497) acc 84.3750 (74.9716) alaph_mean 0.3287 (0.3187) alpha_min -0.0000 (0.0000) alpha_max 0.5067 (0.5152) lr 1.8443e-03 eta 0:36:46
epoch [11/50] batch [240/288] time 0.190 (0.195) data 0.000 (0.001) loss 1.7006 (1.5671) teacher_loss 0.9907 (0.9301) loss_zs_kd 0.1513 (0.1685) loss_oracle 0.6343 (0.5527) acc 75.0000 (75.0521) alaph_mean 0.2756 (0.3168) alpha_min -0.0000 (0.0000) alpha_max 0.5046 (0.5147) lr 1.8443e-03 eta 0:36:41
epoch [11/50] batch [260/288] time 0.194 (0.195) data 0.000 (0.001) loss 1.4829 (1.5680) teacher_loss 0.8740 (0.9314) loss_zs_kd 0.1468 (0.1683) loss_oracle 0.5355 (0.5524) acc 81.2500 (75.0721) alaph_mean 0.3370 (0.3166) alpha_min 0.0000 (0.0000) alpha_max 0.5098 (0.5140) lr 1.8443e-03 eta 0:36:36
epoch [11/50] batch [280/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.2257 (1.5581) teacher_loss 0.6729 (0.9233) loss_zs_kd 0.1212 (0.1675) loss_oracle 0.4923 (0.5511) acc 81.2500 (75.2902) alaph_mean 0.3440 (0.3172) alpha_min 0.0000 (0.0000) alpha_max 0.5073 (0.5148) lr 1.8443e-03 eta 0:36:31
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,408
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,016
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.7%
******* Domain a best val acc:      86.6%, epoch: 10 *******
******* Domain a best val test acc: 83.3%, epoch: 10 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [12/50] batch [20/288] time 0.199 (0.214) data 0.000 (0.015) loss 1.2706 (1.5787) teacher_loss 0.8389 (0.9287) loss_zs_kd 0.1311 (0.1758) loss_oracle 0.3662 (0.5622) acc 84.3750 (76.8750) alaph_mean 0.4061 (0.3135) alpha_min 0.0000 (0.0000) alpha_max 0.5041 (0.5244) lr 1.8090e-03 eta 0:39:58
epoch [12/50] batch [40/288] time 0.195 (0.204) data 0.000 (0.008) loss 1.6598 (1.5626) teacher_loss 1.0273 (0.9082) loss_zs_kd 0.1146 (0.1751) loss_oracle 0.5751 (0.5669) acc 75.0000 (76.3281) alaph_mean 0.3004 (0.3122) alpha_min -0.0000 (0.0000) alpha_max 0.6687 (0.5226) lr 1.8090e-03 eta 0:38:01
epoch [12/50] batch [60/288] time 0.193 (0.200) data 0.000 (0.005) loss 1.7374 (1.5884) teacher_loss 1.1143 (0.9339) loss_zs_kd 0.1621 (0.1733) loss_oracle 0.5421 (0.5679) acc 68.7500 (75.0000) alaph_mean 0.3199 (0.3141) alpha_min 0.0000 (0.0000) alpha_max 0.5062 (0.5266) lr 1.8090e-03 eta 0:37:18
epoch [12/50] batch [80/288] time 0.194 (0.199) data 0.000 (0.004) loss 2.6425 (1.5950) teacher_loss 1.8887 (0.9396) loss_zs_kd 0.1951 (0.1745) loss_oracle 0.6563 (0.5681) acc 53.1250 (75.2344) alaph_mean 0.2811 (0.3166) alpha_min 0.0000 (0.0000) alpha_max 0.5090 (0.5226) lr 1.8090e-03 eta 0:36:56
epoch [12/50] batch [100/288] time 0.195 (0.198) data 0.000 (0.003) loss 1.6529 (1.5811) teacher_loss 0.9907 (0.9233) loss_zs_kd 0.1827 (0.1718) loss_oracle 0.5709 (0.5719) acc 71.8750 (75.5625) alaph_mean 0.3134 (0.3147) alpha_min 0.0000 (0.0000) alpha_max 0.5056 (0.5197) lr 1.8090e-03 eta 0:36:40
epoch [12/50] batch [120/288] time 0.190 (0.197) data 0.000 (0.003) loss 1.9291 (1.5746) teacher_loss 1.2490 (0.9182) loss_zs_kd 0.1798 (0.1710) loss_oracle 0.5902 (0.5708) acc 68.7500 (75.7812) alaph_mean 0.2716 (0.3159) alpha_min 0.0000 (0.0000) alpha_max 0.5075 (0.5193) lr 1.8090e-03 eta 0:36:29
epoch [12/50] batch [140/288] time 0.197 (0.197) data 0.000 (0.002) loss 1.0891 (1.5793) teacher_loss 0.4758 (0.9240) loss_zs_kd 0.1503 (0.1724) loss_oracle 0.5381 (0.5691) acc 90.6250 (75.6696) alaph_mean 0.3268 (0.3174) alpha_min 0.0000 (0.0000) alpha_max 0.5184 (0.5177) lr 1.8090e-03 eta 0:36:23
epoch [12/50] batch [160/288] time 0.193 (0.197) data 0.000 (0.002) loss 1.6392 (1.5760) teacher_loss 0.9287 (0.9209) loss_zs_kd 0.2279 (0.1729) loss_oracle 0.5965 (0.5687) acc 71.8750 (75.5664) alaph_mean 0.3023 (0.3184) alpha_min -0.0000 (0.0000) alpha_max 0.5026 (0.5193) lr 1.8090e-03 eta 0:36:15
epoch [12/50] batch [180/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.5156 (1.5915) teacher_loss 0.7983 (0.9328) loss_zs_kd 0.2050 (0.1741) loss_oracle 0.6147 (0.5717) acc 87.5000 (75.2604) alaph_mean 0.3130 (0.3173) alpha_min -0.0000 (0.0000) alpha_max 0.5084 (0.5187) lr 1.8090e-03 eta 0:36:08
epoch [12/50] batch [200/288] time 0.190 (0.196) data 0.000 (0.002) loss 1.8057 (1.5976) teacher_loss 1.0029 (0.9361) loss_zs_kd 0.1887 (0.1742) loss_oracle 0.7084 (0.5744) acc 78.1250 (75.3125) alaph_mean 0.2371 (0.3158) alpha_min -0.0000 (0.0000) alpha_max 0.5059 (0.5204) lr 1.8090e-03 eta 0:36:01
epoch [12/50] batch [220/288] time 0.190 (0.196) data 0.000 (0.002) loss 1.5352 (1.5983) teacher_loss 0.7334 (0.9353) loss_zs_kd 0.2191 (0.1741) loss_oracle 0.6922 (0.5760) acc 78.1250 (75.3551) alaph_mean 0.2530 (0.3141) alpha_min -0.0000 (0.0000) alpha_max 0.5133 (0.5236) lr 1.8090e-03 eta 0:35:55
epoch [12/50] batch [240/288] time 0.197 (0.196) data 0.000 (0.001) loss 1.7884 (1.5993) teacher_loss 1.2109 (0.9391) loss_zs_kd 0.2139 (0.1730) loss_oracle 0.4705 (0.5737) acc 62.5000 (75.2344) alaph_mean 0.4176 (0.3146) alpha_min 0.0000 (0.0000) alpha_max 0.5100 (0.5232) lr 1.8090e-03 eta 0:35:49
epoch [12/50] batch [260/288] time 0.194 (0.195) data 0.000 (0.001) loss 1.4317 (1.5959) teacher_loss 0.7549 (0.9380) loss_zs_kd 0.1824 (0.1721) loss_oracle 0.5857 (0.5718) acc 84.3750 (75.2043) alaph_mean 0.3286 (0.3152) alpha_min 0.0000 (0.0000) alpha_max 0.9888 (0.5243) lr 1.8090e-03 eta 0:35:44
epoch [12/50] batch [280/288] time 0.182 (0.195) data 0.000 (0.001) loss 1.5581 (1.5893) teacher_loss 0.8921 (0.9366) loss_zs_kd 0.1358 (0.1709) loss_oracle 0.5981 (0.5672) acc 75.0000 (75.1228) alaph_mean 0.2881 (0.3169) alpha_min 0.0000 (0.0000) alpha_max 0.5623 (0.5234) lr 1.8090e-03 eta 0:35:39
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,398
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,011
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.4%
******* Domain a best val acc:      86.6%, epoch: 10 *******
******* Domain a best val test acc: 83.3%, epoch: 10 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [13/50] batch [20/288] time 0.199 (0.204) data 0.000 (0.012) loss 1.3994 (1.4908) teacher_loss 0.7788 (0.8827) loss_zs_kd 0.1837 (0.1655) loss_oracle 0.5287 (0.5254) acc 78.1250 (75.4688) alaph_mean 0.3685 (0.3359) alpha_min 0.0000 (0.0000) alpha_max 0.7818 (0.5235) lr 1.7705e-03 eta 0:37:04
epoch [13/50] batch [40/288] time 0.196 (0.199) data 0.000 (0.006) loss 1.3859 (1.5760) teacher_loss 0.7725 (0.9627) loss_zs_kd 0.2164 (0.1704) loss_oracle 0.5052 (0.5280) acc 81.2500 (74.0625) alaph_mean 0.3459 (0.3312) alpha_min 0.0000 (0.0000) alpha_max 0.5144 (0.5168) lr 1.7705e-03 eta 0:36:08
epoch [13/50] batch [60/288] time 0.191 (0.197) data 0.000 (0.004) loss 2.4889 (1.6588) teacher_loss 1.5752 (1.0187) loss_zs_kd 0.2113 (0.1734) loss_oracle 0.8081 (0.5534) acc 62.5000 (72.5000) alaph_mean 0.1899 (0.3152) alpha_min -0.0000 (0.0000) alpha_max 0.5065 (0.5142) lr 1.7705e-03 eta 0:35:42
epoch [13/50] batch [80/288] time 0.196 (0.196) data 0.000 (0.003) loss 1.8676 (1.6353) teacher_loss 1.2246 (0.9934) loss_zs_kd 0.2240 (0.1732) loss_oracle 0.5310 (0.5553) acc 68.7500 (73.3594) alaph_mean 0.3499 (0.3165) alpha_min 0.0000 (0.0000) alpha_max 0.5067 (0.5136) lr 1.7705e-03 eta 0:35:31
epoch [13/50] batch [100/288] time 0.205 (0.196) data 0.000 (0.003) loss 1.1146 (1.6045) teacher_loss 0.5674 (0.9613) loss_zs_kd 0.0917 (0.1692) loss_oracle 0.5014 (0.5586) acc 87.5000 (74.3125) alaph_mean 0.3562 (0.3153) alpha_min 0.0000 (0.0000) alpha_max 0.5077 (0.5140) lr 1.7705e-03 eta 0:35:21
epoch [13/50] batch [120/288] time 0.191 (0.195) data 0.000 (0.002) loss 1.6733 (1.5950) teacher_loss 0.9653 (0.9523) loss_zs_kd 0.1665 (0.1692) loss_oracle 0.6247 (0.5580) acc 78.1250 (74.7396) alaph_mean 0.2641 (0.3159) alpha_min -0.0000 (0.0000) alpha_max 0.5025 (0.5132) lr 1.7705e-03 eta 0:35:14
epoch [13/50] batch [140/288] time 0.194 (0.195) data 0.000 (0.002) loss 1.6747 (1.6030) teacher_loss 0.9492 (0.9585) loss_zs_kd 0.1760 (0.1693) loss_oracle 0.6375 (0.5599) acc 75.0000 (74.5759) alaph_mean 0.2812 (0.3161) alpha_min 0.0000 (0.0000) alpha_max 0.5031 (0.5142) lr 1.7705e-03 eta 0:35:09
epoch [13/50] batch [160/288] time 0.193 (0.195) data 0.000 (0.002) loss 1.6122 (1.5907) teacher_loss 1.0293 (0.9443) loss_zs_kd 0.1518 (0.1699) loss_oracle 0.5070 (0.5615) acc 75.0000 (74.9609) alaph_mean 0.3201 (0.3160) alpha_min -0.0000 (0.0000) alpha_max 0.5042 (0.5133) lr 1.7705e-03 eta 0:35:04
epoch [13/50] batch [180/288] time 0.199 (0.195) data 0.000 (0.002) loss 1.1294 (1.5970) teacher_loss 0.6533 (0.9520) loss_zs_kd 0.1253 (0.1713) loss_oracle 0.4134 (0.5594) acc 81.2500 (74.7396) alaph_mean 0.3734 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5172 (0.5146) lr 1.7705e-03 eta 0:34:59
epoch [13/50] batch [200/288] time 0.185 (0.195) data 0.000 (0.001) loss 1.9802 (1.5969) teacher_loss 1.2949 (0.9498) loss_zs_kd 0.1295 (0.1728) loss_oracle 0.6205 (0.5607) acc 65.6250 (74.6719) alaph_mean 0.3141 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5142 (0.5140) lr 1.7705e-03 eta 0:34:53
epoch [13/50] batch [220/288] time 0.186 (0.195) data 0.000 (0.001) loss 1.7609 (1.5908) teacher_loss 0.8291 (0.9444) loss_zs_kd 0.2059 (0.1735) loss_oracle 0.8289 (0.5597) acc 78.1250 (74.8580) alaph_mean 0.1893 (0.3175) alpha_min -0.0000 (0.0000) alpha_max 0.5056 (0.5153) lr 1.7705e-03 eta 0:34:50
epoch [13/50] batch [240/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.6409 (1.5860) teacher_loss 1.1191 (0.9413) loss_zs_kd 0.1707 (0.1722) loss_oracle 0.4364 (0.5586) acc 75.0000 (74.9870) alaph_mean 0.3784 (0.3180) alpha_min 0.0000 (0.0000) alpha_max 0.5056 (0.5156) lr 1.7705e-03 eta 0:34:45
epoch [13/50] batch [260/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.6117 (1.5724) teacher_loss 0.9692 (0.9296) loss_zs_kd 0.1520 (0.1700) loss_oracle 0.5665 (0.5578) acc 71.8750 (75.3005) alaph_mean 0.3425 (0.3186) alpha_min 0.0000 (0.0000) alpha_max 0.5051 (0.5156) lr 1.7705e-03 eta 0:34:40
epoch [13/50] batch [280/288] time 0.194 (0.195) data 0.000 (0.001) loss 1.2849 (1.5723) teacher_loss 0.6357 (0.9318) loss_zs_kd 0.1536 (0.1692) loss_oracle 0.5724 (0.5558) acc 81.2500 (75.1786) alaph_mean 0.3401 (0.3195) alpha_min -0.0000 (0.0000) alpha_max 0.5019 (0.5151) lr 1.7705e-03 eta 0:34:36
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,396
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.8%
******* Domain a best val acc:      86.6%, epoch: 10 *******
******* Domain a best val test acc: 83.3%, epoch: 10 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [14/50] batch [20/288] time 0.197 (0.215) data 0.000 (0.013) loss 1.4940 (1.5803) teacher_loss 0.9878 (0.9411) loss_zs_kd 0.1580 (0.1643) loss_oracle 0.4272 (0.5570) acc 68.7500 (75.3125) alaph_mean 0.3898 (0.3239) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5053) lr 1.7290e-03 eta 0:38:05
epoch [14/50] batch [40/288] time 0.202 (0.203) data 0.000 (0.007) loss 1.5534 (1.5770) teacher_loss 0.7651 (0.9181) loss_zs_kd 0.2155 (0.1644) loss_oracle 0.6806 (0.5767) acc 78.1250 (75.5469) alaph_mean 0.2506 (0.3162) alpha_min 0.0000 (0.0000) alpha_max 0.5052 (0.5061) lr 1.7290e-03 eta 0:35:59
epoch [14/50] batch [60/288] time 0.195 (0.200) data 0.000 (0.005) loss 1.4634 (1.5652) teacher_loss 0.8584 (0.8972) loss_zs_kd 0.1526 (0.1679) loss_oracle 0.5287 (0.5840) acc 75.0000 (75.6250) alaph_mean 0.3484 (0.3120) alpha_min 0.0000 (0.0000) alpha_max 0.5048 (0.5112) lr 1.7290e-03 eta 0:35:23
epoch [14/50] batch [80/288] time 0.192 (0.199) data 0.000 (0.004) loss 1.8118 (1.5852) teacher_loss 1.0693 (0.9126) loss_zs_kd 0.1682 (0.1730) loss_oracle 0.6584 (0.5861) acc 75.0000 (75.4297) alaph_mean 0.2685 (0.3105) alpha_min 0.0000 (0.0000) alpha_max 0.8519 (0.5154) lr 1.7290e-03 eta 0:35:02
epoch [14/50] batch [100/288] time 0.171 (0.198) data 0.000 (0.003) loss 1.6356 (1.5799) teacher_loss 1.0117 (0.9122) loss_zs_kd 0.1820 (0.1755) loss_oracle 0.5329 (0.5799) acc 71.8750 (75.2812) alaph_mean 0.3446 (0.3124) alpha_min 0.0000 (0.0000) alpha_max 0.5097 (0.5193) lr 1.7290e-03 eta 0:34:52
epoch [14/50] batch [120/288] time 0.208 (0.198) data 0.000 (0.002) loss 1.2496 (1.5860) teacher_loss 0.7148 (0.9221) loss_zs_kd 0.1601 (0.1740) loss_oracle 0.4547 (0.5769) acc 84.3750 (75.1823) alaph_mean 0.3598 (0.3117) alpha_min 0.0000 (0.0000) alpha_max 0.5076 (0.5175) lr 1.7290e-03 eta 0:34:42
epoch [14/50] batch [140/288] time 0.199 (0.197) data 0.000 (0.002) loss 1.2374 (1.5929) teacher_loss 0.7720 (0.9349) loss_zs_kd 0.1414 (0.1731) loss_oracle 0.3947 (0.5715) acc 81.2500 (75.0670) alaph_mean 0.3896 (0.3125) alpha_min 0.0000 (0.0000) alpha_max 0.5996 (0.5191) lr 1.7290e-03 eta 0:34:33
epoch [14/50] batch [160/288] time 0.205 (0.197) data 0.000 (0.002) loss 1.1859 (1.5784) teacher_loss 0.6685 (0.9236) loss_zs_kd 0.1532 (0.1712) loss_oracle 0.4409 (0.5692) acc 78.1250 (75.3906) alaph_mean 0.3613 (0.3127) alpha_min 0.0000 (0.0000) alpha_max 0.6457 (0.5189) lr 1.7290e-03 eta 0:34:24
epoch [14/50] batch [180/288] time 0.200 (0.197) data 0.000 (0.002) loss 1.0374 (1.5632) teacher_loss 0.5742 (0.9122) loss_zs_kd 0.1213 (0.1697) loss_oracle 0.4025 (0.5661) acc 87.5000 (75.5556) alaph_mean 0.4062 (0.3127) alpha_min -0.0000 (0.0000) alpha_max 0.5080 (0.5181) lr 1.7290e-03 eta 0:34:19
epoch [14/50] batch [200/288] time 0.188 (0.196) data 0.000 (0.002) loss 1.8750 (1.5610) teacher_loss 1.0283 (0.9172) loss_zs_kd 0.2091 (0.1698) loss_oracle 0.7421 (0.5589) acc 68.7500 (75.3750) alaph_mean 0.2473 (0.3158) alpha_min 0.0000 (0.0000) alpha_max 0.5104 (0.5175) lr 1.7290e-03 eta 0:34:13
epoch [14/50] batch [220/288] time 0.195 (0.196) data 0.000 (0.001) loss 0.9547 (1.5536) teacher_loss 0.3428 (0.9115) loss_zs_kd 0.1343 (0.1697) loss_oracle 0.5448 (0.5572) acc 93.7500 (75.6676) alaph_mean 0.3298 (0.3164) alpha_min -0.0000 (0.0000) alpha_max 0.5060 (0.5174) lr 1.7290e-03 eta 0:34:08
epoch [14/50] batch [240/288] time 0.193 (0.196) data 0.000 (0.001) loss 1.3336 (1.5479) teacher_loss 0.7466 (0.9056) loss_zs_kd 0.1762 (0.1701) loss_oracle 0.4989 (0.5572) acc 84.3750 (75.8594) alaph_mean 0.3308 (0.3163) alpha_min 0.0000 (0.0000) alpha_max 0.5050 (0.5167) lr 1.7290e-03 eta 0:33:59
epoch [14/50] batch [260/288] time 0.200 (0.196) data 0.000 (0.001) loss 1.6737 (1.5489) teacher_loss 1.0977 (0.9067) loss_zs_kd 0.1868 (0.1703) loss_oracle 0.4826 (0.5571) acc 71.8750 (75.8654) alaph_mean 0.3374 (0.3166) alpha_min 0.0000 (0.0000) alpha_max 0.5843 (0.5166) lr 1.7290e-03 eta 0:33:54
epoch [14/50] batch [280/288] time 0.188 (0.196) data 0.000 (0.001) loss 1.0951 (1.5455) teacher_loss 0.5093 (0.9049) loss_zs_kd 0.1518 (0.1697) loss_oracle 0.5099 (0.5557) acc 84.3750 (75.9375) alaph_mean 0.3430 (0.3175) alpha_min 0.0000 (0.0000) alpha_max 0.5024 (0.5166) lr 1.7290e-03 eta 0:33:49
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,391
* accuracy: 86.1%
* error: 13.9%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,017
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.8%
******* Domain a best val acc:      86.6%, epoch: 10 *******
******* Domain a best val test acc: 83.3%, epoch: 10 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [15/50] batch [20/288] time 0.199 (0.209) data 0.000 (0.013) loss 1.7714 (1.5467) teacher_loss 1.0244 (0.9011) loss_zs_kd 0.1539 (0.1774) loss_oracle 0.6700 (0.5569) acc 78.1250 (76.5625) alaph_mean 0.2549 (0.3177) alpha_min 0.0000 (0.0000) alpha_max 0.5191 (0.5215) lr 1.6845e-03 eta 0:35:58
epoch [15/50] batch [40/288] time 0.195 (0.202) data 0.000 (0.007) loss 1.8443 (1.5940) teacher_loss 1.0986 (0.9562) loss_zs_kd 0.1526 (0.1761) loss_oracle 0.6693 (0.5497) acc 78.1250 (75.0000) alaph_mean 0.2674 (0.3268) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5242) lr 1.6845e-03 eta 0:34:44
epoch [15/50] batch [60/288] time 0.194 (0.199) data 0.000 (0.005) loss 1.2693 (1.5578) teacher_loss 0.6128 (0.9144) loss_zs_kd 0.1480 (0.1761) loss_oracle 0.5825 (0.5554) acc 81.2500 (76.1458) alaph_mean 0.3062 (0.3258) alpha_min -0.0000 (0.0000) alpha_max 0.5047 (0.5213) lr 1.6845e-03 eta 0:34:16
epoch [15/50] batch [80/288] time 0.192 (0.198) data 0.000 (0.003) loss 1.5858 (1.5356) teacher_loss 0.8936 (0.8892) loss_zs_kd 0.1437 (0.1730) loss_oracle 0.6204 (0.5599) acc 78.1250 (76.5625) alaph_mean 0.3077 (0.3246) alpha_min -0.0000 (0.0000) alpha_max 0.5110 (0.5188) lr 1.6845e-03 eta 0:33:56
epoch [15/50] batch [100/288] time 0.194 (0.197) data 0.000 (0.003) loss 1.5940 (1.5241) teacher_loss 0.9404 (0.8782) loss_zs_kd 0.1617 (0.1735) loss_oracle 0.5727 (0.5591) acc 75.0000 (76.7500) alaph_mean 0.3158 (0.3270) alpha_min -0.0000 (0.0000) alpha_max 0.5035 (0.5216) lr 1.6845e-03 eta 0:33:44
epoch [15/50] batch [120/288] time 0.204 (0.197) data 0.000 (0.002) loss 1.8280 (1.5253) teacher_loss 1.1592 (0.8813) loss_zs_kd 0.2277 (0.1723) loss_oracle 0.5550 (0.5579) acc 75.0000 (76.6667) alaph_mean 0.3360 (0.3280) alpha_min 0.0000 (0.0000) alpha_max 0.7185 (0.5238) lr 1.6845e-03 eta 0:33:36
epoch [15/50] batch [140/288] time 0.206 (0.196) data 0.000 (0.002) loss 1.6936 (1.5358) teacher_loss 1.0859 (0.8899) loss_zs_kd 0.1515 (0.1729) loss_oracle 0.5319 (0.5594) acc 68.7500 (76.2723) alaph_mean 0.3430 (0.3277) alpha_min -0.0000 (0.0000) alpha_max 0.5035 (0.5233) lr 1.6845e-03 eta 0:33:28
epoch [15/50] batch [160/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.5881 (1.5421) teacher_loss 1.0371 (0.8961) loss_zs_kd 0.1310 (0.1727) loss_oracle 0.4855 (0.5596) acc 68.7500 (76.1133) alaph_mean 0.3318 (0.3272) alpha_min -0.0000 (0.0000) alpha_max 0.5103 (0.5219) lr 1.6845e-03 eta 0:33:21
epoch [15/50] batch [180/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.9683 (1.5484) teacher_loss 1.3457 (0.9053) loss_zs_kd 0.1854 (0.1728) loss_oracle 0.5299 (0.5567) acc 71.8750 (75.8333) alaph_mean 0.3300 (0.3277) alpha_min 0.0000 (0.0000) alpha_max 0.5105 (0.5226) lr 1.6845e-03 eta 0:33:15
epoch [15/50] batch [200/288] time 0.195 (0.196) data 0.000 (0.001) loss 1.2772 (1.5497) teacher_loss 0.6792 (0.9106) loss_zs_kd 0.2223 (0.1733) loss_oracle 0.4869 (0.5525) acc 78.1250 (75.6875) alaph_mean 0.3648 (0.3287) alpha_min -0.0000 (0.0000) alpha_max 0.5039 (0.5216) lr 1.6845e-03 eta 0:33:09
epoch [15/50] batch [220/288] time 0.173 (0.196) data 0.000 (0.001) loss 1.6861 (1.5506) teacher_loss 1.0342 (0.9106) loss_zs_kd 0.1673 (0.1737) loss_oracle 0.5682 (0.5531) acc 75.0000 (75.6676) alaph_mean 0.3086 (0.3270) alpha_min -0.0000 (0.0000) alpha_max 0.5050 (0.5202) lr 1.6845e-03 eta 0:33:04
epoch [15/50] batch [240/288] time 0.202 (0.195) data 0.000 (0.001) loss 1.5979 (1.5491) teacher_loss 0.7588 (0.9070) loss_zs_kd 0.2693 (0.1749) loss_oracle 0.7044 (0.5547) acc 75.0000 (75.8333) alaph_mean 0.2827 (0.3268) alpha_min 0.0000 (0.0000) alpha_max 0.5324 (0.5200) lr 1.6845e-03 eta 0:32:59
epoch [15/50] batch [260/288] time 0.197 (0.195) data 0.000 (0.001) loss 1.5180 (1.5507) teacher_loss 0.9702 (0.9089) loss_zs_kd 0.1426 (0.1753) loss_oracle 0.4765 (0.5541) acc 81.2500 (75.8534) alaph_mean 0.3750 (0.3270) alpha_min 0.0000 (0.0000) alpha_max 0.5083 (0.5206) lr 1.6845e-03 eta 0:32:55
epoch [15/50] batch [280/288] time 0.204 (0.195) data 0.000 (0.001) loss 1.5365 (1.5492) teacher_loss 0.9966 (0.9084) loss_zs_kd 0.1337 (0.1744) loss_oracle 0.4730 (0.5536) acc 71.8750 (75.7589) alaph_mean 0.3450 (0.3264) alpha_min 0.0000 (0.0000) alpha_max 0.5060 (0.5201) lr 1.6845e-03 eta 0:32:49
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,416
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.0%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,017
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 80.0%
******* Domain a best val acc:      86.7%, epoch: 15 *******
******* Domain a best val test acc: 83.1%, epoch: 15 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [16/50] batch [20/288] time 0.190 (0.204) data 0.000 (0.012) loss 2.2059 (1.5942) teacher_loss 1.2979 (0.9476) loss_zs_kd 0.1801 (0.1691) loss_oracle 0.8180 (0.5621) acc 71.8750 (75.7812) alaph_mean 0.1796 (0.3219) alpha_min 0.0000 (0.0000) alpha_max 0.5078 (0.5151) lr 1.6374e-03 eta 0:34:16
epoch [16/50] batch [40/288] time 0.195 (0.198) data 0.000 (0.006) loss 1.7418 (1.5582) teacher_loss 1.0527 (0.9050) loss_zs_kd 0.1849 (0.1725) loss_oracle 0.5966 (0.5670) acc 71.8750 (75.6250) alaph_mean 0.3050 (0.3189) alpha_min 0.0000 (0.0000) alpha_max 0.5056 (0.5269) lr 1.6374e-03 eta 0:33:04
epoch [16/50] batch [60/288] time 0.183 (0.195) data 0.000 (0.004) loss 2.2227 (1.5718) teacher_loss 1.1846 (0.9077) loss_zs_kd 0.2148 (0.1739) loss_oracle 0.9308 (0.5771) acc 75.0000 (75.4688) alaph_mean 0.1017 (0.3152) alpha_min -0.0000 (0.0000) alpha_max 0.5029 (0.5202) lr 1.6374e-03 eta 0:32:35
epoch [16/50] batch [80/288] time 0.204 (0.195) data 0.000 (0.003) loss 1.9830 (1.5760) teacher_loss 1.1494 (0.9048) loss_zs_kd 0.2198 (0.1738) loss_oracle 0.7236 (0.5843) acc 65.6250 (75.7812) alaph_mean 0.2730 (0.3111) alpha_min 0.0000 (0.0000) alpha_max 0.5093 (0.5170) lr 1.6374e-03 eta 0:32:28
epoch [16/50] batch [100/288] time 0.197 (0.195) data 0.000 (0.002) loss 1.5824 (1.5859) teacher_loss 0.9976 (0.9187) loss_zs_kd 0.2049 (0.1760) loss_oracle 0.4823 (0.5791) acc 65.6250 (75.0625) alaph_mean 0.3808 (0.3133) alpha_min 0.0000 (0.0000) alpha_max 0.5149 (0.5151) lr 1.6374e-03 eta 0:32:23
epoch [16/50] batch [120/288] time 0.192 (0.195) data 0.000 (0.002) loss 1.5117 (1.5795) teacher_loss 0.6528 (0.9140) loss_zs_kd 0.2146 (0.1726) loss_oracle 0.7516 (0.5793) acc 90.6250 (74.9740) alaph_mean 0.2505 (0.3129) alpha_min 0.0000 (0.0000) alpha_max 0.5079 (0.5148) lr 1.6374e-03 eta 0:32:18
epoch [16/50] batch [140/288] time 0.193 (0.195) data 0.000 (0.002) loss 1.5247 (1.5558) teacher_loss 0.9194 (0.8947) loss_zs_kd 0.1533 (0.1716) loss_oracle 0.5286 (0.5752) acc 81.2500 (75.6250) alaph_mean 0.3523 (0.3156) alpha_min 0.0000 (0.0000) alpha_max 0.5080 (0.5180) lr 1.6374e-03 eta 0:32:14
epoch [16/50] batch [160/288] time 0.194 (0.195) data 0.000 (0.002) loss 1.7034 (1.5568) teacher_loss 1.0557 (0.8933) loss_zs_kd 0.1521 (0.1727) loss_oracle 0.5717 (0.5771) acc 78.1250 (75.8008) alaph_mean 0.3142 (0.3152) alpha_min -0.0000 (0.0000) alpha_max 0.5067 (0.5190) lr 1.6374e-03 eta 0:32:09
epoch [16/50] batch [180/288] time 0.197 (0.194) data 0.000 (0.001) loss 1.4326 (1.5631) teacher_loss 0.7373 (0.9023) loss_zs_kd 0.1492 (0.1736) loss_oracle 0.6207 (0.5740) acc 81.2500 (75.8333) alaph_mean 0.3130 (0.3172) alpha_min 0.0000 (0.0000) alpha_max 0.5055 (0.5195) lr 1.6374e-03 eta 0:32:05
epoch [16/50] batch [200/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.5030 (1.5672) teacher_loss 0.8960 (0.9078) loss_zs_kd 0.1984 (0.1728) loss_oracle 0.5079 (0.5731) acc 78.1250 (75.6094) alaph_mean 0.3433 (0.3174) alpha_min -0.0000 (0.0000) alpha_max 0.5040 (0.5193) lr 1.6374e-03 eta 0:32:01
epoch [16/50] batch [220/288] time 0.194 (0.194) data 0.000 (0.001) loss 1.2075 (1.5682) teacher_loss 0.6528 (0.9068) loss_zs_kd 0.1506 (0.1731) loss_oracle 0.4794 (0.5749) acc 81.2500 (75.6108) alaph_mean 0.3839 (0.3160) alpha_min 0.0000 (0.0000) alpha_max 0.5089 (0.5191) lr 1.6374e-03 eta 0:31:57
epoch [16/50] batch [240/288] time 0.191 (0.194) data 0.000 (0.001) loss 1.4125 (1.5592) teacher_loss 0.7163 (0.8983) loss_zs_kd 0.2219 (0.1735) loss_oracle 0.5853 (0.5742) acc 84.3750 (75.8203) alaph_mean 0.2960 (0.3165) alpha_min 0.0000 (0.0000) alpha_max 0.5047 (0.5186) lr 1.6374e-03 eta 0:31:53
epoch [16/50] batch [260/288] time 0.194 (0.194) data 0.000 (0.001) loss 1.4551 (1.5619) teacher_loss 0.8330 (0.9020) loss_zs_kd 0.1412 (0.1739) loss_oracle 0.5515 (0.5729) acc 78.1250 (75.7692) alaph_mean 0.3308 (0.3175) alpha_min 0.0000 (0.0000) alpha_max 0.5436 (0.5191) lr 1.6374e-03 eta 0:31:49
epoch [16/50] batch [280/288] time 0.190 (0.194) data 0.000 (0.001) loss 2.1839 (1.5705) teacher_loss 1.4844 (0.9099) loss_zs_kd 0.2061 (0.1741) loss_oracle 0.5964 (0.5736) acc 62.5000 (75.6138) alaph_mean 0.2919 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5084 (0.5185) lr 1.6374e-03 eta 0:31:45
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,404
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,017
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.8%
******* Domain a best val acc:      86.7%, epoch: 15 *******
******* Domain a best val test acc: 83.1%, epoch: 15 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [17/50] batch [20/288] time 0.193 (0.199) data 0.000 (0.012) loss 1.8731 (1.4516) teacher_loss 1.3096 (0.7656) loss_zs_kd 0.1501 (0.1557) loss_oracle 0.4885 (0.6082) acc 78.1250 (80.7812) alaph_mean 0.3555 (0.3039) alpha_min -0.0000 (0.0000) alpha_max 0.5067 (0.5194) lr 1.5878e-03 eta 0:32:20
epoch [17/50] batch [40/288] time 0.193 (0.196) data 0.000 (0.006) loss 1.3990 (1.5281) teacher_loss 0.7891 (0.8522) loss_zs_kd 0.2143 (0.1644) loss_oracle 0.5028 (0.5937) acc 87.5000 (78.9844) alaph_mean 0.3833 (0.3120) alpha_min 0.0000 (0.0000) alpha_max 0.5097 (0.5217) lr 1.5878e-03 eta 0:31:56
epoch [17/50] batch [60/288] time 0.199 (0.196) data 0.001 (0.004) loss 2.0558 (1.5554) teacher_loss 1.3057 (0.8739) loss_zs_kd 0.1618 (0.1691) loss_oracle 0.6693 (0.5969) acc 71.8750 (77.9688) alaph_mean 0.2468 (0.3128) alpha_min 0.0000 (0.0000) alpha_max 0.5030 (0.5237) lr 1.5878e-03 eta 0:31:44
epoch [17/50] batch [80/288] time 0.192 (0.195) data 0.000 (0.003) loss 1.4627 (1.5525) teacher_loss 0.7803 (0.8700) loss_zs_kd 0.1846 (0.1700) loss_oracle 0.5901 (0.5975) acc 84.3750 (78.0469) alaph_mean 0.3067 (0.3116) alpha_min 0.0000 (0.0000) alpha_max 0.5073 (0.5225) lr 1.5878e-03 eta 0:31:37
epoch [17/50] batch [100/288] time 0.195 (0.195) data 0.000 (0.003) loss 1.5720 (1.5503) teacher_loss 0.9707 (0.8714) loss_zs_kd 0.1582 (0.1702) loss_oracle 0.5222 (0.5938) acc 68.7500 (77.6562) alaph_mean 0.3459 (0.3127) alpha_min -0.0000 (0.0000) alpha_max 0.5048 (0.5201) lr 1.5878e-03 eta 0:31:31
epoch [17/50] batch [120/288] time 0.198 (0.195) data 0.000 (0.002) loss 1.5044 (1.5566) teacher_loss 1.0488 (0.8816) loss_zs_kd 0.1251 (0.1704) loss_oracle 0.3930 (0.5899) acc 75.0000 (76.9531) alaph_mean 0.4078 (0.3135) alpha_min -0.0000 (0.0000) alpha_max 0.5093 (0.5189) lr 1.5878e-03 eta 0:31:26
epoch [17/50] batch [140/288] time 0.196 (0.195) data 0.000 (0.002) loss 1.7063 (1.5718) teacher_loss 1.0332 (0.9005) loss_zs_kd 0.1795 (0.1723) loss_oracle 0.5834 (0.5852) acc 75.0000 (76.4062) alaph_mean 0.3435 (0.3174) alpha_min 0.0000 (0.0000) alpha_max 0.5095 (0.5188) lr 1.5878e-03 eta 0:31:21
epoch [17/50] batch [160/288] time 0.194 (0.195) data 0.000 (0.002) loss 1.6649 (1.5706) teacher_loss 1.0342 (0.9018) loss_zs_kd 0.1645 (0.1733) loss_oracle 0.5484 (0.5822) acc 71.8750 (76.2891) alaph_mean 0.3102 (0.3189) alpha_min -0.0000 (0.0000) alpha_max 0.5022 (0.5194) lr 1.5878e-03 eta 0:31:16
epoch [17/50] batch [180/288] time 0.193 (0.195) data 0.000 (0.002) loss 1.4146 (1.5635) teacher_loss 0.7681 (0.8939) loss_zs_kd 0.1458 (0.1725) loss_oracle 0.5736 (0.5834) acc 75.0000 (76.4236) alaph_mean 0.3032 (0.3186) alpha_min 0.0000 (0.0000) alpha_max 0.5143 (0.5181) lr 1.5878e-03 eta 0:31:12
epoch [17/50] batch [200/288] time 0.177 (0.195) data 0.000 (0.001) loss 1.3221 (1.5624) teacher_loss 0.7568 (0.8930) loss_zs_kd 0.1221 (0.1726) loss_oracle 0.5043 (0.5831) acc 71.8750 (76.3281) alaph_mean 0.3556 (0.3185) alpha_min 0.0000 (0.0000) alpha_max 0.5065 (0.5178) lr 1.5878e-03 eta 0:31:06
epoch [17/50] batch [220/288] time 0.197 (0.195) data 0.000 (0.001) loss 1.2526 (1.5620) teacher_loss 0.6182 (0.8917) loss_zs_kd 0.1324 (0.1733) loss_oracle 0.5683 (0.5837) acc 84.3750 (76.2642) alaph_mean 0.3440 (0.3176) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5170) lr 1.5878e-03 eta 0:31:02
epoch [17/50] batch [240/288] time 0.185 (0.195) data 0.000 (0.001) loss 1.5361 (1.5610) teacher_loss 0.9590 (0.8913) loss_zs_kd 0.1822 (0.1735) loss_oracle 0.4860 (0.5830) acc 71.8750 (76.3151) alaph_mean 0.3827 (0.3177) alpha_min 0.0000 (0.0000) alpha_max 0.5057 (0.5179) lr 1.5878e-03 eta 0:30:58
epoch [17/50] batch [260/288] time 0.190 (0.195) data 0.000 (0.001) loss 2.0831 (1.5649) teacher_loss 1.2617 (0.8938) loss_zs_kd 0.1879 (0.1747) loss_oracle 0.7274 (0.5837) acc 62.5000 (76.2139) alaph_mean 0.2424 (0.3172) alpha_min -0.0000 (0.0000) alpha_max 0.5604 (0.5193) lr 1.5878e-03 eta 0:30:54
epoch [17/50] batch [280/288] time 0.189 (0.194) data 0.000 (0.001) loss 1.8486 (1.5649) teacher_loss 1.1357 (0.8916) loss_zs_kd 0.2114 (0.1763) loss_oracle 0.6072 (0.5851) acc 68.7500 (76.2835) alaph_mean 0.3291 (0.3166) alpha_min 0.0000 (0.0000) alpha_max 0.6831 (0.5195) lr 1.5878e-03 eta 0:30:50
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,395
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,998
* accuracy: 82.3%
* error: 17.7%
* macro_f1: 78.9%
******* Domain a best val acc:      86.7%, epoch: 15 *******
******* Domain a best val test acc: 83.1%, epoch: 15 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [18/50] batch [20/288] time 0.185 (0.210) data 0.000 (0.014) loss 1.9450 (1.5523) teacher_loss 1.2363 (0.8855) loss_zs_kd 0.1696 (0.1870) loss_oracle 0.6239 (0.5733) acc 68.7500 (77.8125) alaph_mean 0.2887 (0.3316) alpha_min 0.0000 (0.0000) alpha_max 0.5996 (0.5135) lr 1.5358e-03 eta 0:33:13
epoch [18/50] batch [40/288] time 0.193 (0.201) data 0.000 (0.007) loss 1.9299 (1.5939) teacher_loss 1.2725 (0.9181) loss_zs_kd 0.2471 (0.1852) loss_oracle 0.5339 (0.5833) acc 65.6250 (75.8594) alaph_mean 0.3444 (0.3285) alpha_min 0.0000 (0.0123) alpha_max 0.5084 (0.5118) lr 1.5358e-03 eta 0:31:43
epoch [18/50] batch [60/288] time 0.197 (0.199) data 0.000 (0.005) loss 1.3564 (1.5994) teacher_loss 0.5728 (0.9153) loss_zs_kd 0.1982 (0.1825) loss_oracle 0.6846 (0.5929) acc 84.3750 (75.8333) alaph_mean 0.3073 (0.3204) alpha_min -0.0000 (0.0082) alpha_max 0.5089 (0.5111) lr 1.5358e-03 eta 0:31:21
epoch [18/50] batch [80/288] time 0.172 (0.199) data 0.000 (0.004) loss 2.4364 (1.6144) teacher_loss 1.6904 (0.9371) loss_zs_kd 0.2562 (0.1832) loss_oracle 0.6178 (0.5857) acc 53.1250 (75.3125) alaph_mean 0.3009 (0.3233) alpha_min -0.0000 (0.0061) alpha_max 0.5046 (0.5108) lr 1.5358e-03 eta 0:31:10
epoch [18/50] batch [100/288] time 0.184 (0.198) data 0.000 (0.003) loss 1.6527 (1.5959) teacher_loss 0.8804 (0.9253) loss_zs_kd 0.1415 (0.1819) loss_oracle 0.7016 (0.5796) acc 81.2500 (75.5000) alaph_mean 0.2327 (0.3239) alpha_min 0.0000 (0.0049) alpha_max 0.5130 (0.5135) lr 1.5358e-03 eta 0:30:57
epoch [18/50] batch [120/288] time 0.196 (0.197) data 0.001 (0.003) loss 1.5693 (1.5800) teacher_loss 1.0078 (0.9100) loss_zs_kd 0.1782 (0.1803) loss_oracle 0.4724 (0.5798) acc 78.1250 (75.8073) alaph_mean 0.3965 (0.3245) alpha_min 0.0000 (0.0041) alpha_max 0.5070 (0.5144) lr 1.5358e-03 eta 0:30:50
epoch [18/50] batch [140/288] time 0.206 (0.197) data 0.000 (0.002) loss 1.4078 (1.5867) teacher_loss 0.8364 (0.9179) loss_zs_kd 0.1896 (0.1807) loss_oracle 0.4766 (0.5784) acc 75.0000 (75.5134) alaph_mean 0.3762 (0.3243) alpha_min 0.0000 (0.0035) alpha_max 0.5108 (0.5134) lr 1.5358e-03 eta 0:30:42
epoch [18/50] batch [160/288] time 0.193 (0.196) data 0.000 (0.002) loss 1.4771 (1.5789) teacher_loss 0.7412 (0.9110) loss_zs_kd 0.2247 (0.1820) loss_oracle 0.6235 (0.5769) acc 81.2500 (75.7031) alaph_mean 0.2815 (0.3232) alpha_min -0.0000 (0.0031) alpha_max 0.5043 (0.5125) lr 1.5358e-03 eta 0:30:35
epoch [18/50] batch [180/288] time 0.197 (0.196) data 0.000 (0.002) loss 1.4562 (1.5724) teacher_loss 0.8179 (0.9058) loss_zs_kd 0.2421 (0.1831) loss_oracle 0.5173 (0.5750) acc 68.7500 (75.8854) alaph_mean 0.3205 (0.3232) alpha_min 0.0000 (0.0027) alpha_max 0.5109 (0.5121) lr 1.5358e-03 eta 0:30:29
epoch [18/50] batch [200/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.7719 (1.5616) teacher_loss 1.1465 (0.8969) loss_zs_kd 0.1850 (0.1821) loss_oracle 0.5330 (0.5737) acc 68.7500 (76.0469) alaph_mean 0.3590 (0.3236) alpha_min 0.0000 (0.0025) alpha_max 0.5049 (0.5127) lr 1.5358e-03 eta 0:30:24
epoch [18/50] batch [220/288] time 0.195 (0.196) data 0.000 (0.001) loss 2.0833 (1.5672) teacher_loss 1.4092 (0.9024) loss_zs_kd 0.1660 (0.1821) loss_oracle 0.5911 (0.5738) acc 59.3750 (75.8523) alaph_mean 0.3142 (0.3234) alpha_min -0.0000 (0.0022) alpha_max 0.5091 (0.5122) lr 1.5358e-03 eta 0:30:17
epoch [18/50] batch [240/288] time 0.195 (0.196) data 0.000 (0.001) loss 1.2425 (1.5690) teacher_loss 0.6411 (0.9030) loss_zs_kd 0.2065 (0.1824) loss_oracle 0.4981 (0.5748) acc 84.3750 (75.7031) alaph_mean 0.3896 (0.3232) alpha_min 0.0000 (0.0020) alpha_max 0.5104 (0.5125) lr 1.5358e-03 eta 0:30:12
epoch [18/50] batch [260/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.4038 (1.5664) teacher_loss 0.8994 (0.9019) loss_zs_kd 0.1170 (0.1819) loss_oracle 0.4459 (0.5736) acc 81.2500 (75.8173) alaph_mean 0.3793 (0.3233) alpha_min 0.0000 (0.0019) alpha_max 0.5072 (0.5122) lr 1.5358e-03 eta 0:30:06
epoch [18/50] batch [280/288] time 0.179 (0.195) data 0.000 (0.001) loss 1.7434 (1.5708) teacher_loss 1.1045 (0.9071) loss_zs_kd 0.2174 (0.1818) loss_oracle 0.5302 (0.5728) acc 65.6250 (75.6585) alaph_mean 0.3382 (0.3233) alpha_min 0.0000 (0.0018) alpha_max 0.5070 (0.5119) lr 1.5358e-03 eta 0:30:01
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,406
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.0%
******* Domain a best val acc:      86.7%, epoch: 15 *******
******* Domain a best val test acc: 83.1%, epoch: 15 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [19/50] batch [20/288] time 0.178 (0.199) data 0.000 (0.014) loss 1.5369 (1.5121) teacher_loss 0.9277 (0.8363) loss_zs_kd 0.1538 (0.1817) loss_oracle 0.5323 (0.5850) acc 75.0000 (77.5000) alaph_mean 0.3037 (0.3135) alpha_min 0.0000 (0.0000) alpha_max 0.6242 (0.5144) lr 1.4818e-03 eta 0:30:32
epoch [19/50] batch [40/288] time 0.192 (0.196) data 0.000 (0.007) loss 1.8370 (1.5784) teacher_loss 1.0596 (0.8972) loss_zs_kd 0.1961 (0.1832) loss_oracle 0.6794 (0.5896) acc 68.7500 (75.7812) alaph_mean 0.2933 (0.3142) alpha_min 0.0000 (0.0000) alpha_max 0.5078 (0.5125) lr 1.4818e-03 eta 0:30:02
epoch [19/50] batch [60/288] time 0.175 (0.196) data 0.001 (0.005) loss 1.8903 (1.6351) teacher_loss 1.1602 (0.9444) loss_zs_kd 0.1865 (0.1859) loss_oracle 0.6369 (0.5977) acc 75.0000 (74.7396) alaph_mean 0.2912 (0.3135) alpha_min -0.0000 (0.0000) alpha_max 0.5058 (0.5122) lr 1.4818e-03 eta 0:29:53
epoch [19/50] batch [80/288] time 0.183 (0.195) data 0.000 (0.004) loss 1.5842 (1.5892) teacher_loss 0.8774 (0.9016) loss_zs_kd 0.1611 (0.1835) loss_oracle 0.6262 (0.5958) acc 68.7500 (75.6641) alaph_mean 0.2828 (0.3128) alpha_min -0.0000 (0.0000) alpha_max 0.5049 (0.5127) lr 1.4818e-03 eta 0:29:45
epoch [19/50] batch [100/288] time 0.199 (0.195) data 0.000 (0.003) loss 1.4614 (1.5766) teacher_loss 0.9985 (0.8958) loss_zs_kd 0.1696 (0.1831) loss_oracle 0.3781 (0.5893) acc 75.0000 (75.7812) alaph_mean 0.4323 (0.3158) alpha_min 0.0000 (0.0000) alpha_max 0.8417 (0.5150) lr 1.4818e-03 eta 0:29:34
epoch [19/50] batch [120/288] time 0.193 (0.195) data 0.000 (0.003) loss 1.9120 (1.5698) teacher_loss 1.3066 (0.8918) loss_zs_kd 0.1882 (0.1831) loss_oracle 0.5112 (0.5865) acc 68.7500 (76.0417) alaph_mean 0.3626 (0.3173) alpha_min -0.0000 (0.0000) alpha_max 0.5137 (0.5143) lr 1.4818e-03 eta 0:29:29
epoch [19/50] batch [140/288] time 0.191 (0.194) data 0.000 (0.002) loss 1.8190 (1.5728) teacher_loss 1.2158 (0.8936) loss_zs_kd 0.1446 (0.1818) loss_oracle 0.5309 (0.5883) acc 78.1250 (76.0938) alaph_mean 0.3194 (0.3168) alpha_min -0.0000 (0.0000) alpha_max 0.5069 (0.5152) lr 1.4818e-03 eta 0:29:23
epoch [19/50] batch [160/288] time 0.194 (0.194) data 0.000 (0.002) loss 1.6096 (1.5770) teacher_loss 0.9443 (0.8975) loss_zs_kd 0.1718 (0.1789) loss_oracle 0.5794 (0.5901) acc 75.0000 (75.8984) alaph_mean 0.3018 (0.3154) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5163) lr 1.4818e-03 eta 0:29:19
epoch [19/50] batch [180/288] time 0.197 (0.194) data 0.000 (0.002) loss 1.9031 (1.5824) teacher_loss 1.1230 (0.9017) loss_zs_kd 0.1757 (0.1785) loss_oracle 0.6922 (0.5914) acc 71.8750 (75.6771) alaph_mean 0.2562 (0.3151) alpha_min -0.0000 (0.0000) alpha_max 0.5095 (0.5162) lr 1.4818e-03 eta 0:29:15
epoch [19/50] batch [200/288] time 0.194 (0.194) data 0.000 (0.002) loss 1.2153 (1.5768) teacher_loss 0.6479 (0.8960) loss_zs_kd 0.2335 (0.1788) loss_oracle 0.4507 (0.5914) acc 84.3750 (75.6406) alaph_mean 0.3786 (0.3151) alpha_min 0.0000 (0.0000) alpha_max 0.5499 (0.5156) lr 1.4818e-03 eta 0:29:12
epoch [19/50] batch [220/288] time 0.200 (0.194) data 0.000 (0.001) loss 1.3256 (1.5837) teacher_loss 0.8130 (0.9037) loss_zs_kd 0.2090 (0.1812) loss_oracle 0.4081 (0.5893) acc 78.1250 (75.5682) alaph_mean 0.4171 (0.3164) alpha_min 0.0000 (0.0000) alpha_max 0.5100 (0.5158) lr 1.4818e-03 eta 0:29:07
epoch [19/50] batch [240/288] time 0.194 (0.194) data 0.000 (0.001) loss 1.3489 (1.5828) teacher_loss 0.6533 (0.9030) loss_zs_kd 0.2270 (0.1815) loss_oracle 0.5820 (0.5891) acc 81.2500 (75.5990) alaph_mean 0.2928 (0.3157) alpha_min 0.0000 (0.0000) alpha_max 0.5046 (0.5162) lr 1.4818e-03 eta 0:29:03
epoch [19/50] batch [260/288] time 0.195 (0.194) data 0.000 (0.001) loss 1.5957 (1.5792) teacher_loss 0.9917 (0.8991) loss_zs_kd 0.1689 (0.1820) loss_oracle 0.5195 (0.5892) acc 75.0000 (75.7572) alaph_mean 0.3242 (0.3161) alpha_min 0.0000 (0.0000) alpha_max 0.5088 (0.5181) lr 1.4818e-03 eta 0:28:59
epoch [19/50] batch [280/288] time 0.191 (0.194) data 0.000 (0.001) loss 2.0467 (1.5799) teacher_loss 1.3379 (0.9000) loss_zs_kd 0.2524 (0.1827) loss_oracle 0.5826 (0.5885) acc 62.5000 (75.7701) alaph_mean 0.3031 (0.3162) alpha_min 0.0000 (0.0000) alpha_max 0.5048 (0.5183) lr 1.4818e-03 eta 0:28:55
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,420
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.1%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,005
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.2%
******* Domain a best val acc:      86.8%, epoch: 19 *******
******* Domain a best val test acc: 82.6%, epoch: 19 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [20/50] batch [20/288] time 0.196 (0.198) data 0.000 (0.011) loss 2.2087 (1.5653) teacher_loss 1.3379 (0.8823) loss_zs_kd 0.1917 (0.1877) loss_oracle 0.7749 (0.5891) acc 65.6250 (76.0938) alaph_mean 0.1938 (0.3172) alpha_min 0.0000 (0.0000) alpha_max 0.5045 (0.5131) lr 1.4258e-03 eta 0:29:25
epoch [20/50] batch [40/288] time 0.195 (0.196) data 0.000 (0.005) loss 1.6536 (1.5643) teacher_loss 0.9067 (0.8832) loss_zs_kd 0.2781 (0.1937) loss_oracle 0.6078 (0.5842) acc 78.1250 (75.9375) alaph_mean 0.3565 (0.3246) alpha_min 0.0000 (0.0000) alpha_max 0.5066 (0.5125) lr 1.4258e-03 eta 0:29:00
epoch [20/50] batch [60/288] time 0.193 (0.193) data 0.000 (0.004) loss 1.3238 (1.5642) teacher_loss 0.7227 (0.8773) loss_zs_kd 0.1137 (0.1910) loss_oracle 0.5443 (0.5914) acc 84.3750 (76.5625) alaph_mean 0.3517 (0.3183) alpha_min 0.0000 (0.0000) alpha_max 0.5028 (0.5158) lr 1.4258e-03 eta 0:28:32
epoch [20/50] batch [80/288] time 0.192 (0.193) data 0.000 (0.003) loss 1.4805 (1.5613) teacher_loss 0.7671 (0.8794) loss_zs_kd 0.1478 (0.1890) loss_oracle 0.6395 (0.5874) acc 81.2500 (76.6797) alaph_mean 0.2978 (0.3214) alpha_min 0.0000 (0.0000) alpha_max 0.5019 (0.5178) lr 1.4258e-03 eta 0:28:31
epoch [20/50] batch [100/288] time 0.194 (0.194) data 0.000 (0.002) loss 1.6027 (1.5584) teacher_loss 1.0703 (0.8769) loss_zs_kd 0.1688 (0.1850) loss_oracle 0.4480 (0.5890) acc 71.8750 (76.8750) alaph_mean 0.3987 (0.3195) alpha_min 0.0000 (0.0000) alpha_max 0.5051 (0.5171) lr 1.4258e-03 eta 0:28:28
epoch [20/50] batch [120/288] time 0.190 (0.193) data 0.000 (0.002) loss 1.0538 (1.5735) teacher_loss 0.5225 (0.8949) loss_zs_kd 0.0888 (0.1824) loss_oracle 0.4869 (0.5874) acc 84.3750 (76.5104) alaph_mean 0.3309 (0.3183) alpha_min 0.0000 (0.0000) alpha_max 0.5194 (0.5178) lr 1.4258e-03 eta 0:28:21
epoch [20/50] batch [140/288] time 0.193 (0.193) data 0.000 (0.002) loss 1.4047 (1.5802) teacher_loss 0.6812 (0.8979) loss_zs_kd 0.1895 (0.1836) loss_oracle 0.6289 (0.5905) acc 81.2500 (76.2946) alaph_mean 0.3288 (0.3183) alpha_min 0.0000 (0.0000) alpha_max 0.5492 (0.5171) lr 1.4258e-03 eta 0:28:19
epoch [20/50] batch [160/288] time 0.195 (0.193) data 0.000 (0.001) loss 1.5512 (1.5781) teacher_loss 0.9531 (0.8929) loss_zs_kd 0.1238 (0.1835) loss_oracle 0.5362 (0.5935) acc 78.1250 (76.4453) alaph_mean 0.3538 (0.3191) alpha_min 0.0000 (0.0000) alpha_max 0.5133 (0.5165) lr 1.4258e-03 eta 0:28:16
epoch [20/50] batch [180/288] time 0.192 (0.194) data 0.000 (0.001) loss 1.3678 (1.5731) teacher_loss 0.7178 (0.8873) loss_zs_kd 0.1712 (0.1838) loss_oracle 0.5644 (0.5939) acc 81.2500 (76.4236) alaph_mean 0.3581 (0.3196) alpha_min 0.0000 (0.0000) alpha_max 0.5040 (0.5165) lr 1.4258e-03 eta 0:28:13
epoch [20/50] batch [200/288] time 0.195 (0.194) data 0.000 (0.001) loss 2.0628 (1.5721) teacher_loss 1.3379 (0.8897) loss_zs_kd 0.3253 (0.1842) loss_oracle 0.5623 (0.5904) acc 65.6250 (76.2969) alaph_mean 0.3568 (0.3222) alpha_min -0.0000 (0.0000) alpha_max 0.5066 (0.5165) lr 1.4258e-03 eta 0:28:09
epoch [20/50] batch [220/288] time 0.174 (0.193) data 0.000 (0.001) loss 1.3338 (1.5630) teacher_loss 0.7529 (0.8801) loss_zs_kd 0.1730 (0.1846) loss_oracle 0.4943 (0.5905) acc 78.1250 (76.5341) alaph_mean 0.3671 (0.3218) alpha_min 0.0000 (0.0000) alpha_max 0.5081 (0.5160) lr 1.4258e-03 eta 0:28:04
epoch [20/50] batch [240/288] time 0.189 (0.194) data 0.000 (0.001) loss 1.7203 (1.5612) teacher_loss 0.9766 (0.8759) loss_zs_kd 0.1680 (0.1856) loss_oracle 0.6597 (0.5925) acc 71.8750 (76.5885) alaph_mean 0.2946 (0.3211) alpha_min -0.0000 (0.0000) alpha_max 0.5031 (0.5164) lr 1.4258e-03 eta 0:28:01
epoch [20/50] batch [260/288] time 0.190 (0.194) data 0.000 (0.001) loss 1.8149 (1.5550) teacher_loss 1.1670 (0.8720) loss_zs_kd 0.1415 (0.1853) loss_oracle 0.5771 (0.5903) acc 68.7500 (76.6106) alaph_mean 0.3464 (0.3223) alpha_min 0.0000 (0.0000) alpha_max 0.5052 (0.5163) lr 1.4258e-03 eta 0:27:58
epoch [20/50] batch [280/288] time 0.196 (0.194) data 0.000 (0.001) loss 1.1899 (1.5527) teacher_loss 0.6118 (0.8697) loss_zs_kd 0.1480 (0.1845) loss_oracle 0.5041 (0.5908) acc 84.3750 (76.6518) alaph_mean 0.3581 (0.3221) alpha_min 0.0000 (0.0000) alpha_max 0.5077 (0.5159) lr 1.4258e-03 eta 0:27:54
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,412
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 80.1%
******* Domain a best val acc:      86.8%, epoch: 19 *******
******* Domain a best val test acc: 82.6%, epoch: 19 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [21/50] batch [20/288] time 0.188 (0.191) data 0.000 (0.011) loss 1.1624 (1.5327) teacher_loss 0.4336 (0.8428) loss_zs_kd 0.1734 (0.1945) loss_oracle 0.6422 (0.5926) acc 90.6250 (76.0938) alaph_mean 0.2604 (0.3174) alpha_min 0.0000 (0.0000) alpha_max 0.5157 (0.5178) lr 1.3681e-03 eta 0:27:24
epoch [21/50] batch [40/288] time 0.197 (0.190) data 0.000 (0.005) loss 1.2008 (1.5344) teacher_loss 0.5747 (0.8409) loss_zs_kd 0.1904 (0.1866) loss_oracle 0.5309 (0.6002) acc 84.3750 (76.0938) alaph_mean 0.3278 (0.3110) alpha_min 0.0000 (0.0000) alpha_max 0.5075 (0.5201) lr 1.3681e-03 eta 0:27:15
epoch [21/50] batch [60/288] time 0.195 (0.192) data 0.000 (0.004) loss 1.4176 (1.5477) teacher_loss 0.7461 (0.8627) loss_zs_kd 0.1691 (0.1807) loss_oracle 0.5869 (0.5947) acc 78.1250 (76.4583) alaph_mean 0.3028 (0.3109) alpha_min 0.0000 (0.0000) alpha_max 0.5099 (0.5189) lr 1.3681e-03 eta 0:27:23
epoch [21/50] batch [80/288] time 0.187 (0.192) data 0.000 (0.003) loss 1.4148 (1.5696) teacher_loss 0.5942 (0.8900) loss_zs_kd 0.1386 (0.1757) loss_oracle 0.7513 (0.5918) acc 81.2500 (75.7812) alaph_mean 0.2477 (0.3113) alpha_min 0.0000 (0.0000) alpha_max 0.5133 (0.5180) lr 1.3681e-03 eta 0:27:24
epoch [21/50] batch [100/288] time 0.194 (0.192) data 0.000 (0.002) loss 1.4274 (1.5510) teacher_loss 0.7866 (0.8740) loss_zs_kd 0.1353 (0.1747) loss_oracle 0.5731 (0.5897) acc 78.1250 (76.2188) alaph_mean 0.3439 (0.3131) alpha_min 0.0000 (0.0000) alpha_max 0.5063 (0.5186) lr 1.3681e-03 eta 0:27:23
epoch [21/50] batch [120/288] time 0.181 (0.193) data 0.000 (0.002) loss 1.5303 (1.5619) teacher_loss 0.6797 (0.8790) loss_zs_kd 0.1451 (0.1759) loss_oracle 0.7780 (0.5949) acc 84.3750 (76.3021) alaph_mean 0.1993 (0.3104) alpha_min 0.0000 (0.0000) alpha_max 0.5084 (0.5172) lr 1.3681e-03 eta 0:27:20
epoch [21/50] batch [140/288] time 0.197 (0.193) data 0.000 (0.002) loss 1.5725 (1.5727) teacher_loss 0.8789 (0.8912) loss_zs_kd 0.1865 (0.1767) loss_oracle 0.6004 (0.5932) acc 81.2500 (75.9375) alaph_mean 0.3030 (0.3114) alpha_min -0.0000 (0.0000) alpha_max 0.5052 (0.5182) lr 1.3681e-03 eta 0:27:19
epoch [21/50] batch [160/288] time 0.197 (0.193) data 0.000 (0.002) loss 1.4706 (1.5883) teacher_loss 0.8760 (0.9045) loss_zs_kd 0.1733 (0.1794) loss_oracle 0.5080 (0.5941) acc 75.0000 (75.6641) alaph_mean 0.3457 (0.3122) alpha_min 0.0000 (0.0000) alpha_max 0.5049 (0.5209) lr 1.3681e-03 eta 0:27:16
epoch [21/50] batch [180/288] time 0.189 (0.193) data 0.000 (0.001) loss 1.9541 (1.5935) teacher_loss 1.1133 (0.9068) loss_zs_kd 0.1593 (0.1807) loss_oracle 0.7611 (0.5964) acc 75.0000 (75.5903) alaph_mean 0.2437 (0.3120) alpha_min -0.0000 (0.0000) alpha_max 0.5063 (0.5208) lr 1.3681e-03 eta 0:27:13
epoch [21/50] batch [200/288] time 0.193 (0.193) data 0.000 (0.001) loss 1.7947 (1.5867) teacher_loss 1.0957 (0.9013) loss_zs_kd 0.1580 (0.1803) loss_oracle 0.6200 (0.5952) acc 75.0000 (75.7812) alaph_mean 0.2672 (0.3122) alpha_min 0.0000 (0.0000) alpha_max 0.5091 (0.5203) lr 1.3681e-03 eta 0:27:10
epoch [21/50] batch [220/288] time 0.157 (0.193) data 0.000 (0.001) loss 1.4733 (1.5853) teacher_loss 0.7837 (0.8978) loss_zs_kd 0.1586 (0.1804) loss_oracle 0.6103 (0.5973) acc 75.0000 (75.7955) alaph_mean 0.2973 (0.3107) alpha_min 0.0000 (0.0000) alpha_max 0.5069 (0.5202) lr 1.3681e-03 eta 0:27:05
epoch [21/50] batch [240/288] time 0.200 (0.193) data 0.000 (0.001) loss 1.5973 (1.5919) teacher_loss 0.9062 (0.9065) loss_zs_kd 0.2276 (0.1797) loss_oracle 0.5772 (0.5955) acc 81.2500 (75.6510) alaph_mean 0.3133 (0.3114) alpha_min 0.0000 (0.0000) alpha_max 0.5130 (0.5198) lr 1.3681e-03 eta 0:26:58
epoch [21/50] batch [260/288] time 0.194 (0.193) data 0.000 (0.001) loss 1.4592 (1.5875) teacher_loss 0.7607 (0.9014) loss_zs_kd 0.1647 (0.1809) loss_oracle 0.6161 (0.5956) acc 78.1250 (75.7332) alaph_mean 0.3104 (0.3119) alpha_min -0.0000 (0.0000) alpha_max 0.5087 (0.5196) lr 1.3681e-03 eta 0:26:56
epoch [21/50] batch [280/288] time 0.193 (0.193) data 0.000 (0.001) loss 1.4708 (1.5870) teacher_loss 0.7212 (0.8996) loss_zs_kd 0.1384 (0.1809) loss_oracle 0.6804 (0.5970) acc 78.1250 (75.8147) alaph_mean 0.2991 (0.3111) alpha_min -0.0000 (0.0000) alpha_max 0.5035 (0.5190) lr 1.3681e-03 eta 0:26:52
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,417
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,014
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.7%
******* Domain a best val acc:      86.8%, epoch: 19 *******
******* Domain a best val test acc: 82.6%, epoch: 19 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [22/50] batch [20/288] time 0.199 (0.208) data 0.001 (0.013) loss 2.2952 (1.5797) teacher_loss 1.3691 (0.8704) loss_zs_kd 0.1947 (0.1926) loss_oracle 0.8287 (0.6129) acc 62.5000 (75.7812) alaph_mean 0.2342 (0.3070) alpha_min 0.0000 (0.0000) alpha_max 0.5066 (0.5173) lr 1.3090e-03 eta 0:28:52
epoch [22/50] batch [40/288] time 0.206 (0.202) data 0.000 (0.007) loss 1.4087 (1.5428) teacher_loss 0.8730 (0.8591) loss_zs_kd 0.1407 (0.1856) loss_oracle 0.4653 (0.5909) acc 81.2500 (76.3281) alaph_mean 0.3769 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5202) lr 1.3090e-03 eta 0:27:57
epoch [22/50] batch [60/288] time 0.191 (0.198) data 0.000 (0.005) loss 1.3448 (1.5533) teacher_loss 0.6123 (0.8754) loss_zs_kd 0.2416 (0.1888) loss_oracle 0.6117 (0.5835) acc 81.2500 (75.7812) alaph_mean 0.3035 (0.3241) alpha_min 0.0000 (0.0000) alpha_max 0.5111 (0.5247) lr 1.3090e-03 eta 0:27:19
epoch [22/50] batch [80/288] time 0.186 (0.197) data 0.000 (0.004) loss 2.0816 (1.5492) teacher_loss 1.2754 (0.8694) loss_zs_kd 0.1999 (0.1885) loss_oracle 0.7063 (0.5855) acc 71.8750 (76.2500) alaph_mean 0.2646 (0.3233) alpha_min -0.0000 (0.0000) alpha_max 0.5097 (0.5247) lr 1.3090e-03 eta 0:27:08
epoch [22/50] batch [100/288] time 0.194 (0.196) data 0.000 (0.003) loss 1.2450 (1.5503) teacher_loss 0.5889 (0.8680) loss_zs_kd 0.1223 (0.1841) loss_oracle 0.5950 (0.5903) acc 87.5000 (76.2188) alaph_mean 0.3447 (0.3198) alpha_min -0.0000 (0.0000) alpha_max 0.5129 (0.5230) lr 1.3090e-03 eta 0:26:58
epoch [22/50] batch [120/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.5262 (1.5595) teacher_loss 0.8247 (0.8725) loss_zs_kd 0.2277 (0.1872) loss_oracle 0.5876 (0.5934) acc 81.2500 (76.0677) alaph_mean 0.3128 (0.3192) alpha_min 0.0000 (0.0000) alpha_max 0.5071 (0.5264) lr 1.3090e-03 eta 0:26:50
epoch [22/50] batch [140/288] time 0.192 (0.195) data 0.000 (0.002) loss 1.5667 (1.5516) teacher_loss 0.8340 (0.8687) loss_zs_kd 0.2124 (0.1845) loss_oracle 0.6265 (0.5907) acc 75.0000 (76.3393) alaph_mean 0.3297 (0.3207) alpha_min 0.0000 (0.0000) alpha_max 0.5079 (0.5286) lr 1.3090e-03 eta 0:26:45
epoch [22/50] batch [160/288] time 0.197 (0.195) data 0.001 (0.002) loss 1.1538 (1.5686) teacher_loss 0.5488 (0.8805) loss_zs_kd 0.1224 (0.1844) loss_oracle 0.5438 (0.5959) acc 90.6250 (76.2500) alaph_mean 0.3757 (0.3184) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5264) lr 1.3090e-03 eta 0:26:39
epoch [22/50] batch [180/288] time 0.201 (0.195) data 0.000 (0.002) loss 1.4953 (1.5630) teacher_loss 0.8735 (0.8737) loss_zs_kd 0.1203 (0.1843) loss_oracle 0.5616 (0.5971) acc 81.2500 (76.6667) alaph_mean 0.3199 (0.3183) alpha_min -0.0000 (0.0000) alpha_max 0.5073 (0.5254) lr 1.3090e-03 eta 0:26:34
epoch [22/50] batch [200/288] time 0.207 (0.195) data 0.000 (0.002) loss 1.5164 (1.5670) teacher_loss 0.7817 (0.8768) loss_zs_kd 0.1652 (0.1848) loss_oracle 0.6520 (0.5978) acc 81.2500 (76.6406) alaph_mean 0.2829 (0.3183) alpha_min 0.0000 (0.0000) alpha_max 0.5069 (0.5267) lr 1.3090e-03 eta 0:26:29
epoch [22/50] batch [220/288] time 0.207 (0.195) data 0.000 (0.001) loss 1.0696 (1.5688) teacher_loss 0.5317 (0.8771) loss_zs_kd 0.1873 (0.1860) loss_oracle 0.4442 (0.5987) acc 84.3750 (76.4915) alaph_mean 0.3869 (0.3183) alpha_min 0.0000 (0.0000) alpha_max 0.5067 (0.5266) lr 1.3090e-03 eta 0:26:24
epoch [22/50] batch [240/288] time 0.193 (0.195) data 0.000 (0.001) loss 1.5572 (1.5720) teacher_loss 0.8706 (0.8817) loss_zs_kd 0.1666 (0.1850) loss_oracle 0.6033 (0.5978) acc 78.1250 (76.5234) alaph_mean 0.3406 (0.3186) alpha_min 0.0000 (0.0000) alpha_max 0.5094 (0.5250) lr 1.3090e-03 eta 0:26:20
epoch [22/50] batch [260/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.5002 (1.5704) teacher_loss 0.7695 (0.8821) loss_zs_kd 0.1663 (0.1838) loss_oracle 0.6475 (0.5964) acc 81.2500 (76.5865) alaph_mean 0.2898 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.7562 (0.5257) lr 1.3090e-03 eta 0:26:16
epoch [22/50] batch [280/288] time 0.199 (0.195) data 0.000 (0.001) loss 1.0745 (1.5763) teacher_loss 0.5190 (0.8861) loss_zs_kd 0.1623 (0.1834) loss_oracle 0.4743 (0.5985) acc 87.5000 (76.4174) alaph_mean 0.3592 (0.3172) alpha_min 0.0000 (0.0000) alpha_max 0.5035 (0.5267) lr 1.3090e-03 eta 0:26:11
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,417
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.1%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,016
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.8%
******* Domain a best val acc:      86.8%, epoch: 19 *******
******* Domain a best val test acc: 82.6%, epoch: 19 *******
******* Domain a best test acc:     83.4%, epoch: 7 *******
epoch [23/50] batch [20/288] time 0.203 (0.175) data 0.000 (0.011) loss 1.6325 (1.5299) teacher_loss 0.9575 (0.8461) loss_zs_kd 0.1823 (0.1766) loss_oracle 0.5838 (0.5955) acc 75.0000 (78.2812) alaph_mean 0.3440 (0.3170) alpha_min 0.0000 (0.0000) alpha_max 0.5115 (0.5164) lr 1.2487e-03 eta 0:23:25
epoch [23/50] batch [40/288] time 0.200 (0.184) data 0.000 (0.006) loss 1.4481 (1.5321) teacher_loss 0.8862 (0.8541) loss_zs_kd 0.1429 (0.1794) loss_oracle 0.4904 (0.5882) acc 75.0000 (77.2656) alaph_mean 0.3749 (0.3170) alpha_min 0.0000 (0.0000) alpha_max 0.5080 (0.5137) lr 1.2487e-03 eta 0:24:36
epoch [23/50] batch [60/288] time 0.196 (0.187) data 0.001 (0.004) loss 1.9970 (1.5626) teacher_loss 1.2627 (0.8879) loss_zs_kd 0.2314 (0.1839) loss_oracle 0.6186 (0.5827) acc 59.3750 (76.3542) alaph_mean 0.2892 (0.3181) alpha_min -0.0000 (0.0000) alpha_max 0.5057 (0.5140) lr 1.2487e-03 eta 0:25:00
epoch [23/50] batch [80/288] time 0.187 (0.189) data 0.000 (0.003) loss 1.8839 (1.5493) teacher_loss 1.1807 (0.8727) loss_zs_kd 0.1885 (0.1853) loss_oracle 0.6090 (0.5840) acc 71.8750 (76.4453) alaph_mean 0.3127 (0.3185) alpha_min -0.0000 (0.0000) alpha_max 0.5081 (0.5173) lr 1.2487e-03 eta 0:25:05
epoch [23/50] batch [100/288] time 0.193 (0.189) data 0.000 (0.002) loss 1.3825 (1.5375) teacher_loss 0.6685 (0.8614) loss_zs_kd 0.2263 (0.1867) loss_oracle 0.6009 (0.5827) acc 84.3750 (76.9688) alaph_mean 0.3127 (0.3198) alpha_min 0.0000 (0.0000) alpha_max 0.5087 (0.5258) lr 1.2487e-03 eta 0:25:04
epoch [23/50] batch [120/288] time 0.192 (0.190) data 0.000 (0.002) loss 1.3084 (1.5442) teacher_loss 0.6431 (0.8651) loss_zs_kd 0.2187 (0.1894) loss_oracle 0.5560 (0.5843) acc 81.2500 (77.0052) alaph_mean 0.3444 (0.3193) alpha_min 0.0000 (0.0000) alpha_max 0.5098 (0.5229) lr 1.2487e-03 eta 0:25:09
epoch [23/50] batch [140/288] time 0.196 (0.191) data 0.000 (0.002) loss 1.2394 (1.5361) teacher_loss 0.8125 (0.8597) loss_zs_kd 0.1752 (0.1889) loss_oracle 0.3393 (0.5820) acc 81.2500 (76.8750) alaph_mean 0.4684 (0.3204) alpha_min 0.0000 (0.0000) alpha_max 0.5085 (0.5207) lr 1.2487e-03 eta 0:25:11
epoch [23/50] batch [160/288] time 0.192 (0.191) data 0.000 (0.002) loss 1.8317 (1.5419) teacher_loss 1.0322 (0.8679) loss_zs_kd 0.1741 (0.1884) loss_oracle 0.7124 (0.5798) acc 78.1250 (76.6602) alaph_mean 0.2394 (0.3217) alpha_min 0.0000 (0.0000) alpha_max 0.5021 (0.5219) lr 1.2487e-03 eta 0:25:11
epoch [23/50] batch [180/288] time 0.189 (0.192) data 0.001 (0.001) loss 1.3963 (1.5369) teacher_loss 0.8037 (0.8640) loss_zs_kd 0.1185 (0.1876) loss_oracle 0.5334 (0.5791) acc 78.1250 (76.8576) alaph_mean 0.3437 (0.3218) alpha_min 0.0000 (0.0000) alpha_max 0.5071 (0.5224) lr 1.2487e-03 eta 0:25:09
epoch [23/50] batch [200/288] time 0.194 (0.192) data 0.000 (0.001) loss 1.4065 (1.5418) teacher_loss 0.6997 (0.8671) loss_zs_kd 0.1731 (0.1874) loss_oracle 0.6203 (0.5810) acc 81.2500 (76.9844) alaph_mean 0.3129 (0.3213) alpha_min 0.0000 (0.0000) alpha_max 0.5143 (0.5243) lr 1.2487e-03 eta 0:25:07
epoch [23/50] batch [220/288] time 0.222 (0.192) data 0.000 (0.001) loss 1.7157 (1.5490) teacher_loss 0.9775 (0.8701) loss_zs_kd 0.1872 (0.1869) loss_oracle 0.6446 (0.5854) acc 71.8750 (76.8608) alaph_mean 0.3023 (0.3193) alpha_min -0.0000 (0.0000) alpha_max 0.5063 (0.5236) lr 1.2487e-03 eta 0:25:05
epoch [23/50] batch [240/288] time 0.194 (0.192) data 0.000 (0.001) loss 1.3746 (1.5566) teacher_loss 0.7534 (0.8791) loss_zs_kd 0.1742 (0.1852) loss_oracle 0.5341 (0.5848) acc 75.0000 (76.6797) alaph_mean 0.3466 (0.3197) alpha_min 0.0000 (0.0000) alpha_max 0.5085 (0.5223) lr 1.2487e-03 eta 0:25:02
epoch [23/50] batch [260/288] time 0.192 (0.192) data 0.000 (0.001) loss 1.3078 (1.5674) teacher_loss 0.5933 (0.8874) loss_zs_kd 0.1555 (0.1858) loss_oracle 0.6368 (0.5871) acc 84.3750 (76.3341) alaph_mean 0.2857 (0.3183) alpha_min -0.0000 (0.0000) alpha_max 0.5063 (0.5245) lr 1.2487e-03 eta 0:24:59
epoch [23/50] batch [280/288] time 0.185 (0.192) data 0.000 (0.001) loss 1.6662 (1.5600) teacher_loss 0.9116 (0.8819) loss_zs_kd 0.2099 (0.1852) loss_oracle 0.6496 (0.5855) acc 71.8750 (76.5513) alaph_mean 0.2658 (0.3188) alpha_min 0.0000 (0.0000) alpha_max 0.5038 (0.5255) lr 1.2487e-03 eta 0:24:56
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,414
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,026
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.2%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.8%, epoch: 19 *******
******* Domain a best val test acc: 82.6%, epoch: 19 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [24/50] batch [20/288] time 0.192 (0.232) data 0.000 (0.013) loss 1.5168 (1.5794) teacher_loss 0.9897 (0.8701) loss_zs_kd 0.1544 (0.1732) loss_oracle 0.4499 (0.6227) acc 81.2500 (76.0938) alaph_mean 0.3858 (0.3049) alpha_min 0.0000 (0.0000) alpha_max 0.5076 (0.5521) lr 1.1874e-03 eta 0:29:56
epoch [24/50] batch [40/288] time 0.197 (0.212) data 0.000 (0.006) loss 1.4416 (1.5967) teacher_loss 0.8096 (0.8885) loss_zs_kd 0.2374 (0.1798) loss_oracle 0.5133 (0.6183) acc 78.1250 (75.6250) alaph_mean 0.3874 (0.3069) alpha_min 0.0000 (0.0000) alpha_max 0.5140 (0.5324) lr 1.1874e-03 eta 0:27:18
epoch [24/50] batch [60/288] time 0.190 (0.206) data 0.000 (0.004) loss 1.4663 (1.5982) teacher_loss 0.7505 (0.8951) loss_zs_kd 0.1845 (0.1874) loss_oracle 0.6236 (0.6093) acc 78.1250 (75.6771) alaph_mean 0.2786 (0.3127) alpha_min -0.0000 (0.0000) alpha_max 0.5071 (0.5281) lr 1.1874e-03 eta 0:26:28
epoch [24/50] batch [80/288] time 0.195 (0.203) data 0.000 (0.003) loss 1.4920 (1.5731) teacher_loss 0.8291 (0.8717) loss_zs_kd 0.2223 (0.1873) loss_oracle 0.5517 (0.6078) acc 75.0000 (76.3281) alaph_mean 0.3602 (0.3132) alpha_min -0.0000 (0.0000) alpha_max 0.5122 (0.5239) lr 1.1874e-03 eta 0:26:01
epoch [24/50] batch [100/288] time 0.199 (0.202) data 0.000 (0.003) loss 1.4196 (1.5783) teacher_loss 0.8018 (0.8791) loss_zs_kd 0.2071 (0.1846) loss_oracle 0.5143 (0.6070) acc 81.2500 (76.3125) alaph_mean 0.3516 (0.3130) alpha_min 0.0000 (0.0000) alpha_max 0.5056 (0.5231) lr 1.1874e-03 eta 0:25:48
epoch [24/50] batch [120/288] time 0.193 (0.201) data 0.000 (0.002) loss 1.6268 (1.5657) teacher_loss 0.9878 (0.8708) loss_zs_kd 0.1606 (0.1832) loss_oracle 0.5587 (0.6033) acc 68.7500 (76.3542) alaph_mean 0.3279 (0.3151) alpha_min 0.0000 (0.0000) alpha_max 0.5058 (0.5225) lr 1.1874e-03 eta 0:25:37
epoch [24/50] batch [140/288] time 0.197 (0.200) data 0.000 (0.002) loss 1.6118 (1.5614) teacher_loss 0.9214 (0.8682) loss_zs_kd 0.1446 (0.1836) loss_oracle 0.6181 (0.6014) acc 78.1250 (76.5625) alaph_mean 0.2950 (0.3167) alpha_min 0.0000 (0.0000) alpha_max 0.5096 (0.5207) lr 1.1874e-03 eta 0:25:24
epoch [24/50] batch [160/288] time 0.194 (0.199) data 0.000 (0.002) loss 1.6811 (1.5629) teacher_loss 1.0537 (0.8727) loss_zs_kd 0.1836 (0.1865) loss_oracle 0.5356 (0.5970) acc 75.0000 (76.5625) alaph_mean 0.3435 (0.3178) alpha_min 0.0000 (0.0000) alpha_max 0.5046 (0.5190) lr 1.1874e-03 eta 0:25:14
epoch [24/50] batch [180/288] time 0.190 (0.198) data 0.000 (0.002) loss 1.6671 (1.5709) teacher_loss 0.8936 (0.8826) loss_zs_kd 0.1320 (0.1858) loss_oracle 0.7075 (0.5954) acc 75.0000 (76.1979) alaph_mean 0.2297 (0.3172) alpha_min -0.0000 (0.0000) alpha_max 0.5140 (0.5181) lr 1.1874e-03 eta 0:25:05
epoch [24/50] batch [200/288] time 0.190 (0.198) data 0.000 (0.001) loss 1.7222 (1.5751) teacher_loss 0.9692 (0.8838) loss_zs_kd 0.1856 (0.1861) loss_oracle 0.6601 (0.5982) acc 71.8750 (76.1094) alaph_mean 0.2556 (0.3157) alpha_min 0.0000 (0.0000) alpha_max 0.6457 (0.5212) lr 1.1874e-03 eta 0:24:58
epoch [24/50] batch [220/288] time 0.192 (0.198) data 0.000 (0.001) loss 1.4548 (1.5714) teacher_loss 0.6636 (0.8835) loss_zs_kd 0.1866 (0.1851) loss_oracle 0.6980 (0.5953) acc 78.1250 (76.1790) alaph_mean 0.3121 (0.3177) alpha_min 0.0000 (0.0000) alpha_max 0.5052 (0.5218) lr 1.1874e-03 eta 0:24:52
epoch [24/50] batch [240/288] time 0.191 (0.197) data 0.000 (0.001) loss 1.6365 (1.5732) teacher_loss 0.8398 (0.8834) loss_zs_kd 0.1829 (0.1850) loss_oracle 0.7052 (0.5973) acc 81.2500 (76.1979) alaph_mean 0.2444 (0.3177) alpha_min -0.0000 (0.0000) alpha_max 0.5059 (0.5224) lr 1.1874e-03 eta 0:24:46
epoch [24/50] batch [260/288] time 0.197 (0.197) data 0.000 (0.001) loss 1.1857 (1.5769) teacher_loss 0.5322 (0.8864) loss_zs_kd 0.1786 (0.1844) loss_oracle 0.5642 (0.5982) acc 90.6250 (76.1418) alaph_mean 0.3281 (0.3174) alpha_min -0.0000 (0.0000) alpha_max 0.5025 (0.5216) lr 1.1874e-03 eta 0:24:40
epoch [24/50] batch [280/288] time 0.195 (0.197) data 0.000 (0.001) loss 1.4242 (1.5768) teacher_loss 0.9302 (0.8886) loss_zs_kd 0.1899 (0.1846) loss_oracle 0.3991 (0.5959) acc 71.8750 (76.1719) alaph_mean 0.4097 (0.3182) alpha_min 0.0000 (0.0000) alpha_max 0.5063 (0.5212) lr 1.1874e-03 eta 0:24:35
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,394
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,005
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.3%
******* Domain a best val acc:      86.8%, epoch: 19 *******
******* Domain a best val test acc: 82.6%, epoch: 19 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [25/50] batch [20/288] time 0.176 (0.234) data 0.000 (0.014) loss 1.8147 (1.5601) teacher_loss 1.1055 (0.8917) loss_zs_kd 0.1719 (0.1859) loss_oracle 0.6233 (0.5755) acc 68.7500 (75.1562) alaph_mean 0.3293 (0.3342) alpha_min 0.0000 (0.0000) alpha_max 0.5096 (0.5155) lr 1.1253e-03 eta 0:29:07
epoch [25/50] batch [40/288] time 0.180 (0.215) data 0.000 (0.007) loss 1.5432 (1.5923) teacher_loss 0.7974 (0.9115) loss_zs_kd 0.1630 (0.1870) loss_oracle 0.6643 (0.5873) acc 81.2500 (75.4688) alaph_mean 0.2856 (0.3229) alpha_min 0.0000 (0.0000) alpha_max 0.5088 (0.5137) lr 1.1253e-03 eta 0:26:37
epoch [25/50] batch [60/288] time 0.195 (0.208) data 0.000 (0.005) loss 1.8620 (1.5887) teacher_loss 1.1855 (0.9125) loss_zs_kd 0.2083 (0.1870) loss_oracle 0.5724 (0.5827) acc 65.6250 (75.4688) alaph_mean 0.3339 (0.3260) alpha_min 0.0000 (0.0000) alpha_max 0.5060 (0.5291) lr 1.1253e-03 eta 0:25:44
epoch [25/50] batch [80/288] time 0.195 (0.205) data 0.000 (0.004) loss 1.4269 (1.5920) teacher_loss 0.8193 (0.9141) loss_zs_kd 0.2292 (0.1861) loss_oracle 0.4929 (0.5848) acc 78.1250 (75.7812) alaph_mean 0.3949 (0.3261) alpha_min 0.0000 (0.0000) alpha_max 0.5079 (0.5276) lr 1.1253e-03 eta 0:25:15
epoch [25/50] batch [100/288] time 0.201 (0.203) data 0.000 (0.003) loss 1.4634 (1.5946) teacher_loss 0.7646 (0.9056) loss_zs_kd 0.2129 (0.1923) loss_oracle 0.5923 (0.5929) acc 71.8750 (75.8438) alaph_mean 0.3496 (0.3230) alpha_min 0.0000 (0.0000) alpha_max 0.6549 (0.5279) lr 1.1253e-03 eta 0:24:57
epoch [25/50] batch [120/288] time 0.190 (0.201) data 0.000 (0.002) loss 1.7779 (1.5843) teacher_loss 1.0938 (0.8955) loss_zs_kd 0.1601 (0.1897) loss_oracle 0.6042 (0.5939) acc 62.5000 (76.0417) alaph_mean 0.2725 (0.3201) alpha_min 0.0000 (0.0000) alpha_max 0.5057 (0.5244) lr 1.1253e-03 eta 0:24:37
epoch [25/50] batch [140/288] time 0.198 (0.200) data 0.000 (0.002) loss 0.9097 (1.5861) teacher_loss 0.3276 (0.8945) loss_zs_kd 0.1339 (0.1895) loss_oracle 0.5151 (0.5968) acc 90.6250 (76.0938) alaph_mean 0.3515 (0.3186) alpha_min -0.0000 (0.0000) alpha_max 0.5080 (0.5270) lr 1.1253e-03 eta 0:24:29
epoch [25/50] batch [160/288] time 0.193 (0.199) data 0.000 (0.002) loss 1.5367 (1.5782) teacher_loss 0.8350 (0.8826) loss_zs_kd 0.1593 (0.1882) loss_oracle 0.6221 (0.6015) acc 81.2500 (76.3672) alaph_mean 0.2872 (0.3162) alpha_min -0.0000 (0.0000) alpha_max 0.5081 (0.5246) lr 1.1253e-03 eta 0:24:19
epoch [25/50] batch [180/288] time 0.190 (0.199) data 0.000 (0.002) loss 1.6859 (1.5806) teacher_loss 0.8604 (0.8872) loss_zs_kd 0.1921 (0.1889) loss_oracle 0.7295 (0.5990) acc 81.2500 (76.3542) alaph_mean 0.2494 (0.3172) alpha_min 0.0000 (0.0000) alpha_max 0.5035 (0.5253) lr 1.1253e-03 eta 0:24:12
epoch [25/50] batch [200/288] time 0.195 (0.198) data 0.000 (0.002) loss 1.2121 (1.5702) teacher_loss 0.4722 (0.8830) loss_zs_kd 0.2301 (0.1886) loss_oracle 0.6249 (0.5929) acc 90.6250 (76.4688) alaph_mean 0.3452 (0.3202) alpha_min 0.0000 (0.0000) alpha_max 0.5122 (0.5264) lr 1.1253e-03 eta 0:24:06
epoch [25/50] batch [220/288] time 0.159 (0.198) data 0.000 (0.001) loss 1.5608 (1.5642) teacher_loss 0.7578 (0.8782) loss_zs_kd 0.2241 (0.1882) loss_oracle 0.6910 (0.5919) acc 78.1250 (76.5625) alaph_mean 0.2633 (0.3203) alpha_min 0.0000 (0.0000) alpha_max 0.5040 (0.5266) lr 1.1253e-03 eta 0:23:58
epoch [25/50] batch [240/288] time 0.198 (0.198) data 0.000 (0.001) loss 2.1164 (1.5641) teacher_loss 1.3125 (0.8783) loss_zs_kd 0.1929 (0.1879) loss_oracle 0.7075 (0.5918) acc 65.6250 (76.4974) alaph_mean 0.3120 (0.3207) alpha_min 0.0000 (0.0000) alpha_max 0.5080 (0.5251) lr 1.1253e-03 eta 0:23:54
epoch [25/50] batch [260/288] time 0.218 (0.198) data 0.000 (0.001) loss 1.3954 (1.5648) teacher_loss 0.7710 (0.8774) loss_zs_kd 0.1999 (0.1888) loss_oracle 0.5245 (0.5930) acc 78.1250 (76.5144) alaph_mean 0.3613 (0.3201) alpha_min 0.0000 (0.0000) alpha_max 0.5082 (0.5263) lr 1.1253e-03 eta 0:23:49
epoch [25/50] batch [280/288] time 0.229 (0.198) data 0.000 (0.001) loss 1.4032 (1.5660) teacher_loss 0.8276 (0.8776) loss_zs_kd 0.1616 (0.1883) loss_oracle 0.4947 (0.5942) acc 78.1250 (76.5290) alaph_mean 0.3794 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.5067 (0.5269) lr 1.1253e-03 eta 0:23:43
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,409
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 86.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,005
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.3%
******* Domain a best val acc:      86.8%, epoch: 19 *******
******* Domain a best val test acc: 82.6%, epoch: 19 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [26/50] batch [20/288] time 0.195 (0.210) data 0.000 (0.013) loss 1.2655 (1.4606) teacher_loss 0.6353 (0.7718) loss_zs_kd 0.1427 (0.1877) loss_oracle 0.5589 (0.5949) acc 78.1250 (79.6875) alaph_mean 0.3059 (0.3172) alpha_min -0.0000 (0.0000) alpha_max 0.5078 (0.5266) lr 1.0628e-03 eta 0:25:10
epoch [26/50] batch [40/288] time 0.191 (0.201) data 0.000 (0.007) loss 1.7178 (1.5132) teacher_loss 1.0127 (0.8336) loss_zs_kd 0.1296 (0.1860) loss_oracle 0.6403 (0.5867) acc 75.0000 (77.1875) alaph_mean 0.2795 (0.3214) alpha_min 0.0000 (0.0000) alpha_max 0.5112 (0.5196) lr 1.0628e-03 eta 0:24:00
epoch [26/50] batch [60/288] time 0.193 (0.197) data 0.001 (0.005) loss 1.4859 (1.5157) teacher_loss 0.8535 (0.8355) loss_zs_kd 0.1891 (0.1885) loss_oracle 0.5378 (0.5859) acc 65.6250 (77.2396) alaph_mean 0.3635 (0.3208) alpha_min 0.0000 (0.0000) alpha_max 0.5078 (0.5172) lr 1.0628e-03 eta 0:23:29
epoch [26/50] batch [80/288] time 0.217 (0.197) data 0.000 (0.004) loss 1.8966 (1.5299) teacher_loss 1.2451 (0.8448) loss_zs_kd 0.1921 (0.1888) loss_oracle 0.5554 (0.5907) acc 71.8750 (77.3047) alaph_mean 0.3444 (0.3182) alpha_min 0.0000 (0.0000) alpha_max 0.5102 (0.5198) lr 1.0628e-03 eta 0:23:22
epoch [26/50] batch [100/288] time 0.200 (0.197) data 0.000 (0.003) loss 1.1976 (1.5381) teacher_loss 0.6465 (0.8567) loss_zs_kd 0.1471 (0.1879) loss_oracle 0.4775 (0.5874) acc 84.3750 (76.6250) alaph_mean 0.3828 (0.3181) alpha_min 0.0000 (0.0000) alpha_max 0.5052 (0.5195) lr 1.0628e-03 eta 0:23:17
epoch [26/50] batch [120/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.6450 (1.5379) teacher_loss 0.9541 (0.8598) loss_zs_kd 0.2310 (0.1863) loss_oracle 0.5753 (0.5849) acc 75.0000 (76.7188) alaph_mean 0.3283 (0.3191) alpha_min 0.0000 (0.0000) alpha_max 0.5083 (0.5190) lr 1.0628e-03 eta 0:23:10
epoch [26/50] batch [140/288] time 0.192 (0.196) data 0.000 (0.002) loss 1.6351 (1.5398) teacher_loss 0.9097 (0.8581) loss_zs_kd 0.1743 (0.1847) loss_oracle 0.6383 (0.5893) acc 78.1250 (76.9643) alaph_mean 0.3100 (0.3173) alpha_min -0.0000 (0.0000) alpha_max 0.5104 (0.5180) lr 1.0628e-03 eta 0:23:03
epoch [26/50] batch [160/288] time 0.197 (0.196) data 0.000 (0.002) loss 1.6332 (1.5327) teacher_loss 1.0078 (0.8555) loss_zs_kd 0.2067 (0.1853) loss_oracle 0.5221 (0.5845) acc 71.8750 (77.1289) alaph_mean 0.3425 (0.3211) alpha_min 0.0000 (0.0000) alpha_max 0.5078 (0.5196) lr 1.0628e-03 eta 0:22:59
epoch [26/50] batch [180/288] time 0.193 (0.196) data 0.000 (0.002) loss 1.3059 (1.5364) teacher_loss 0.6470 (0.8536) loss_zs_kd 0.1540 (0.1858) loss_oracle 0.5819 (0.5899) acc 81.2500 (76.9965) alaph_mean 0.3227 (0.3187) alpha_min 0.0000 (0.0000) alpha_max 0.5147 (0.5183) lr 1.0628e-03 eta 0:22:53
epoch [26/50] batch [200/288] time 0.193 (0.195) data 0.000 (0.002) loss 1.6041 (1.5426) teacher_loss 0.9580 (0.8569) loss_zs_kd 0.1801 (0.1866) loss_oracle 0.5560 (0.5924) acc 84.3750 (76.8438) alaph_mean 0.3096 (0.3180) alpha_min -0.0000 (0.0000) alpha_max 0.5100 (0.5192) lr 1.0628e-03 eta 0:22:47
epoch [26/50] batch [220/288] time 0.192 (0.195) data 0.000 (0.001) loss 0.9057 (1.5467) teacher_loss 0.2793 (0.8594) loss_zs_kd 0.1322 (0.1866) loss_oracle 0.5603 (0.5940) acc 93.7500 (76.8040) alaph_mean 0.3062 (0.3176) alpha_min 0.0000 (0.0000) alpha_max 0.5027 (0.5199) lr 1.0628e-03 eta 0:22:42
epoch [26/50] batch [240/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.6346 (1.5510) teacher_loss 0.9531 (0.8642) loss_zs_kd 0.2090 (0.1870) loss_oracle 0.5770 (0.5933) acc 78.1250 (76.6276) alaph_mean 0.3208 (0.3180) alpha_min 0.0000 (0.0000) alpha_max 0.5105 (0.5198) lr 1.0628e-03 eta 0:22:38
epoch [26/50] batch [260/288] time 0.194 (0.195) data 0.000 (0.001) loss 1.6045 (1.5614) teacher_loss 0.8726 (0.8705) loss_zs_kd 0.2182 (0.1884) loss_oracle 0.6228 (0.5967) acc 65.6250 (76.3942) alaph_mean 0.2873 (0.3166) alpha_min -0.0000 (0.0000) alpha_max 0.5096 (0.5190) lr 1.0628e-03 eta 0:22:33
epoch [26/50] batch [280/288] time 0.192 (0.195) data 0.000 (0.001) loss 1.9976 (1.5660) teacher_loss 1.1357 (0.8739) loss_zs_kd 0.2196 (0.1896) loss_oracle 0.7521 (0.5974) acc 59.3750 (76.2500) alaph_mean 0.2228 (0.3163) alpha_min 0.0000 (0.0000) alpha_max 0.5060 (0.5185) lr 1.0628e-03 eta 0:22:29
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,411
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 86.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,002
* accuracy: 82.5%
* error: 17.5%
* macro_f1: 79.1%
******* Domain a best val acc:      86.8%, epoch: 19 *******
******* Domain a best val test acc: 82.6%, epoch: 19 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [27/50] batch [20/288] time 0.204 (0.191) data 0.000 (0.013) loss 1.0371 (1.5007) teacher_loss 0.5918 (0.8134) loss_zs_kd 0.1635 (0.2030) loss_oracle 0.3636 (0.5858) acc 81.2500 (79.5312) alaph_mean 0.4880 (0.3371) alpha_min 0.0000 (0.0000) alpha_max 0.5982 (0.5225) lr 1.0000e-03 eta 0:21:55
epoch [27/50] batch [40/288] time 0.192 (0.193) data 0.000 (0.007) loss 1.0739 (1.5429) teacher_loss 0.4316 (0.8709) loss_zs_kd 0.1249 (0.1932) loss_oracle 0.5798 (0.5754) acc 93.7500 (77.5781) alaph_mean 0.2872 (0.3355) alpha_min -0.0000 (0.0000) alpha_max 0.5019 (0.5190) lr 1.0000e-03 eta 0:22:03
epoch [27/50] batch [60/288] time 0.186 (0.193) data 0.001 (0.005) loss 1.9404 (1.5287) teacher_loss 1.2373 (0.8519) loss_zs_kd 0.2920 (0.1949) loss_oracle 0.5571 (0.5793) acc 65.6250 (77.7604) alaph_mean 0.3313 (0.3340) alpha_min 0.0000 (0.0000) alpha_max 0.5055 (0.5165) lr 1.0000e-03 eta 0:22:04
epoch [27/50] batch [80/288] time 0.193 (0.194) data 0.000 (0.004) loss 1.6137 (1.5333) teacher_loss 0.7510 (0.8470) loss_zs_kd 0.1874 (0.1908) loss_oracle 0.7691 (0.5909) acc 78.1250 (77.3047) alaph_mean 0.2420 (0.3262) alpha_min -0.0000 (0.0000) alpha_max 0.5045 (0.5157) lr 1.0000e-03 eta 0:22:04
epoch [27/50] batch [100/288] time 0.195 (0.195) data 0.000 (0.003) loss 1.4119 (1.5446) teacher_loss 0.6890 (0.8575) loss_zs_kd 0.2445 (0.1924) loss_oracle 0.6007 (0.5909) acc 87.5000 (77.1250) alaph_mean 0.3364 (0.3262) alpha_min -0.0000 (0.0000) alpha_max 0.5099 (0.5190) lr 1.0000e-03 eta 0:22:07
epoch [27/50] batch [120/288] time 0.199 (0.195) data 0.000 (0.002) loss 1.1809 (1.5229) teacher_loss 0.6973 (0.8406) loss_zs_kd 0.1539 (0.1895) loss_oracle 0.4067 (0.5876) acc 81.2500 (77.7344) alaph_mean 0.4060 (0.3279) alpha_min -0.0000 (0.0000) alpha_max 0.5075 (0.5218) lr 1.0000e-03 eta 0:22:03
epoch [27/50] batch [140/288] time 0.208 (0.195) data 0.000 (0.002) loss 1.4055 (1.5337) teacher_loss 0.8423 (0.8524) loss_zs_kd 0.2718 (0.1916) loss_oracle 0.4273 (0.5855) acc 78.1250 (77.4777) alaph_mean 0.3933 (0.3279) alpha_min 0.0000 (0.0000) alpha_max 0.5616 (0.5219) lr 1.0000e-03 eta 0:22:00
epoch [27/50] batch [160/288] time 0.183 (0.195) data 0.000 (0.002) loss 1.8561 (1.5387) teacher_loss 0.9761 (0.8489) loss_zs_kd 0.2094 (0.1919) loss_oracle 0.7753 (0.5939) acc 68.7500 (77.5781) alaph_mean 0.2342 (0.3240) alpha_min 0.0000 (0.0000) alpha_max 0.5075 (0.5220) lr 1.0000e-03 eta 0:21:55
epoch [27/50] batch [180/288] time 0.193 (0.195) data 0.000 (0.002) loss 1.4031 (1.5419) teacher_loss 0.6611 (0.8528) loss_zs_kd 0.2133 (0.1914) loss_oracle 0.6353 (0.5934) acc 78.1250 (77.5347) alaph_mean 0.3169 (0.3239) alpha_min 0.0000 (0.0000) alpha_max 0.5086 (0.5226) lr 1.0000e-03 eta 0:21:50
epoch [27/50] batch [200/288] time 0.191 (0.195) data 0.000 (0.002) loss 1.6102 (1.5339) teacher_loss 0.9419 (0.8458) loss_zs_kd 0.1527 (0.1906) loss_oracle 0.5920 (0.5928) acc 78.1250 (77.7344) alaph_mean 0.3381 (0.3246) alpha_min 0.0000 (0.0000) alpha_max 0.7729 (0.5248) lr 1.0000e-03 eta 0:21:46
epoch [27/50] batch [220/288] time 0.199 (0.195) data 0.000 (0.001) loss 1.5900 (1.5334) teacher_loss 0.8794 (0.8445) loss_zs_kd 0.3222 (0.1898) loss_oracle 0.5495 (0.5940) acc 75.0000 (77.6847) alaph_mean 0.3440 (0.3231) alpha_min 0.0000 (0.0000) alpha_max 0.5142 (0.5233) lr 1.0000e-03 eta 0:21:43
epoch [27/50] batch [240/288] time 0.202 (0.195) data 0.000 (0.001) loss 1.0881 (1.5469) teacher_loss 0.5376 (0.8578) loss_zs_kd 0.1345 (0.1901) loss_oracle 0.4833 (0.5941) acc 84.3750 (77.1354) alaph_mean 0.3927 (0.3229) alpha_min 0.0000 (0.0000) alpha_max 0.5083 (0.5254) lr 1.0000e-03 eta 0:21:39
epoch [27/50] batch [260/288] time 0.195 (0.195) data 0.000 (0.001) loss 1.2388 (1.5479) teacher_loss 0.6377 (0.8606) loss_zs_kd 0.1504 (0.1895) loss_oracle 0.5259 (0.5926) acc 78.1250 (77.0553) alaph_mean 0.3460 (0.3228) alpha_min 0.0000 (0.0000) alpha_max 0.5093 (0.5244) lr 1.0000e-03 eta 0:21:35
epoch [27/50] batch [280/288] time 0.180 (0.195) data 0.000 (0.001) loss 1.9147 (1.5524) teacher_loss 1.1299 (0.8651) loss_zs_kd 0.2378 (0.1892) loss_oracle 0.6659 (0.5926) acc 62.5000 (76.8527) alaph_mean 0.2838 (0.3227) alpha_min -0.0000 (0.0000) alpha_max 0.5077 (0.5246) lr 1.0000e-03 eta 0:21:30
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,418
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.1%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,004
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.5%
******* Domain a best val acc:      86.8%, epoch: 19 *******
******* Domain a best val test acc: 82.6%, epoch: 19 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [28/50] batch [20/288] time 0.199 (0.210) data 0.000 (0.016) loss 1.2682 (1.5355) teacher_loss 0.7031 (0.8475) loss_zs_kd 0.1693 (0.1929) loss_oracle 0.4804 (0.5915) acc 81.2500 (79.0625) alaph_mean 0.3755 (0.3148) alpha_min 0.0000 (0.0000) alpha_max 0.5069 (0.5162) lr 9.3721e-04 eta 0:23:07
epoch [28/50] batch [40/288] time 0.198 (0.203) data 0.000 (0.008) loss 1.1099 (1.5515) teacher_loss 0.5161 (0.8819) loss_zs_kd 0.1283 (0.1788) loss_oracle 0.5296 (0.5801) acc 90.6250 (77.1875) alaph_mean 0.3508 (0.3201) alpha_min 0.0000 (0.0000) alpha_max 0.5142 (0.5306) lr 9.3721e-04 eta 0:22:15
epoch [28/50] batch [60/288] time 0.195 (0.198) data 0.001 (0.005) loss 1.2460 (1.5639) teacher_loss 0.5825 (0.8929) loss_zs_kd 0.1908 (0.1787) loss_oracle 0.5681 (0.5816) acc 81.2500 (76.4583) alaph_mean 0.3435 (0.3196) alpha_min 0.0000 (0.0000) alpha_max 0.5102 (0.5259) lr 9.3721e-04 eta 0:21:36
epoch [28/50] batch [80/288] time 0.199 (0.197) data 0.000 (0.004) loss 1.3096 (1.5744) teacher_loss 0.7280 (0.9018) loss_zs_kd 0.1412 (0.1791) loss_oracle 0.5110 (0.5831) acc 75.0000 (76.0156) alaph_mean 0.3755 (0.3185) alpha_min 0.0000 (0.0000) alpha_max 0.5067 (0.5285) lr 9.3721e-04 eta 0:21:28
epoch [28/50] batch [100/288] time 0.193 (0.196) data 0.000 (0.003) loss 1.4826 (1.5699) teacher_loss 0.7598 (0.8926) loss_zs_kd 0.1421 (0.1793) loss_oracle 0.6518 (0.5876) acc 84.3750 (76.2812) alaph_mean 0.2657 (0.3156) alpha_min -0.0000 (0.0000) alpha_max 0.5044 (0.5295) lr 9.3721e-04 eta 0:21:20
epoch [28/50] batch [120/288] time 0.197 (0.196) data 0.000 (0.003) loss 1.4472 (1.5579) teacher_loss 0.7607 (0.8821) loss_zs_kd 0.1448 (0.1760) loss_oracle 0.6141 (0.5877) acc 84.3750 (76.4844) alaph_mean 0.3140 (0.3157) alpha_min 0.0000 (0.0000) alpha_max 0.5081 (0.5284) lr 9.3721e-04 eta 0:21:14
epoch [28/50] batch [140/288] time 0.194 (0.196) data 0.000 (0.002) loss 0.9910 (1.5499) teacher_loss 0.3721 (0.8740) loss_zs_kd 0.1458 (0.1770) loss_oracle 0.5460 (0.5874) acc 93.7500 (76.6964) alaph_mean 0.3453 (0.3170) alpha_min 0.0000 (0.0000) alpha_max 0.5030 (0.5288) lr 9.3721e-04 eta 0:21:08
epoch [28/50] batch [160/288] time 0.205 (0.195) data 0.000 (0.002) loss 1.8157 (1.5414) teacher_loss 1.1523 (0.8663) loss_zs_kd 0.2337 (0.1770) loss_oracle 0.5465 (0.5867) acc 65.6250 (76.9727) alaph_mean 0.3439 (0.3190) alpha_min 0.0000 (0.0000) alpha_max 0.5053 (0.5288) lr 9.3721e-04 eta 0:21:02
epoch [28/50] batch [180/288] time 0.197 (0.195) data 0.000 (0.002) loss 1.3579 (1.5385) teacher_loss 0.6968 (0.8613) loss_zs_kd 0.2258 (0.1782) loss_oracle 0.5482 (0.5881) acc 75.0000 (77.0833) alaph_mean 0.3436 (0.3192) alpha_min -0.0000 (0.0000) alpha_max 0.5067 (0.5278) lr 9.3721e-04 eta 0:20:58
epoch [28/50] batch [200/288] time 0.196 (0.195) data 0.000 (0.002) loss 1.9507 (1.5475) teacher_loss 1.2314 (0.8677) loss_zs_kd 0.2586 (0.1800) loss_oracle 0.5900 (0.5897) acc 68.7500 (76.9375) alaph_mean 0.3311 (0.3191) alpha_min -0.0000 (0.0000) alpha_max 0.5034 (0.5262) lr 9.3721e-04 eta 0:20:53
epoch [28/50] batch [220/288] time 0.194 (0.195) data 0.000 (0.002) loss 1.5512 (1.5460) teacher_loss 0.8633 (0.8653) loss_zs_kd 0.1738 (0.1819) loss_oracle 0.6010 (0.5898) acc 78.1250 (76.9602) alaph_mean 0.3277 (0.3201) alpha_min 0.0000 (0.0000) alpha_max 0.5093 (0.5249) lr 9.3721e-04 eta 0:20:49
epoch [28/50] batch [240/288] time 0.194 (0.195) data 0.000 (0.001) loss 0.9571 (1.5484) teacher_loss 0.3518 (0.8645) loss_zs_kd 0.1555 (0.1834) loss_oracle 0.5276 (0.5922) acc 93.7500 (76.9271) alaph_mean 0.3589 (0.3197) alpha_min 0.0000 (0.0000) alpha_max 0.5066 (0.5244) lr 9.3721e-04 eta 0:20:45
epoch [28/50] batch [260/288] time 0.193 (0.195) data 0.000 (0.001) loss 1.3266 (1.5516) teacher_loss 0.6372 (0.8664) loss_zs_kd 0.1627 (0.1841) loss_oracle 0.6080 (0.5931) acc 81.2500 (76.9712) alaph_mean 0.3217 (0.3198) alpha_min -0.0000 (0.0000) alpha_max 0.5057 (0.5232) lr 9.3721e-04 eta 0:20:40
epoch [28/50] batch [280/288] time 0.193 (0.195) data 0.000 (0.001) loss 1.3151 (1.5502) teacher_loss 0.6094 (0.8621) loss_zs_kd 0.2115 (0.1857) loss_oracle 0.6000 (0.5952) acc 81.2500 (77.0312) alaph_mean 0.3153 (0.3195) alpha_min 0.0000 (0.0000) alpha_max 0.5046 (0.5257) lr 9.3721e-04 eta 0:20:36
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,424
* accuracy: 86.9%
* error: 13.1%
* macro_f1: 86.3%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,007
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 79.5%
******* Domain a best val acc:      86.9%, epoch: 28 *******
******* Domain a best val test acc: 82.7%, epoch: 28 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [29/50] batch [20/288] time 0.197 (0.198) data 0.000 (0.011) loss 1.3907 (1.4488) teacher_loss 0.5322 (0.7392) loss_zs_kd 0.2518 (0.2018) loss_oracle 0.7326 (0.6087) acc 81.2500 (80.4688) alaph_mean 0.2655 (0.3163) alpha_min -0.0000 (0.0000) alpha_max 0.5075 (0.5195) lr 8.7467e-04 eta 0:20:50
epoch [29/50] batch [40/288] time 0.190 (0.197) data 0.000 (0.005) loss 1.4708 (1.5199) teacher_loss 0.8076 (0.8290) loss_zs_kd 0.1798 (0.1973) loss_oracle 0.5733 (0.5922) acc 81.2500 (78.2031) alaph_mean 0.3877 (0.3259) alpha_min 0.0000 (0.0000) alpha_max 0.7440 (0.5243) lr 8.7467e-04 eta 0:20:37
epoch [29/50] batch [60/288] time 0.201 (0.196) data 0.000 (0.004) loss 2.2212 (1.5579) teacher_loss 1.6045 (0.8750) loss_zs_kd 0.2103 (0.1926) loss_oracle 0.5115 (0.5866) acc 59.3750 (77.0312) alaph_mean 0.3607 (0.3283) alpha_min -0.0000 (0.0000) alpha_max 0.5179 (0.5195) lr 8.7467e-04 eta 0:20:29
epoch [29/50] batch [80/288] time 0.194 (0.195) data 0.000 (0.003) loss 1.3568 (1.5649) teacher_loss 0.6479 (0.8765) loss_zs_kd 0.2238 (0.1957) loss_oracle 0.5970 (0.5906) acc 87.5000 (76.6797) alaph_mean 0.3469 (0.3261) alpha_min -0.0000 (0.0000) alpha_max 0.5046 (0.5182) lr 8.7467e-04 eta 0:20:22
epoch [29/50] batch [100/288] time 0.194 (0.195) data 0.000 (0.002) loss 1.8522 (1.5887) teacher_loss 1.2080 (0.8995) loss_zs_kd 0.1794 (0.1960) loss_oracle 0.5545 (0.5912) acc 71.8750 (76.3125) alaph_mean 0.3174 (0.3249) alpha_min 0.0000 (0.0000) alpha_max 0.5124 (0.5175) lr 8.7467e-04 eta 0:20:18
epoch [29/50] batch [120/288] time 0.194 (0.195) data 0.000 (0.002) loss 1.4669 (1.5751) teacher_loss 0.8091 (0.8858) loss_zs_kd 0.1335 (0.1927) loss_oracle 0.5910 (0.5930) acc 75.0000 (76.7708) alaph_mean 0.2848 (0.3222) alpha_min -0.0000 (0.0000) alpha_max 0.5015 (0.5193) lr 8.7467e-04 eta 0:20:11
epoch [29/50] batch [140/288] time 0.187 (0.195) data 0.000 (0.002) loss 1.7650 (1.5620) teacher_loss 0.9873 (0.8753) loss_zs_kd 0.1184 (0.1893) loss_oracle 0.7185 (0.5920) acc 71.8750 (76.8304) alaph_mean 0.2166 (0.3218) alpha_min 0.0000 (0.0000) alpha_max 0.5009 (0.5192) lr 8.7467e-04 eta 0:20:07
epoch [29/50] batch [160/288] time 0.199 (0.195) data 0.000 (0.002) loss 1.3358 (1.5635) teacher_loss 0.6191 (0.8751) loss_zs_kd 0.2348 (0.1909) loss_oracle 0.5993 (0.5929) acc 87.5000 (76.8555) alaph_mean 0.3196 (0.3217) alpha_min -0.0000 (0.0000) alpha_max 0.5051 (0.5178) lr 8.7467e-04 eta 0:20:03
epoch [29/50] batch [180/288] time 0.196 (0.195) data 0.000 (0.001) loss 1.7845 (1.5645) teacher_loss 1.1387 (0.8788) loss_zs_kd 0.2263 (0.1926) loss_oracle 0.5327 (0.5895) acc 68.7500 (76.8576) alaph_mean 0.3452 (0.3239) alpha_min -0.0000 (0.0000) alpha_max 0.5067 (0.5201) lr 8.7467e-04 eta 0:19:58
epoch [29/50] batch [200/288] time 0.188 (0.195) data 0.000 (0.001) loss 1.8768 (1.5649) teacher_loss 1.1172 (0.8769) loss_zs_kd 0.1851 (0.1928) loss_oracle 0.6671 (0.5916) acc 75.0000 (76.9688) alaph_mean 0.2917 (0.3234) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5220) lr 8.7467e-04 eta 0:19:54
epoch [29/50] batch [220/288] time 0.190 (0.194) data 0.000 (0.001) loss 1.9574 (1.5745) teacher_loss 1.0371 (0.8821) loss_zs_kd 0.1941 (0.1915) loss_oracle 0.8233 (0.5966) acc 68.7500 (76.9744) alaph_mean 0.1920 (0.3210) alpha_min 0.0000 (0.0000) alpha_max 0.6026 (0.5227) lr 8.7467e-04 eta 0:19:49
epoch [29/50] batch [240/288] time 0.192 (0.194) data 0.000 (0.001) loss 1.4615 (1.5756) teacher_loss 0.7451 (0.8810) loss_zs_kd 0.1646 (0.1914) loss_oracle 0.6340 (0.5989) acc 84.3750 (77.0052) alaph_mean 0.3197 (0.3199) alpha_min 0.0000 (0.0000) alpha_max 0.5087 (0.5238) lr 8.7467e-04 eta 0:19:45
epoch [29/50] batch [260/288] time 0.198 (0.194) data 0.000 (0.001) loss 1.3852 (1.5761) teacher_loss 0.5552 (0.8801) loss_zs_kd 0.2454 (0.1916) loss_oracle 0.7073 (0.6001) acc 84.3750 (77.0913) alaph_mean 0.3058 (0.3198) alpha_min 0.0000 (0.0000) alpha_max 0.5087 (0.5239) lr 8.7467e-04 eta 0:19:41
epoch [29/50] batch [280/288] time 0.205 (0.194) data 0.000 (0.001) loss 2.1019 (1.5779) teacher_loss 1.3193 (0.8812) loss_zs_kd 0.1933 (0.1920) loss_oracle 0.6859 (0.6007) acc 68.7500 (77.0424) alaph_mean 0.2967 (0.3197) alpha_min -0.0000 (0.0000) alpha_max 0.5095 (0.5247) lr 8.7467e-04 eta 0:19:37
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,430
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.4%
Checkpoint saved to icml/multi-dg/oracle/13_entropybaseweight_mixlogits/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,008
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 79.5%
******* Domain a best val acc:      87.1%, epoch: 29 *******
******* Domain a best val test acc: 82.7%, epoch: 29 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [30/50] batch [20/288] time 0.198 (0.212) data 0.000 (0.016) loss 1.2520 (1.5487) teacher_loss 0.5654 (0.8495) loss_zs_kd 0.2062 (0.1861) loss_oracle 0.5835 (0.6061) acc 87.5000 (76.0938) alaph_mean 0.3148 (0.3201) alpha_min -0.0000 (0.0000) alpha_max 0.5063 (0.5286) lr 8.1262e-04 eta 0:21:15
epoch [30/50] batch [40/288] time 0.183 (0.204) data 0.000 (0.008) loss 1.4834 (1.4766) teacher_loss 0.8062 (0.7754) loss_zs_kd 0.1484 (0.1835) loss_oracle 0.6030 (0.6095) acc 71.8750 (77.9688) alaph_mean 0.3101 (0.3185) alpha_min 0.0000 (0.0000) alpha_max 0.5066 (0.5260) lr 8.1262e-04 eta 0:20:27
epoch [30/50] batch [60/288] time 0.195 (0.201) data 0.001 (0.005) loss 1.6733 (1.5299) teacher_loss 0.9995 (0.8189) loss_zs_kd 0.2484 (0.1907) loss_oracle 0.5496 (0.6157) acc 65.6250 (77.4479) alaph_mean 0.3714 (0.3155) alpha_min 0.0000 (0.0000) alpha_max 0.5113 (0.5251) lr 8.1262e-04 eta 0:20:05
epoch [30/50] batch [80/288] time 0.190 (0.199) data 0.000 (0.004) loss 1.4143 (1.5421) teacher_loss 0.7129 (0.8335) loss_zs_kd 0.1767 (0.1907) loss_oracle 0.6130 (0.6133) acc 78.1250 (77.1484) alaph_mean 0.3224 (0.3157) alpha_min -0.0000 (0.0000) alpha_max 0.6629 (0.5248) lr 8.1262e-04 eta 0:19:49
epoch [30/50] batch [100/288] time 0.193 (0.198) data 0.000 (0.003) loss 1.3158 (1.5565) teacher_loss 0.5610 (0.8471) loss_zs_kd 0.1751 (0.1895) loss_oracle 0.6672 (0.6147) acc 84.3750 (76.9688) alaph_mean 0.2841 (0.3164) alpha_min 0.0000 (0.0000) alpha_max 0.5089 (0.5308) lr 8.1262e-04 eta 0:19:39
epoch [30/50] batch [120/288] time 0.194 (0.198) data 0.000 (0.003) loss 1.5503 (1.5572) teacher_loss 0.9087 (0.8509) loss_zs_kd 0.1669 (0.1900) loss_oracle 0.5582 (0.6112) acc 75.0000 (76.9010) alaph_mean 0.3768 (0.3191) alpha_min 0.0000 (0.0000) alpha_max 0.6256 (0.5363) lr 8.1262e-04 eta 0:19:31
epoch [30/50] batch [140/288] time 0.196 (0.197) data 0.000 (0.002) loss 1.6234 (1.5754) teacher_loss 1.0615 (0.8669) loss_zs_kd 0.1617 (0.1931) loss_oracle 0.4810 (0.6120) acc 68.7500 (76.7411) alaph_mean 0.3964 (0.3205) alpha_min 0.0000 (0.0000) alpha_max 0.5057 (0.5376) lr 8.1262e-04 eta 0:19:24
epoch [30/50] batch [160/288] time 0.193 (0.197) data 0.000 (0.002) loss 1.2778 (1.5690) teacher_loss 0.5957 (0.8613) loss_zs_kd 0.2062 (0.1947) loss_oracle 0.5790 (0.6103) acc 84.3750 (76.8750) alaph_mean 0.3503 (0.3223) alpha_min 0.0000 (0.0000) alpha_max 0.6876 (0.5395) lr 8.1262e-04 eta 0:19:18
epoch [30/50] batch [180/288] time 0.200 (0.197) data 0.000 (0.002) loss 1.6385 (1.5783) teacher_loss 0.9502 (0.8688) loss_zs_kd 0.2033 (0.1952) loss_oracle 0.5866 (0.6119) acc 71.8750 (76.6493) alaph_mean 0.3614 (0.3223) alpha_min 0.0000 (0.0000) alpha_max 0.5059 (0.5380) lr 8.1262e-04 eta 0:19:13
epoch [30/50] batch [200/288] time 0.205 (0.196) data 0.000 (0.002) loss 1.7090 (1.5748) teacher_loss 1.1396 (0.8663) loss_zs_kd 0.2165 (0.1958) loss_oracle 0.4611 (0.6106) acc 65.6250 (76.6250) alaph_mean 0.4132 (0.3222) alpha_min 0.0000 (0.0000) alpha_max 0.5077 (0.5367) lr 8.1262e-04 eta 0:19:07
epoch [30/50] batch [220/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.7928 (1.5769) teacher_loss 0.9814 (0.8688) loss_zs_kd 0.1978 (0.1960) loss_oracle 0.7124 (0.6101) acc 75.0000 (76.5341) alaph_mean 0.2457 (0.3225) alpha_min 0.0000 (0.0000) alpha_max 0.5062 (0.5358) lr 8.1262e-04 eta 0:19:03
epoch [30/50] batch [240/288] time 0.197 (0.196) data 0.000 (0.002) loss 1.1614 (1.5767) teacher_loss 0.5410 (0.8706) loss_zs_kd 0.1594 (0.1957) loss_oracle 0.5407 (0.6083) acc 90.6250 (76.5365) alaph_mean 0.3671 (0.3237) alpha_min 0.0000 (0.0000) alpha_max 0.5065 (0.5341) lr 8.1262e-04 eta 0:18:58
epoch [30/50] batch [260/288] time 0.193 (0.196) data 0.000 (0.001) loss 1.4571 (1.5829) teacher_loss 0.7788 (0.8787) loss_zs_kd 0.1594 (0.1956) loss_oracle 0.5986 (0.6064) acc 81.2500 (76.4183) alaph_mean 0.2527 (0.3242) alpha_min -0.0000 (0.0000) alpha_max 0.5056 (0.5338) lr 8.1262e-04 eta 0:18:53
epoch [30/50] batch [280/288] time 0.188 (0.196) data 0.000 (0.001) loss 1.4132 (1.5752) teacher_loss 0.6191 (0.8714) loss_zs_kd 0.1597 (0.1944) loss_oracle 0.7142 (0.6066) acc 81.2500 (76.7076) alaph_mean 0.2537 (0.3236) alpha_min 0.0000 (0.0000) alpha_max 0.5026 (0.5345) lr 8.1262e-04 eta 0:18:49
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,414
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 80.1%
******* Domain a best val acc:      87.1%, epoch: 29 *******
******* Domain a best val test acc: 82.7%, epoch: 29 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [31/50] batch [20/288] time 0.194 (0.213) data 0.000 (0.016) loss 0.9191 (1.5218) teacher_loss 0.3433 (0.8089) loss_zs_kd 0.1476 (0.2092) loss_oracle 0.5020 (0.6083) acc 90.6250 (79.3750) alaph_mean 0.3780 (0.3251) alpha_min 0.0000 (0.0000) alpha_max 0.5068 (0.5183) lr 7.5131e-04 eta 0:20:21
epoch [31/50] batch [40/288] time 0.192 (0.203) data 0.000 (0.008) loss 1.3003 (1.5794) teacher_loss 0.6704 (0.8666) loss_zs_kd 0.1664 (0.1985) loss_oracle 0.5467 (0.6135) acc 87.5000 (77.7344) alaph_mean 0.3337 (0.3193) alpha_min 0.0000 (0.0000) alpha_max 0.5112 (0.5244) lr 7.5131e-04 eta 0:19:21
epoch [31/50] batch [60/288] time 0.211 (0.200) data 0.000 (0.006) loss 1.5433 (1.5874) teacher_loss 0.8662 (0.8781) loss_zs_kd 0.1950 (0.1957) loss_oracle 0.5795 (0.6114) acc 78.1250 (76.9271) alaph_mean 0.3331 (0.3199) alpha_min 0.0000 (0.0000) alpha_max 0.5124 (0.5209) lr 7.5131e-04 eta 0:19:00
epoch [31/50] batch [80/288] time 0.210 (0.199) data 0.000 (0.004) loss 1.3917 (1.5685) teacher_loss 0.7344 (0.8591) loss_zs_kd 0.1685 (0.1976) loss_oracle 0.5731 (0.6106) acc 81.2500 (76.9922) alaph_mean 0.3411 (0.3199) alpha_min 0.0000 (0.0000) alpha_max 0.6103 (0.5204) lr 7.5131e-04 eta 0:18:48
epoch [31/50] batch [100/288] time 0.195 (0.198) data 0.000 (0.003) loss 1.6930 (1.5566) teacher_loss 0.9678 (0.8501) loss_zs_kd 0.2356 (0.1957) loss_oracle 0.6074 (0.6087) acc 75.0000 (77.1250) alaph_mean 0.3072 (0.3222) alpha_min -0.0000 (0.0000) alpha_max 0.8139 (0.5250) lr 7.5131e-04 eta 0:18:40
epoch [31/50] batch [120/288] time 0.196 (0.196) data 0.000 (0.003) loss 1.1691 (1.5702) teacher_loss 0.4863 (0.8558) loss_zs_kd 0.1773 (0.1985) loss_oracle 0.5941 (0.6152) acc 87.5000 (77.1615) alaph_mean 0.3278 (0.3194) alpha_min 0.0000 (0.0000) alpha_max 0.6787 (0.5267) lr 7.5131e-04 eta 0:18:27
epoch [31/50] batch [140/288] time 0.199 (0.196) data 0.000 (0.003) loss 1.1394 (1.5628) teacher_loss 0.5068 (0.8515) loss_zs_kd 0.2634 (0.1977) loss_oracle 0.5009 (0.6125) acc 84.3750 (77.4107) alaph_mean 0.3940 (0.3209) alpha_min 0.0000 (0.0000) alpha_max 0.5125 (0.5310) lr 7.5131e-04 eta 0:18:22
epoch [31/50] batch [160/288] time 0.187 (0.196) data 0.000 (0.002) loss 1.6221 (1.5684) teacher_loss 0.7705 (0.8576) loss_zs_kd 0.2035 (0.1972) loss_oracle 0.7499 (0.6122) acc 81.2500 (77.3242) alaph_mean 0.2426 (0.3203) alpha_min 0.0000 (0.0000) alpha_max 0.5953 (0.5291) lr 7.5131e-04 eta 0:18:16
epoch [31/50] batch [180/288] time 0.198 (0.196) data 0.000 (0.002) loss 1.8321 (1.5657) teacher_loss 1.2197 (0.8591) loss_zs_kd 0.1958 (0.1952) loss_oracle 0.5145 (0.6089) acc 68.7500 (77.1528) alaph_mean 0.3444 (0.3212) alpha_min 0.0000 (0.0000) alpha_max 0.5089 (0.5294) lr 7.5131e-04 eta 0:18:11
epoch [31/50] batch [200/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.4976 (1.5641) teacher_loss 0.8706 (0.8575) loss_zs_kd 0.1857 (0.1956) loss_oracle 0.5342 (0.6087) acc 84.3750 (77.2344) alaph_mean 0.3639 (0.3225) alpha_min 0.0000 (0.0000) alpha_max 0.5051 (0.5310) lr 7.5131e-04 eta 0:18:07
epoch [31/50] batch [220/288] time 0.193 (0.196) data 0.000 (0.002) loss 1.7235 (1.5666) teacher_loss 0.8887 (0.8594) loss_zs_kd 0.2216 (0.1962) loss_oracle 0.7240 (0.6091) acc 65.6250 (77.2301) alaph_mean 0.2572 (0.3226) alpha_min -0.0000 (0.0000) alpha_max 0.5046 (0.5292) lr 7.5131e-04 eta 0:18:03
epoch [31/50] batch [240/288] time 0.188 (0.195) data 0.000 (0.002) loss 1.7093 (1.5652) teacher_loss 0.9316 (0.8572) loss_zs_kd 0.1697 (0.1967) loss_oracle 0.6929 (0.6096) acc 75.0000 (77.1875) alaph_mean 0.2517 (0.3222) alpha_min 0.0000 (0.0000) alpha_max 0.5073 (0.5291) lr 7.5131e-04 eta 0:17:58
epoch [31/50] batch [260/288] time 0.170 (0.195) data 0.000 (0.001) loss 1.2719 (1.5681) teacher_loss 0.5132 (0.8599) loss_zs_kd 0.1355 (0.1955) loss_oracle 0.6910 (0.6104) acc 90.6250 (77.1635) alaph_mean 0.2656 (0.3214) alpha_min 0.0000 (0.0000) alpha_max 0.5036 (0.5283) lr 7.5131e-04 eta 0:17:53
epoch [31/50] batch [280/288] time 0.190 (0.195) data 0.000 (0.001) loss 1.4134 (1.5568) teacher_loss 0.6973 (0.8517) loss_zs_kd 0.1609 (0.1943) loss_oracle 0.6357 (0.6079) acc 84.3750 (77.5446) alaph_mean 0.2894 (0.3220) alpha_min 0.0000 (0.0000) alpha_max 0.5084 (0.5277) lr 7.5131e-04 eta 0:17:49
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,411
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 86.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,013
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.7%
******* Domain a best val acc:      87.1%, epoch: 29 *******
******* Domain a best val test acc: 82.7%, epoch: 29 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [32/50] batch [20/288] time 0.163 (0.192) data 0.000 (0.013) loss 1.4428 (1.6354) teacher_loss 0.8467 (0.9042) loss_zs_kd 0.1346 (0.1864) loss_oracle 0.5288 (0.6380) acc 78.1250 (75.9375) alaph_mean 0.3133 (0.3007) alpha_min 0.0000 (0.0000) alpha_max 0.5052 (0.5262) lr 6.9098e-04 eta 0:17:25
epoch [32/50] batch [40/288] time 0.200 (0.193) data 0.000 (0.007) loss 0.9904 (1.6010) teacher_loss 0.4370 (0.8896) loss_zs_kd 0.1146 (0.1963) loss_oracle 0.4960 (0.6133) acc 87.5000 (76.7188) alaph_mean 0.3846 (0.3115) alpha_min 0.0000 (0.0000) alpha_max 0.5070 (0.5167) lr 6.9098e-04 eta 0:17:29
epoch [32/50] batch [60/288] time 0.196 (0.193) data 0.001 (0.005) loss 1.2746 (1.6212) teacher_loss 0.6865 (0.8983) loss_zs_kd 0.2026 (0.1994) loss_oracle 0.4868 (0.6232) acc 81.2500 (76.5104) alaph_mean 0.3612 (0.3071) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5162) lr 6.9098e-04 eta 0:17:26
epoch [32/50] batch [80/288] time 0.196 (0.194) data 0.000 (0.003) loss 1.9748 (1.5897) teacher_loss 1.2959 (0.8769) loss_zs_kd 0.1802 (0.1965) loss_oracle 0.5888 (0.6146) acc 62.5000 (76.9531) alaph_mean 0.3005 (0.3108) alpha_min 0.0000 (0.0000) alpha_max 0.5052 (0.5159) lr 6.9098e-04 eta 0:17:23
epoch [32/50] batch [100/288] time 0.198 (0.194) data 0.000 (0.003) loss 1.1705 (1.5862) teacher_loss 0.4446 (0.8775) loss_zs_kd 0.1832 (0.1939) loss_oracle 0.6343 (0.6117) acc 90.6250 (77.0000) alaph_mean 0.3213 (0.3104) alpha_min -0.0000 (0.0000) alpha_max 0.5049 (0.5145) lr 6.9098e-04 eta 0:17:20
epoch [32/50] batch [120/288] time 0.194 (0.194) data 0.000 (0.002) loss 1.1046 (1.5769) teacher_loss 0.5439 (0.8732) loss_zs_kd 0.1487 (0.1943) loss_oracle 0.4863 (0.6066) acc 78.1250 (76.8229) alaph_mean 0.3674 (0.3135) alpha_min -0.0000 (0.0000) alpha_max 0.5078 (0.5146) lr 6.9098e-04 eta 0:17:17
epoch [32/50] batch [140/288] time 0.196 (0.194) data 0.000 (0.002) loss 1.5912 (1.5741) teacher_loss 0.9126 (0.8722) loss_zs_kd 0.2497 (0.1937) loss_oracle 0.5537 (0.6050) acc 81.2500 (76.7857) alaph_mean 0.3304 (0.3134) alpha_min 0.0000 (0.0000) alpha_max 0.5053 (0.5157) lr 6.9098e-04 eta 0:17:13
epoch [32/50] batch [160/288] time 0.204 (0.194) data 0.000 (0.002) loss 1.3178 (1.5756) teacher_loss 0.6138 (0.8724) loss_zs_kd 0.2069 (0.1934) loss_oracle 0.6006 (0.6066) acc 84.3750 (76.6406) alaph_mean 0.3298 (0.3122) alpha_min -0.0000 (0.0000) alpha_max 0.5111 (0.5147) lr 6.9098e-04 eta 0:17:09
epoch [32/50] batch [180/288] time 0.197 (0.194) data 0.000 (0.002) loss 1.4012 (1.5821) teacher_loss 0.7451 (0.8790) loss_zs_kd 0.1986 (0.1934) loss_oracle 0.5568 (0.6064) acc 81.2500 (76.2153) alaph_mean 0.3594 (0.3125) alpha_min -0.0000 (0.0000) alpha_max 0.5089 (0.5163) lr 6.9098e-04 eta 0:17:05
epoch [32/50] batch [200/288] time 0.192 (0.194) data 0.000 (0.002) loss 1.5790 (1.5754) teacher_loss 0.8779 (0.8743) loss_zs_kd 0.1952 (0.1931) loss_oracle 0.6035 (0.6046) acc 78.1250 (76.3594) alaph_mean 0.2979 (0.3132) alpha_min 0.0000 (0.0000) alpha_max 0.5054 (0.5187) lr 6.9098e-04 eta 0:17:01
epoch [32/50] batch [220/288] time 0.194 (0.194) data 0.000 (0.001) loss 1.3898 (1.5717) teacher_loss 0.6460 (0.8766) loss_zs_kd 0.1434 (0.1925) loss_oracle 0.6721 (0.5988) acc 81.2500 (76.3920) alaph_mean 0.2683 (0.3159) alpha_min 0.0000 (0.0000) alpha_max 0.5103 (0.5183) lr 6.9098e-04 eta 0:16:57
epoch [32/50] batch [240/288] time 0.194 (0.194) data 0.000 (0.001) loss 1.3759 (1.5602) teacher_loss 0.8892 (0.8674) loss_zs_kd 0.1536 (0.1910) loss_oracle 0.4099 (0.5973) acc 84.3750 (76.6927) alaph_mean 0.3706 (0.3172) alpha_min 0.0000 (0.0000) alpha_max 0.5083 (0.5177) lr 6.9098e-04 eta 0:16:54
epoch [32/50] batch [260/288] time 0.195 (0.194) data 0.000 (0.001) loss 1.5860 (1.5653) teacher_loss 0.9741 (0.8746) loss_zs_kd 0.1273 (0.1909) loss_oracle 0.5482 (0.5953) acc 78.1250 (76.6106) alaph_mean 0.3148 (0.3179) alpha_min 0.0000 (0.0000) alpha_max 0.5119 (0.5190) lr 6.9098e-04 eta 0:16:49
epoch [32/50] batch [280/288] time 0.206 (0.194) data 0.000 (0.001) loss 1.1816 (1.5630) teacher_loss 0.6431 (0.8728) loss_zs_kd 0.1557 (0.1905) loss_oracle 0.4607 (0.5949) acc 78.1250 (76.6406) alaph_mean 0.3974 (0.3186) alpha_min 0.0000 (0.0000) alpha_max 0.5055 (0.5191) lr 6.9098e-04 eta 0:16:46
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,413
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 86.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,019
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 80.1%
******* Domain a best val acc:      87.1%, epoch: 29 *******
******* Domain a best val test acc: 82.7%, epoch: 29 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [33/50] batch [20/288] time 0.192 (0.193) data 0.000 (0.012) loss 1.4325 (1.5934) teacher_loss 0.9146 (0.9133) loss_zs_kd 0.1931 (0.1992) loss_oracle 0.4214 (0.5805) acc 81.2500 (75.7812) alaph_mean 0.4157 (0.3185) alpha_min 0.0000 (0.0000) alpha_max 0.5083 (0.5284) lr 6.3188e-04 eta 0:16:37
epoch [33/50] batch [40/288] time 0.192 (0.192) data 0.000 (0.006) loss 1.5362 (1.6114) teacher_loss 0.8457 (0.9244) loss_zs_kd 0.1707 (0.1951) loss_oracle 0.6052 (0.5894) acc 84.3750 (75.2344) alaph_mean 0.3388 (0.3104) alpha_min 0.0000 (0.0000) alpha_max 0.5092 (0.5256) lr 6.3188e-04 eta 0:16:27
epoch [33/50] batch [60/288] time 0.193 (0.193) data 0.001 (0.004) loss 1.6497 (1.6088) teacher_loss 0.9077 (0.9133) loss_zs_kd 0.2250 (0.1961) loss_oracle 0.6295 (0.5975) acc 68.7500 (75.3646) alaph_mean 0.2865 (0.3080) alpha_min 0.0000 (0.0000) alpha_max 0.5037 (0.5287) lr 6.3188e-04 eta 0:16:26
epoch [33/50] batch [80/288] time 0.193 (0.193) data 0.000 (0.003) loss 1.4772 (1.6059) teacher_loss 0.7598 (0.9118) loss_zs_kd 0.1418 (0.1932) loss_oracle 0.6465 (0.5976) acc 78.1250 (75.3516) alaph_mean 0.2826 (0.3103) alpha_min 0.0000 (0.0000) alpha_max 0.5124 (0.5261) lr 6.3188e-04 eta 0:16:25
epoch [33/50] batch [100/288] time 0.202 (0.193) data 0.000 (0.003) loss 1.5746 (1.5759) teacher_loss 0.8130 (0.8890) loss_zs_kd 0.1657 (0.1935) loss_oracle 0.6788 (0.5901) acc 81.2500 (76.0312) alaph_mean 0.2973 (0.3152) alpha_min 0.0000 (0.0000) alpha_max 0.5086 (0.5243) lr 6.3188e-04 eta 0:16:23
epoch [33/50] batch [120/288] time 0.197 (0.194) data 0.000 (0.002) loss 1.9890 (1.5692) teacher_loss 1.4199 (0.8858) loss_zs_kd 0.1875 (0.1933) loss_oracle 0.4753 (0.5868) acc 62.5000 (76.2500) alaph_mean 0.3877 (0.3182) alpha_min -0.0000 (0.0000) alpha_max 0.5038 (0.5229) lr 6.3188e-04 eta 0:16:20
epoch [33/50] batch [140/288] time 0.200 (0.194) data 0.000 (0.002) loss 1.6929 (1.5675) teacher_loss 1.0674 (0.8816) loss_zs_kd 0.2365 (0.1935) loss_oracle 0.5073 (0.5891) acc 71.8750 (76.5179) alaph_mean 0.4067 (0.3182) alpha_min 0.0000 (0.0000) alpha_max 0.5114 (0.5216) lr 6.3188e-04 eta 0:16:17
epoch [33/50] batch [160/288] time 0.194 (0.194) data 0.000 (0.002) loss 1.7281 (1.5659) teacher_loss 0.9526 (0.8781) loss_zs_kd 0.2376 (0.1927) loss_oracle 0.6566 (0.5914) acc 75.0000 (76.6797) alaph_mean 0.2976 (0.3171) alpha_min 0.0000 (0.0000) alpha_max 0.5150 (0.5205) lr 6.3188e-04 eta 0:16:13
epoch [33/50] batch [180/288] time 0.194 (0.194) data 0.000 (0.002) loss 1.7954 (1.5708) teacher_loss 1.1201 (0.8784) loss_zs_kd 0.1585 (0.1909) loss_oracle 0.5960 (0.5970) acc 75.0000 (76.6840) alaph_mean 0.3182 (0.3155) alpha_min 0.0000 (0.0000) alpha_max 0.5088 (0.5200) lr 6.3188e-04 eta 0:16:09
epoch [33/50] batch [200/288] time 0.192 (0.194) data 0.000 (0.001) loss 1.5346 (1.5662) teacher_loss 0.8477 (0.8737) loss_zs_kd 0.1613 (0.1896) loss_oracle 0.6063 (0.5977) acc 75.0000 (76.6875) alaph_mean 0.3321 (0.3160) alpha_min 0.0000 (0.0000) alpha_max 0.5117 (0.5213) lr 6.3188e-04 eta 0:16:05
epoch [33/50] batch [220/288] time 0.195 (0.194) data 0.000 (0.001) loss 1.5612 (1.5652) teacher_loss 0.9761 (0.8709) loss_zs_kd 0.1053 (0.1897) loss_oracle 0.5325 (0.5994) acc 75.0000 (76.6903) alaph_mean 0.3490 (0.3158) alpha_min 0.0000 (0.0000) alpha_max 0.5056 (0.5213) lr 6.3188e-04 eta 0:16:01
epoch [33/50] batch [240/288] time 0.194 (0.194) data 0.000 (0.001) loss 1.1812 (1.5552) teacher_loss 0.5132 (0.8609) loss_zs_kd 0.1852 (0.1903) loss_oracle 0.5754 (0.5992) acc 84.3750 (76.9010) alaph_mean 0.3480 (0.3168) alpha_min 0.0000 (0.0000) alpha_max 0.6466 (0.5215) lr 6.3188e-04 eta 0:15:58
epoch [33/50] batch [260/288] time 0.196 (0.194) data 0.000 (0.001) loss 1.3683 (1.5580) teacher_loss 0.8320 (0.8654) loss_zs_kd 0.1251 (0.1906) loss_oracle 0.4737 (0.5973) acc 84.3750 (76.8630) alaph_mean 0.3403 (0.3182) alpha_min 0.0000 (0.0000) alpha_max 0.5099 (0.5225) lr 6.3188e-04 eta 0:15:54
epoch [33/50] batch [280/288] time 0.197 (0.194) data 0.000 (0.001) loss 1.4082 (1.5599) teacher_loss 0.7852 (0.8658) loss_zs_kd 0.1749 (0.1912) loss_oracle 0.5355 (0.5984) acc 75.0000 (76.8415) alaph_mean 0.3513 (0.3178) alpha_min 0.0000 (0.0000) alpha_max 0.5106 (0.5229) lr 6.3188e-04 eta 0:15:50
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,420
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 80.1%
******* Domain a best val acc:      87.1%, epoch: 29 *******
******* Domain a best val test acc: 82.7%, epoch: 29 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [34/50] batch [20/288] time 0.195 (0.206) data 0.000 (0.011) loss 1.7749 (1.4978) teacher_loss 1.1641 (0.8203) loss_zs_kd 0.2238 (0.1909) loss_oracle 0.4989 (0.5820) acc 65.6250 (77.3438) alaph_mean 0.3605 (0.3173) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5161) lr 5.7422e-04 eta 0:16:43
epoch [34/50] batch [40/288] time 0.172 (0.200) data 0.000 (0.006) loss 1.5425 (1.5316) teacher_loss 0.7041 (0.8517) loss_zs_kd 0.1503 (0.1911) loss_oracle 0.7633 (0.5843) acc 87.5000 (76.7188) alaph_mean 0.2342 (0.3218) alpha_min -0.0000 (0.0000) alpha_max 0.5034 (0.5183) lr 5.7422e-04 eta 0:16:09
epoch [34/50] batch [60/288] time 0.193 (0.198) data 0.001 (0.004) loss 1.4009 (1.5292) teacher_loss 0.6592 (0.8547) loss_zs_kd 0.1317 (0.1889) loss_oracle 0.6759 (0.5801) acc 81.2500 (76.4583) alaph_mean 0.2819 (0.3247) alpha_min -0.0000 (0.0000) alpha_max 0.5082 (0.5296) lr 5.7422e-04 eta 0:15:58
epoch [34/50] batch [80/288] time 0.193 (0.197) data 0.000 (0.003) loss 1.5601 (1.5232) teacher_loss 0.9541 (0.8529) loss_zs_kd 0.2231 (0.1903) loss_oracle 0.4944 (0.5751) acc 75.0000 (77.3047) alaph_mean 0.3512 (0.3283) alpha_min 0.0000 (0.0000) alpha_max 0.5049 (0.5299) lr 5.7422e-04 eta 0:15:49
epoch [34/50] batch [100/288] time 0.193 (0.197) data 0.000 (0.002) loss 1.6288 (1.5132) teacher_loss 0.9102 (0.8394) loss_zs_kd 0.2312 (0.1900) loss_oracle 0.6031 (0.5787) acc 78.1250 (77.8750) alaph_mean 0.3126 (0.3256) alpha_min 0.0000 (0.0000) alpha_max 0.5086 (0.5337) lr 5.7422e-04 eta 0:15:42
epoch [34/50] batch [120/288] time 0.187 (0.196) data 0.000 (0.002) loss 1.6378 (1.5272) teacher_loss 0.8296 (0.8466) loss_zs_kd 0.1813 (0.1904) loss_oracle 0.7176 (0.5854) acc 75.0000 (77.7083) alaph_mean 0.2420 (0.3233) alpha_min -0.0000 (0.0000) alpha_max 0.5091 (0.5293) lr 5.7422e-04 eta 0:15:36
epoch [34/50] batch [140/288] time 0.184 (0.196) data 0.000 (0.002) loss 1.6383 (1.5230) teacher_loss 0.8696 (0.8376) loss_zs_kd 0.1910 (0.1920) loss_oracle 0.6732 (0.5893) acc 71.8750 (78.0134) alaph_mean 0.2542 (0.3227) alpha_min 0.0000 (0.0000) alpha_max 0.5077 (0.5293) lr 5.7422e-04 eta 0:15:31
epoch [34/50] batch [160/288] time 0.207 (0.196) data 0.000 (0.002) loss 0.9246 (1.5241) teacher_loss 0.4954 (0.8365) loss_zs_kd 0.1667 (0.1933) loss_oracle 0.3459 (0.5910) acc 84.3750 (77.9102) alaph_mean 0.4702 (0.3227) alpha_min 0.0000 (0.0000) alpha_max 0.5113 (0.5275) lr 5.7422e-04 eta 0:15:26
epoch [34/50] batch [180/288] time 0.188 (0.195) data 0.000 (0.001) loss 1.5714 (1.5280) teacher_loss 0.9409 (0.8441) loss_zs_kd 0.2031 (0.1946) loss_oracle 0.5289 (0.5865) acc 68.7500 (77.4306) alaph_mean 0.3298 (0.3250) alpha_min -0.0000 (0.0000) alpha_max 0.5869 (0.5287) lr 5.7422e-04 eta 0:15:21
epoch [34/50] batch [200/288] time 0.191 (0.195) data 0.000 (0.001) loss 2.1524 (1.5454) teacher_loss 1.3164 (0.8570) loss_zs_kd 0.2365 (0.1950) loss_oracle 0.7177 (0.5909) acc 68.7500 (77.2188) alaph_mean 0.2795 (0.3235) alpha_min 0.0000 (0.0000) alpha_max 0.8024 (0.5299) lr 5.7422e-04 eta 0:15:16
epoch [34/50] batch [220/288] time 0.197 (0.195) data 0.000 (0.001) loss 1.7723 (1.5493) teacher_loss 1.1328 (0.8578) loss_zs_kd 0.1575 (0.1955) loss_oracle 0.5608 (0.5937) acc 65.6250 (77.0739) alaph_mean 0.3353 (0.3231) alpha_min 0.0000 (0.0000) alpha_max 0.5095 (0.5280) lr 5.7422e-04 eta 0:15:12
epoch [34/50] batch [240/288] time 0.185 (0.195) data 0.000 (0.001) loss 1.6472 (1.5554) teacher_loss 0.8877 (0.8634) loss_zs_kd 0.2093 (0.1956) loss_oracle 0.6549 (0.5942) acc 65.6250 (76.9661) alaph_mean 0.2624 (0.3232) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5266) lr 5.7422e-04 eta 0:15:08
epoch [34/50] batch [260/288] time 0.194 (0.195) data 0.000 (0.001) loss 1.3344 (1.5497) teacher_loss 0.7236 (0.8578) loss_zs_kd 0.2057 (0.1955) loss_oracle 0.5079 (0.5941) acc 84.3750 (77.1755) alaph_mean 0.3764 (0.3232) alpha_min 0.0000 (0.0000) alpha_max 0.5129 (0.5275) lr 5.7422e-04 eta 0:15:04
epoch [34/50] batch [280/288] time 0.186 (0.195) data 0.000 (0.001) loss 1.8705 (1.5461) teacher_loss 0.9663 (0.8536) loss_zs_kd 0.2829 (0.1947) loss_oracle 0.7627 (0.5952) acc 65.6250 (77.2879) alaph_mean 0.2305 (0.3227) alpha_min 0.0000 (0.0000) alpha_max 0.5092 (0.5262) lr 5.7422e-04 eta 0:15:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,423
* accuracy: 86.9%
* error: 13.1%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,013
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.8%
******* Domain a best val acc:      87.1%, epoch: 29 *******
******* Domain a best val test acc: 82.7%, epoch: 29 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [35/50] batch [20/288] time 0.196 (0.222) data 0.000 (0.016) loss 1.7810 (1.5925) teacher_loss 1.1641 (0.8687) loss_zs_kd 0.2244 (0.2176) loss_oracle 0.5048 (0.6151) acc 78.1250 (77.8125) alaph_mean 0.3743 (0.3292) alpha_min 0.0000 (0.0000) alpha_max 0.5029 (0.5175) lr 5.1825e-04 eta 0:16:59
epoch [35/50] batch [40/288] time 0.199 (0.209) data 0.000 (0.008) loss 1.6276 (1.6504) teacher_loss 0.9429 (0.9215) loss_zs_kd 0.1530 (0.2161) loss_oracle 0.6082 (0.6209) acc 71.8750 (75.7812) alaph_mean 0.3289 (0.3254) alpha_min 0.0000 (0.0000) alpha_max 0.5117 (0.5260) lr 5.1825e-04 eta 0:15:53
epoch [35/50] batch [60/288] time 0.181 (0.203) data 0.000 (0.005) loss 2.2055 (1.6395) teacher_loss 1.4102 (0.9043) loss_zs_kd 0.1648 (0.2088) loss_oracle 0.7130 (0.6308) acc 71.8750 (76.6146) alaph_mean 0.2751 (0.3184) alpha_min 0.0000 (0.0000) alpha_max 0.5087 (0.5244) lr 5.1825e-04 eta 0:15:24
epoch [35/50] batch [80/288] time 0.195 (0.201) data 0.000 (0.004) loss 1.7035 (1.6350) teacher_loss 0.9180 (0.9049) loss_zs_kd 0.2480 (0.2079) loss_oracle 0.6615 (0.6262) acc 78.1250 (76.4844) alaph_mean 0.2965 (0.3184) alpha_min 0.0000 (0.0000) alpha_max 0.5044 (0.5214) lr 5.1825e-04 eta 0:15:11
epoch [35/50] batch [100/288] time 0.192 (0.200) data 0.000 (0.003) loss 1.6486 (1.6285) teacher_loss 0.9775 (0.9026) loss_zs_kd 0.1795 (0.2080) loss_oracle 0.5813 (0.6219) acc 75.0000 (76.4375) alaph_mean 0.3676 (0.3202) alpha_min 0.0000 (0.0000) alpha_max 0.5069 (0.5193) lr 5.1825e-04 eta 0:15:00
epoch [35/50] batch [120/288] time 0.195 (0.199) data 0.000 (0.003) loss 1.3911 (1.6133) teacher_loss 0.7246 (0.8922) loss_zs_kd 0.2487 (0.2070) loss_oracle 0.5421 (0.6176) acc 78.1250 (76.8229) alaph_mean 0.3617 (0.3209) alpha_min 0.0000 (0.0000) alpha_max 0.6022 (0.5214) lr 5.1825e-04 eta 0:14:52
epoch [35/50] batch [140/288] time 0.195 (0.198) data 0.000 (0.002) loss 1.1470 (1.6093) teacher_loss 0.4851 (0.8908) loss_zs_kd 0.0972 (0.2042) loss_oracle 0.6133 (0.6164) acc 90.6250 (76.8973) alaph_mean 0.3058 (0.3185) alpha_min 0.0000 (0.0000) alpha_max 0.5051 (0.5222) lr 5.1825e-04 eta 0:14:45
epoch [35/50] batch [160/288] time 0.196 (0.198) data 0.000 (0.002) loss 1.4241 (1.6026) teacher_loss 0.8120 (0.8865) loss_zs_kd 0.1764 (0.2032) loss_oracle 0.5239 (0.6144) acc 75.0000 (76.9727) alaph_mean 0.3256 (0.3189) alpha_min 0.0000 (0.0000) alpha_max 0.5102 (0.5232) lr 5.1825e-04 eta 0:14:39
epoch [35/50] batch [180/288] time 0.195 (0.197) data 0.000 (0.002) loss 1.2468 (1.5891) teacher_loss 0.5967 (0.8712) loss_zs_kd 0.1242 (0.2001) loss_oracle 0.5881 (0.6179) acc 81.2500 (77.2396) alaph_mean 0.3440 (0.3159) alpha_min 0.0000 (0.0000) alpha_max 0.5090 (0.5254) lr 5.1825e-04 eta 0:14:33
epoch [35/50] batch [200/288] time 0.197 (0.197) data 0.000 (0.002) loss 1.1357 (1.5831) teacher_loss 0.4844 (0.8647) loss_zs_kd 0.1829 (0.1994) loss_oracle 0.5599 (0.6187) acc 87.5000 (77.3594) alaph_mean 0.3589 (0.3151) alpha_min 0.0000 (0.0000) alpha_max 0.5082 (0.5249) lr 5.1825e-04 eta 0:14:28
epoch [35/50] batch [220/288] time 0.195 (0.197) data 0.000 (0.002) loss 1.3146 (1.5773) teacher_loss 0.6138 (0.8607) loss_zs_kd 0.1482 (0.1987) loss_oracle 0.6267 (0.6172) acc 84.3750 (77.3722) alaph_mean 0.2906 (0.3156) alpha_min 0.0000 (0.0000) alpha_max 0.5063 (0.5249) lr 5.1825e-04 eta 0:14:23
epoch [35/50] batch [240/288] time 0.194 (0.196) data 0.000 (0.002) loss 1.2624 (1.5745) teacher_loss 0.5088 (0.8585) loss_zs_kd 0.2211 (0.1984) loss_oracle 0.6431 (0.6168) acc 78.1250 (77.3568) alaph_mean 0.2509 (0.3158) alpha_min 0.0000 (0.0000) alpha_max 0.5084 (0.5260) lr 5.1825e-04 eta 0:14:18
epoch [35/50] batch [260/288] time 0.193 (0.196) data 0.000 (0.001) loss 1.2317 (1.5633) teacher_loss 0.6494 (0.8491) loss_zs_kd 0.1805 (0.1978) loss_oracle 0.4920 (0.6153) acc 87.5000 (77.6442) alaph_mean 0.3322 (0.3154) alpha_min -0.0000 (0.0000) alpha_max 0.5033 (0.5250) lr 5.1825e-04 eta 0:14:13
epoch [35/50] batch [280/288] time 0.190 (0.196) data 0.000 (0.001) loss 1.4437 (1.5611) teacher_loss 0.8540 (0.8505) loss_zs_kd 0.1824 (0.1985) loss_oracle 0.4985 (0.6113) acc 75.0000 (77.5558) alaph_mean 0.3932 (0.3171) alpha_min 0.0000 (0.0000) alpha_max 0.5884 (0.5264) lr 5.1825e-04 eta 0:14:08
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,421
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.5%
******* Domain a best val acc:      87.1%, epoch: 29 *******
******* Domain a best val test acc: 82.7%, epoch: 29 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [36/50] batch [20/288] time 0.164 (0.190) data 0.000 (0.011) loss 2.0991 (1.5430) teacher_loss 1.3213 (0.8229) loss_zs_kd 0.1701 (0.1892) loss_oracle 0.6927 (0.6255) acc 65.6250 (79.8438) alaph_mean 0.2661 (0.3038) alpha_min -0.0000 (0.0000) alpha_max 0.5039 (0.5073) lr 4.6417e-04 eta 0:13:37
epoch [36/50] batch [40/288] time 0.205 (0.189) data 0.000 (0.006) loss 1.3811 (1.5656) teacher_loss 0.7939 (0.8575) loss_zs_kd 0.1700 (0.1979) loss_oracle 0.5022 (0.6092) acc 78.1250 (78.0469) alaph_mean 0.3751 (0.3131) alpha_min 0.0000 (0.0000) alpha_max 0.5097 (0.5242) lr 4.6417e-04 eta 0:13:28
epoch [36/50] batch [60/288] time 0.198 (0.191) data 0.000 (0.004) loss 1.3098 (1.5535) teacher_loss 0.6064 (0.8525) loss_zs_kd 0.2384 (0.1971) loss_oracle 0.5841 (0.6024) acc 84.3750 (78.0208) alaph_mean 0.3389 (0.3195) alpha_min 0.0000 (0.0000) alpha_max 0.5034 (0.5230) lr 4.6417e-04 eta 0:13:33
epoch [36/50] batch [80/288] time 0.172 (0.192) data 0.000 (0.003) loss 1.6532 (1.5306) teacher_loss 0.9487 (0.8364) loss_zs_kd 0.1887 (0.1966) loss_oracle 0.6101 (0.5959) acc 75.0000 (78.3203) alaph_mean 0.2920 (0.3223) alpha_min 0.0000 (0.0000) alpha_max 0.5053 (0.5248) lr 4.6417e-04 eta 0:13:32
epoch [36/50] batch [100/288] time 0.193 (0.191) data 0.000 (0.002) loss 1.7752 (1.5118) teacher_loss 1.0967 (0.8114) loss_zs_kd 0.1728 (0.1969) loss_oracle 0.5921 (0.6019) acc 71.8750 (79.0000) alaph_mean 0.2817 (0.3200) alpha_min -0.0000 (0.0000) alpha_max 0.5083 (0.5259) lr 4.6417e-04 eta 0:13:26
epoch [36/50] batch [120/288] time 0.189 (0.191) data 0.000 (0.002) loss 1.2299 (1.5205) teacher_loss 0.5083 (0.8218) loss_zs_kd 0.1712 (0.1941) loss_oracle 0.6360 (0.6016) acc 90.6250 (78.8281) alaph_mean 0.3535 (0.3212) alpha_min -0.0000 (0.0000) alpha_max 0.5092 (0.5234) lr 4.6417e-04 eta 0:13:21
epoch [36/50] batch [140/288] time 0.197 (0.191) data 0.000 (0.002) loss 1.3998 (1.5281) teacher_loss 0.8608 (0.8251) loss_zs_kd 0.1887 (0.1949) loss_oracle 0.4446 (0.6055) acc 78.1250 (78.4598) alaph_mean 0.3879 (0.3193) alpha_min 0.0000 (0.0000) alpha_max 0.5058 (0.5226) lr 4.6417e-04 eta 0:13:19
epoch [36/50] batch [160/288] time 0.198 (0.192) data 0.000 (0.002) loss 1.3933 (1.5239) teacher_loss 0.7222 (0.8193) loss_zs_kd 0.1772 (0.1970) loss_oracle 0.5825 (0.6061) acc 78.1250 (78.5156) alaph_mean 0.3441 (0.3199) alpha_min -0.0000 (0.0000) alpha_max 0.5048 (0.5208) lr 4.6417e-04 eta 0:13:17
epoch [36/50] batch [180/288] time 0.193 (0.192) data 0.000 (0.001) loss 1.9161 (1.5316) teacher_loss 1.1777 (0.8257) loss_zs_kd 0.1618 (0.1973) loss_oracle 0.6574 (0.6073) acc 75.0000 (78.3507) alaph_mean 0.2712 (0.3202) alpha_min -0.0000 (0.0000) alpha_max 0.5098 (0.5248) lr 4.6417e-04 eta 0:13:14
epoch [36/50] batch [200/288] time 0.204 (0.192) data 0.000 (0.001) loss 1.5502 (1.5426) teacher_loss 0.9619 (0.8334) loss_zs_kd 0.1860 (0.1976) loss_oracle 0.4953 (0.6104) acc 65.6250 (78.2031) alaph_mean 0.3604 (0.3184) alpha_min 0.0000 (0.0000) alpha_max 0.5408 (0.5255) lr 4.6417e-04 eta 0:13:11
epoch [36/50] batch [220/288] time 0.194 (0.192) data 0.000 (0.001) loss 1.7303 (1.5404) teacher_loss 0.9019 (0.8323) loss_zs_kd 0.2072 (0.1965) loss_oracle 0.7249 (0.6099) acc 75.0000 (78.1392) alaph_mean 0.2807 (0.3186) alpha_min 0.0000 (0.0000) alpha_max 0.5066 (0.5255) lr 4.6417e-04 eta 0:13:08
epoch [36/50] batch [240/288] time 0.192 (0.192) data 0.000 (0.001) loss 1.4505 (1.5382) teacher_loss 0.5923 (0.8284) loss_zs_kd 0.2123 (0.1968) loss_oracle 0.7521 (0.6114) acc 81.2500 (78.1641) alaph_mean 0.2434 (0.3183) alpha_min 0.0000 (0.0000) alpha_max 0.5108 (0.5256) lr 4.6417e-04 eta 0:13:04
epoch [36/50] batch [260/288] time 0.193 (0.193) data 0.000 (0.001) loss 1.3061 (1.5342) teacher_loss 0.5547 (0.8251) loss_zs_kd 0.2063 (0.1966) loss_oracle 0.6483 (0.6109) acc 84.3750 (78.1731) alaph_mean 0.3153 (0.3189) alpha_min 0.0000 (0.0000) alpha_max 0.6316 (0.5265) lr 4.6417e-04 eta 0:13:01
epoch [36/50] batch [280/288] time 0.194 (0.193) data 0.000 (0.001) loss 1.6107 (1.5399) teacher_loss 0.8193 (0.8299) loss_zs_kd 0.1703 (0.1985) loss_oracle 0.7062 (0.6108) acc 84.3750 (77.9241) alaph_mean 0.2474 (0.3198) alpha_min -0.0000 (0.0000) alpha_max 0.5060 (0.5269) lr 4.6417e-04 eta 0:12:57
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,425
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.3%
******* Domain a best val acc:      87.1%, epoch: 29 *******
******* Domain a best val test acc: 82.7%, epoch: 29 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [37/50] batch [20/288] time 0.202 (0.222) data 0.000 (0.012) loss 1.7112 (1.5923) teacher_loss 0.8906 (0.8506) loss_zs_kd 0.1753 (0.2118) loss_oracle 0.7329 (0.6358) acc 75.0000 (77.8125) alaph_mean 0.2821 (0.3153) alpha_min 0.0000 (0.0000) alpha_max 0.5101 (0.5353) lr 4.1221e-04 eta 0:14:51
epoch [37/50] batch [40/288] time 0.198 (0.208) data 0.000 (0.006) loss 1.5675 (1.6070) teacher_loss 0.7773 (0.8764) loss_zs_kd 0.2500 (0.2038) loss_oracle 0.6651 (0.6287) acc 84.3750 (77.3438) alaph_mean 0.3266 (0.3169) alpha_min 0.0000 (0.0000) alpha_max 0.9263 (0.5383) lr 4.1221e-04 eta 0:13:50
epoch [37/50] batch [60/288] time 0.180 (0.203) data 0.000 (0.004) loss 1.4980 (1.5881) teacher_loss 0.7065 (0.8676) loss_zs_kd 0.1945 (0.1981) loss_oracle 0.6942 (0.6214) acc 84.3750 (77.1354) alaph_mean 0.2718 (0.3176) alpha_min -0.0000 (0.0000) alpha_max 0.5040 (0.5289) lr 4.1221e-04 eta 0:13:27
epoch [37/50] batch [80/288] time 0.196 (0.201) data 0.000 (0.003) loss 1.1546 (1.5769) teacher_loss 0.5342 (0.8525) loss_zs_kd 0.1464 (0.2012) loss_oracle 0.5472 (0.6238) acc 87.5000 (77.5000) alaph_mean 0.3585 (0.3162) alpha_min -0.0000 (0.0000) alpha_max 0.5041 (0.5351) lr 4.1221e-04 eta 0:13:14
epoch [37/50] batch [100/288] time 0.194 (0.200) data 0.000 (0.003) loss 1.4036 (1.5601) teacher_loss 0.6699 (0.8346) loss_zs_kd 0.1879 (0.2017) loss_oracle 0.6397 (0.6247) acc 87.5000 (78.1562) alaph_mean 0.3324 (0.3164) alpha_min 0.0000 (0.0000) alpha_max 0.5076 (0.5298) lr 4.1221e-04 eta 0:13:05
epoch [37/50] batch [120/288] time 0.205 (0.199) data 0.000 (0.002) loss 1.6749 (1.5762) teacher_loss 1.1553 (0.8484) loss_zs_kd 0.1413 (0.2040) loss_oracle 0.4490 (0.6258) acc 68.7500 (77.6562) alaph_mean 0.3975 (0.3166) alpha_min 0.0000 (0.0000) alpha_max 0.5058 (0.5327) lr 4.1221e-04 eta 0:12:57
epoch [37/50] batch [140/288] time 0.189 (0.198) data 0.000 (0.002) loss 1.7206 (1.5826) teacher_loss 0.8511 (0.8517) loss_zs_kd 0.1720 (0.2036) loss_oracle 0.7835 (0.6291) acc 81.2500 (77.5670) alaph_mean 0.2435 (0.3155) alpha_min -0.0000 (0.0000) alpha_max 0.5024 (0.5292) lr 4.1221e-04 eta 0:12:51
epoch [37/50] batch [160/288] time 0.171 (0.197) data 0.000 (0.002) loss 1.5269 (1.5754) teacher_loss 0.7207 (0.8481) loss_zs_kd 0.1592 (0.2037) loss_oracle 0.7266 (0.6254) acc 78.1250 (77.6562) alaph_mean 0.2469 (0.3170) alpha_min -0.0000 (0.0000) alpha_max 0.5135 (0.5277) lr 4.1221e-04 eta 0:12:44
epoch [37/50] batch [180/288] time 0.186 (0.197) data 0.000 (0.002) loss 1.5748 (1.5662) teacher_loss 0.7944 (0.8389) loss_zs_kd 0.1914 (0.2029) loss_oracle 0.6847 (0.6259) acc 78.1250 (77.8472) alaph_mean 0.3091 (0.3167) alpha_min 0.0000 (0.0000) alpha_max 0.5141 (0.5268) lr 4.1221e-04 eta 0:12:39
epoch [37/50] batch [200/288] time 0.195 (0.197) data 0.000 (0.001) loss 1.3016 (1.5708) teacher_loss 0.6777 (0.8416) loss_zs_kd 0.1987 (0.2027) loss_oracle 0.5245 (0.6278) acc 78.1250 (77.7969) alaph_mean 0.3674 (0.3154) alpha_min 0.0000 (0.0000) alpha_max 0.5066 (0.5250) lr 4.1221e-04 eta 0:12:34
epoch [37/50] batch [220/288] time 0.194 (0.197) data 0.000 (0.001) loss 1.8162 (1.5708) teacher_loss 1.1016 (0.8449) loss_zs_kd 0.2685 (0.2023) loss_oracle 0.5804 (0.6248) acc 68.7500 (77.6847) alaph_mean 0.3282 (0.3162) alpha_min 0.0000 (0.0000) alpha_max 0.5085 (0.5245) lr 4.1221e-04 eta 0:12:29
epoch [37/50] batch [240/288] time 0.182 (0.197) data 0.000 (0.001) loss 1.2931 (1.5714) teacher_loss 0.6992 (0.8465) loss_zs_kd 0.1913 (0.2015) loss_oracle 0.4982 (0.6241) acc 78.1250 (77.5130) alaph_mean 0.3975 (0.3161) alpha_min 0.0000 (0.0000) alpha_max 0.5115 (0.5254) lr 4.1221e-04 eta 0:12:25
epoch [37/50] batch [260/288] time 0.196 (0.196) data 0.000 (0.001) loss 1.7731 (1.5752) teacher_loss 1.0078 (0.8499) loss_zs_kd 0.2188 (0.2015) loss_oracle 0.6559 (0.6246) acc 81.2500 (77.4159) alaph_mean 0.3098 (0.3162) alpha_min -0.0000 (0.0000) alpha_max 0.5065 (0.5249) lr 4.1221e-04 eta 0:12:21
epoch [37/50] batch [280/288] time 0.194 (0.196) data 0.000 (0.001) loss 1.3401 (1.5722) teacher_loss 0.6113 (0.8471) loss_zs_kd 0.2247 (0.2018) loss_oracle 0.6165 (0.6241) acc 87.5000 (77.5223) alaph_mean 0.2945 (0.3155) alpha_min 0.0000 (0.0000) alpha_max 0.5086 (0.5276) lr 4.1221e-04 eta 0:12:16
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,422
* accuracy: 86.9%
* error: 13.1%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.3%
******* Domain a best val acc:      87.1%, epoch: 29 *******
******* Domain a best val test acc: 82.7%, epoch: 29 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [38/50] batch [20/288] time 0.187 (0.233) data 0.000 (0.016) loss 1.5401 (1.6405) teacher_loss 0.9370 (0.9117) loss_zs_kd 0.1548 (0.2018) loss_oracle 0.5257 (0.6279) acc 68.7500 (75.9375) alaph_mean 0.3477 (0.3092) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5319) lr 3.6258e-04 eta 0:14:26
epoch [38/50] batch [40/288] time 0.193 (0.212) data 0.000 (0.008) loss 1.1146 (1.5718) teacher_loss 0.3047 (0.8349) loss_zs_kd 0.3064 (0.2010) loss_oracle 0.6568 (0.6365) acc 87.5000 (77.5781) alaph_mean 0.2968 (0.3059) alpha_min -0.0000 (0.0000) alpha_max 0.5066 (0.5220) lr 3.6258e-04 eta 0:13:04
epoch [38/50] batch [60/288] time 0.198 (0.206) data 0.000 (0.005) loss 1.2737 (1.5632) teacher_loss 0.6113 (0.8342) loss_zs_kd 0.2030 (0.2004) loss_oracle 0.5609 (0.6289) acc 81.2500 (77.1875) alaph_mean 0.3421 (0.3071) alpha_min 0.0000 (0.0000) alpha_max 0.5080 (0.5218) lr 3.6258e-04 eta 0:12:37
epoch [38/50] batch [80/288] time 0.195 (0.203) data 0.000 (0.004) loss 1.7803 (1.5732) teacher_loss 0.9937 (0.8458) loss_zs_kd 0.1594 (0.2000) loss_oracle 0.7070 (0.6273) acc 81.2500 (77.2656) alaph_mean 0.2817 (0.3075) alpha_min 0.0000 (0.0000) alpha_max 0.5052 (0.5187) lr 3.6258e-04 eta 0:12:23
epoch [38/50] batch [100/288] time 0.188 (0.201) data 0.000 (0.003) loss 1.6856 (1.5886) teacher_loss 0.9043 (0.8658) loss_zs_kd 0.1653 (0.1997) loss_oracle 0.6987 (0.6229) acc 78.1250 (77.0312) alaph_mean 0.2579 (0.3094) alpha_min 0.0000 (0.0000) alpha_max 0.5126 (0.5204) lr 3.6258e-04 eta 0:12:13
epoch [38/50] batch [120/288] time 0.198 (0.200) data 0.000 (0.003) loss 1.8620 (1.5928) teacher_loss 1.2002 (0.8687) loss_zs_kd 0.2036 (0.1981) loss_oracle 0.5600 (0.6250) acc 68.7500 (76.6667) alaph_mean 0.3783 (0.3081) alpha_min 0.0000 (0.0000) alpha_max 0.5088 (0.5205) lr 3.6258e-04 eta 0:12:05
epoch [38/50] batch [140/288] time 0.192 (0.199) data 0.000 (0.002) loss 1.5156 (1.5985) teacher_loss 0.7764 (0.8739) loss_zs_kd 0.1889 (0.2006) loss_oracle 0.6447 (0.6243) acc 81.2500 (76.5625) alaph_mean 0.2820 (0.3095) alpha_min 0.0000 (0.0000) alpha_max 0.5073 (0.5232) lr 3.6258e-04 eta 0:11:57
epoch [38/50] batch [160/288] time 0.185 (0.198) data 0.000 (0.002) loss 1.3046 (1.6077) teacher_loss 0.6309 (0.8846) loss_zs_kd 0.1253 (0.2023) loss_oracle 0.6111 (0.6220) acc 84.3750 (76.1914) alaph_mean 0.3218 (0.3120) alpha_min 0.0000 (0.0000) alpha_max 0.5073 (0.5232) lr 3.6258e-04 eta 0:11:51
epoch [38/50] batch [180/288] time 0.181 (0.198) data 0.000 (0.002) loss 1.8736 (1.6162) teacher_loss 0.9814 (0.8919) loss_zs_kd 0.2650 (0.2052) loss_oracle 0.7596 (0.6217) acc 75.0000 (76.0243) alaph_mean 0.2532 (0.3129) alpha_min -0.0000 (0.0000) alpha_max 0.5083 (0.5221) lr 3.6258e-04 eta 0:11:45
epoch [38/50] batch [200/288] time 0.191 (0.197) data 0.000 (0.002) loss 1.2365 (1.6021) teacher_loss 0.4773 (0.8818) loss_zs_kd 0.1502 (0.2038) loss_oracle 0.6841 (0.6184) acc 84.3750 (76.2500) alaph_mean 0.2939 (0.3141) alpha_min -0.0000 (0.0000) alpha_max 0.5100 (0.5224) lr 3.6258e-04 eta 0:11:38
epoch [38/50] batch [220/288] time 0.190 (0.197) data 0.000 (0.002) loss 1.3272 (1.5962) teacher_loss 0.4946 (0.8778) loss_zs_kd 0.1963 (0.2030) loss_oracle 0.7344 (0.6168) acc 81.2500 (76.3494) alaph_mean 0.2635 (0.3150) alpha_min -0.0000 (0.0000) alpha_max 0.5146 (0.5220) lr 3.6258e-04 eta 0:11:33
epoch [38/50] batch [240/288] time 0.186 (0.196) data 0.000 (0.002) loss 1.2274 (1.5944) teacher_loss 0.4199 (0.8777) loss_zs_kd 0.1795 (0.2030) loss_oracle 0.7177 (0.6152) acc 90.6250 (76.4974) alaph_mean 0.2505 (0.3156) alpha_min -0.0000 (0.0000) alpha_max 0.5052 (0.5245) lr 3.6258e-04 eta 0:11:28
epoch [38/50] batch [260/288] time 0.193 (0.196) data 0.000 (0.001) loss 1.5081 (1.5944) teacher_loss 0.7744 (0.8755) loss_zs_kd 0.1453 (0.2020) loss_oracle 0.6610 (0.6179) acc 84.3750 (76.6346) alaph_mean 0.2973 (0.3139) alpha_min 0.0000 (0.0000) alpha_max 0.5061 (0.5236) lr 3.6258e-04 eta 0:11:22
epoch [38/50] batch [280/288] time 0.200 (0.195) data 0.000 (0.001) loss 1.3578 (1.5885) teacher_loss 0.5757 (0.8697) loss_zs_kd 0.1547 (0.2008) loss_oracle 0.7048 (0.6184) acc 87.5000 (76.7188) alaph_mean 0.2951 (0.3140) alpha_min 0.0000 (0.0000) alpha_max 0.5083 (0.5239) lr 3.6258e-04 eta 0:11:17
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,426
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 80.1%
******* Domain a best val acc:      87.1%, epoch: 29 *******
******* Domain a best val test acc: 82.7%, epoch: 29 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [39/50] batch [20/288] time 0.188 (0.260) data 0.000 (0.013) loss 1.5261 (1.5112) teacher_loss 0.8110 (0.8000) loss_zs_kd 0.1675 (0.1915) loss_oracle 0.6313 (0.6155) acc 81.2500 (76.7188) alaph_mean 0.3031 (0.3190) alpha_min -0.0000 (0.0000) alpha_max 0.5029 (0.5179) lr 3.1545e-04 eta 0:14:53
epoch [39/50] batch [40/288] time 0.195 (0.225) data 0.000 (0.007) loss 1.7320 (1.5432) teacher_loss 1.0186 (0.8339) loss_zs_kd 0.2276 (0.2004) loss_oracle 0.5996 (0.6091) acc 68.7500 (77.0312) alaph_mean 0.3176 (0.3250) alpha_min 0.0000 (0.0000) alpha_max 0.6694 (0.5286) lr 3.1545e-04 eta 0:12:47
epoch [39/50] batch [60/288] time 0.193 (0.215) data 0.000 (0.005) loss 1.3740 (1.5388) teacher_loss 0.6636 (0.8343) loss_zs_kd 0.1399 (0.2003) loss_oracle 0.6404 (0.6043) acc 78.1250 (76.6146) alaph_mean 0.3128 (0.3266) alpha_min 0.0000 (0.0000) alpha_max 0.5078 (0.5255) lr 3.1545e-04 eta 0:12:08
epoch [39/50] batch [80/288] time 0.198 (0.211) data 0.000 (0.004) loss 1.7103 (1.5445) teacher_loss 1.0410 (0.8377) loss_zs_kd 0.1831 (0.2006) loss_oracle 0.5777 (0.6065) acc 71.8750 (76.7578) alaph_mean 0.3597 (0.3235) alpha_min 0.0000 (0.0000) alpha_max 0.5080 (0.5244) lr 3.1545e-04 eta 0:11:50
epoch [39/50] batch [100/288] time 0.195 (0.208) data 0.000 (0.003) loss 1.3794 (1.5514) teacher_loss 0.8145 (0.8460) loss_zs_kd 0.2135 (0.1997) loss_oracle 0.4582 (0.6056) acc 78.1250 (76.5312) alaph_mean 0.4064 (0.3229) alpha_min 0.0000 (0.0000) alpha_max 0.5076 (0.5226) lr 3.1545e-04 eta 0:11:38
epoch [39/50] batch [120/288] time 0.195 (0.205) data 0.000 (0.002) loss 1.2724 (1.5538) teacher_loss 0.5410 (0.8462) loss_zs_kd 0.1511 (0.2003) loss_oracle 0.6558 (0.6074) acc 81.2500 (76.6406) alaph_mean 0.3115 (0.3227) alpha_min 0.0000 (0.0000) alpha_max 0.5095 (0.5241) lr 3.1545e-04 eta 0:11:25
epoch [39/50] batch [140/288] time 0.186 (0.204) data 0.000 (0.002) loss 1.6586 (1.5527) teacher_loss 0.9121 (0.8457) loss_zs_kd 0.2374 (0.2035) loss_oracle 0.6278 (0.6052) acc 71.8750 (76.7411) alaph_mean 0.3282 (0.3252) alpha_min 0.0000 (0.0000) alpha_max 0.5080 (0.5252) lr 3.1545e-04 eta 0:11:14
epoch [39/50] batch [160/288] time 0.175 (0.203) data 0.000 (0.002) loss 1.7856 (1.5460) teacher_loss 0.9541 (0.8433) loss_zs_kd 0.2151 (0.2034) loss_oracle 0.7240 (0.6010) acc 75.0000 (76.9141) alaph_mean 0.2854 (0.3280) alpha_min 0.0000 (0.0000) alpha_max 0.5077 (0.5288) lr 3.1545e-04 eta 0:11:07
epoch [39/50] batch [180/288] time 0.196 (0.201) data 0.000 (0.002) loss 1.1875 (1.5448) teacher_loss 0.5469 (0.8382) loss_zs_kd 0.2193 (0.2021) loss_oracle 0.5309 (0.6055) acc 81.2500 (76.9618) alaph_mean 0.3592 (0.3269) alpha_min 0.0000 (0.0000) alpha_max 0.5086 (0.5278) lr 3.1545e-04 eta 0:10:59
epoch [39/50] batch [200/288] time 0.196 (0.200) data 0.000 (0.002) loss 1.6451 (1.5630) teacher_loss 0.7661 (0.8511) loss_zs_kd 0.2171 (0.2027) loss_oracle 0.7704 (0.6106) acc 78.1250 (76.7656) alaph_mean 0.2309 (0.3237) alpha_min -0.0000 (0.0000) alpha_max 0.5067 (0.5298) lr 3.1545e-04 eta 0:10:52
epoch [39/50] batch [220/288] time 0.191 (0.200) data 0.000 (0.001) loss 1.8662 (1.5584) teacher_loss 1.0791 (0.8489) loss_zs_kd 0.2270 (0.2021) loss_oracle 0.6736 (0.6085) acc 78.1250 (76.8892) alaph_mean 0.3310 (0.3244) alpha_min -0.0000 (0.0000) alpha_max 0.5568 (0.5294) lr 3.1545e-04 eta 0:10:46
epoch [39/50] batch [240/288] time 0.202 (0.199) data 0.000 (0.001) loss 1.6568 (1.5637) teacher_loss 0.7769 (0.8526) loss_zs_kd 0.2288 (0.2040) loss_oracle 0.7656 (0.6092) acc 71.8750 (76.7188) alaph_mean 0.2272 (0.3243) alpha_min -0.0000 (0.0000) alpha_max 0.5052 (0.5286) lr 3.1545e-04 eta 0:10:41
epoch [39/50] batch [260/288] time 0.194 (0.199) data 0.000 (0.001) loss 1.1534 (1.5625) teacher_loss 0.4429 (0.8507) loss_zs_kd 0.1364 (0.2026) loss_oracle 0.6423 (0.6105) acc 90.6250 (76.7548) alaph_mean 0.3091 (0.3245) alpha_min 0.0000 (0.0000) alpha_max 0.5041 (0.5290) lr 3.1545e-04 eta 0:10:35
epoch [39/50] batch [280/288] time 0.182 (0.199) data 0.000 (0.001) loss 1.4871 (1.5608) teacher_loss 0.8105 (0.8498) loss_zs_kd 0.1631 (0.2023) loss_oracle 0.5951 (0.6099) acc 78.1250 (76.7188) alaph_mean 0.3191 (0.3250) alpha_min -0.0000 (0.0000) alpha_max 0.5049 (0.5298) lr 3.1545e-04 eta 0:10:30
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,424
* accuracy: 86.9%
* error: 13.1%
* macro_f1: 86.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,015
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 80.0%
******* Domain a best val acc:      87.1%, epoch: 29 *******
******* Domain a best val test acc: 82.7%, epoch: 29 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [40/50] batch [20/288] time 0.096 (0.251) data 0.000 (0.014) loss 1.6639 (1.5881) teacher_loss 0.8193 (0.8627) loss_zs_kd 0.1960 (0.1895) loss_oracle 0.7466 (0.6306) acc 75.0000 (76.2500) alaph_mean 0.2528 (0.3039) alpha_min 0.0000 (0.0000) alpha_max 0.5089 (0.5394) lr 2.7103e-04 eta 0:13:09
epoch [40/50] batch [40/288] time 0.189 (0.220) data 0.000 (0.007) loss 2.0273 (1.6099) teacher_loss 1.2197 (0.8799) loss_zs_kd 0.2565 (0.1977) loss_oracle 0.6793 (0.6312) acc 71.8750 (76.6406) alaph_mean 0.2582 (0.3091) alpha_min -0.0000 (0.0000) alpha_max 0.5057 (0.5475) lr 2.7103e-04 eta 0:11:27
epoch [40/50] batch [60/288] time 0.191 (0.211) data 0.000 (0.005) loss 1.6426 (1.5727) teacher_loss 0.7515 (0.8548) loss_zs_kd 0.2024 (0.1959) loss_oracle 0.7899 (0.6200) acc 81.2500 (77.1354) alaph_mean 0.1996 (0.3119) alpha_min 0.0000 (0.0000) alpha_max 0.5064 (0.5408) lr 2.7103e-04 eta 0:10:55
epoch [40/50] batch [80/288] time 0.194 (0.207) data 0.000 (0.004) loss 1.9764 (1.5799) teacher_loss 1.1973 (0.8591) loss_zs_kd 0.2073 (0.1986) loss_oracle 0.6755 (0.6215) acc 68.7500 (76.9922) alaph_mean 0.2975 (0.3127) alpha_min 0.0000 (0.0000) alpha_max 0.5150 (0.5375) lr 2.7103e-04 eta 0:10:39
epoch [40/50] batch [100/288] time 0.195 (0.204) data 0.000 (0.003) loss 1.4374 (1.5854) teacher_loss 0.7568 (0.8626) loss_zs_kd 0.1696 (0.1981) loss_oracle 0.5958 (0.6238) acc 81.2500 (76.8438) alaph_mean 0.3389 (0.3134) alpha_min 0.0000 (0.0000) alpha_max 0.6147 (0.5354) lr 2.7103e-04 eta 0:10:26
epoch [40/50] batch [120/288] time 0.195 (0.203) data 0.000 (0.002) loss 1.5625 (1.5741) teacher_loss 0.8867 (0.8606) loss_zs_kd 0.2386 (0.1985) loss_oracle 0.5565 (0.6143) acc 71.8750 (76.9010) alaph_mean 0.3418 (0.3172) alpha_min 0.0000 (0.0000) alpha_max 0.5071 (0.5323) lr 2.7103e-04 eta 0:10:17
epoch [40/50] batch [140/288] time 0.193 (0.201) data 0.000 (0.002) loss 1.7305 (1.5770) teacher_loss 1.0283 (0.8625) loss_zs_kd 0.1615 (0.1977) loss_oracle 0.6214 (0.6156) acc 65.6250 (76.8304) alaph_mean 0.2783 (0.3162) alpha_min 0.0000 (0.0000) alpha_max 0.5057 (0.5347) lr 2.7103e-04 eta 0:10:09
epoch [40/50] batch [160/288] time 0.201 (0.200) data 0.000 (0.002) loss 1.8133 (1.5704) teacher_loss 1.1641 (0.8640) loss_zs_kd 0.2259 (0.1980) loss_oracle 0.5363 (0.6074) acc 78.1250 (76.9727) alaph_mean 0.3514 (0.3196) alpha_min 0.0000 (0.0000) alpha_max 0.5053 (0.5344) lr 2.7103e-04 eta 0:10:02
epoch [40/50] batch [180/288] time 0.193 (0.200) data 0.000 (0.002) loss 1.4310 (1.5811) teacher_loss 0.7393 (0.8741) loss_zs_kd 0.1679 (0.1987) loss_oracle 0.6078 (0.6076) acc 81.2500 (76.9618) alaph_mean 0.3289 (0.3194) alpha_min 0.0000 (0.0000) alpha_max 0.5094 (0.5347) lr 2.7103e-04 eta 0:09:56
epoch [40/50] batch [200/288] time 0.190 (0.199) data 0.000 (0.002) loss 1.5524 (1.5728) teacher_loss 0.7754 (0.8654) loss_zs_kd 0.2361 (0.1985) loss_oracle 0.6590 (0.6081) acc 68.7500 (76.9844) alaph_mean 0.2974 (0.3199) alpha_min 0.0000 (0.0000) alpha_max 0.5109 (0.5355) lr 2.7103e-04 eta 0:09:51
epoch [40/50] batch [220/288] time 0.195 (0.199) data 0.000 (0.001) loss 1.2501 (1.5703) teacher_loss 0.6274 (0.8640) loss_zs_kd 0.0966 (0.1989) loss_oracle 0.5744 (0.6068) acc 87.5000 (77.1591) alaph_mean 0.3387 (0.3212) alpha_min 0.0000 (0.0000) alpha_max 0.5098 (0.5355) lr 2.7103e-04 eta 0:09:46
epoch [40/50] batch [240/288] time 0.192 (0.198) data 0.000 (0.001) loss 1.1569 (1.5580) teacher_loss 0.4929 (0.8536) loss_zs_kd 0.1980 (0.1980) loss_oracle 0.5649 (0.6054) acc 81.2500 (77.4219) alaph_mean 0.2971 (0.3222) alpha_min 0.0000 (0.0000) alpha_max 0.5055 (0.5342) lr 2.7103e-04 eta 0:09:40
epoch [40/50] batch [260/288] time 0.195 (0.198) data 0.000 (0.001) loss 1.7626 (1.5547) teacher_loss 0.9976 (0.8488) loss_zs_kd 0.2356 (0.1994) loss_oracle 0.6472 (0.6062) acc 71.8750 (77.5000) alaph_mean 0.2937 (0.3217) alpha_min 0.0000 (0.0000) alpha_max 0.5079 (0.5327) lr 2.7103e-04 eta 0:09:36
epoch [40/50] batch [280/288] time 0.197 (0.198) data 0.000 (0.001) loss 1.7758 (1.5556) teacher_loss 1.0156 (0.8469) loss_zs_kd 0.2368 (0.1986) loss_oracle 0.6418 (0.6094) acc 75.0000 (77.5781) alaph_mean 0.2905 (0.3201) alpha_min 0.0000 (0.0000) alpha_max 0.5136 (0.5331) lr 2.7103e-04 eta 0:09:30
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,425
* accuracy: 87.0%
* error: 13.0%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 80.1%
******* Domain a best val acc:      87.1%, epoch: 29 *******
******* Domain a best val test acc: 82.7%, epoch: 29 *******
******* Domain a best test acc:     83.5%, epoch: 23 *******
epoch [41/50] batch [20/288] time 0.169 (0.296) data 0.000 (0.012) loss 1.3180 (1.5825) teacher_loss 0.7334 (0.8429) loss_zs_kd 0.1959 (0.2112) loss_oracle 0.4866 (0.6340) acc 78.1250 (77.6562) alaph_mean 0.3580 (0.3110) alpha_min -0.0000 (0.0000) alpha_max 0.5482 (0.5410) lr 2.2949e-04 eta 0:14:07
