Loading trainer: TRIP
Loading dataset: SPG_OfficeHome
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------------------------------
Dataset    SPG_OfficeHome
Source     ['clipart', 'product', 'real_world']
Target     ['art']
# classes  65
# train_x  9,222
# val      3,939
# test     2,427
---------  ------------------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,195
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/288] time 0.194 (0.246) data 0.000 (0.022) loss 3.3908 (3.7790) teacher_loss 3.3202 (3.7062) loss_zs_kd 0.0003 (0.0001) loss_oracle 0.0706 (0.0728) acc 71.8750 (68.4375) lr 1.0000e-05 eta 0:58:51
epoch [1/50] batch [40/288] time 0.197 (0.220) data 0.000 (0.011) loss 3.6922 (3.8335) teacher_loss 3.6106 (3.7626) loss_zs_kd 0.0016 (0.0004) loss_oracle 0.0816 (0.0709) acc 65.6250 (68.2031) lr 1.0000e-05 eta 0:52:41
epoch [1/50] batch [60/288] time 0.191 (0.211) data 0.000 (0.007) loss 4.1885 (3.8436) teacher_loss 4.1342 (3.7697) loss_zs_kd 0.0072 (0.0010) loss_oracle 0.0543 (0.0739) acc 62.5000 (67.7604) lr 1.0000e-05 eta 0:50:30
epoch [1/50] batch [80/288] time 0.183 (0.207) data 0.000 (0.006) loss 3.7780 (3.8458) teacher_loss 3.7089 (3.7683) loss_zs_kd 0.0097 (0.0019) loss_oracle 0.0691 (0.0775) acc 59.3750 (67.6953) lr 1.0000e-05 eta 0:49:26
epoch [1/50] batch [100/288] time 0.177 (0.204) data 0.000 (0.005) loss 3.3020 (3.7548) teacher_loss 3.2215 (3.6782) loss_zs_kd 0.0065 (0.0030) loss_oracle 0.0804 (0.0766) acc 62.5000 (68.2812) lr 1.0000e-05 eta 0:48:42
epoch [1/50] batch [120/288] time 0.195 (0.203) data 0.001 (0.004) loss 3.0586 (3.7311) teacher_loss 2.9750 (3.6537) loss_zs_kd 0.0065 (0.0042) loss_oracle 0.0836 (0.0774) acc 75.0000 (68.5677) lr 1.0000e-05 eta 0:48:16
epoch [1/50] batch [140/288] time 0.193 (0.202) data 0.000 (0.003) loss 4.3645 (3.6813) teacher_loss 4.2763 (3.6029) loss_zs_kd 0.0121 (0.0052) loss_oracle 0.0881 (0.0784) acc 56.2500 (68.7054) lr 1.0000e-05 eta 0:47:55
epoch [1/50] batch [160/288] time 0.199 (0.201) data 0.000 (0.003) loss 3.2308 (3.6677) teacher_loss 3.1998 (3.5900) loss_zs_kd 0.0134 (0.0064) loss_oracle 0.0311 (0.0777) acc 68.7500 (68.9648) lr 1.0000e-05 eta 0:47:39
epoch [1/50] batch [180/288] time 0.205 (0.200) data 0.000 (0.003) loss 2.9498 (3.6725) teacher_loss 2.8876 (3.5953) loss_zs_kd 0.0233 (0.0077) loss_oracle 0.0623 (0.0772) acc 68.7500 (68.8542) lr 1.0000e-05 eta 0:47:29
epoch [1/50] batch [200/288] time 0.466 (0.207) data 0.000 (0.002) loss 4.1476 (3.6617) teacher_loss 4.0542 (3.5847) loss_zs_kd 0.0276 (0.0091) loss_oracle 0.0934 (0.0770) acc 68.7500 (68.8906) lr 1.0000e-05 eta 0:49:02
epoch [1/50] batch [220/288] time 0.197 (0.208) data 0.000 (0.002) loss 4.0918 (3.6404) teacher_loss 4.0774 (3.5639) loss_zs_kd 0.0250 (0.0108) loss_oracle 0.0144 (0.0764) acc 68.7500 (69.2188) lr 1.0000e-05 eta 0:49:13
epoch [1/50] batch [240/288] time 0.188 (0.207) data 0.000 (0.002) loss 2.8926 (3.6275) teacher_loss 2.8240 (3.5515) loss_zs_kd 0.0165 (0.0124) loss_oracle 0.0686 (0.0760) acc 81.2500 (69.4792) lr 1.0000e-05 eta 0:48:51
epoch [1/50] batch [260/288] time 0.186 (0.206) data 0.000 (0.002) loss 3.6152 (3.6331) teacher_loss 3.5092 (3.5571) loss_zs_kd 0.0516 (0.0143) loss_oracle 0.1060 (0.0760) acc 71.8750 (69.4591) lr 1.0000e-05 eta 0:48:33
epoch [1/50] batch [280/288] time 0.198 (0.205) data 0.000 (0.002) loss 4.4438 (3.6290) teacher_loss 4.3984 (3.5530) loss_zs_kd 0.0541 (0.0164) loss_oracle 0.0454 (0.0759) acc 65.6250 (69.5982) lr 1.0000e-05 eta 0:48:16
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,335
* accuracy: 84.7%
* error: 15.3%
* macro_f1: 83.8%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,004
* accuracy: 82.6%
* error: 17.4%
* macro_f1: 79.0%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      84.7%, epoch: 1 *******
******* Domain a best val test acc: 82.6%, epoch: 1 *******
******* Domain a best test acc:     82.6%, epoch: 1 *******
epoch [2/50] batch [20/288] time 0.195 (0.212) data 0.000 (0.015) loss 3.4145 (3.6282) teacher_loss 3.3501 (3.5538) loss_zs_kd 0.2341 (0.1918) loss_oracle 0.0645 (0.0744) acc 65.6250 (67.9688) lr 2.0000e-03 eta 0:49:53
epoch [2/50] batch [40/288] time 0.229 (0.207) data 0.000 (0.008) loss 2.0709 (3.4190) teacher_loss 1.9990 (3.3460) loss_zs_kd 0.1701 (0.2243) loss_oracle 0.0718 (0.0730) acc 84.3750 (70.8594) lr 2.0000e-03 eta 0:48:38
epoch [2/50] batch [60/288] time 0.219 (0.205) data 0.000 (0.005) loss 4.6408 (3.4202) teacher_loss 4.5598 (3.3462) loss_zs_kd 0.3136 (0.2278) loss_oracle 0.0810 (0.0740) acc 65.6250 (70.9375) lr 2.0000e-03 eta 0:47:55
epoch [2/50] batch [80/288] time 0.193 (0.202) data 0.000 (0.004) loss 2.4101 (3.3665) teacher_loss 2.3251 (3.2908) loss_zs_kd 0.2570 (0.2345) loss_oracle 0.0850 (0.0758) acc 71.8750 (71.4453) lr 2.0000e-03 eta 0:47:08
epoch [2/50] batch [100/288] time 0.230 (0.200) data 0.000 (0.003) loss 3.9210 (3.3362) teacher_loss 3.8514 (3.2609) loss_zs_kd 0.2688 (0.2422) loss_oracle 0.0696 (0.0754) acc 71.8750 (71.7188) lr 2.0000e-03 eta 0:46:47
epoch [2/50] batch [120/288] time 0.190 (0.200) data 0.000 (0.003) loss 3.2472 (3.3098) teacher_loss 3.1588 (3.2347) loss_zs_kd 0.1729 (0.2452) loss_oracle 0.0884 (0.0751) acc 71.8750 (71.8750) lr 2.0000e-03 eta 0:46:34
epoch [2/50] batch [140/288] time 0.192 (0.199) data 0.000 (0.002) loss 3.5554 (3.3087) teacher_loss 3.5365 (3.2347) loss_zs_kd 0.2931 (0.2441) loss_oracle 0.0188 (0.0739) acc 71.8750 (71.8527) lr 2.0000e-03 eta 0:46:18
epoch [2/50] batch [160/288] time 0.196 (0.198) data 0.000 (0.002) loss 3.3174 (3.3171) teacher_loss 3.2500 (3.2427) loss_zs_kd 0.4138 (0.2463) loss_oracle 0.0674 (0.0744) acc 71.8750 (71.7383) lr 2.0000e-03 eta 0:46:00
epoch [2/50] batch [180/288] time 0.096 (0.203) data 0.000 (0.002) loss 2.8689 (3.3042) teacher_loss 2.7777 (3.2298) loss_zs_kd 0.2808 (0.2505) loss_oracle 0.0912 (0.0744) acc 75.0000 (71.6840) lr 2.0000e-03 eta 0:47:04
epoch [2/50] batch [200/288] time 0.191 (0.202) data 0.000 (0.002) loss 5.2978 (3.3131) teacher_loss 5.2017 (3.2390) loss_zs_kd 0.3515 (0.2535) loss_oracle 0.0961 (0.0741) acc 53.1250 (71.5000) lr 2.0000e-03 eta 0:46:44
epoch [2/50] batch [220/288] time 0.222 (0.201) data 0.000 (0.002) loss 3.2745 (3.2798) teacher_loss 3.2121 (3.2059) loss_zs_kd 0.2833 (0.2544) loss_oracle 0.0623 (0.0739) acc 75.0000 (71.6477) lr 2.0000e-03 eta 0:46:35
epoch [2/50] batch [240/288] time 0.202 (0.201) data 0.000 (0.001) loss 3.8678 (3.2661) teacher_loss 3.8152 (3.1923) loss_zs_kd 0.2854 (0.2577) loss_oracle 0.0526 (0.0737) acc 62.5000 (71.6536) lr 2.0000e-03 eta 0:46:22
epoch [2/50] batch [260/288] time 0.189 (0.200) data 0.000 (0.001) loss 3.2272 (3.2520) teacher_loss 3.1485 (3.1780) loss_zs_kd 0.3552 (0.2594) loss_oracle 0.0787 (0.0741) acc 75.0000 (71.6466) lr 2.0000e-03 eta 0:46:10
epoch [2/50] batch [280/288] time 0.182 (0.200) data 0.000 (0.001) loss 2.5603 (3.2406) teacher_loss 2.4527 (3.1664) loss_zs_kd 0.2912 (0.2586) loss_oracle 0.1076 (0.0742) acc 65.6250 (71.6964) lr 2.0000e-03 eta 0:46:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,379
* accuracy: 85.8%
* error: 14.2%
* macro_f1: 85.1%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,999
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.2%
******* Domain a best val acc:      85.8%, epoch: 2 *******
******* Domain a best val test acc: 82.4%, epoch: 2 *******
******* Domain a best test acc:     82.6%, epoch: 1 *******
epoch [3/50] batch [20/288] time 0.200 (0.204) data 0.000 (0.013) loss 4.2626 (3.1615) teacher_loss 4.2387 (3.0881) loss_zs_kd 0.3720 (0.2855) loss_oracle 0.0239 (0.0734) acc 56.2500 (72.1875) lr 1.9980e-03 eta 0:46:55
epoch [3/50] batch [40/288] time 0.194 (0.199) data 0.000 (0.006) loss 2.4994 (2.9798) teacher_loss 2.3988 (2.9075) loss_zs_kd 0.2130 (0.2758) loss_oracle 0.1007 (0.0723) acc 87.5000 (73.2812) lr 1.9980e-03 eta 0:45:38
epoch [3/50] batch [60/288] time 0.198 (0.197) data 0.000 (0.004) loss 3.2752 (3.1570) teacher_loss 3.2145 (3.0821) loss_zs_kd 0.1786 (0.2742) loss_oracle 0.0607 (0.0749) acc 75.0000 (72.1875) lr 1.9980e-03 eta 0:45:10
epoch [3/50] batch [80/288] time 0.168 (0.196) data 0.000 (0.003) loss 3.0193 (3.1568) teacher_loss 2.9473 (3.0818) loss_zs_kd 0.2339 (0.2678) loss_oracle 0.0721 (0.0751) acc 75.0000 (72.2266) lr 1.9980e-03 eta 0:44:48
epoch [3/50] batch [100/288] time 0.191 (0.195) data 0.000 (0.003) loss 2.7862 (3.1782) teacher_loss 2.7005 (3.1018) loss_zs_kd 0.2230 (0.2673) loss_oracle 0.0857 (0.0764) acc 71.8750 (72.2812) lr 1.9980e-03 eta 0:44:38
epoch [3/50] batch [120/288] time 0.193 (0.195) data 0.000 (0.002) loss 2.3817 (3.1368) teacher_loss 2.3053 (3.0612) loss_zs_kd 0.2978 (0.2737) loss_oracle 0.0765 (0.0756) acc 75.0000 (72.5521) lr 1.9980e-03 eta 0:44:31
epoch [3/50] batch [140/288] time 0.189 (0.195) data 0.000 (0.002) loss 3.1129 (3.1279) teacher_loss 3.0412 (3.0525) loss_zs_kd 0.1970 (0.2743) loss_oracle 0.0717 (0.0754) acc 78.1250 (72.5223) lr 1.9980e-03 eta 0:44:25
epoch [3/50] batch [160/288] time 0.192 (0.195) data 0.000 (0.002) loss 4.9742 (3.1374) teacher_loss 4.9018 (3.0619) loss_zs_kd 0.3132 (0.2785) loss_oracle 0.0724 (0.0755) acc 59.3750 (72.4219) lr 1.9980e-03 eta 0:44:20
epoch [3/50] batch [180/288] time 0.084 (0.203) data 0.000 (0.002) loss 3.3072 (3.1317) teacher_loss 3.2420 (3.0563) loss_zs_kd 0.2502 (0.2794) loss_oracle 0.0653 (0.0754) acc 68.7500 (72.6389) lr 1.9980e-03 eta 0:46:10
epoch [3/50] batch [200/288] time 0.194 (0.203) data 0.000 (0.001) loss 3.0107 (3.1177) teacher_loss 2.9464 (3.0421) loss_zs_kd 0.2258 (0.2796) loss_oracle 0.0643 (0.0756) acc 84.3750 (72.8906) lr 1.9980e-03 eta 0:46:07
epoch [3/50] batch [220/288] time 0.189 (0.202) data 0.000 (0.001) loss 2.5316 (3.1462) teacher_loss 2.4426 (3.0707) loss_zs_kd 0.2705 (0.2816) loss_oracle 0.0890 (0.0755) acc 71.8750 (72.8409) lr 1.9980e-03 eta 0:45:51
epoch [3/50] batch [240/288] time 0.191 (0.202) data 0.000 (0.001) loss 4.4556 (3.1634) teacher_loss 4.3694 (3.0879) loss_zs_kd 0.2423 (0.2823) loss_oracle 0.0862 (0.0755) acc 59.3750 (72.7214) lr 1.9980e-03 eta 0:45:38
epoch [3/50] batch [260/288] time 0.198 (0.201) data 0.000 (0.001) loss 1.7607 (3.1486) teacher_loss 1.7117 (3.0732) loss_zs_kd 0.2842 (0.2817) loss_oracle 0.0490 (0.0754) acc 84.3750 (72.8005) lr 1.9980e-03 eta 0:45:26
epoch [3/50] batch [280/288] time 0.183 (0.200) data 0.000 (0.001) loss 3.1311 (3.1327) teacher_loss 3.0572 (3.0577) loss_zs_kd 0.6079 (0.2872) loss_oracle 0.0739 (0.0751) acc 78.1250 (73.0246) lr 1.9980e-03 eta 0:45:15
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,404
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,998
* accuracy: 82.3%
* error: 17.7%
* macro_f1: 78.9%
******* Domain a best val acc:      86.4%, epoch: 3 *******
******* Domain a best val test acc: 82.3%, epoch: 3 *******
******* Domain a best test acc:     82.6%, epoch: 1 *******
epoch [4/50] batch [20/288] time 0.194 (0.214) data 0.000 (0.020) loss 2.3795 (2.9060) teacher_loss 2.3100 (2.8317) loss_zs_kd 0.2319 (0.3096) loss_oracle 0.0695 (0.0743) acc 84.3750 (73.4375) lr 1.9921e-03 eta 0:48:08
epoch [4/50] batch [40/288] time 0.202 (0.204) data 0.000 (0.010) loss 3.1178 (2.9740) teacher_loss 3.0750 (2.9013) loss_zs_kd 0.5091 (0.3062) loss_oracle 0.0428 (0.0727) acc 71.8750 (73.9844) lr 1.9921e-03 eta 0:45:49
epoch [4/50] batch [60/288] time 0.191 (0.201) data 0.000 (0.007) loss 3.7703 (2.9804) teacher_loss 3.6706 (2.9076) loss_zs_kd 0.2343 (0.3043) loss_oracle 0.0997 (0.0728) acc 65.6250 (74.1146) lr 1.9921e-03 eta 0:45:01
epoch [4/50] batch [80/288] time 0.197 (0.199) data 0.000 (0.005) loss 4.1966 (3.0383) teacher_loss 4.1416 (2.9651) loss_zs_kd 0.2536 (0.3006) loss_oracle 0.0551 (0.0732) acc 65.6250 (73.7109) lr 1.9921e-03 eta 0:44:38
epoch [4/50] batch [100/288] time 0.196 (0.198) data 0.000 (0.004) loss 2.4931 (3.0790) teacher_loss 2.4141 (3.0067) loss_zs_kd 0.2160 (0.2939) loss_oracle 0.0789 (0.0722) acc 81.2500 (73.3750) lr 1.9921e-03 eta 0:44:18
epoch [4/50] batch [120/288] time 0.188 (0.197) data 0.000 (0.003) loss 2.9902 (3.1069) teacher_loss 2.8577 (3.0348) loss_zs_kd 0.2296 (0.2869) loss_oracle 0.1325 (0.0720) acc 68.7500 (73.0729) lr 1.9921e-03 eta 0:44:05
epoch [4/50] batch [140/288] time 0.190 (0.197) data 0.000 (0.003) loss 2.2498 (3.1214) teacher_loss 2.1442 (3.0478) loss_zs_kd 0.2634 (0.2849) loss_oracle 0.1056 (0.0736) acc 75.0000 (72.8348) lr 1.9921e-03 eta 0:43:57
epoch [4/50] batch [160/288] time 0.191 (0.197) data 0.000 (0.003) loss 3.5125 (3.0988) teacher_loss 3.4572 (3.0254) loss_zs_kd 0.3204 (0.2888) loss_oracle 0.0553 (0.0734) acc 68.7500 (72.9883) lr 1.9921e-03 eta 0:43:48
epoch [4/50] batch [180/288] time 0.090 (0.203) data 0.000 (0.002) loss 4.3842 (3.0937) teacher_loss 4.2954 (3.0203) loss_zs_kd 0.3940 (0.2906) loss_oracle 0.0888 (0.0734) acc 56.2500 (73.0556) lr 1.9921e-03 eta 0:45:13
epoch [4/50] batch [200/288] time 0.193 (0.205) data 0.000 (0.002) loss 3.4117 (3.0835) teacher_loss 3.3040 (3.0097) loss_zs_kd 0.3215 (0.2973) loss_oracle 0.1077 (0.0738) acc 71.8750 (73.2344) lr 1.9921e-03 eta 0:45:38
epoch [4/50] batch [220/288] time 0.195 (0.204) data 0.000 (0.002) loss 3.7248 (3.0831) teacher_loss 3.6388 (3.0095) loss_zs_kd 0.2925 (0.3008) loss_oracle 0.0859 (0.0735) acc 65.6250 (73.1250) lr 1.9921e-03 eta 0:45:21
epoch [4/50] batch [240/288] time 0.189 (0.203) data 0.000 (0.002) loss 3.6999 (3.0804) teacher_loss 3.5860 (3.0069) loss_zs_kd 0.2757 (0.2988) loss_oracle 0.1138 (0.0735) acc 71.8750 (73.1901) lr 1.9921e-03 eta 0:45:05
epoch [4/50] batch [260/288] time 0.190 (0.203) data 0.000 (0.002) loss 2.5660 (3.0943) teacher_loss 2.4699 (3.0208) loss_zs_kd 0.3544 (0.2993) loss_oracle 0.0960 (0.0735) acc 71.8750 (72.9567) lr 1.9921e-03 eta 0:44:51
epoch [4/50] batch [280/288] time 0.190 (0.202) data 0.000 (0.002) loss 2.2115 (3.0794) teacher_loss 2.1297 (3.0062) loss_zs_kd 0.2445 (0.2983) loss_oracle 0.0819 (0.0732) acc 81.2500 (73.2366) lr 1.9921e-03 eta 0:44:38
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,000
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.2%
******* Domain a best val acc:      86.6%, epoch: 4 *******
******* Domain a best val test acc: 82.4%, epoch: 4 *******
******* Domain a best test acc:     82.6%, epoch: 1 *******
epoch [5/50] batch [20/288] time 0.193 (0.216) data 0.000 (0.016) loss 4.2870 (3.4395) teacher_loss 4.2010 (3.3666) loss_zs_kd 0.3417 (0.3313) loss_oracle 0.0860 (0.0729) acc 62.5000 (69.6875) lr 1.9823e-03 eta 0:47:40
epoch [5/50] batch [40/288] time 0.189 (0.205) data 0.000 (0.008) loss 1.4488 (3.1280) teacher_loss 1.3672 (3.0554) loss_zs_kd 0.4473 (0.3371) loss_oracle 0.0816 (0.0726) acc 90.6250 (72.1875) lr 1.9823e-03 eta 0:45:12
epoch [5/50] batch [60/288] time 0.197 (0.202) data 0.000 (0.006) loss 3.2869 (3.0197) teacher_loss 3.2486 (2.9465) loss_zs_kd 0.2353 (0.3345) loss_oracle 0.0382 (0.0731) acc 68.7500 (73.5938) lr 1.9823e-03 eta 0:44:26
epoch [5/50] batch [80/288] time 0.199 (0.199) data 0.000 (0.004) loss 4.3780 (2.9775) teacher_loss 4.3115 (2.9048) loss_zs_kd 0.3042 (0.3352) loss_oracle 0.0665 (0.0727) acc 71.8750 (74.0625) lr 1.9823e-03 eta 0:43:40
epoch [5/50] batch [100/288] time 0.198 (0.198) data 0.000 (0.003) loss 1.7173 (3.0013) teacher_loss 1.6529 (2.9286) loss_zs_kd 0.3764 (0.3338) loss_oracle 0.0644 (0.0727) acc 81.2500 (73.7812) lr 1.9823e-03 eta 0:43:23
epoch [5/50] batch [120/288] time 0.196 (0.197) data 0.000 (0.003) loss 3.3691 (2.9914) teacher_loss 3.2780 (2.9193) loss_zs_kd 0.3908 (0.3308) loss_oracle 0.0911 (0.0721) acc 75.0000 (73.8802) lr 1.9823e-03 eta 0:43:11
epoch [5/50] batch [140/288] time 0.200 (0.197) data 0.000 (0.002) loss 3.1305 (3.0063) teacher_loss 3.1023 (2.9338) loss_zs_kd 0.2766 (0.3294) loss_oracle 0.0283 (0.0724) acc 68.7500 (73.9062) lr 1.9823e-03 eta 0:43:02
epoch [5/50] batch [160/288] time 0.198 (0.197) data 0.000 (0.002) loss 3.9209 (3.0128) teacher_loss 3.8444 (2.9403) loss_zs_kd 0.3340 (0.3286) loss_oracle 0.0765 (0.0724) acc 65.6250 (73.6523) lr 1.9823e-03 eta 0:42:53
epoch [5/50] batch [180/288] time 0.092 (0.201) data 0.000 (0.002) loss 2.6031 (3.0325) teacher_loss 2.5402 (2.9592) loss_zs_kd 0.3065 (0.3319) loss_oracle 0.0629 (0.0733) acc 71.8750 (73.4375) lr 1.9823e-03 eta 0:43:47
epoch [5/50] batch [200/288] time 0.193 (0.204) data 0.000 (0.002) loss 2.8935 (3.0471) teacher_loss 2.8149 (2.9734) loss_zs_kd 0.3509 (0.3284) loss_oracle 0.0786 (0.0737) acc 84.3750 (73.5625) lr 1.9823e-03 eta 0:44:19
epoch [5/50] batch [220/288] time 0.189 (0.203) data 0.000 (0.002) loss 3.6771 (3.0608) teacher_loss 3.5865 (2.9870) loss_zs_kd 0.3346 (0.3282) loss_oracle 0.0906 (0.0738) acc 71.8750 (73.3807) lr 1.9823e-03 eta 0:44:03
epoch [5/50] batch [240/288] time 0.189 (0.202) data 0.000 (0.002) loss 2.5396 (3.0520) teacher_loss 2.4709 (2.9788) loss_zs_kd 0.2617 (0.3284) loss_oracle 0.0687 (0.0732) acc 78.1250 (73.3984) lr 1.9823e-03 eta 0:43:51
epoch [5/50] batch [260/288] time 0.189 (0.202) data 0.000 (0.001) loss 3.4578 (3.0422) teacher_loss 3.3644 (2.9693) loss_zs_kd 0.1935 (0.3283) loss_oracle 0.0935 (0.0730) acc 65.6250 (73.3413) lr 1.9823e-03 eta 0:43:39
epoch [5/50] batch [280/288] time 0.193 (0.201) data 0.000 (0.001) loss 2.0615 (3.0130) teacher_loss 1.9868 (2.9405) loss_zs_kd 0.3308 (0.3272) loss_oracle 0.0747 (0.0725) acc 81.2500 (73.5268) lr 1.9823e-03 eta 0:43:28
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,406
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.6%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.6%, epoch: 4 *******
******* Domain a best val test acc: 82.4%, epoch: 4 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [6/50] batch [20/288] time 0.194 (0.207) data 0.000 (0.014) loss 3.2728 (2.7329) teacher_loss 3.1867 (2.6574) loss_zs_kd 0.2289 (0.3015) loss_oracle 0.0861 (0.0755) acc 68.7500 (73.5938) lr 1.9686e-03 eta 0:44:35
epoch [6/50] batch [40/288] time 0.194 (0.200) data 0.000 (0.007) loss 2.9180 (2.8735) teacher_loss 2.8174 (2.7994) loss_zs_kd 0.3244 (0.3193) loss_oracle 0.1005 (0.0741) acc 75.0000 (73.5156) lr 1.9686e-03 eta 0:43:05
epoch [6/50] batch [60/288] time 0.192 (0.198) data 0.000 (0.005) loss 3.0210 (2.8206) teacher_loss 2.9594 (2.7471) loss_zs_kd 0.4600 (0.3239) loss_oracle 0.0615 (0.0734) acc 68.7500 (73.9062) lr 1.9686e-03 eta 0:42:34
epoch [6/50] batch [80/288] time 0.194 (0.197) data 0.000 (0.004) loss 2.8423 (2.8473) teacher_loss 2.7851 (2.7742) loss_zs_kd 0.4576 (0.3381) loss_oracle 0.0571 (0.0731) acc 78.1250 (73.6719) lr 1.9686e-03 eta 0:42:16
epoch [6/50] batch [100/288] time 0.193 (0.196) data 0.000 (0.003) loss 3.6060 (2.8904) teacher_loss 3.5170 (2.8167) loss_zs_kd 0.3757 (0.3382) loss_oracle 0.0889 (0.0737) acc 68.7500 (73.7500) lr 1.9686e-03 eta 0:42:05
epoch [6/50] batch [120/288] time 0.193 (0.196) data 0.000 (0.002) loss 3.2198 (2.8874) teacher_loss 3.1676 (2.8143) loss_zs_kd 0.3739 (0.3356) loss_oracle 0.0522 (0.0732) acc 71.8750 (73.9844) lr 1.9686e-03 eta 0:41:56
epoch [6/50] batch [140/288] time 0.189 (0.196) data 0.000 (0.002) loss 2.3676 (2.8897) teacher_loss 2.3104 (2.8180) loss_zs_kd 0.1962 (0.3322) loss_oracle 0.0572 (0.0717) acc 84.3750 (74.0625) lr 1.9686e-03 eta 0:41:52
epoch [6/50] batch [160/288] time 0.085 (0.193) data 0.000 (0.002) loss 2.0248 (2.8962) teacher_loss 1.9595 (2.8233) loss_zs_kd 0.4685 (0.3379) loss_oracle 0.0653 (0.0729) acc 78.1250 (73.9258) lr 1.9686e-03 eta 0:41:13
epoch [6/50] batch [180/288] time 0.468 (0.203) data 0.000 (0.002) loss 2.2520 (2.9013) teacher_loss 2.1681 (2.8287) loss_zs_kd 0.4221 (0.3406) loss_oracle 0.0838 (0.0727) acc 81.2500 (73.8542) lr 1.9686e-03 eta 0:43:13
epoch [6/50] batch [200/288] time 0.197 (0.203) data 0.000 (0.002) loss 2.7310 (2.9135) teacher_loss 2.6977 (2.8411) loss_zs_kd 0.3118 (0.3393) loss_oracle 0.0333 (0.0724) acc 68.7500 (73.8125) lr 1.9686e-03 eta 0:43:12
epoch [6/50] batch [220/288] time 0.198 (0.203) data 0.000 (0.001) loss 3.6325 (2.9227) teacher_loss 3.5606 (2.8505) loss_zs_kd 0.4692 (0.3432) loss_oracle 0.0719 (0.0722) acc 75.0000 (73.8352) lr 1.9686e-03 eta 0:43:00
epoch [6/50] batch [240/288] time 0.201 (0.202) data 0.000 (0.001) loss 2.8413 (2.9390) teacher_loss 2.8080 (2.8670) loss_zs_kd 0.1828 (0.3435) loss_oracle 0.0333 (0.0720) acc 75.0000 (73.7370) lr 1.9686e-03 eta 0:42:47
epoch [6/50] batch [260/288] time 0.190 (0.201) data 0.000 (0.001) loss 3.8228 (2.9488) teacher_loss 3.7174 (2.8757) loss_zs_kd 0.2821 (0.3444) loss_oracle 0.1054 (0.0731) acc 68.7500 (73.7260) lr 1.9686e-03 eta 0:42:34
epoch [6/50] batch [280/288] time 0.161 (0.200) data 0.000 (0.001) loss 3.2509 (2.9495) teacher_loss 3.1743 (2.8763) loss_zs_kd 0.5056 (0.3454) loss_oracle 0.0765 (0.0731) acc 75.0000 (73.8170) lr 1.9686e-03 eta 0:42:19
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,401
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,000
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.5%
******* Domain a best val acc:      86.6%, epoch: 4 *******
******* Domain a best val test acc: 82.4%, epoch: 4 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [7/50] batch [20/288] time 0.211 (0.216) data 0.000 (0.017) loss 3.4273 (3.1091) teacher_loss 3.3485 (3.0323) loss_zs_kd 0.2587 (0.3260) loss_oracle 0.0789 (0.0768) acc 68.7500 (75.1562) lr 1.9511e-03 eta 0:45:30
epoch [7/50] batch [40/288] time 0.190 (0.205) data 0.000 (0.009) loss 1.9450 (2.9213) teacher_loss 1.8493 (2.8421) loss_zs_kd 0.3410 (0.3282) loss_oracle 0.0958 (0.0791) acc 78.1250 (75.7031) lr 1.9511e-03 eta 0:43:06
epoch [7/50] batch [60/288] time 0.199 (0.201) data 0.000 (0.006) loss 3.5953 (2.8758) teacher_loss 3.5282 (2.7980) loss_zs_kd 0.4523 (0.3433) loss_oracle 0.0672 (0.0779) acc 68.7500 (75.6250) lr 1.9511e-03 eta 0:42:18
epoch [7/50] batch [80/288] time 0.194 (0.199) data 0.000 (0.004) loss 2.4329 (2.8638) teacher_loss 2.3636 (2.7882) loss_zs_kd 0.3828 (0.3442) loss_oracle 0.0693 (0.0756) acc 81.2500 (75.2344) lr 1.9511e-03 eta 0:41:52
epoch [7/50] batch [100/288] time 0.195 (0.198) data 0.000 (0.004) loss 1.7913 (2.8637) teacher_loss 1.6955 (2.7886) loss_zs_kd 0.2887 (0.3392) loss_oracle 0.0958 (0.0750) acc 84.3750 (74.9062) lr 1.9511e-03 eta 0:41:34
epoch [7/50] batch [120/288] time 0.190 (0.198) data 0.000 (0.003) loss 4.1769 (2.8556) teacher_loss 4.0787 (2.7791) loss_zs_kd 0.3811 (0.3415) loss_oracle 0.0982 (0.0765) acc 71.8750 (75.0260) lr 1.9511e-03 eta 0:41:19
epoch [7/50] batch [140/288] time 0.188 (0.197) data 0.001 (0.003) loss 3.0849 (2.8742) teacher_loss 2.9793 (2.7979) loss_zs_kd 0.3276 (0.3450) loss_oracle 0.1056 (0.0763) acc 71.8750 (74.8214) lr 1.9511e-03 eta 0:41:12
epoch [7/50] batch [160/288] time 0.082 (0.195) data 0.000 (0.002) loss 2.8443 (2.9261) teacher_loss 2.7543 (2.8491) loss_zs_kd 0.3524 (0.3454) loss_oracle 0.0900 (0.0769) acc 78.1250 (74.3555) lr 1.9511e-03 eta 0:40:39
epoch [7/50] batch [180/288] time 0.466 (0.204) data 0.000 (0.002) loss 3.1842 (2.9440) teacher_loss 3.1219 (2.8680) loss_zs_kd 0.3731 (0.3457) loss_oracle 0.0622 (0.0759) acc 78.1250 (74.2882) lr 1.9511e-03 eta 0:42:28
epoch [7/50] batch [200/288] time 0.193 (0.205) data 0.000 (0.002) loss 1.7826 (2.9179) teacher_loss 1.7135 (2.8419) loss_zs_kd 0.4288 (0.3482) loss_oracle 0.0691 (0.0760) acc 78.1250 (74.3594) lr 1.9511e-03 eta 0:42:34
epoch [7/50] batch [220/288] time 0.190 (0.204) data 0.000 (0.002) loss 3.6338 (2.9228) teacher_loss 3.5417 (2.8468) loss_zs_kd 0.3708 (0.3482) loss_oracle 0.0921 (0.0759) acc 71.8750 (74.2472) lr 1.9511e-03 eta 0:42:18
epoch [7/50] batch [240/288] time 0.191 (0.203) data 0.000 (0.002) loss 4.4146 (2.9480) teacher_loss 4.3214 (2.8722) loss_zs_kd 0.4423 (0.3497) loss_oracle 0.0932 (0.0758) acc 62.5000 (74.1536) lr 1.9511e-03 eta 0:42:03
epoch [7/50] batch [260/288] time 0.211 (0.202) data 0.000 (0.002) loss 3.0299 (2.9566) teacher_loss 2.9588 (2.8806) loss_zs_kd 0.2657 (0.3494) loss_oracle 0.0711 (0.0760) acc 75.0000 (74.1466) lr 1.9511e-03 eta 0:41:50
epoch [7/50] batch [280/288] time 0.219 (0.202) data 0.000 (0.001) loss 2.1102 (2.9394) teacher_loss 2.0256 (2.8635) loss_zs_kd 0.2748 (0.3465) loss_oracle 0.0846 (0.0758) acc 78.1250 (74.3415) lr 1.9511e-03 eta 0:41:37
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,413
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 86.1%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,015
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.8%
******* Domain a best val acc:      86.6%, epoch: 7 *******
******* Domain a best val test acc: 83.0%, epoch: 7 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [8/50] batch [20/288] time 0.189 (0.208) data 0.000 (0.016) loss 2.6500 (2.8941) teacher_loss 2.5712 (2.8199) loss_zs_kd 0.4893 (0.3560) loss_oracle 0.0788 (0.0742) acc 81.2500 (73.9062) lr 1.9298e-03 eta 0:42:51
epoch [8/50] batch [40/288] time 0.186 (0.202) data 0.000 (0.008) loss 2.5420 (2.8657) teacher_loss 2.4706 (2.7943) loss_zs_kd 0.3850 (0.3367) loss_oracle 0.0714 (0.0715) acc 78.1250 (73.9844) lr 1.9298e-03 eta 0:41:27
epoch [8/50] batch [60/288] time 0.186 (0.199) data 0.000 (0.005) loss 4.4154 (2.8102) teacher_loss 4.2920 (2.7357) loss_zs_kd 0.2018 (0.3328) loss_oracle 0.1235 (0.0745) acc 62.5000 (74.1146) lr 1.9298e-03 eta 0:40:51
epoch [8/50] batch [80/288] time 0.191 (0.198) data 0.000 (0.004) loss 3.2883 (2.7836) teacher_loss 3.1939 (2.7105) loss_zs_kd 0.2587 (0.3395) loss_oracle 0.0944 (0.0731) acc 68.7500 (74.5312) lr 1.9298e-03 eta 0:40:31
epoch [8/50] batch [100/288] time 0.219 (0.197) data 0.000 (0.003) loss 2.0649 (2.8092) teacher_loss 2.0077 (2.7365) loss_zs_kd 0.5539 (0.3474) loss_oracle 0.0571 (0.0727) acc 81.2500 (74.3438) lr 1.9298e-03 eta 0:40:18
epoch [8/50] batch [120/288] time 0.192 (0.196) data 0.000 (0.003) loss 2.3177 (2.7836) teacher_loss 2.2072 (2.7111) loss_zs_kd 0.4207 (0.3534) loss_oracle 0.1105 (0.0725) acc 78.1250 (74.7396) lr 1.9298e-03 eta 0:40:08
epoch [8/50] batch [140/288] time 0.208 (0.196) data 0.000 (0.002) loss 2.6905 (2.8192) teacher_loss 2.6522 (2.7459) loss_zs_kd 0.4360 (0.3571) loss_oracle 0.0383 (0.0733) acc 62.5000 (74.3973) lr 1.9298e-03 eta 0:40:02
epoch [8/50] batch [160/288] time 0.091 (0.192) data 0.000 (0.002) loss 3.3773 (2.8741) teacher_loss 3.3102 (2.8005) loss_zs_kd 0.4803 (0.3585) loss_oracle 0.0672 (0.0735) acc 62.5000 (73.9258) lr 1.9298e-03 eta 0:39:01
epoch [8/50] batch [180/288] time 0.471 (0.201) data 0.000 (0.002) loss 3.9387 (2.8839) teacher_loss 3.8716 (2.8097) loss_zs_kd 0.3071 (0.3562) loss_oracle 0.0672 (0.0742) acc 65.6250 (73.8021) lr 1.9298e-03 eta 0:40:48
epoch [8/50] batch [200/288] time 0.188 (0.201) data 0.000 (0.002) loss 3.8846 (2.8917) teacher_loss 3.8201 (2.8177) loss_zs_kd 0.3283 (0.3550) loss_oracle 0.0645 (0.0741) acc 68.7500 (73.7188) lr 1.9298e-03 eta 0:40:52
epoch [8/50] batch [220/288] time 0.197 (0.200) data 0.000 (0.002) loss 2.0950 (2.8795) teacher_loss 2.0317 (2.8051) loss_zs_kd 0.2965 (0.3532) loss_oracle 0.0634 (0.0744) acc 87.5000 (73.8920) lr 1.9298e-03 eta 0:40:35
epoch [8/50] batch [240/288] time 0.195 (0.200) data 0.000 (0.001) loss 2.7832 (2.8903) teacher_loss 2.7549 (2.8165) loss_zs_kd 0.4167 (0.3541) loss_oracle 0.0283 (0.0737) acc 75.0000 (73.8672) lr 1.9298e-03 eta 0:40:25
epoch [8/50] batch [260/288] time 0.193 (0.199) data 0.000 (0.001) loss 2.5703 (2.8881) teacher_loss 2.5225 (2.8141) loss_zs_kd 0.5155 (0.3530) loss_oracle 0.0478 (0.0740) acc 78.1250 (74.0385) lr 1.9298e-03 eta 0:40:16
epoch [8/50] batch [280/288] time 0.196 (0.199) data 0.000 (0.001) loss 4.2547 (2.9069) teacher_loss 4.1947 (2.8332) loss_zs_kd 0.4209 (0.3523) loss_oracle 0.0599 (0.0738) acc 56.2500 (73.9062) lr 1.9298e-03 eta 0:40:07
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,417
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.1%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,014
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.5%
******* Domain a best val acc:      86.7%, epoch: 8 *******
******* Domain a best val test acc: 83.0%, epoch: 8 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [9/50] batch [20/288] time 0.195 (0.205) data 0.000 (0.012) loss 3.4301 (2.9333) teacher_loss 3.3874 (2.8681) loss_zs_kd 0.3063 (0.3598) loss_oracle 0.0427 (0.0652) acc 71.8750 (75.1562) lr 1.9048e-03 eta 0:41:20
epoch [9/50] batch [40/288] time 0.205 (0.199) data 0.000 (0.006) loss 1.7935 (2.7714) teacher_loss 1.7557 (2.7013) loss_zs_kd 0.2059 (0.3446) loss_oracle 0.0377 (0.0701) acc 81.2500 (76.1719) lr 1.9048e-03 eta 0:40:04
epoch [9/50] batch [60/288] time 0.193 (0.198) data 0.000 (0.004) loss 3.3318 (2.7805) teacher_loss 3.2551 (2.7092) loss_zs_kd 0.3292 (0.3525) loss_oracle 0.0767 (0.0713) acc 62.5000 (75.6771) lr 1.9048e-03 eta 0:39:40
epoch [9/50] batch [80/288] time 0.194 (0.197) data 0.000 (0.003) loss 2.3600 (2.8677) teacher_loss 2.2884 (2.7969) loss_zs_kd 0.3577 (0.3651) loss_oracle 0.0716 (0.0708) acc 84.3750 (75.1172) lr 1.9048e-03 eta 0:39:26
epoch [9/50] batch [100/288] time 0.198 (0.196) data 0.000 (0.003) loss 2.2324 (2.8722) teacher_loss 2.1505 (2.8018) loss_zs_kd 0.3223 (0.3596) loss_oracle 0.0819 (0.0704) acc 71.8750 (75.0312) lr 1.9048e-03 eta 0:39:16
epoch [9/50] batch [120/288] time 0.194 (0.196) data 0.000 (0.002) loss 2.8333 (2.9237) teacher_loss 2.7590 (2.8535) loss_zs_kd 0.2908 (0.3594) loss_oracle 0.0742 (0.0702) acc 75.0000 (74.3229) lr 1.9048e-03 eta 0:39:08
epoch [9/50] batch [140/288] time 0.192 (0.196) data 0.000 (0.002) loss 2.9957 (2.9515) teacher_loss 2.9335 (2.8808) loss_zs_kd 0.1377 (0.3564) loss_oracle 0.0622 (0.0707) acc 78.1250 (74.2857) lr 1.9048e-03 eta 0:38:58
epoch [9/50] batch [160/288] time 0.081 (0.194) data 0.000 (0.002) loss 2.1036 (2.9326) teacher_loss 1.9814 (2.8609) loss_zs_kd 0.4120 (0.3600) loss_oracle 0.1222 (0.0717) acc 87.5000 (74.2188) lr 1.9048e-03 eta 0:38:31
epoch [9/50] batch [180/288] time 0.454 (0.201) data 0.000 (0.002) loss 4.7896 (2.9365) teacher_loss 4.7369 (2.8652) loss_zs_kd 0.4612 (0.3588) loss_oracle 0.0527 (0.0713) acc 65.6250 (74.1493) lr 1.9048e-03 eta 0:39:51
epoch [9/50] batch [200/288] time 0.196 (0.203) data 0.000 (0.001) loss 3.9724 (2.9632) teacher_loss 3.9297 (2.8924) loss_zs_kd 0.4293 (0.3615) loss_oracle 0.0427 (0.0708) acc 71.8750 (73.9531) lr 1.9048e-03 eta 0:40:18
epoch [9/50] batch [220/288] time 0.194 (0.202) data 0.001 (0.001) loss 1.9204 (2.9604) teacher_loss 1.8322 (2.8893) loss_zs_kd 0.4708 (0.3611) loss_oracle 0.0883 (0.0711) acc 78.1250 (73.9631) lr 1.9048e-03 eta 0:40:04
epoch [9/50] batch [240/288] time 0.198 (0.202) data 0.000 (0.001) loss 2.7603 (2.9550) teacher_loss 2.7219 (2.8835) loss_zs_kd 0.4975 (0.3611) loss_oracle 0.0384 (0.0715) acc 78.1250 (74.0625) lr 1.9048e-03 eta 0:39:52
epoch [9/50] batch [260/288] time 0.196 (0.201) data 0.000 (0.001) loss 2.6829 (2.9666) teacher_loss 2.6285 (2.8951) loss_zs_kd 0.2856 (0.3610) loss_oracle 0.0543 (0.0716) acc 71.8750 (73.9183) lr 1.9048e-03 eta 0:39:41
epoch [9/50] batch [280/288] time 0.199 (0.201) data 0.000 (0.001) loss 2.9615 (2.9455) teacher_loss 2.9092 (2.8740) loss_zs_kd 0.2060 (0.3609) loss_oracle 0.0522 (0.0715) acc 75.0000 (74.0737) lr 1.9048e-03 eta 0:39:30
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 86.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,001
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.4%
******* Domain a best val acc:      86.7%, epoch: 8 *******
******* Domain a best val test acc: 83.0%, epoch: 8 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [10/50] batch [20/288] time 0.195 (0.215) data 0.000 (0.017) loss 2.3533 (3.0547) teacher_loss 2.2772 (2.9817) loss_zs_kd 0.2748 (0.3333) loss_oracle 0.0761 (0.0730) acc 84.3750 (75.1562) lr 1.8763e-03 eta 0:42:14
epoch [10/50] batch [40/288] time 0.196 (0.205) data 0.000 (0.009) loss 3.6421 (2.8889) teacher_loss 3.5682 (2.8162) loss_zs_kd 0.4551 (0.3533) loss_oracle 0.0739 (0.0726) acc 68.7500 (75.8594) lr 1.8763e-03 eta 0:40:09
epoch [10/50] batch [60/288] time 0.198 (0.202) data 0.000 (0.006) loss 2.5728 (2.7725) teacher_loss 2.4961 (2.6994) loss_zs_kd 0.3061 (0.3575) loss_oracle 0.0767 (0.0732) acc 78.1250 (76.5104) lr 1.8763e-03 eta 0:39:28
epoch [10/50] batch [80/288] time 0.194 (0.200) data 0.000 (0.004) loss 3.3742 (2.8566) teacher_loss 3.3099 (2.7837) loss_zs_kd 0.2354 (0.3708) loss_oracle 0.0643 (0.0729) acc 78.1250 (75.8984) lr 1.8763e-03 eta 0:39:04
epoch [10/50] batch [100/288] time 0.193 (0.199) data 0.000 (0.004) loss 3.7216 (2.8266) teacher_loss 3.6443 (2.7548) loss_zs_kd 0.3540 (0.3617) loss_oracle 0.0773 (0.0717) acc 75.0000 (76.0938) lr 1.8763e-03 eta 0:38:49
epoch [10/50] batch [120/288] time 0.191 (0.198) data 0.000 (0.003) loss 2.8543 (2.8284) teacher_loss 2.7639 (2.7568) loss_zs_kd 0.3505 (0.3600) loss_oracle 0.0904 (0.0716) acc 78.1250 (76.0417) lr 1.8763e-03 eta 0:38:35
epoch [10/50] batch [140/288] time 0.193 (0.198) data 0.000 (0.003) loss 3.7639 (2.8662) teacher_loss 3.6951 (2.7944) loss_zs_kd 0.3552 (0.3661) loss_oracle 0.0687 (0.0718) acc 71.8750 (75.7589) lr 1.8763e-03 eta 0:38:25
epoch [10/50] batch [160/288] time 0.083 (0.196) data 0.000 (0.002) loss 1.8645 (2.8586) teacher_loss 1.8262 (2.7868) loss_zs_kd 0.2948 (0.3642) loss_oracle 0.0383 (0.0718) acc 90.6250 (75.6445) lr 1.8763e-03 eta 0:38:00
epoch [10/50] batch [180/288] time 0.453 (0.202) data 0.000 (0.002) loss 3.4740 (2.8964) teacher_loss 3.4069 (2.8239) loss_zs_kd 0.2969 (0.3645) loss_oracle 0.0671 (0.0725) acc 71.8750 (75.1562) lr 1.8763e-03 eta 0:39:13
epoch [10/50] batch [200/288] time 0.195 (0.205) data 0.000 (0.002) loss 3.8840 (2.9135) teacher_loss 3.7764 (2.8404) loss_zs_kd 0.3328 (0.3645) loss_oracle 0.1076 (0.0730) acc 62.5000 (74.9844) lr 1.8763e-03 eta 0:39:38
epoch [10/50] batch [220/288] time 0.194 (0.204) data 0.000 (0.002) loss 3.0247 (2.9164) teacher_loss 2.9770 (2.8436) loss_zs_kd 0.3691 (0.3620) loss_oracle 0.0477 (0.0728) acc 71.8750 (75.0426) lr 1.8763e-03 eta 0:39:24
epoch [10/50] batch [240/288] time 0.196 (0.203) data 0.000 (0.002) loss 2.3958 (2.9221) teacher_loss 2.3388 (2.8488) loss_zs_kd 0.4122 (0.3605) loss_oracle 0.0571 (0.0732) acc 84.3750 (74.9609) lr 1.8763e-03 eta 0:39:12
epoch [10/50] batch [260/288] time 0.197 (0.203) data 0.000 (0.002) loss 3.3153 (2.9179) teacher_loss 3.3009 (2.8452) loss_zs_kd 0.3537 (0.3625) loss_oracle 0.0144 (0.0727) acc 78.1250 (74.9399) lr 1.8763e-03 eta 0:39:00
epoch [10/50] batch [280/288] time 0.191 (0.202) data 0.000 (0.001) loss 4.5497 (2.9428) teacher_loss 4.4666 (2.8699) loss_zs_kd 0.4627 (0.3647) loss_oracle 0.0831 (0.0729) acc 68.7500 (74.7991) lr 1.8763e-03 eta 0:38:48
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,417
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.1%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,016
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.9%
******* Domain a best val acc:      86.7%, epoch: 8 *******
******* Domain a best val test acc: 83.0%, epoch: 8 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [11/50] batch [20/288] time 0.198 (0.209) data 0.000 (0.014) loss 4.3611 (3.0156) teacher_loss 4.2872 (2.9459) loss_zs_kd 0.3912 (0.3628) loss_oracle 0.0739 (0.0697) acc 62.5000 (73.4375) lr 1.8443e-03 eta 0:40:06
epoch [11/50] batch [40/288] time 0.191 (0.201) data 0.000 (0.007) loss 3.4633 (2.9922) teacher_loss 3.3671 (2.9182) loss_zs_kd 0.4071 (0.3739) loss_oracle 0.0962 (0.0740) acc 68.7500 (73.1250) lr 1.8443e-03 eta 0:38:32
epoch [11/50] batch [60/288] time 0.199 (0.199) data 0.000 (0.005) loss 2.2355 (2.9626) teacher_loss 2.2022 (2.8883) loss_zs_kd 0.4916 (0.3694) loss_oracle 0.0333 (0.0743) acc 78.1250 (72.9167) lr 1.8443e-03 eta 0:38:04
epoch [11/50] batch [80/288] time 0.193 (0.198) data 0.000 (0.004) loss 3.4644 (2.8833) teacher_loss 3.3977 (2.8092) loss_zs_kd 0.2697 (0.3666) loss_oracle 0.0667 (0.0741) acc 65.6250 (73.8281) lr 1.8443e-03 eta 0:37:45
epoch [11/50] batch [100/288] time 0.200 (0.197) data 0.000 (0.003) loss 3.9001 (2.8444) teacher_loss 3.8479 (2.7718) loss_zs_kd 0.4447 (0.3684) loss_oracle 0.0521 (0.0726) acc 68.7500 (74.2812) lr 1.8443e-03 eta 0:37:33
epoch [11/50] batch [120/288] time 0.191 (0.197) data 0.000 (0.002) loss 4.4380 (2.8454) teacher_loss 4.3693 (2.7728) loss_zs_kd 0.3061 (0.3685) loss_oracle 0.0688 (0.0727) acc 50.0000 (74.2448) lr 1.8443e-03 eta 0:37:23
epoch [11/50] batch [140/288] time 0.195 (0.197) data 0.000 (0.002) loss 2.6507 (2.8554) teacher_loss 2.5628 (2.7830) loss_zs_kd 0.3091 (0.3692) loss_oracle 0.0879 (0.0724) acc 81.2500 (74.2857) lr 1.8443e-03 eta 0:37:16
epoch [11/50] batch [160/288] time 0.083 (0.195) data 0.000 (0.002) loss 3.6807 (2.9012) teacher_loss 3.6042 (2.8285) loss_zs_kd 0.3043 (0.3715) loss_oracle 0.0765 (0.0727) acc 65.6250 (73.8672) lr 1.8443e-03 eta 0:36:53
epoch [11/50] batch [180/288] time 0.102 (0.200) data 0.000 (0.002) loss 2.9218 (2.8886) teacher_loss 2.8403 (2.8160) loss_zs_kd 0.3868 (0.3720) loss_oracle 0.0815 (0.0726) acc 81.2500 (74.0451) lr 1.8443e-03 eta 0:37:45
epoch [11/50] batch [200/288] time 0.195 (0.203) data 0.000 (0.002) loss 3.0337 (2.8773) teacher_loss 2.9655 (2.8042) loss_zs_kd 0.6629 (0.3729) loss_oracle 0.0682 (0.0731) acc 78.1250 (74.2500) lr 1.8443e-03 eta 0:38:18
epoch [11/50] batch [220/288] time 0.198 (0.202) data 0.000 (0.001) loss 3.1052 (2.8838) teacher_loss 3.0409 (2.8095) loss_zs_kd 0.3771 (0.3806) loss_oracle 0.0644 (0.0743) acc 84.3750 (74.2045) lr 1.8443e-03 eta 0:38:05
epoch [11/50] batch [240/288] time 0.191 (0.201) data 0.000 (0.001) loss 2.9640 (2.8734) teacher_loss 2.8858 (2.7984) loss_zs_kd 0.2908 (0.3810) loss_oracle 0.0782 (0.0750) acc 71.8750 (74.4010) lr 1.8443e-03 eta 0:37:52
epoch [11/50] batch [260/288] time 0.193 (0.201) data 0.000 (0.001) loss 2.6376 (2.8737) teacher_loss 2.5697 (2.7983) loss_zs_kd 0.3661 (0.3809) loss_oracle 0.0679 (0.0753) acc 75.0000 (74.3510) lr 1.8443e-03 eta 0:37:42
epoch [11/50] batch [280/288] time 0.197 (0.200) data 0.000 (0.001) loss 1.9617 (2.8489) teacher_loss 1.8997 (2.7740) loss_zs_kd 0.2724 (0.3807) loss_oracle 0.0621 (0.0749) acc 81.2500 (74.5871) lr 1.8443e-03 eta 0:37:32
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,420
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.2%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,010
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.7%
******* Domain a best val acc:      86.8%, epoch: 11 *******
******* Domain a best val test acc: 82.8%, epoch: 11 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [12/50] batch [20/288] time 0.161 (0.203) data 0.000 (0.013) loss 2.7787 (2.9134) teacher_loss 2.7403 (2.8359) loss_zs_kd 0.2926 (0.4066) loss_oracle 0.0384 (0.0776) acc 81.2500 (74.2188) lr 1.8090e-03 eta 0:37:52
epoch [12/50] batch [40/288] time 0.194 (0.198) data 0.000 (0.007) loss 3.1158 (2.8033) teacher_loss 3.0240 (2.7264) loss_zs_kd 0.2606 (0.3965) loss_oracle 0.0918 (0.0768) acc 78.1250 (74.1406) lr 1.8090e-03 eta 0:36:57
epoch [12/50] batch [60/288] time 0.193 (0.197) data 0.000 (0.005) loss 3.5800 (2.8779) teacher_loss 3.5099 (2.8019) loss_zs_kd 0.3839 (0.3917) loss_oracle 0.0701 (0.0760) acc 78.1250 (74.1146) lr 1.8090e-03 eta 0:36:44
epoch [12/50] batch [80/288] time 0.195 (0.196) data 0.001 (0.004) loss 5.9251 (2.9136) teacher_loss 5.8440 (2.8395) loss_zs_kd 0.4423 (0.3932) loss_oracle 0.0811 (0.0740) acc 56.2500 (74.4531) lr 1.8090e-03 eta 0:36:27
epoch [12/50] batch [100/288] time 0.191 (0.196) data 0.000 (0.003) loss 2.7162 (2.8668) teacher_loss 2.6397 (2.7921) loss_zs_kd 0.3761 (0.3825) loss_oracle 0.0765 (0.0747) acc 65.6250 (74.7188) lr 1.8090e-03 eta 0:36:17
epoch [12/50] batch [120/288] time 0.192 (0.195) data 0.000 (0.002) loss 3.7444 (2.8505) teacher_loss 3.6632 (2.7768) loss_zs_kd 0.3586 (0.3780) loss_oracle 0.0811 (0.0737) acc 65.6250 (74.9479) lr 1.8090e-03 eta 0:36:10
epoch [12/50] batch [140/288] time 0.196 (0.195) data 0.000 (0.002) loss 1.5761 (2.8558) teacher_loss 1.5094 (2.7828) loss_zs_kd 0.3097 (0.3831) loss_oracle 0.0667 (0.0730) acc 87.5000 (74.7098) lr 1.8090e-03 eta 0:36:04
epoch [12/50] batch [160/288] time 0.082 (0.194) data 0.000 (0.002) loss 3.3156 (2.8554) teacher_loss 3.2318 (2.7823) loss_zs_kd 0.5723 (0.3894) loss_oracle 0.0838 (0.0731) acc 71.8750 (74.6680) lr 1.8090e-03 eta 0:35:49
epoch [12/50] batch [180/288] time 0.123 (0.200) data 0.000 (0.002) loss 2.3983 (2.8985) teacher_loss 2.3412 (2.8251) loss_zs_kd 0.4058 (0.3911) loss_oracle 0.0571 (0.0734) acc 78.1250 (74.1146) lr 1.8090e-03 eta 0:36:44
epoch [12/50] batch [200/288] time 0.192 (0.203) data 0.000 (0.002) loss 3.3550 (2.9043) teacher_loss 3.2451 (2.8301) loss_zs_kd 0.3787 (0.3885) loss_oracle 0.1099 (0.0742) acc 65.6250 (74.2344) lr 1.8090e-03 eta 0:37:24
epoch [12/50] batch [220/288] time 0.188 (0.202) data 0.000 (0.001) loss 2.0307 (2.8927) teacher_loss 1.9330 (2.8178) loss_zs_kd 0.5911 (0.3917) loss_oracle 0.0977 (0.0750) acc 78.1250 (74.5028) lr 1.8090e-03 eta 0:37:09
epoch [12/50] batch [240/288] time 0.194 (0.201) data 0.000 (0.001) loss 3.7807 (2.8995) teacher_loss 3.7423 (2.8247) loss_zs_kd 0.4606 (0.3933) loss_oracle 0.0384 (0.0749) acc 65.6250 (74.4922) lr 1.8090e-03 eta 0:36:54
epoch [12/50] batch [260/288] time 0.192 (0.201) data 0.000 (0.001) loss 2.0835 (2.8914) teacher_loss 2.0027 (2.8168) loss_zs_kd 0.6273 (0.3953) loss_oracle 0.0807 (0.0746) acc 78.1250 (74.4351) lr 1.8090e-03 eta 0:36:42
epoch [12/50] batch [280/288] time 0.186 (0.200) data 0.000 (0.001) loss 2.8375 (2.8900) teacher_loss 2.7395 (2.8161) loss_zs_kd 0.3036 (0.3955) loss_oracle 0.0980 (0.0739) acc 75.0000 (74.4754) lr 1.8090e-03 eta 0:36:33
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,405
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,000
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.6%
******* Domain a best val acc:      86.8%, epoch: 11 *******
******* Domain a best val test acc: 82.8%, epoch: 11 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [13/50] batch [20/288] time 0.196 (0.212) data 0.000 (0.014) loss 2.0540 (2.7623) teacher_loss 1.9869 (2.6936) loss_zs_kd 0.4204 (0.3970) loss_oracle 0.0672 (0.0687) acc 81.2500 (75.7812) lr 1.7705e-03 eta 0:38:40
epoch [13/50] batch [40/288] time 0.196 (0.203) data 0.000 (0.007) loss 1.9407 (2.9932) teacher_loss 1.8835 (2.9233) loss_zs_kd 0.5477 (0.4034) loss_oracle 0.0572 (0.0699) acc 84.3750 (72.9688) lr 1.7705e-03 eta 0:36:58
epoch [13/50] batch [60/288] time 0.190 (0.200) data 0.000 (0.005) loss 4.5368 (3.1214) teacher_loss 4.4219 (3.0459) loss_zs_kd 0.4647 (0.3922) loss_oracle 0.1149 (0.0755) acc 62.5000 (72.0312) lr 1.7705e-03 eta 0:36:17
epoch [13/50] batch [80/288] time 0.198 (0.199) data 0.000 (0.004) loss 3.7268 (3.0406) teacher_loss 3.6595 (2.9651) loss_zs_kd 0.4905 (0.3922) loss_oracle 0.0672 (0.0756) acc 68.7500 (73.0469) lr 1.7705e-03 eta 0:36:00
epoch [13/50] batch [100/288] time 0.194 (0.198) data 0.000 (0.003) loss 2.3151 (2.9549) teacher_loss 2.2427 (2.8787) loss_zs_kd 0.1780 (0.3888) loss_oracle 0.0724 (0.0762) acc 84.3750 (74.0312) lr 1.7705e-03 eta 0:35:43
epoch [13/50] batch [120/288] time 0.190 (0.197) data 0.000 (0.003) loss 2.8178 (2.9314) teacher_loss 2.7096 (2.8555) loss_zs_kd 0.4031 (0.3890) loss_oracle 0.1082 (0.0758) acc 75.0000 (74.4531) lr 1.7705e-03 eta 0:35:34
epoch [13/50] batch [140/288] time 0.193 (0.197) data 0.000 (0.002) loss 2.5231 (2.9622) teacher_loss 2.4381 (2.8865) loss_zs_kd 0.3756 (0.3866) loss_oracle 0.0850 (0.0757) acc 78.1250 (74.1518) lr 1.7705e-03 eta 0:35:25
epoch [13/50] batch [160/288] time 0.192 (0.196) data 0.000 (0.002) loss 3.4318 (2.9259) teacher_loss 3.3429 (2.8504) loss_zs_kd 0.3523 (0.3829) loss_oracle 0.0889 (0.0755) acc 78.1250 (74.3359) lr 1.7705e-03 eta 0:35:16
epoch [13/50] batch [180/288] time 0.097 (0.201) data 0.000 (0.002) loss 2.1953 (2.9429) teacher_loss 2.1404 (2.8675) loss_zs_kd 0.2660 (0.3872) loss_oracle 0.0549 (0.0754) acc 78.1250 (74.2361) lr 1.7705e-03 eta 0:36:04
epoch [13/50] batch [200/288] time 0.191 (0.202) data 0.000 (0.002) loss 4.1929 (2.9354) teacher_loss 4.1212 (2.8602) loss_zs_kd 0.3099 (0.3934) loss_oracle 0.0717 (0.0752) acc 65.6250 (74.3125) lr 1.7705e-03 eta 0:36:05
epoch [13/50] batch [220/288] time 0.190 (0.201) data 0.000 (0.001) loss 2.3520 (2.9212) teacher_loss 2.2410 (2.8464) loss_zs_kd 0.5722 (0.3964) loss_oracle 0.1109 (0.0748) acc 81.2500 (74.3324) lr 1.7705e-03 eta 0:35:55
epoch [13/50] batch [240/288] time 0.196 (0.200) data 0.000 (0.001) loss 3.5710 (2.9079) teacher_loss 3.5209 (2.8335) loss_zs_kd 0.4132 (0.3934) loss_oracle 0.0502 (0.0745) acc 71.8750 (74.4141) lr 1.7705e-03 eta 0:35:45
epoch [13/50] batch [260/288] time 0.198 (0.200) data 0.000 (0.001) loss 2.8866 (2.8696) teacher_loss 2.8272 (2.7954) loss_zs_kd 0.3086 (0.3910) loss_oracle 0.0594 (0.0743) acc 78.1250 (74.7356) lr 1.7705e-03 eta 0:35:36
epoch [13/50] batch [280/288] time 0.196 (0.199) data 0.000 (0.001) loss 1.8841 (2.8747) teacher_loss 1.8184 (2.8009) loss_zs_kd 0.3226 (0.3893) loss_oracle 0.0657 (0.0738) acc 81.2500 (74.6652) lr 1.7705e-03 eta 0:35:27
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,400
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,992
* accuracy: 82.1%
* error: 17.9%
* macro_f1: 79.0%
******* Domain a best val acc:      86.8%, epoch: 11 *******
******* Domain a best val test acc: 82.8%, epoch: 11 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [14/50] batch [20/288] time 0.200 (0.209) data 0.000 (0.014) loss 2.9865 (2.9311) teacher_loss 2.9387 (2.8583) loss_zs_kd 0.3909 (0.3701) loss_oracle 0.0478 (0.0728) acc 75.0000 (73.1250) lr 1.7290e-03 eta 0:37:04
epoch [14/50] batch [40/288] time 0.191 (0.202) data 0.000 (0.007) loss 2.1015 (2.8077) teacher_loss 1.9981 (2.7322) loss_zs_kd 0.5408 (0.3818) loss_oracle 0.1033 (0.0755) acc 78.1250 (74.2188) lr 1.7290e-03 eta 0:35:39
epoch [14/50] batch [60/288] time 0.196 (0.199) data 0.001 (0.005) loss 2.2296 (2.7373) teacher_loss 2.1819 (2.6611) loss_zs_kd 0.3758 (0.3962) loss_oracle 0.0477 (0.0762) acc 84.3750 (74.9479) lr 1.7290e-03 eta 0:35:09
epoch [14/50] batch [80/288] time 0.189 (0.198) data 0.000 (0.004) loss 3.1373 (2.8187) teacher_loss 3.0266 (2.7412) loss_zs_kd 0.3476 (0.4044) loss_oracle 0.1107 (0.0774) acc 75.0000 (74.5703) lr 1.7290e-03 eta 0:34:51
epoch [14/50] batch [100/288] time 0.199 (0.197) data 0.000 (0.003) loss 3.3081 (2.8149) teacher_loss 3.2459 (2.7384) loss_zs_kd 0.4651 (0.4159) loss_oracle 0.0622 (0.0765) acc 78.1250 (74.5938) lr 1.7290e-03 eta 0:34:39
epoch [14/50] batch [120/288] time 0.195 (0.197) data 0.000 (0.003) loss 2.1224 (2.8596) teacher_loss 2.0633 (2.7831) loss_zs_kd 0.3816 (0.4078) loss_oracle 0.0591 (0.0764) acc 84.3750 (74.5833) lr 1.7290e-03 eta 0:34:30
epoch [14/50] batch [140/288] time 0.199 (0.196) data 0.000 (0.002) loss 2.2071 (2.8932) teacher_loss 2.1593 (2.8170) loss_zs_kd 0.3612 (0.4019) loss_oracle 0.0478 (0.0762) acc 84.3750 (74.5089) lr 1.7290e-03 eta 0:34:22
epoch [14/50] batch [160/288] time 0.189 (0.196) data 0.000 (0.002) loss 1.9063 (2.8493) teacher_loss 1.8319 (2.7729) loss_zs_kd 0.3311 (0.3971) loss_oracle 0.0744 (0.0764) acc 81.2500 (74.8828) lr 1.7290e-03 eta 0:34:18
epoch [14/50] batch [180/288] time 0.310 (0.204) data 0.000 (0.002) loss 1.9500 (2.8214) teacher_loss 1.9066 (2.7450) loss_zs_kd 0.2517 (0.3936) loss_oracle 0.0434 (0.0764) acc 84.3750 (74.9653) lr 1.7290e-03 eta 0:35:37
epoch [14/50] batch [200/288] time 0.150 (0.202) data 0.000 (0.002) loss 3.3098 (2.8343) teacher_loss 3.2021 (2.7595) loss_zs_kd 0.4473 (0.3936) loss_oracle 0.1077 (0.0748) acc 59.3750 (74.8125) lr 1.7290e-03 eta 0:35:17
epoch [14/50] batch [220/288] time 0.196 (0.202) data 0.000 (0.001) loss 1.1362 (2.8175) teacher_loss 1.0712 (2.7428) loss_zs_kd 0.4169 (0.3968) loss_oracle 0.0650 (0.0747) acc 84.3750 (74.9432) lr 1.7290e-03 eta 0:35:04
epoch [14/50] batch [240/288] time 0.179 (0.201) data 0.000 (0.001) loss 2.6072 (2.7995) teacher_loss 2.5122 (2.7246) loss_zs_kd 0.4520 (0.3982) loss_oracle 0.0950 (0.0749) acc 81.2500 (74.9870) lr 1.7290e-03 eta 0:34:53
epoch [14/50] batch [260/288] time 0.200 (0.200) data 0.000 (0.001) loss 3.3789 (2.8067) teacher_loss 3.3073 (2.7320) loss_zs_kd 0.4854 (0.3992) loss_oracle 0.0716 (0.0748) acc 75.0000 (74.9880) lr 1.7290e-03 eta 0:34:41
epoch [14/50] batch [280/288] time 0.166 (0.200) data 0.000 (0.001) loss 1.3829 (2.8027) teacher_loss 1.3035 (2.7281) loss_zs_kd 0.3086 (0.3975) loss_oracle 0.0794 (0.0746) acc 90.6250 (75.1004) lr 1.7290e-03 eta 0:34:31
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,419
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,992
* accuracy: 82.1%
* error: 17.9%
* macro_f1: 79.3%
******* Domain a best val acc:      86.8%, epoch: 11 *******
******* Domain a best val test acc: 82.8%, epoch: 11 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [15/50] batch [20/288] time 0.181 (0.208) data 0.000 (0.015) loss 3.0493 (2.8297) teacher_loss 2.9690 (2.7598) loss_zs_kd 0.3085 (0.3831) loss_oracle 0.0803 (0.0698) acc 75.0000 (75.6250) lr 1.6845e-03 eta 0:35:51
epoch [15/50] batch [40/288] time 0.195 (0.201) data 0.000 (0.008) loss 3.1162 (2.9935) teacher_loss 3.0207 (2.9247) loss_zs_kd 0.3851 (0.3842) loss_oracle 0.0955 (0.0688) acc 75.0000 (74.3750) lr 1.6845e-03 eta 0:34:40
epoch [15/50] batch [60/288] time 0.191 (0.199) data 0.001 (0.005) loss 1.7675 (2.8771) teacher_loss 1.6844 (2.8074) loss_zs_kd 0.2647 (0.3851) loss_oracle 0.0831 (0.0697) acc 84.3750 (75.1042) lr 1.6845e-03 eta 0:34:07
epoch [15/50] batch [80/288] time 0.195 (0.198) data 0.000 (0.004) loss 2.9438 (2.7732) teacher_loss 2.8621 (2.7020) loss_zs_kd 0.3351 (0.3856) loss_oracle 0.0816 (0.0712) acc 78.1250 (75.7031) lr 1.6845e-03 eta 0:33:51
epoch [15/50] batch [100/288] time 0.196 (0.197) data 0.001 (0.003) loss 2.8439 (2.7346) teacher_loss 2.7672 (2.6642) loss_zs_kd 0.3106 (0.3908) loss_oracle 0.0767 (0.0704) acc 71.8750 (75.6875) lr 1.6845e-03 eta 0:33:41
epoch [15/50] batch [120/288] time 0.198 (0.196) data 0.000 (0.003) loss 3.6530 (2.7551) teacher_loss 3.5965 (2.6852) loss_zs_kd 0.5456 (0.3897) loss_oracle 0.0565 (0.0699) acc 68.7500 (75.6250) lr 1.6845e-03 eta 0:33:32
epoch [15/50] batch [140/288] time 0.198 (0.196) data 0.000 (0.002) loss 2.8736 (2.7861) teacher_loss 2.8115 (2.7162) loss_zs_kd 0.3409 (0.3878) loss_oracle 0.0622 (0.0699) acc 71.8750 (75.2232) lr 1.6845e-03 eta 0:33:25
epoch [15/50] batch [160/288] time 0.199 (0.196) data 0.000 (0.002) loss 3.3213 (2.8137) teacher_loss 3.2596 (2.7438) loss_zs_kd 0.2683 (0.3874) loss_oracle 0.0617 (0.0700) acc 71.8750 (74.9609) lr 1.6845e-03 eta 0:33:19
epoch [15/50] batch [180/288] time 0.525 (0.196) data 0.001 (0.002) loss 4.3420 (2.8391) teacher_loss 4.2783 (2.7696) loss_zs_kd 0.4643 (0.3845) loss_oracle 0.0637 (0.0695) acc 71.8750 (74.9479) lr 1.6845e-03 eta 0:33:19
epoch [15/50] batch [200/288] time 0.085 (0.206) data 0.000 (0.002) loss 2.0225 (2.8506) teacher_loss 1.9579 (2.7812) loss_zs_kd 0.4601 (0.3847) loss_oracle 0.0646 (0.0695) acc 78.1250 (74.9531) lr 1.6845e-03 eta 0:34:51
epoch [15/50] batch [220/288] time 0.193 (0.203) data 0.000 (0.002) loss 3.4740 (2.8509) teacher_loss 3.3779 (2.7807) loss_zs_kd 0.3709 (0.3844) loss_oracle 0.0961 (0.0702) acc 75.0000 (74.9006) lr 1.6845e-03 eta 0:34:21
epoch [15/50] batch [240/288] time 0.195 (0.202) data 0.000 (0.001) loss 2.3666 (2.8308) teacher_loss 2.2734 (2.7605) loss_zs_kd 0.6878 (0.3881) loss_oracle 0.0932 (0.0703) acc 75.0000 (75.1562) lr 1.6845e-03 eta 0:34:10
epoch [15/50] batch [260/288] time 0.202 (0.202) data 0.000 (0.001) loss 3.3749 (2.8420) teacher_loss 3.3270 (2.7719) loss_zs_kd 0.3047 (0.3913) loss_oracle 0.0479 (0.0701) acc 75.0000 (75.0962) lr 1.6845e-03 eta 0:34:02
epoch [15/50] batch [280/288] time 0.192 (0.201) data 0.000 (0.001) loss 2.6932 (2.8341) teacher_loss 2.6332 (2.7637) loss_zs_kd 0.3770 (0.3892) loss_oracle 0.0600 (0.0704) acc 71.8750 (74.9888) lr 1.6845e-03 eta 0:33:48
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,436
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.6%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,999
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.0%
******* Domain a best val acc:      87.2%, epoch: 15 *******
******* Domain a best val test acc: 82.4%, epoch: 15 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [16/50] batch [20/288] time 0.161 (0.211) data 0.000 (0.016) loss 3.9971 (2.9209) teacher_loss 3.8542 (2.8442) loss_zs_kd 0.4917 (0.3891) loss_oracle 0.1430 (0.0767) acc 71.8750 (75.3125) lr 1.6374e-03 eta 0:35:25
epoch [16/50] batch [40/288] time 0.193 (0.204) data 0.000 (0.008) loss 3.1745 (2.8272) teacher_loss 3.0961 (2.7511) loss_zs_kd 0.3845 (0.3931) loss_oracle 0.0783 (0.0761) acc 71.8750 (75.5469) lr 1.6374e-03 eta 0:34:06
epoch [16/50] batch [60/288] time 0.186 (0.200) data 0.001 (0.005) loss 3.4233 (2.8018) teacher_loss 3.2522 (2.7246) loss_zs_kd 0.5405 (0.3945) loss_oracle 0.1711 (0.0773) acc 78.1250 (75.8854) lr 1.6374e-03 eta 0:33:28
epoch [16/50] batch [80/288] time 0.191 (0.199) data 0.000 (0.004) loss 3.8175 (2.7992) teacher_loss 3.7388 (2.7209) loss_zs_kd 0.3763 (0.3907) loss_oracle 0.0788 (0.0782) acc 65.6250 (75.4688) lr 1.6374e-03 eta 0:33:07
epoch [16/50] batch [100/288] time 0.195 (0.198) data 0.000 (0.003) loss 3.1767 (2.8597) teacher_loss 3.1289 (2.7829) loss_zs_kd 0.4235 (0.3934) loss_oracle 0.0478 (0.0768) acc 65.6250 (74.5000) lr 1.6374e-03 eta 0:32:54
epoch [16/50] batch [120/288] time 0.192 (0.197) data 0.000 (0.003) loss 2.3190 (2.8415) teacher_loss 2.2395 (2.7651) loss_zs_kd 0.4653 (0.3886) loss_oracle 0.0795 (0.0764) acc 84.3750 (74.6615) lr 1.6374e-03 eta 0:32:44
epoch [16/50] batch [140/288] time 0.193 (0.197) data 0.000 (0.002) loss 3.2147 (2.7844) teacher_loss 3.1402 (2.7092) loss_zs_kd 0.4005 (0.3885) loss_oracle 0.0746 (0.0752) acc 71.8750 (75.0000) lr 1.6374e-03 eta 0:32:36
epoch [16/50] batch [160/288] time 0.231 (0.197) data 0.000 (0.002) loss 3.0996 (2.7773) teacher_loss 3.0280 (2.7018) loss_zs_kd 0.3361 (0.3926) loss_oracle 0.0716 (0.0755) acc 78.1250 (75.1367) lr 1.6374e-03 eta 0:32:32
epoch [16/50] batch [180/288] time 0.468 (0.197) data 0.000 (0.002) loss 2.2330 (2.8146) teacher_loss 2.1615 (2.7402) loss_zs_kd 0.2882 (0.3920) loss_oracle 0.0716 (0.0744) acc 78.1250 (75.1215) lr 1.6374e-03 eta 0:32:32
epoch [16/50] batch [200/288] time 0.086 (0.205) data 0.000 (0.002) loss 1.9393 (2.8196) teacher_loss 1.8771 (2.7453) loss_zs_kd 0.4541 (0.3900) loss_oracle 0.0622 (0.0743) acc 75.0000 (75.0469) lr 1.6374e-03 eta 0:33:45
epoch [16/50] batch [220/288] time 0.200 (0.203) data 0.000 (0.002) loss 1.9375 (2.8074) teacher_loss 1.8925 (2.7327) loss_zs_kd 0.3840 (0.3915) loss_oracle 0.0450 (0.0747) acc 84.3750 (75.2273) lr 1.6374e-03 eta 0:33:19
epoch [16/50] batch [240/288] time 0.190 (0.202) data 0.000 (0.002) loss 2.0069 (2.7818) teacher_loss 1.9397 (2.7075) loss_zs_kd 0.5233 (0.3927) loss_oracle 0.0672 (0.0742) acc 84.3750 (75.4557) lr 1.6374e-03 eta 0:33:07
epoch [16/50] batch [260/288] time 0.196 (0.201) data 0.000 (0.001) loss 2.6170 (2.7937) teacher_loss 2.5453 (2.7197) loss_zs_kd 0.3194 (0.3949) loss_oracle 0.0717 (0.0740) acc 81.2500 (75.3846) lr 1.6374e-03 eta 0:32:58
epoch [16/50] batch [280/288] time 0.192 (0.201) data 0.000 (0.001) loss 3.9739 (2.8173) teacher_loss 3.8855 (2.7430) loss_zs_kd 0.4767 (0.3972) loss_oracle 0.0884 (0.0743) acc 65.6250 (75.2232) lr 1.6374e-03 eta 0:32:48
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,416
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,999
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 79.3%
******* Domain a best val acc:      87.2%, epoch: 15 *******
******* Domain a best val test acc: 82.4%, epoch: 15 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [17/50] batch [20/288] time 0.187 (0.206) data 0.000 (0.013) loss 4.3921 (2.4563) teacher_loss 4.3248 (2.3720) loss_zs_kd 0.3353 (0.3559) loss_oracle 0.0673 (0.0843) acc 75.0000 (79.6875) lr 1.5878e-03 eta 0:33:31
epoch [17/50] batch [40/288] time 0.190 (0.200) data 0.000 (0.007) loss 2.8160 (2.7134) teacher_loss 2.7493 (2.6331) loss_zs_kd 0.4445 (0.3718) loss_oracle 0.0667 (0.0804) acc 71.8750 (77.6562) lr 1.5878e-03 eta 0:32:28
epoch [17/50] batch [60/288] time 0.190 (0.198) data 0.000 (0.005) loss 4.0557 (2.7423) teacher_loss 3.9596 (2.6631) loss_zs_kd 0.3527 (0.3795) loss_oracle 0.0961 (0.0792) acc 65.6250 (76.8229) lr 1.5878e-03 eta 0:32:06
epoch [17/50] batch [80/288] time 0.191 (0.197) data 0.000 (0.003) loss 2.1040 (2.7337) teacher_loss 2.0171 (2.6537) loss_zs_kd 0.4197 (0.3864) loss_oracle 0.0869 (0.0800) acc 78.1250 (76.6406) lr 1.5878e-03 eta 0:31:52
epoch [17/50] batch [100/288] time 0.203 (0.196) data 0.001 (0.003) loss 3.1287 (2.7214) teacher_loss 3.0716 (2.6432) loss_zs_kd 0.3506 (0.3893) loss_oracle 0.0572 (0.0782) acc 65.6250 (76.5000) lr 1.5878e-03 eta 0:31:44
epoch [17/50] batch [120/288] time 0.195 (0.196) data 0.000 (0.002) loss 2.9688 (2.7363) teacher_loss 2.9305 (2.6587) loss_zs_kd 0.2649 (0.3891) loss_oracle 0.0383 (0.0776) acc 75.0000 (75.9375) lr 1.5878e-03 eta 0:31:36
epoch [17/50] batch [140/288] time 0.196 (0.196) data 0.000 (0.002) loss 3.5481 (2.8084) teacher_loss 3.4860 (2.7329) loss_zs_kd 0.3682 (0.3933) loss_oracle 0.0622 (0.0754) acc 68.7500 (75.2902) lr 1.5878e-03 eta 0:31:30
epoch [17/50] batch [160/288] time 0.192 (0.196) data 0.000 (0.002) loss 2.8537 (2.8110) teacher_loss 2.7755 (2.7364) loss_zs_kd 0.3528 (0.3933) loss_oracle 0.0782 (0.0746) acc 71.8750 (75.3711) lr 1.5878e-03 eta 0:31:24
epoch [17/50] batch [180/288] time 0.094 (0.193) data 0.000 (0.002) loss 2.3824 (2.7848) teacher_loss 2.2965 (2.7105) loss_zs_kd 0.3329 (0.3905) loss_oracle 0.0860 (0.0743) acc 81.2500 (75.4688) lr 1.5878e-03 eta 0:30:57
epoch [17/50] batch [200/288] time 0.098 (0.198) data 0.000 (0.002) loss 2.6988 (2.7783) teacher_loss 2.6421 (2.7040) loss_zs_kd 0.3025 (0.3912) loss_oracle 0.0568 (0.0743) acc 75.0000 (75.5625) lr 1.5878e-03 eta 0:31:40
epoch [17/50] batch [220/288] time 0.196 (0.201) data 0.000 (0.001) loss 2.1598 (2.7735) teacher_loss 2.1026 (2.6989) loss_zs_kd 0.2520 (0.3971) loss_oracle 0.0571 (0.0746) acc 81.2500 (75.5966) lr 1.5878e-03 eta 0:32:05
epoch [17/50] batch [240/288] time 0.194 (0.200) data 0.000 (0.001) loss 2.5037 (2.7662) teacher_loss 2.4582 (2.6921) loss_zs_kd 0.3571 (0.3980) loss_oracle 0.0455 (0.0741) acc 78.1250 (75.5990) lr 1.5878e-03 eta 0:31:54
epoch [17/50] batch [260/288] time 0.190 (0.200) data 0.000 (0.001) loss 3.8601 (2.7688) teacher_loss 3.7553 (2.6945) loss_zs_kd 0.3936 (0.4012) loss_oracle 0.1049 (0.0743) acc 68.7500 (75.5048) lr 1.5878e-03 eta 0:31:44
epoch [17/50] batch [280/288] time 0.195 (0.199) data 0.000 (0.001) loss 3.2265 (2.7617) teacher_loss 3.1405 (2.6870) loss_zs_kd 0.5991 (0.4057) loss_oracle 0.0860 (0.0747) acc 71.8750 (75.5357) lr 1.5878e-03 eta 0:31:36
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,416
* accuracy: 86.7%
* error: 13.3%
* macro_f1: 86.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,985
* accuracy: 81.8%
* error: 18.2%
* macro_f1: 78.7%
******* Domain a best val acc:      87.2%, epoch: 15 *******
******* Domain a best val test acc: 82.4%, epoch: 15 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [18/50] batch [20/288] time 0.186 (0.207) data 0.000 (0.014) loss 3.4042 (2.7520) teacher_loss 3.3088 (2.6803) loss_zs_kd 0.5095 (0.4446) loss_oracle 0.0954 (0.0717) acc 68.7500 (76.4062) lr 1.5358e-03 eta 0:32:45
epoch [18/50] batch [40/288] time 0.195 (0.199) data 0.000 (0.007) loss 4.1403 (2.8598) teacher_loss 4.0615 (2.7890) loss_zs_kd 0.6630 (0.4458) loss_oracle 0.0788 (0.0708) acc 62.5000 (74.4531) lr 1.5358e-03 eta 0:31:22
epoch [18/50] batch [60/288] time 0.193 (0.193) data 0.000 (0.005) loss 2.1292 (2.8566) teacher_loss 2.0505 (2.7833) loss_zs_kd 0.4701 (0.4306) loss_oracle 0.0788 (0.0733) acc 81.2500 (74.7917) lr 1.5358e-03 eta 0:30:23
epoch [18/50] batch [80/288] time 0.192 (0.194) data 0.000 (0.004) loss 5.7067 (2.9183) teacher_loss 5.6279 (2.8467) loss_zs_kd 0.5275 (0.4253) loss_oracle 0.0788 (0.0716) acc 53.1250 (74.4922) lr 1.5358e-03 eta 0:30:24
epoch [18/50] batch [100/288] time 0.192 (0.194) data 0.000 (0.003) loss 2.4143 (2.8785) teacher_loss 2.3143 (2.8076) loss_zs_kd 0.3512 (0.4213) loss_oracle 0.1000 (0.0710) acc 84.3750 (74.9688) lr 1.5358e-03 eta 0:30:21
epoch [18/50] batch [120/288] time 0.193 (0.194) data 0.000 (0.003) loss 3.4051 (2.8480) teacher_loss 3.3540 (2.7771) loss_zs_kd 0.3990 (0.4179) loss_oracle 0.0511 (0.0709) acc 75.0000 (75.2344) lr 1.5358e-03 eta 0:30:17
epoch [18/50] batch [140/288] time 0.216 (0.194) data 0.000 (0.002) loss 3.1638 (2.8645) teacher_loss 3.1305 (2.7940) loss_zs_kd 0.3628 (0.4186) loss_oracle 0.0333 (0.0705) acc 71.8750 (74.9330) lr 1.5358e-03 eta 0:30:14
epoch [18/50] batch [160/288] time 0.193 (0.194) data 0.000 (0.002) loss 2.7698 (2.8469) teacher_loss 2.6736 (2.7760) loss_zs_kd 0.5424 (0.4205) loss_oracle 0.0961 (0.0709) acc 75.0000 (75.2148) lr 1.5358e-03 eta 0:30:10
epoch [18/50] batch [180/288] time 0.199 (0.194) data 0.000 (0.002) loss 2.6173 (2.8223) teacher_loss 2.5406 (2.7509) loss_zs_kd 0.6650 (0.4242) loss_oracle 0.0767 (0.0715) acc 68.7500 (75.3993) lr 1.5358e-03 eta 0:30:08
epoch [18/50] batch [200/288] time 0.092 (0.202) data 0.000 (0.002) loss 3.4469 (2.7983) teacher_loss 3.3819 (2.7267) loss_zs_kd 0.4504 (0.4239) loss_oracle 0.0650 (0.0716) acc 68.7500 (75.4531) lr 1.5358e-03 eta 0:31:21
epoch [18/50] batch [220/288] time 0.194 (0.204) data 0.000 (0.001) loss 4.0017 (2.8037) teacher_loss 3.9300 (2.7321) loss_zs_kd 0.3546 (0.4251) loss_oracle 0.0716 (0.0717) acc 62.5000 (75.3693) lr 1.5358e-03 eta 0:31:37
epoch [18/50] batch [240/288] time 0.194 (0.203) data 0.000 (0.001) loss 2.0346 (2.8144) teacher_loss 2.0158 (2.7429) loss_zs_kd 0.4453 (0.4277) loss_oracle 0.0189 (0.0715) acc 81.2500 (75.1953) lr 1.5358e-03 eta 0:31:23
epoch [18/50] batch [260/288] time 0.192 (0.203) data 0.000 (0.001) loss 3.2382 (2.8158) teacher_loss 3.1854 (2.7441) loss_zs_kd 0.2412 (0.4234) loss_oracle 0.0528 (0.0717) acc 75.0000 (75.3125) lr 1.5358e-03 eta 0:31:12
epoch [18/50] batch [280/288] time 0.203 (0.202) data 0.000 (0.001) loss 3.5203 (2.8291) teacher_loss 3.4488 (2.7573) loss_zs_kd 0.5806 (0.4227) loss_oracle 0.0715 (0.0717) acc 68.7500 (75.2679) lr 1.5358e-03 eta 0:30:59
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,421
* accuracy: 86.8%
* error: 13.2%
* macro_f1: 86.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,994
* accuracy: 82.2%
* error: 17.8%
* macro_f1: 79.0%
******* Domain a best val acc:      87.2%, epoch: 15 *******
******* Domain a best val test acc: 82.4%, epoch: 15 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [19/50] batch [20/288] time 0.189 (0.204) data 0.000 (0.013) loss 2.7630 (2.6141) teacher_loss 2.6818 (2.5408) loss_zs_kd 0.3436 (0.4075) loss_oracle 0.0813 (0.0733) acc 75.0000 (76.2500) lr 1.4818e-03 eta 0:31:12
epoch [19/50] batch [40/288] time 0.194 (0.197) data 0.000 (0.006) loss 2.9332 (2.8324) teacher_loss 2.8571 (2.7567) loss_zs_kd 0.4288 (0.4160) loss_oracle 0.0760 (0.0757) acc 68.7500 (74.2188) lr 1.4818e-03 eta 0:30:04
epoch [19/50] batch [60/288] time 0.197 (0.196) data 0.000 (0.004) loss 3.7548 (2.9829) teacher_loss 3.6737 (2.9065) loss_zs_kd 0.4866 (0.4295) loss_oracle 0.0811 (0.0764) acc 65.6250 (73.1771) lr 1.4818e-03 eta 0:29:51
epoch [19/50] batch [80/288] time 0.194 (0.195) data 0.000 (0.003) loss 2.3996 (2.8530) teacher_loss 2.3236 (2.7762) loss_zs_kd 0.3979 (0.4256) loss_oracle 0.0760 (0.0768) acc 68.7500 (74.0625) lr 1.4818e-03 eta 0:29:42
epoch [19/50] batch [100/288] time 0.197 (0.195) data 0.000 (0.003) loss 2.9760 (2.8346) teacher_loss 2.9478 (2.7594) loss_zs_kd 0.3917 (0.4295) loss_oracle 0.0283 (0.0752) acc 75.0000 (74.2812) lr 1.4818e-03 eta 0:29:37
epoch [19/50] batch [120/288] time 0.193 (0.195) data 0.000 (0.002) loss 4.7364 (2.8318) teacher_loss 4.6836 (2.7567) loss_zs_kd 0.4299 (0.4321) loss_oracle 0.0528 (0.0752) acc 59.3750 (74.4010) lr 1.4818e-03 eta 0:29:32
epoch [19/50] batch [140/288] time 0.191 (0.195) data 0.000 (0.002) loss 3.5725 (2.8320) teacher_loss 3.4985 (2.7572) loss_zs_kd 0.3277 (0.4302) loss_oracle 0.0740 (0.0749) acc 78.1250 (74.7098) lr 1.4818e-03 eta 0:29:26
epoch [19/50] batch [160/288] time 0.194 (0.195) data 0.001 (0.002) loss 2.4915 (2.8387) teacher_loss 2.3977 (2.7630) loss_zs_kd 0.3708 (0.4215) loss_oracle 0.0938 (0.0757) acc 78.1250 (74.6289) lr 1.4818e-03 eta 0:29:21
epoch [19/50] batch [180/288] time 0.190 (0.195) data 0.000 (0.002) loss 2.7763 (2.8353) teacher_loss 2.6852 (2.7596) loss_zs_kd 0.3264 (0.4189) loss_oracle 0.0911 (0.0757) acc 75.0000 (74.5833) lr 1.4818e-03 eta 0:29:23
epoch [19/50] batch [200/288] time 0.082 (0.202) data 0.000 (0.001) loss 1.6724 (2.8067) teacher_loss 1.6053 (2.7311) loss_zs_kd 0.4495 (0.4180) loss_oracle 0.0671 (0.0756) acc 87.5000 (74.8438) lr 1.4818e-03 eta 0:30:25
epoch [19/50] batch [220/288] time 0.205 (0.203) data 0.000 (0.001) loss 2.8905 (2.8274) teacher_loss 2.8472 (2.7525) loss_zs_kd 0.6021 (0.4264) loss_oracle 0.0434 (0.0749) acc 75.0000 (74.7869) lr 1.4818e-03 eta 0:30:25
epoch [19/50] batch [240/288] time 0.190 (0.202) data 0.000 (0.001) loss 1.9137 (2.8187) teacher_loss 1.8255 (2.7434) loss_zs_kd 0.6887 (0.4276) loss_oracle 0.0882 (0.0753) acc 84.3750 (74.9740) lr 1.4818e-03 eta 0:30:14
epoch [19/50] batch [260/288] time 0.193 (0.202) data 0.000 (0.001) loss 3.0455 (2.8049) teacher_loss 2.9635 (2.7298) loss_zs_kd 0.4335 (0.4279) loss_oracle 0.0820 (0.0751) acc 75.0000 (75.1082) lr 1.4818e-03 eta 0:30:05
epoch [19/50] batch [280/288] time 0.194 (0.201) data 0.000 (0.001) loss 4.6256 (2.8103) teacher_loss 4.5272 (2.7352) loss_zs_kd 0.5095 (0.4275) loss_oracle 0.0984 (0.0751) acc 56.2500 (74.9888) lr 1.4818e-03 eta 0:29:56
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,439
* accuracy: 87.3%
* error: 12.7%
* macro_f1: 86.7%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,998
* accuracy: 82.3%
* error: 17.7%
* macro_f1: 79.0%
******* Domain a best val acc:      87.3%, epoch: 19 *******
******* Domain a best val test acc: 82.3%, epoch: 19 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [20/50] batch [20/288] time 0.192 (0.217) data 0.000 (0.014) loss 4.0343 (2.6691) teacher_loss 3.9049 (2.5932) loss_zs_kd 0.4370 (0.4228) loss_oracle 0.1294 (0.0759) acc 62.5000 (75.6250) lr 1.4258e-03 eta 0:32:09
epoch [20/50] batch [40/288] time 0.195 (0.206) data 0.000 (0.007) loss 2.8187 (2.6799) teacher_loss 2.7616 (2.6075) loss_zs_kd 0.6799 (0.4470) loss_oracle 0.0571 (0.0724) acc 78.1250 (75.6250) lr 1.4258e-03 eta 0:30:27
epoch [20/50] batch [60/288] time 0.198 (0.202) data 0.000 (0.005) loss 2.4684 (2.6742) teacher_loss 2.4112 (2.5982) loss_zs_kd 0.2370 (0.4408) loss_oracle 0.0572 (0.0759) acc 78.1250 (76.0417) lr 1.4258e-03 eta 0:29:48
epoch [20/50] batch [80/288] time 0.194 (0.200) data 0.000 (0.004) loss 2.0747 (2.6890) teacher_loss 2.0000 (2.6147) loss_zs_kd 0.3537 (0.4398) loss_oracle 0.0747 (0.0743) acc 81.2500 (75.9375) lr 1.4258e-03 eta 0:29:27
epoch [20/50] batch [100/288] time 0.191 (0.199) data 0.000 (0.003) loss 3.2941 (2.6888) teacher_loss 3.2486 (2.6138) loss_zs_kd 0.3773 (0.4341) loss_oracle 0.0455 (0.0750) acc 71.8750 (76.0312) lr 1.4258e-03 eta 0:29:13
epoch [20/50] batch [120/288] time 0.191 (0.198) data 0.000 (0.002) loss 1.4598 (2.7511) teacher_loss 1.3824 (2.6755) loss_zs_kd 0.2324 (0.4323) loss_oracle 0.0774 (0.0755) acc 87.5000 (75.7292) lr 1.4258e-03 eta 0:29:03
epoch [20/50] batch [140/288] time 0.195 (0.197) data 0.000 (0.002) loss 2.3312 (2.7604) teacher_loss 2.2574 (2.6852) loss_zs_kd 0.3959 (0.4355) loss_oracle 0.0738 (0.0752) acc 81.2500 (75.7589) lr 1.4258e-03 eta 0:28:54
epoch [20/50] batch [160/288] time 0.203 (0.197) data 0.000 (0.002) loss 3.0740 (2.7483) teacher_loss 3.0069 (2.6738) loss_zs_kd 0.3224 (0.4355) loss_oracle 0.0671 (0.0745) acc 75.0000 (75.8789) lr 1.4258e-03 eta 0:28:47
epoch [20/50] batch [180/288] time 0.190 (0.197) data 0.000 (0.002) loss 2.4373 (2.7243) teacher_loss 2.3773 (2.6503) loss_zs_kd 0.3465 (0.4349) loss_oracle 0.0601 (0.0740) acc 81.2500 (76.0243) lr 1.4258e-03 eta 0:28:41
epoch [20/50] batch [200/288] time 0.082 (0.202) data 0.000 (0.002) loss 3.9490 (2.7350) teacher_loss 3.9042 (2.6623) loss_zs_kd 0.6476 (0.4334) loss_oracle 0.0449 (0.0727) acc 59.3750 (75.7188) lr 1.4258e-03 eta 0:29:20
epoch [20/50] batch [220/288] time 0.193 (0.204) data 0.000 (0.001) loss 2.2106 (2.7023) teacher_loss 2.1600 (2.6294) loss_zs_kd 0.3509 (0.4330) loss_oracle 0.0505 (0.0729) acc 78.1250 (75.8807) lr 1.4258e-03 eta 0:29:37
epoch [20/50] batch [240/288] time 0.190 (0.203) data 0.000 (0.001) loss 3.3885 (2.6994) teacher_loss 3.3024 (2.6261) loss_zs_kd 0.3753 (0.4348) loss_oracle 0.0861 (0.0732) acc 65.6250 (75.9115) lr 1.4258e-03 eta 0:29:23
epoch [20/50] batch [260/288] time 0.194 (0.202) data 0.000 (0.001) loss 3.7121 (2.6841) teacher_loss 3.6429 (2.6111) loss_zs_kd 0.3353 (0.4349) loss_oracle 0.0691 (0.0730) acc 68.7500 (76.1058) lr 1.4258e-03 eta 0:29:13
epoch [20/50] batch [280/288] time 0.188 (0.202) data 0.000 (0.001) loss 1.7740 (2.6784) teacher_loss 1.7068 (2.6054) loss_zs_kd 0.4027 (0.4320) loss_oracle 0.0672 (0.0730) acc 87.5000 (76.2277) lr 1.4258e-03 eta 0:29:02
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,435
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,991
* accuracy: 82.0%
* error: 18.0%
* macro_f1: 78.9%
******* Domain a best val acc:      87.3%, epoch: 19 *******
******* Domain a best val test acc: 82.3%, epoch: 19 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [21/50] batch [20/288] time 0.188 (0.212) data 0.000 (0.013) loss 1.1978 (2.5148) teacher_loss 1.1016 (2.4438) loss_zs_kd 0.4835 (0.4589) loss_oracle 0.0962 (0.0710) acc 90.6250 (77.6562) lr 1.3681e-03 eta 0:30:26
epoch [21/50] batch [40/288] time 0.162 (0.202) data 0.000 (0.007) loss 1.4057 (2.5444) teacher_loss 1.3193 (2.4694) loss_zs_kd 0.5009 (0.4459) loss_oracle 0.0864 (0.0751) acc 87.5000 (76.2500) lr 1.3681e-03 eta 0:28:57
epoch [21/50] batch [60/288] time 0.190 (0.200) data 0.000 (0.004) loss 2.2532 (2.6337) teacher_loss 2.1851 (2.5593) loss_zs_kd 0.4438 (0.4353) loss_oracle 0.0681 (0.0744) acc 81.2500 (75.8333) lr 1.3681e-03 eta 0:28:35
epoch [21/50] batch [80/288] time 0.188 (0.198) data 0.000 (0.003) loss 1.7391 (2.7288) teacher_loss 1.6415 (2.6533) loss_zs_kd 0.2410 (0.4312) loss_oracle 0.0977 (0.0754) acc 87.5000 (75.3125) lr 1.3681e-03 eta 0:28:17
epoch [21/50] batch [100/288] time 0.196 (0.197) data 0.000 (0.003) loss 2.9458 (2.6838) teacher_loss 2.8864 (2.6089) loss_zs_kd 0.2779 (0.4294) loss_oracle 0.0594 (0.0749) acc 78.1250 (75.8438) lr 1.3681e-03 eta 0:28:06
epoch [21/50] batch [120/288] time 0.194 (0.197) data 0.000 (0.002) loss 2.1767 (2.6993) teacher_loss 2.0719 (2.6240) loss_zs_kd 0.3418 (0.4289) loss_oracle 0.1048 (0.0753) acc 81.2500 (75.9375) lr 1.3681e-03 eta 0:27:57
epoch [21/50] batch [140/288] time 0.199 (0.197) data 0.000 (0.002) loss 2.9061 (2.7367) teacher_loss 2.8200 (2.6613) loss_zs_kd 0.4775 (0.4263) loss_oracle 0.0861 (0.0754) acc 78.1250 (75.6027) lr 1.3681e-03 eta 0:27:50
epoch [21/50] batch [160/288] time 0.199 (0.196) data 0.000 (0.002) loss 2.7896 (2.7763) teacher_loss 2.7274 (2.7008) loss_zs_kd 0.4851 (0.4368) loss_oracle 0.0622 (0.0755) acc 65.6250 (75.1367) lr 1.3681e-03 eta 0:27:43
epoch [21/50] batch [180/288] time 0.186 (0.196) data 0.000 (0.002) loss 3.3320 (2.7888) teacher_loss 3.2342 (2.7128) loss_zs_kd 0.3360 (0.4410) loss_oracle 0.0978 (0.0760) acc 68.7500 (74.9653) lr 1.3681e-03 eta 0:27:38
epoch [21/50] batch [200/288] time 0.082 (0.201) data 0.000 (0.001) loss 3.8270 (2.7719) teacher_loss 3.7188 (2.6959) loss_zs_kd 0.3473 (0.4388) loss_oracle 0.1082 (0.0761) acc 68.7500 (75.0625) lr 1.3681e-03 eta 0:28:19
epoch [21/50] batch [220/288] time 0.191 (0.202) data 0.000 (0.001) loss 2.4748 (2.7710) teacher_loss 2.3910 (2.6944) loss_zs_kd 0.2774 (0.4388) loss_oracle 0.0838 (0.0766) acc 81.2500 (75.0000) lr 1.3681e-03 eta 0:28:23
epoch [21/50] batch [240/288] time 0.195 (0.202) data 0.000 (0.001) loss 2.4963 (2.8003) teacher_loss 2.4197 (2.7242) loss_zs_kd 0.5196 (0.4360) loss_oracle 0.0766 (0.0762) acc 87.5000 (74.8828) lr 1.3681e-03 eta 0:28:14
epoch [21/50] batch [260/288] time 0.196 (0.201) data 0.000 (0.001) loss 2.7080 (2.7888) teacher_loss 2.6218 (2.7128) loss_zs_kd 0.4327 (0.4378) loss_oracle 0.0862 (0.0760) acc 75.0000 (74.9639) lr 1.3681e-03 eta 0:28:05
epoch [21/50] batch [280/288] time 0.195 (0.201) data 0.000 (0.001) loss 3.0194 (2.7877) teacher_loss 2.9360 (2.7113) loss_zs_kd 0.3769 (0.4397) loss_oracle 0.0834 (0.0764) acc 78.1250 (75.0670) lr 1.3681e-03 eta 0:27:56
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,434
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,007
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 79.4%
******* Domain a best val acc:      87.3%, epoch: 19 *******
******* Domain a best val test acc: 82.3%, epoch: 19 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [22/50] batch [20/288] time 0.190 (0.203) data 0.000 (0.013) loss 4.0281 (2.7445) teacher_loss 3.9288 (2.6693) loss_zs_kd 0.4449 (0.4615) loss_oracle 0.0993 (0.0752) acc 62.5000 (75.0000) lr 1.3090e-03 eta 0:28:09
epoch [22/50] batch [40/288] time 0.199 (0.199) data 0.000 (0.007) loss 2.9256 (2.6742) teacher_loss 2.8778 (2.6013) loss_zs_kd 0.3444 (0.4428) loss_oracle 0.0477 (0.0729) acc 78.1250 (75.9375) lr 1.3090e-03 eta 0:27:31
epoch [22/50] batch [60/288] time 0.192 (0.197) data 0.000 (0.004) loss 1.7402 (2.7286) teacher_loss 1.6758 (2.6586) loss_zs_kd 0.5366 (0.4429) loss_oracle 0.0644 (0.0700) acc 81.2500 (75.1562) lr 1.3090e-03 eta 0:27:15
epoch [22/50] batch [80/288] time 0.190 (0.197) data 0.000 (0.003) loss 4.2085 (2.6875) teacher_loss 4.1059 (2.6168) loss_zs_kd 0.5363 (0.4494) loss_oracle 0.1026 (0.0707) acc 59.3750 (75.6250) lr 1.3090e-03 eta 0:27:05
epoch [22/50] batch [100/288] time 0.195 (0.196) data 0.000 (0.003) loss 1.7893 (2.6924) teacher_loss 1.7315 (2.6199) loss_zs_kd 0.2626 (0.4373) loss_oracle 0.0579 (0.0725) acc 87.5000 (75.6250) lr 1.3090e-03 eta 0:26:58
epoch [22/50] batch [120/288] time 0.195 (0.196) data 0.000 (0.002) loss 2.8599 (2.7230) teacher_loss 2.7833 (2.6501) loss_zs_kd 0.4312 (0.4373) loss_oracle 0.0766 (0.0729) acc 71.8750 (75.4167) lr 1.3090e-03 eta 0:26:52
epoch [22/50] batch [140/288] time 0.196 (0.196) data 0.000 (0.002) loss 2.6877 (2.6995) teacher_loss 2.6306 (2.6270) loss_zs_kd 0.4393 (0.4292) loss_oracle 0.0572 (0.0725) acc 81.2500 (75.7366) lr 1.3090e-03 eta 0:26:47
epoch [22/50] batch [160/288] time 0.193 (0.195) data 0.000 (0.002) loss 1.9620 (2.7290) teacher_loss 1.9193 (2.6558) loss_zs_kd 0.2943 (0.4301) loss_oracle 0.0427 (0.0733) acc 87.5000 (75.7617) lr 1.3090e-03 eta 0:26:41
epoch [22/50] batch [180/288] time 0.190 (0.196) data 0.000 (0.002) loss 2.8317 (2.7178) teacher_loss 2.7499 (2.6446) loss_zs_kd 0.2275 (0.4291) loss_oracle 0.0818 (0.0732) acc 81.2500 (75.9028) lr 1.3090e-03 eta 0:26:38
epoch [22/50] batch [200/288] time 0.084 (0.203) data 0.000 (0.001) loss 2.7376 (2.7274) teacher_loss 2.6622 (2.6540) loss_zs_kd 0.4213 (0.4279) loss_oracle 0.0754 (0.0734) acc 81.2500 (75.7656) lr 1.3090e-03 eta 0:27:34
epoch [22/50] batch [220/288] time 0.196 (0.203) data 0.000 (0.001) loss 1.2912 (2.7306) teacher_loss 1.2384 (2.6571) loss_zs_kd 0.4201 (0.4306) loss_oracle 0.0528 (0.0734) acc 93.7500 (75.6676) lr 1.3090e-03 eta 0:27:33
epoch [22/50] batch [240/288] time 0.199 (0.203) data 0.000 (0.001) loss 2.8446 (2.7396) teacher_loss 2.7802 (2.6663) loss_zs_kd 0.4564 (0.4305) loss_oracle 0.0644 (0.0733) acc 71.8750 (75.5990) lr 1.3090e-03 eta 0:27:23
epoch [22/50] batch [260/288] time 0.192 (0.202) data 0.000 (0.001) loss 2.4707 (2.7451) teacher_loss 2.3831 (2.6720) loss_zs_kd 0.4251 (0.4284) loss_oracle 0.0876 (0.0731) acc 78.1250 (75.6731) lr 1.3090e-03 eta 0:27:15
epoch [22/50] batch [280/288] time 0.194 (0.202) data 0.000 (0.001) loss 2.0053 (2.7542) teacher_loss 1.9403 (2.6800) loss_zs_kd 0.4113 (0.4289) loss_oracle 0.0650 (0.0741) acc 84.3750 (75.5580) lr 1.3090e-03 eta 0:27:06
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,435
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,970
* accuracy: 81.2%
* error: 18.8%
* macro_f1: 77.6%
******* Domain a best val acc:      87.3%, epoch: 19 *******
******* Domain a best val test acc: 82.3%, epoch: 19 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [23/50] batch [20/288] time 0.197 (0.206) data 0.000 (0.012) loss 3.0730 (2.5952) teacher_loss 3.0158 (2.5237) loss_zs_kd 0.4407 (0.4280) loss_oracle 0.0572 (0.0715) acc 71.8750 (76.4062) lr 1.2487e-03 eta 0:27:35
epoch [23/50] batch [40/288] time 0.199 (0.197) data 0.000 (0.006) loss 2.6259 (2.5976) teacher_loss 2.5683 (2.5256) loss_zs_kd 0.4050 (0.4380) loss_oracle 0.0577 (0.0720) acc 75.0000 (76.4844) lr 1.2487e-03 eta 0:26:22
epoch [23/50] batch [60/288] time 0.191 (0.196) data 0.000 (0.004) loss 3.1194 (2.6777) teacher_loss 3.0233 (2.6055) loss_zs_kd 0.5376 (0.4470) loss_oracle 0.0962 (0.0722) acc 68.7500 (76.0417) lr 1.2487e-03 eta 0:26:11
epoch [23/50] batch [80/288] time 0.192 (0.195) data 0.000 (0.003) loss 3.2748 (2.6567) teacher_loss 3.2040 (2.5850) loss_zs_kd 0.5115 (0.4528) loss_oracle 0.0708 (0.0717) acc 71.8750 (76.4062) lr 1.2487e-03 eta 0:26:00
epoch [23/50] batch [100/288] time 0.191 (0.195) data 0.000 (0.003) loss 2.3351 (2.6442) teacher_loss 2.2586 (2.5719) loss_zs_kd 0.5746 (0.4575) loss_oracle 0.0765 (0.0724) acc 81.2500 (76.5625) lr 1.2487e-03 eta 0:25:54
epoch [23/50] batch [120/288] time 0.191 (0.195) data 0.000 (0.002) loss 1.5879 (2.6587) teacher_loss 1.5207 (2.5861) loss_zs_kd 0.5475 (0.4621) loss_oracle 0.0672 (0.0726) acc 90.6250 (76.5625) lr 1.2487e-03 eta 0:25:49
epoch [23/50] batch [140/288] time 0.199 (0.195) data 0.000 (0.002) loss 2.5757 (2.6585) teacher_loss 2.5613 (2.5865) loss_zs_kd 0.3357 (0.4578) loss_oracle 0.0144 (0.0720) acc 75.0000 (76.6071) lr 1.2487e-03 eta 0:25:45
epoch [23/50] batch [160/288] time 0.194 (0.195) data 0.000 (0.002) loss 2.9210 (2.6822) teacher_loss 2.8132 (2.6104) loss_zs_kd 0.3450 (0.4575) loss_oracle 0.1077 (0.0718) acc 75.0000 (76.4062) lr 1.2487e-03 eta 0:25:41
epoch [23/50] batch [180/288] time 0.195 (0.195) data 0.001 (0.002) loss 2.4292 (2.6698) teacher_loss 2.3672 (2.5976) loss_zs_kd 0.3110 (0.4576) loss_oracle 0.0620 (0.0722) acc 84.3750 (76.5278) lr 1.2487e-03 eta 0:25:37
epoch [23/50] batch [200/288] time 0.469 (0.198) data 0.000 (0.001) loss 2.1261 (2.6671) teacher_loss 2.0595 (2.5945) loss_zs_kd 0.4964 (0.4591) loss_oracle 0.0666 (0.0726) acc 78.1250 (76.7188) lr 1.2487e-03 eta 0:25:57
epoch [23/50] batch [220/288] time 0.086 (0.202) data 0.000 (0.001) loss 3.6274 (2.6804) teacher_loss 3.5588 (2.6068) loss_zs_kd 0.4107 (0.4592) loss_oracle 0.0687 (0.0736) acc 68.7500 (76.4489) lr 1.2487e-03 eta 0:26:24
epoch [23/50] batch [240/288] time 0.190 (0.201) data 0.000 (0.001) loss 2.8042 (2.7123) teacher_loss 2.7370 (2.6387) loss_zs_kd 0.5015 (0.4551) loss_oracle 0.0672 (0.0736) acc 71.8750 (76.1458) lr 1.2487e-03 eta 0:26:15
epoch [23/50] batch [260/288] time 0.192 (0.201) data 0.000 (0.001) loss 2.3127 (2.7343) teacher_loss 2.2266 (2.6602) loss_zs_kd 0.4350 (0.4553) loss_oracle 0.0861 (0.0741) acc 81.2500 (75.9736) lr 1.2487e-03 eta 0:26:06
epoch [23/50] batch [280/288] time 0.193 (0.200) data 0.000 (0.001) loss 2.7588 (2.7219) teacher_loss 2.6705 (2.6481) loss_zs_kd 0.4735 (0.4533) loss_oracle 0.0884 (0.0738) acc 71.8750 (76.0379) lr 1.2487e-03 eta 0:25:58
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,439
* accuracy: 87.3%
* error: 12.7%
* macro_f1: 86.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,996
* accuracy: 82.2%
* error: 17.8%
* macro_f1: 78.9%
******* Domain a best val acc:      87.3%, epoch: 19 *******
******* Domain a best val test acc: 82.3%, epoch: 19 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [24/50] batch [20/288] time 0.199 (0.217) data 0.000 (0.015) loss 3.1198 (2.6554) teacher_loss 3.0771 (2.5704) loss_zs_kd 0.3456 (0.4335) loss_oracle 0.0427 (0.0850) acc 78.1250 (76.7188) lr 1.1874e-03 eta 0:28:01
epoch [24/50] batch [40/288] time 0.189 (0.203) data 0.000 (0.008) loss 2.7338 (2.7732) teacher_loss 2.6811 (2.6919) loss_zs_kd 0.5249 (0.4408) loss_oracle 0.0527 (0.0814) acc 71.8750 (75.6250) lr 1.1874e-03 eta 0:26:11
epoch [24/50] batch [60/288] time 0.192 (0.200) data 0.000 (0.005) loss 2.5641 (2.7933) teacher_loss 2.4659 (2.7151) loss_zs_kd 0.4059 (0.4560) loss_oracle 0.0982 (0.0782) acc 78.1250 (75.3125) lr 1.1874e-03 eta 0:25:46
epoch [24/50] batch [80/288] time 0.198 (0.199) data 0.000 (0.004) loss 2.7735 (2.7258) teacher_loss 2.7136 (2.6482) loss_zs_kd 0.6652 (0.4554) loss_oracle 0.0599 (0.0775) acc 78.1250 (75.7812) lr 1.1874e-03 eta 0:25:31
epoch [24/50] batch [100/288] time 0.196 (0.198) data 0.000 (0.003) loss 1.9768 (2.7377) teacher_loss 1.9248 (2.6603) loss_zs_kd 0.5469 (0.4516) loss_oracle 0.0521 (0.0775) acc 84.3750 (75.7500) lr 1.1874e-03 eta 0:25:20
epoch [24/50] batch [120/288] time 0.197 (0.197) data 0.000 (0.003) loss 2.7911 (2.6982) teacher_loss 2.7195 (2.6212) loss_zs_kd 0.3046 (0.4460) loss_oracle 0.0716 (0.0770) acc 71.8750 (76.2500) lr 1.1874e-03 eta 0:25:11
epoch [24/50] batch [140/288] time 0.196 (0.197) data 0.000 (0.002) loss 2.7585 (2.6809) teacher_loss 2.6674 (2.6053) loss_zs_kd 0.3869 (0.4456) loss_oracle 0.0911 (0.0756) acc 78.1250 (76.4955) lr 1.1874e-03 eta 0:25:04
epoch [24/50] batch [160/288] time 0.197 (0.197) data 0.000 (0.002) loss 3.0650 (2.6934) teacher_loss 2.9853 (2.6188) loss_zs_kd 0.3473 (0.4542) loss_oracle 0.0796 (0.0746) acc 71.8750 (76.4844) lr 1.1874e-03 eta 0:24:58
epoch [24/50] batch [180/288] time 0.220 (0.197) data 0.000 (0.002) loss 2.3728 (2.7214) teacher_loss 2.2774 (2.6465) loss_zs_kd 0.3753 (0.4534) loss_oracle 0.0954 (0.0749) acc 81.2500 (76.0590) lr 1.1874e-03 eta 0:24:53
epoch [24/50] batch [200/288] time 0.417 (0.196) data 0.000 (0.002) loss 2.7849 (2.7297) teacher_loss 2.6778 (2.6540) loss_zs_kd 0.4649 (0.4541) loss_oracle 0.1071 (0.0757) acc 78.1250 (75.9688) lr 1.1874e-03 eta 0:24:46
epoch [24/50] batch [220/288] time 0.112 (0.204) data 0.000 (0.002) loss 2.3237 (2.7377) teacher_loss 2.2471 (2.6623) loss_zs_kd 0.4730 (0.4522) loss_oracle 0.0766 (0.0754) acc 81.2500 (75.9517) lr 1.1874e-03 eta 0:25:38
epoch [24/50] batch [240/288] time 0.195 (0.203) data 0.000 (0.001) loss 2.2320 (2.7407) teacher_loss 2.1243 (2.6656) loss_zs_kd 0.4434 (0.4490) loss_oracle 0.1077 (0.0752) acc 78.1250 (75.9505) lr 1.1874e-03 eta 0:25:27
epoch [24/50] batch [260/288] time 0.195 (0.202) data 0.000 (0.001) loss 1.5543 (2.7557) teacher_loss 1.4999 (2.6805) loss_zs_kd 0.4247 (0.4460) loss_oracle 0.0543 (0.0752) acc 81.2500 (75.6851) lr 1.1874e-03 eta 0:25:18
epoch [24/50] batch [280/288] time 0.194 (0.201) data 0.000 (0.001) loss 2.3519 (2.7628) teacher_loss 2.3086 (2.6878) loss_zs_kd 0.4314 (0.4468) loss_oracle 0.0433 (0.0750) acc 78.1250 (75.6808) lr 1.1874e-03 eta 0:25:10
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,441
* accuracy: 87.4%
* error: 12.6%
* macro_f1: 86.8%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,998
* accuracy: 82.3%
* error: 17.7%
* macro_f1: 79.3%
******* Domain a best val acc:      87.4%, epoch: 24 *******
******* Domain a best val test acc: 82.3%, epoch: 24 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [25/50] batch [20/288] time 0.194 (0.217) data 0.000 (0.017) loss 3.7941 (2.7643) teacher_loss 3.7183 (2.6982) loss_zs_kd 0.4447 (0.4406) loss_oracle 0.0758 (0.0661) acc 65.6250 (75.6250) lr 1.1253e-03 eta 0:27:00
epoch [25/50] batch [40/288] time 0.195 (0.205) data 0.000 (0.009) loss 2.0607 (2.7783) teacher_loss 1.9840 (2.7061) loss_zs_kd 0.3571 (0.4344) loss_oracle 0.0767 (0.0722) acc 81.2500 (75.0781) lr 1.1253e-03 eta 0:25:27
epoch [25/50] batch [60/288] time 0.194 (0.202) data 0.000 (0.006) loss 3.8740 (2.8276) teacher_loss 3.8123 (2.7574) loss_zs_kd 0.4202 (0.4296) loss_oracle 0.0617 (0.0702) acc 62.5000 (74.5312) lr 1.1253e-03 eta 0:24:56
epoch [25/50] batch [80/288] time 0.196 (0.200) data 0.000 (0.004) loss 3.0579 (2.8101) teacher_loss 3.0195 (2.7395) loss_zs_kd 0.5832 (0.4292) loss_oracle 0.0384 (0.0706) acc 75.0000 (74.8047) lr 1.1253e-03 eta 0:24:38
epoch [25/50] batch [100/288] time 0.195 (0.198) data 0.000 (0.004) loss 2.2783 (2.8001) teacher_loss 2.2040 (2.7286) loss_zs_kd 0.4203 (0.4373) loss_oracle 0.0743 (0.0715) acc 78.1250 (75.1250) lr 1.1253e-03 eta 0:24:25
epoch [25/50] batch [120/288] time 0.194 (0.198) data 0.000 (0.003) loss 3.0956 (2.7656) teacher_loss 2.9951 (2.6926) loss_zs_kd 0.4197 (0.4319) loss_oracle 0.1006 (0.0730) acc 68.7500 (75.3385) lr 1.1253e-03 eta 0:24:19
epoch [25/50] batch [140/288] time 0.200 (0.198) data 0.000 (0.003) loss 0.9798 (2.7623) teacher_loss 0.9228 (2.6890) loss_zs_kd 0.3230 (0.4332) loss_oracle 0.0571 (0.0732) acc 93.7500 (75.4911) lr 1.1253e-03 eta 0:24:12
epoch [25/50] batch [160/288] time 0.196 (0.197) data 0.000 (0.002) loss 2.1559 (2.7399) teacher_loss 2.0813 (2.6657) loss_zs_kd 0.3782 (0.4342) loss_oracle 0.0746 (0.0743) acc 84.3750 (75.6250) lr 1.1253e-03 eta 0:24:05
epoch [25/50] batch [180/288] time 0.196 (0.197) data 0.000 (0.002) loss 3.1803 (2.7639) teacher_loss 3.0807 (2.6899) loss_zs_kd 0.3970 (0.4346) loss_oracle 0.0996 (0.0740) acc 75.0000 (75.5208) lr 1.1253e-03 eta 0:23:59
epoch [25/50] batch [200/288] time 0.481 (0.201) data 0.000 (0.002) loss 1.6060 (2.7443) teacher_loss 1.5438 (2.6715) loss_zs_kd 0.5317 (0.4349) loss_oracle 0.0622 (0.0728) acc 87.5000 (75.7500) lr 1.1253e-03 eta 0:24:22
epoch [25/50] batch [220/288] time 0.189 (0.204) data 0.000 (0.002) loss 1.9316 (2.7234) teacher_loss 1.8201 (2.6506) loss_zs_kd 0.5350 (0.4350) loss_oracle 0.1115 (0.0729) acc 84.3750 (75.9517) lr 1.1253e-03 eta 0:24:44
epoch [25/50] batch [240/288] time 0.193 (0.203) data 0.000 (0.002) loss 4.2829 (2.7325) teacher_loss 4.2163 (2.6602) loss_zs_kd 0.4531 (0.4330) loss_oracle 0.0666 (0.0723) acc 65.6250 (75.8724) lr 1.1253e-03 eta 0:24:34
epoch [25/50] batch [260/288] time 0.195 (0.203) data 0.000 (0.002) loss 2.4312 (2.7285) teacher_loss 2.3711 (2.6558) loss_zs_kd 0.3958 (0.4341) loss_oracle 0.0600 (0.0727) acc 81.2500 (75.9736) lr 1.1253e-03 eta 0:24:24
epoch [25/50] batch [280/288] time 0.194 (0.202) data 0.000 (0.001) loss 2.7999 (2.7270) teacher_loss 2.7621 (2.6535) loss_zs_kd 0.3096 (0.4345) loss_oracle 0.0377 (0.0735) acc 75.0000 (76.0156) lr 1.1253e-03 eta 0:24:14
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,440
* accuracy: 87.3%
* error: 12.7%
* macro_f1: 86.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,986
* accuracy: 81.8%
* error: 18.2%
* macro_f1: 78.2%
******* Domain a best val acc:      87.4%, epoch: 24 *******
******* Domain a best val test acc: 82.3%, epoch: 24 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [26/50] batch [20/288] time 0.188 (0.209) data 0.000 (0.013) loss 1.9651 (2.3190) teacher_loss 1.8812 (2.2421) loss_zs_kd 0.2647 (0.4287) loss_oracle 0.0839 (0.0770) acc 78.1250 (78.7500) lr 1.0628e-03 eta 0:24:57
epoch [26/50] batch [40/288] time 0.193 (0.201) data 0.000 (0.007) loss 3.1261 (2.5411) teacher_loss 3.0279 (2.4653) loss_zs_kd 0.2590 (0.4322) loss_oracle 0.0982 (0.0758) acc 68.7500 (76.7188) lr 1.0628e-03 eta 0:24:01
epoch [26/50] batch [60/288] time 0.196 (0.199) data 0.001 (0.004) loss 2.6763 (2.5787) teacher_loss 2.6162 (2.5035) loss_zs_kd 0.4288 (0.4481) loss_oracle 0.0601 (0.0752) acc 68.7500 (77.2917) lr 1.0628e-03 eta 0:23:40
epoch [26/50] batch [80/288] time 0.200 (0.198) data 0.000 (0.003) loss 4.2361 (2.6078) teacher_loss 4.1789 (2.5313) loss_zs_kd 0.4884 (0.4531) loss_oracle 0.0571 (0.0765) acc 65.6250 (77.1875) lr 1.0628e-03 eta 0:23:28
epoch [26/50] batch [100/288] time 0.198 (0.197) data 0.000 (0.003) loss 1.8366 (2.6449) teacher_loss 1.7940 (2.5690) loss_zs_kd 0.3856 (0.4522) loss_oracle 0.0426 (0.0759) acc 90.6250 (76.7812) lr 1.0628e-03 eta 0:23:18
epoch [26/50] batch [120/288] time 0.194 (0.197) data 0.000 (0.002) loss 2.3835 (2.6544) teacher_loss 2.3090 (2.5795) loss_zs_kd 0.6696 (0.4490) loss_oracle 0.0745 (0.0749) acc 75.0000 (76.6667) lr 1.0628e-03 eta 0:23:11
epoch [26/50] batch [140/288] time 0.193 (0.196) data 0.000 (0.002) loss 2.9238 (2.6452) teacher_loss 2.8500 (2.5692) loss_zs_kd 0.4834 (0.4495) loss_oracle 0.0738 (0.0760) acc 81.2500 (76.7634) lr 1.0628e-03 eta 0:23:04
epoch [26/50] batch [160/288] time 0.196 (0.196) data 0.000 (0.002) loss 3.5956 (2.6489) teacher_loss 3.5305 (2.5745) loss_zs_kd 0.5691 (0.4526) loss_oracle 0.0651 (0.0744) acc 75.0000 (76.9141) lr 1.0628e-03 eta 0:22:59
epoch [26/50] batch [180/288] time 0.199 (0.196) data 0.000 (0.002) loss 2.3148 (2.6492) teacher_loss 2.2476 (2.5743) loss_zs_kd 0.3655 (0.4525) loss_oracle 0.0671 (0.0750) acc 81.2500 (76.6146) lr 1.0628e-03 eta 0:22:53
epoch [26/50] batch [200/288] time 0.467 (0.197) data 0.000 (0.002) loss 2.5280 (2.6556) teacher_loss 2.4369 (2.5806) loss_zs_kd 0.4015 (0.4538) loss_oracle 0.0911 (0.0750) acc 84.3750 (76.5000) lr 1.0628e-03 eta 0:22:59
epoch [26/50] batch [220/288] time 0.162 (0.203) data 0.000 (0.001) loss 0.9870 (2.6647) teacher_loss 0.8959 (2.5898) loss_zs_kd 0.2817 (0.4525) loss_oracle 0.0911 (0.0749) acc 93.7500 (76.4773) lr 1.0628e-03 eta 0:23:34
epoch [26/50] batch [240/288] time 0.196 (0.202) data 0.000 (0.001) loss 2.5631 (2.6738) teacher_loss 2.4965 (2.5992) loss_zs_kd 0.5496 (0.4538) loss_oracle 0.0666 (0.0746) acc 81.2500 (76.3281) lr 1.0628e-03 eta 0:23:26
epoch [26/50] batch [260/288] time 0.195 (0.201) data 0.000 (0.001) loss 2.5016 (2.6942) teacher_loss 2.4206 (2.6192) loss_zs_kd 0.5360 (0.4594) loss_oracle 0.0810 (0.0750) acc 75.0000 (76.1298) lr 1.0628e-03 eta 0:23:18
epoch [26/50] batch [280/288] time 0.189 (0.201) data 0.000 (0.001) loss 2.9968 (2.7045) teacher_loss 2.8890 (2.6294) loss_zs_kd 0.5790 (0.4619) loss_oracle 0.1078 (0.0751) acc 65.6250 (76.0491) lr 1.0628e-03 eta 0:23:10
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,449
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 87.0%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,982
* accuracy: 81.7%
* error: 18.3%
* macro_f1: 78.4%
******* Domain a best val acc:      87.6%, epoch: 26 *******
******* Domain a best val test acc: 81.7%, epoch: 26 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [27/50] batch [20/288] time 0.197 (0.206) data 0.000 (0.012) loss 1.9051 (2.5315) teacher_loss 1.8906 (2.4633) loss_zs_kd 0.4684 (0.5116) loss_oracle 0.0144 (0.0681) acc 81.2500 (77.8125) lr 1.0000e-03 eta 0:23:41
epoch [27/50] batch [40/288] time 0.195 (0.200) data 0.000 (0.006) loss 1.4001 (2.7020) teacher_loss 1.3336 (2.6340) loss_zs_kd 0.2859 (0.4801) loss_oracle 0.0666 (0.0680) acc 90.6250 (76.7969) lr 1.0000e-03 eta 0:22:55
epoch [27/50] batch [60/288] time 0.197 (0.198) data 0.000 (0.004) loss 3.7301 (2.6662) teacher_loss 3.6557 (2.5972) loss_zs_kd 0.7199 (0.4805) loss_oracle 0.0744 (0.0690) acc 68.7500 (76.7708) lr 1.0000e-03 eta 0:22:37
epoch [27/50] batch [80/288] time 0.190 (0.197) data 0.001 (0.003) loss 2.1004 (2.6551) teacher_loss 1.9927 (2.5830) loss_zs_kd 0.4004 (0.4590) loss_oracle 0.1076 (0.0721) acc 75.0000 (76.2109) lr 1.0000e-03 eta 0:22:25
epoch [27/50] batch [100/288] time 0.189 (0.196) data 0.000 (0.002) loss 2.0891 (2.6879) teacher_loss 2.0115 (2.6159) loss_zs_kd 0.5742 (0.4561) loss_oracle 0.0776 (0.0720) acc 81.2500 (75.5938) lr 1.0000e-03 eta 0:22:17
epoch [27/50] batch [120/288] time 0.197 (0.196) data 0.000 (0.002) loss 2.3805 (2.6281) teacher_loss 2.3472 (2.5569) loss_zs_kd 0.4146 (0.4540) loss_oracle 0.0333 (0.0712) acc 78.1250 (76.4583) lr 1.0000e-03 eta 0:22:12
epoch [27/50] batch [140/288] time 0.197 (0.196) data 0.000 (0.002) loss 2.3472 (2.6525) teacher_loss 2.3088 (2.5815) loss_zs_kd 0.7132 (0.4605) loss_oracle 0.0383 (0.0710) acc 81.2500 (76.3616) lr 1.0000e-03 eta 0:22:06
epoch [27/50] batch [160/288] time 0.202 (0.196) data 0.000 (0.002) loss 2.8939 (2.6477) teacher_loss 2.8036 (2.5759) loss_zs_kd 0.5089 (0.4621) loss_oracle 0.0904 (0.0719) acc 65.6250 (76.4844) lr 1.0000e-03 eta 0:22:00
epoch [27/50] batch [180/288] time 0.193 (0.195) data 0.000 (0.001) loss 2.0924 (2.6637) teacher_loss 2.0158 (2.5919) loss_zs_kd 0.4043 (0.4601) loss_oracle 0.0766 (0.0718) acc 71.8750 (76.2326) lr 1.0000e-03 eta 0:21:54
epoch [27/50] batch [200/288] time 0.474 (0.201) data 0.000 (0.001) loss 2.8760 (2.6457) teacher_loss 2.8043 (2.5739) loss_zs_kd 0.4046 (0.4594) loss_oracle 0.0717 (0.0719) acc 75.0000 (76.3750) lr 1.0000e-03 eta 0:22:26
epoch [27/50] batch [220/288] time 0.200 (0.204) data 0.000 (0.001) loss 2.9389 (2.6484) teacher_loss 2.8817 (2.5763) loss_zs_kd 0.8149 (0.4595) loss_oracle 0.0572 (0.0721) acc 71.8750 (76.4631) lr 1.0000e-03 eta 0:22:43
epoch [27/50] batch [240/288] time 0.201 (0.203) data 0.000 (0.001) loss 1.6321 (2.6875) teacher_loss 1.5888 (2.6150) loss_zs_kd 0.2905 (0.4588) loss_oracle 0.0434 (0.0724) acc 81.2500 (76.0417) lr 1.0000e-03 eta 0:22:33
epoch [27/50] batch [260/288] time 0.199 (0.202) data 0.000 (0.001) loss 2.1497 (2.6889) teacher_loss 2.0926 (2.6164) loss_zs_kd 0.3968 (0.4596) loss_oracle 0.0571 (0.0725) acc 81.2500 (75.9856) lr 1.0000e-03 eta 0:22:22
epoch [27/50] batch [280/288] time 0.188 (0.201) data 0.000 (0.001) loss 3.7478 (2.7001) teacher_loss 3.6689 (2.6278) loss_zs_kd 0.6715 (0.4609) loss_oracle 0.0789 (0.0723) acc 53.1250 (75.8594) lr 1.0000e-03 eta 0:22:14
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,458
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 87.1%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,981
* accuracy: 81.6%
* error: 18.4%
* macro_f1: 78.3%
******* Domain a best val acc:      87.8%, epoch: 27 *******
******* Domain a best val test acc: 81.6%, epoch: 27 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [28/50] batch [20/288] time 0.190 (0.210) data 0.000 (0.013) loss 2.1396 (2.6347) teacher_loss 2.0919 (2.5591) loss_zs_kd 0.3922 (0.4881) loss_oracle 0.0478 (0.0756) acc 81.2500 (77.0312) lr 9.3721e-04 eta 0:23:06
epoch [28/50] batch [40/288] time 0.192 (0.200) data 0.000 (0.006) loss 1.8716 (2.6926) teacher_loss 1.7972 (2.6173) loss_zs_kd 0.3965 (0.4551) loss_oracle 0.0745 (0.0752) acc 87.5000 (76.0156) lr 9.3721e-04 eta 0:21:54
epoch [28/50] batch [60/288] time 0.199 (0.198) data 0.000 (0.004) loss 2.1549 (2.7257) teacher_loss 2.0926 (2.6521) loss_zs_kd 0.5434 (0.4591) loss_oracle 0.0623 (0.0736) acc 78.1250 (75.6250) lr 9.3721e-04 eta 0:21:38
epoch [28/50] batch [80/288] time 0.195 (0.197) data 0.000 (0.003) loss 2.4682 (2.7610) teacher_loss 2.4204 (2.6867) loss_zs_kd 0.3564 (0.4581) loss_oracle 0.0478 (0.0743) acc 75.0000 (75.1953) lr 9.3721e-04 eta 0:21:28
epoch [28/50] batch [100/288] time 0.192 (0.196) data 0.000 (0.003) loss 2.5477 (2.7387) teacher_loss 2.4549 (2.6633) loss_zs_kd 0.3427 (0.4594) loss_oracle 0.0928 (0.0754) acc 78.1250 (75.4375) lr 9.3721e-04 eta 0:21:19
epoch [28/50] batch [120/288] time 0.195 (0.196) data 0.000 (0.002) loss 2.3893 (2.7094) teacher_loss 2.3178 (2.6336) loss_zs_kd 0.3561 (0.4520) loss_oracle 0.0715 (0.0759) acc 87.5000 (75.7292) lr 9.3721e-04 eta 0:21:13
epoch [28/50] batch [140/288] time 0.195 (0.195) data 0.000 (0.002) loss 1.1535 (2.6798) teacher_loss 1.0913 (2.6047) loss_zs_kd 0.3245 (0.4524) loss_oracle 0.0622 (0.0751) acc 93.7500 (76.0268) lr 9.3721e-04 eta 0:21:06
epoch [28/50] batch [160/288] time 0.187 (0.195) data 0.000 (0.002) loss 3.2527 (2.6470) teacher_loss 3.1955 (2.5730) loss_zs_kd 0.4298 (0.4467) loss_oracle 0.0572 (0.0739) acc 65.6250 (76.4062) lr 9.3721e-04 eta 0:21:02
epoch [28/50] batch [180/288] time 0.194 (0.195) data 0.000 (0.002) loss 2.1546 (2.6338) teacher_loss 2.0897 (2.5604) loss_zs_kd 0.6118 (0.4482) loss_oracle 0.0649 (0.0734) acc 81.2500 (76.6667) lr 9.3721e-04 eta 0:20:54
epoch [28/50] batch [200/288] time 0.549 (0.201) data 0.000 (0.001) loss 3.3457 (2.6505) teacher_loss 3.2763 (2.5771) loss_zs_kd 0.6742 (0.4513) loss_oracle 0.0694 (0.0735) acc 71.8750 (76.5625) lr 9.3721e-04 eta 0:21:30
epoch [28/50] batch [220/288] time 0.200 (0.202) data 0.000 (0.001) loss 2.6111 (2.6544) teacher_loss 2.5445 (2.5812) loss_zs_kd 0.3583 (0.4522) loss_oracle 0.0666 (0.0732) acc 81.2500 (76.5057) lr 9.3721e-04 eta 0:21:34
epoch [28/50] batch [240/288] time 0.195 (0.201) data 0.000 (0.001) loss 1.1287 (2.6542) teacher_loss 1.0721 (2.5810) loss_zs_kd 0.3721 (0.4544) loss_oracle 0.0566 (0.0732) acc 87.5000 (76.4193) lr 9.3721e-04 eta 0:21:26
epoch [28/50] batch [260/288] time 0.197 (0.201) data 0.000 (0.001) loss 2.1419 (2.6672) teacher_loss 2.0583 (2.5938) loss_zs_kd 0.3750 (0.4547) loss_oracle 0.0836 (0.0734) acc 81.2500 (76.3101) lr 9.3721e-04 eta 0:21:18
epoch [28/50] batch [280/288] time 0.195 (0.200) data 0.000 (0.001) loss 2.0775 (2.6634) teacher_loss 2.0011 (2.5898) loss_zs_kd 0.4611 (0.4568) loss_oracle 0.0764 (0.0736) acc 81.2500 (76.3170) lr 9.3721e-04 eta 0:21:10
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,453
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 87.1%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,983
* accuracy: 81.7%
* error: 18.3%
* macro_f1: 78.2%
******* Domain a best val acc:      87.8%, epoch: 27 *******
******* Domain a best val test acc: 81.6%, epoch: 27 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [29/50] batch [20/288] time 0.202 (0.212) data 0.001 (0.015) loss 1.9879 (2.3852) teacher_loss 1.8818 (2.3084) loss_zs_kd 0.6100 (0.4539) loss_oracle 0.1061 (0.0768) acc 81.2500 (78.2812) lr 8.7467e-04 eta 0:22:20
epoch [29/50] batch [40/288] time 0.195 (0.201) data 0.000 (0.007) loss 2.3977 (2.6730) teacher_loss 2.3427 (2.6011) loss_zs_kd 0.3577 (0.4473) loss_oracle 0.0550 (0.0719) acc 84.3750 (75.7031) lr 8.7467e-04 eta 0:21:03
epoch [29/50] batch [60/288] time 0.193 (0.199) data 0.000 (0.005) loss 5.5977 (2.8002) teacher_loss 5.5456 (2.7292) loss_zs_kd 0.5081 (0.4424) loss_oracle 0.0521 (0.0710) acc 56.2500 (75.1042) lr 8.7467e-04 eta 0:20:46
epoch [29/50] batch [80/288] time 0.192 (0.197) data 0.000 (0.004) loss 2.1695 (2.7794) teacher_loss 2.1001 (2.7076) loss_zs_kd 0.5924 (0.4614) loss_oracle 0.0695 (0.0718) acc 87.5000 (75.2734) lr 8.7467e-04 eta 0:20:34
epoch [29/50] batch [100/288] time 0.191 (0.197) data 0.000 (0.003) loss 3.9227 (2.8177) teacher_loss 3.8510 (2.7457) loss_zs_kd 0.3641 (0.4654) loss_oracle 0.0716 (0.0720) acc 71.8750 (75.2812) lr 8.7467e-04 eta 0:20:25
epoch [29/50] batch [120/288] time 0.193 (0.196) data 0.000 (0.003) loss 2.4055 (2.7705) teacher_loss 2.3363 (2.6974) loss_zs_kd 0.2897 (0.4599) loss_oracle 0.0692 (0.0730) acc 68.7500 (75.5208) lr 8.7467e-04 eta 0:20:18
epoch [29/50] batch [140/288] time 0.188 (0.196) data 0.000 (0.002) loss 2.7203 (2.7324) teacher_loss 2.6042 (2.6592) loss_zs_kd 0.2611 (0.4537) loss_oracle 0.1161 (0.0732) acc 75.0000 (75.7812) lr 8.7467e-04 eta 0:20:12
epoch [29/50] batch [160/288] time 0.196 (0.196) data 0.000 (0.002) loss 2.1621 (2.7248) teacher_loss 2.0804 (2.6516) loss_zs_kd 0.5168 (0.4565) loss_oracle 0.0817 (0.0732) acc 81.2500 (75.8594) lr 8.7467e-04 eta 0:20:07
epoch [29/50] batch [180/288] time 0.197 (0.195) data 0.000 (0.002) loss 3.5798 (2.7370) teacher_loss 3.5254 (2.6649) loss_zs_kd 0.6060 (0.4595) loss_oracle 0.0544 (0.0721) acc 65.6250 (75.6424) lr 8.7467e-04 eta 0:20:03
epoch [29/50] batch [200/288] time 0.082 (0.204) data 0.000 (0.002) loss 3.3713 (2.7201) teacher_loss 3.2852 (2.6476) loss_zs_kd 0.4968 (0.4598) loss_oracle 0.0861 (0.0725) acc 65.6250 (75.8125) lr 8.7467e-04 eta 0:20:49
epoch [29/50] batch [220/288] time 0.189 (0.203) data 0.000 (0.002) loss 2.9206 (2.7338) teacher_loss 2.7984 (2.6604) loss_zs_kd 0.4701 (0.4596) loss_oracle 0.1222 (0.0734) acc 75.0000 (75.7955) lr 8.7467e-04 eta 0:20:44
epoch [29/50] batch [240/288] time 0.192 (0.203) data 0.000 (0.001) loss 2.4182 (2.7339) teacher_loss 2.3467 (2.6599) loss_zs_kd 0.3681 (0.4615) loss_oracle 0.0716 (0.0739) acc 81.2500 (75.8203) lr 8.7467e-04 eta 0:20:35
epoch [29/50] batch [260/288] time 0.191 (0.202) data 0.000 (0.001) loss 1.8567 (2.7307) teacher_loss 1.7918 (2.6568) loss_zs_kd 0.5564 (0.4637) loss_oracle 0.0650 (0.0739) acc 81.2500 (75.8534) lr 8.7467e-04 eta 0:20:26
epoch [29/50] batch [280/288] time 0.191 (0.201) data 0.000 (0.001) loss 4.3857 (2.7288) teacher_loss 4.2948 (2.6547) loss_zs_kd 0.4249 (0.4653) loss_oracle 0.0909 (0.0741) acc 56.2500 (75.7589) lr 8.7467e-04 eta 0:20:17
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,450
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 87.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,970
* accuracy: 81.2%
* error: 18.8%
* macro_f1: 77.8%
******* Domain a best val acc:      87.8%, epoch: 27 *******
******* Domain a best val test acc: 81.6%, epoch: 27 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [30/50] batch [20/288] time 0.187 (0.205) data 0.000 (0.014) loss 1.8561 (2.6009) teacher_loss 1.7744 (2.5240) loss_zs_kd 0.5462 (0.4560) loss_oracle 0.0817 (0.0769) acc 75.0000 (74.5312) lr 8.1262e-04 eta 0:20:32
epoch [30/50] batch [40/288] time 0.190 (0.199) data 0.000 (0.007) loss 2.5131 (2.4677) teacher_loss 2.4332 (2.3909) loss_zs_kd 0.3031 (0.4511) loss_oracle 0.0800 (0.0768) acc 75.0000 (76.0156) lr 8.1262e-04 eta 0:19:57
epoch [30/50] batch [60/288] time 0.194 (0.197) data 0.000 (0.005) loss 3.2427 (2.5578) teacher_loss 3.1927 (2.4806) loss_zs_kd 0.7144 (0.4658) loss_oracle 0.0500 (0.0771) acc 68.7500 (76.5104) lr 8.1262e-04 eta 0:19:41
epoch [30/50] batch [80/288] time 0.181 (0.196) data 0.000 (0.004) loss 2.5960 (2.6156) teacher_loss 2.5005 (2.5383) loss_zs_kd 0.3989 (0.4643) loss_oracle 0.0955 (0.0772) acc 78.1250 (76.4062) lr 8.1262e-04 eta 0:19:31
epoch [30/50] batch [100/288] time 0.193 (0.196) data 0.000 (0.003) loss 1.6656 (2.6763) teacher_loss 1.5913 (2.5990) loss_zs_kd 0.4257 (0.4606) loss_oracle 0.0744 (0.0773) acc 84.3750 (75.9062) lr 8.1262e-04 eta 0:19:24
epoch [30/50] batch [120/288] time 0.199 (0.196) data 0.000 (0.003) loss 2.7228 (2.7159) teacher_loss 2.6656 (2.6397) loss_zs_kd 0.3287 (0.4617) loss_oracle 0.0572 (0.0762) acc 75.0000 (75.6510) lr 8.1262e-04 eta 0:19:20
epoch [30/50] batch [140/288] time 0.195 (0.196) data 0.000 (0.002) loss 3.3410 (2.7522) teacher_loss 3.2983 (2.6770) loss_zs_kd 0.2624 (0.4651) loss_oracle 0.0427 (0.0752) acc 65.6250 (75.5804) lr 8.1262e-04 eta 0:19:16
epoch [30/50] batch [160/288] time 0.195 (0.196) data 0.000 (0.002) loss 1.8232 (2.7384) teacher_loss 1.7446 (2.6642) loss_zs_kd 0.4605 (0.4674) loss_oracle 0.0786 (0.0742) acc 81.2500 (75.7422) lr 8.1262e-04 eta 0:19:11
epoch [30/50] batch [180/288] time 0.220 (0.196) data 0.000 (0.002) loss 3.1052 (2.7744) teacher_loss 3.0451 (2.7001) loss_zs_kd 0.4648 (0.4648) loss_oracle 0.0600 (0.0743) acc 78.1250 (75.4340) lr 8.1262e-04 eta 0:19:08
epoch [30/50] batch [200/288] time 0.148 (0.205) data 0.000 (0.002) loss 3.5064 (2.7672) teacher_loss 3.4631 (2.6931) loss_zs_kd 0.4653 (0.4633) loss_oracle 0.0433 (0.0741) acc 71.8750 (75.3750) lr 8.1262e-04 eta 0:19:57
epoch [30/50] batch [220/288] time 0.157 (0.204) data 0.000 (0.002) loss 2.9800 (2.7699) teacher_loss 2.8723 (2.6963) loss_zs_kd 0.3990 (0.4630) loss_oracle 0.1076 (0.0736) acc 71.8750 (75.2983) lr 8.1262e-04 eta 0:19:51
epoch [30/50] batch [240/288] time 0.196 (0.204) data 0.000 (0.001) loss 1.8919 (2.7763) teacher_loss 1.8370 (2.7033) loss_zs_kd 0.3929 (0.4599) loss_oracle 0.0550 (0.0729) acc 87.5000 (75.3125) lr 8.1262e-04 eta 0:19:42
epoch [30/50] batch [260/288] time 0.198 (0.203) data 0.000 (0.001) loss 1.9462 (2.7957) teacher_loss 1.8479 (2.7227) loss_zs_kd 0.4079 (0.4595) loss_oracle 0.0982 (0.0729) acc 84.3750 (75.1562) lr 8.1262e-04 eta 0:19:34
epoch [30/50] batch [280/288] time 0.191 (0.202) data 0.000 (0.001) loss 1.7357 (2.7633) teacher_loss 1.6421 (2.6902) loss_zs_kd 0.3643 (0.4574) loss_oracle 0.0936 (0.0731) acc 81.2500 (75.5692) lr 8.1262e-04 eta 0:19:26
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,433
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,969
* accuracy: 81.1%
* error: 18.9%
* macro_f1: 77.8%
******* Domain a best val acc:      87.8%, epoch: 27 *******
******* Domain a best val test acc: 81.6%, epoch: 27 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [31/50] batch [20/288] time 0.201 (0.206) data 0.000 (0.012) loss 1.1642 (2.5150) teacher_loss 1.1259 (2.4418) loss_zs_kd 0.3247 (0.5175) loss_oracle 0.0383 (0.0732) acc 90.6250 (77.5000) lr 7.5131e-04 eta 0:19:40
epoch [31/50] batch [40/288] time 0.195 (0.202) data 0.000 (0.006) loss 1.8515 (2.6485) teacher_loss 1.7749 (2.5739) loss_zs_kd 0.4328 (0.4806) loss_oracle 0.0767 (0.0747) acc 87.5000 (76.8750) lr 7.5131e-04 eta 0:19:17
epoch [31/50] batch [60/288] time 0.196 (0.200) data 0.001 (0.004) loss 2.4867 (2.6993) teacher_loss 2.4204 (2.6249) loss_zs_kd 0.5029 (0.4667) loss_oracle 0.0663 (0.0745) acc 81.2500 (75.9375) lr 7.5131e-04 eta 0:18:58
epoch [31/50] batch [80/288] time 0.195 (0.198) data 0.000 (0.003) loss 2.5747 (2.6459) teacher_loss 2.5031 (2.5720) loss_zs_kd 0.4169 (0.4777) loss_oracle 0.0716 (0.0739) acc 81.2500 (76.1328) lr 7.5131e-04 eta 0:18:46
epoch [31/50] batch [100/288] time 0.193 (0.198) data 0.000 (0.003) loss 2.9052 (2.5963) teacher_loss 2.8141 (2.5233) loss_zs_kd 0.7388 (0.4770) loss_oracle 0.0911 (0.0730) acc 71.8750 (76.4688) lr 7.5131e-04 eta 0:18:37
epoch [31/50] batch [120/288] time 0.192 (0.197) data 0.000 (0.002) loss 1.5046 (2.6194) teacher_loss 1.4237 (2.5454) loss_zs_kd 0.4190 (0.4862) loss_oracle 0.0809 (0.0739) acc 87.5000 (76.4844) lr 7.5131e-04 eta 0:18:30
epoch [31/50] batch [140/288] time 0.198 (0.196) data 0.000 (0.002) loss 1.6570 (2.6177) teacher_loss 1.6064 (2.5442) loss_zs_kd 0.5726 (0.4866) loss_oracle 0.0505 (0.0734) acc 87.5000 (76.6964) lr 7.5131e-04 eta 0:18:20
epoch [31/50] batch [160/288] time 0.187 (0.196) data 0.000 (0.002) loss 2.2945 (2.6417) teacher_loss 2.1875 (2.5681) loss_zs_kd 0.5090 (0.4880) loss_oracle 0.1070 (0.0736) acc 81.2500 (76.4453) lr 7.5131e-04 eta 0:18:15
epoch [31/50] batch [180/288] time 0.194 (0.195) data 0.000 (0.001) loss 3.8037 (2.6581) teacher_loss 3.7465 (2.5851) loss_zs_kd 0.3910 (0.4793) loss_oracle 0.0572 (0.0730) acc 68.7500 (76.3021) lr 7.5131e-04 eta 0:18:10
epoch [31/50] batch [200/288] time 0.517 (0.204) data 0.000 (0.001) loss 2.6260 (2.6607) teacher_loss 2.5638 (2.5880) loss_zs_kd 0.4077 (0.4804) loss_oracle 0.0622 (0.0727) acc 78.1250 (76.3281) lr 7.5131e-04 eta 0:18:51
epoch [31/50] batch [220/288] time 0.194 (0.205) data 0.000 (0.001) loss 2.8773 (2.6833) teacher_loss 2.7768 (2.6110) loss_zs_kd 0.6394 (0.4811) loss_oracle 0.1006 (0.0723) acc 71.8750 (76.1080) lr 7.5131e-04 eta 0:18:54
epoch [31/50] batch [240/288] time 0.186 (0.204) data 0.000 (0.001) loss 2.7196 (2.6717) teacher_loss 2.6149 (2.5992) loss_zs_kd 0.4330 (0.4796) loss_oracle 0.1046 (0.0726) acc 71.8750 (76.1849) lr 7.5131e-04 eta 0:18:45
epoch [31/50] batch [260/288] time 0.188 (0.203) data 0.000 (0.001) loss 1.6862 (2.6806) teacher_loss 1.5879 (2.6078) loss_zs_kd 0.2775 (0.4746) loss_oracle 0.0983 (0.0729) acc 87.5000 (76.1779) lr 7.5131e-04 eta 0:18:37
epoch [31/50] batch [280/288] time 0.192 (0.202) data 0.000 (0.001) loss 2.2039 (2.6559) teacher_loss 2.1232 (2.5830) loss_zs_kd 0.2874 (0.4706) loss_oracle 0.0807 (0.0728) acc 81.2500 (76.5067) lr 7.5131e-04 eta 0:18:29
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,444
* accuracy: 87.4%
* error: 12.6%
* macro_f1: 86.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,975
* accuracy: 81.4%
* error: 18.6%
* macro_f1: 78.0%
******* Domain a best val acc:      87.8%, epoch: 27 *******
******* Domain a best val test acc: 81.6%, epoch: 27 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [32/50] batch [20/288] time 0.190 (0.210) data 0.000 (0.014) loss 2.8654 (2.7064) teacher_loss 2.7965 (2.6252) loss_zs_kd 0.2878 (0.4410) loss_oracle 0.0689 (0.0811) acc 81.2500 (76.7188) lr 6.9098e-04 eta 0:19:03
epoch [32/50] batch [40/288] time 0.196 (0.205) data 0.000 (0.007) loss 1.3400 (2.6901) teacher_loss 1.2854 (2.6157) loss_zs_kd 0.3150 (0.4733) loss_oracle 0.0546 (0.0744) acc 90.6250 (77.2656) lr 6.9098e-04 eta 0:18:34
epoch [32/50] batch [60/288] time 0.193 (0.201) data 0.000 (0.005) loss 2.0961 (2.7551) teacher_loss 2.0541 (2.6776) loss_zs_kd 0.5050 (0.4744) loss_oracle 0.0420 (0.0775) acc 84.3750 (76.6146) lr 6.9098e-04 eta 0:18:08
epoch [32/50] batch [80/288] time 0.191 (0.199) data 0.000 (0.004) loss 3.8156 (2.6839) teacher_loss 3.7218 (2.6074) loss_zs_kd 0.3406 (0.4705) loss_oracle 0.0939 (0.0765) acc 68.7500 (77.3438) lr 6.9098e-04 eta 0:17:55
epoch [32/50] batch [100/288] time 0.197 (0.198) data 0.000 (0.003) loss 1.1980 (2.6904) teacher_loss 1.1315 (2.6136) loss_zs_kd 0.4813 (0.4638) loss_oracle 0.0665 (0.0767) acc 90.6250 (77.3125) lr 6.9098e-04 eta 0:17:45
epoch [32/50] batch [120/288] time 0.201 (0.198) data 0.000 (0.003) loss 1.3589 (2.6652) teacher_loss 1.3039 (2.5894) loss_zs_kd 0.4079 (0.4671) loss_oracle 0.0550 (0.0758) acc 84.3750 (77.4479) lr 6.9098e-04 eta 0:17:37
epoch [32/50] batch [140/288] time 0.198 (0.197) data 0.000 (0.002) loss 2.5699 (2.6687) teacher_loss 2.5150 (2.5926) loss_zs_kd 0.6058 (0.4682) loss_oracle 0.0549 (0.0762) acc 78.1250 (77.0759) lr 6.9098e-04 eta 0:17:30
epoch [32/50] batch [160/288] time 0.199 (0.197) data 0.000 (0.002) loss 1.8767 (2.6645) teacher_loss 1.8173 (2.5881) loss_zs_kd 0.5297 (0.4667) loss_oracle 0.0594 (0.0763) acc 84.3750 (76.8555) lr 6.9098e-04 eta 0:17:24
epoch [32/50] batch [180/288] time 0.196 (0.196) data 0.000 (0.002) loss 2.5781 (2.7036) teacher_loss 2.5130 (2.6273) loss_zs_kd 0.5396 (0.4672) loss_oracle 0.0651 (0.0763) acc 81.2500 (76.5972) lr 6.9098e-04 eta 0:17:19
epoch [32/50] batch [200/288] time 0.388 (0.204) data 0.000 (0.002) loss 2.7318 (2.6904) teacher_loss 2.6437 (2.6138) loss_zs_kd 0.4599 (0.4645) loss_oracle 0.0881 (0.0766) acc 78.1250 (76.6875) lr 6.9098e-04 eta 0:17:56
epoch [32/50] batch [220/288] time 0.192 (0.205) data 0.000 (0.002) loss 2.2482 (2.7063) teacher_loss 2.1537 (2.6310) loss_zs_kd 0.3304 (0.4631) loss_oracle 0.0945 (0.0753) acc 78.1250 (76.3778) lr 6.9098e-04 eta 0:17:57
epoch [32/50] batch [240/288] time 0.195 (0.204) data 0.000 (0.001) loss 2.7917 (2.6878) teacher_loss 2.7445 (2.6129) loss_zs_kd 0.3378 (0.4598) loss_oracle 0.0471 (0.0749) acc 78.1250 (76.5625) lr 6.9098e-04 eta 0:17:48
epoch [32/50] batch [260/288] time 0.190 (0.204) data 0.000 (0.001) loss 2.3965 (2.7045) teacher_loss 2.3199 (2.6301) loss_zs_kd 0.3153 (0.4602) loss_oracle 0.0766 (0.0744) acc 78.1250 (76.4062) lr 6.9098e-04 eta 0:17:40
epoch [32/50] batch [280/288] time 0.195 (0.203) data 0.000 (0.001) loss 1.8847 (2.6985) teacher_loss 1.8364 (2.6244) loss_zs_kd 0.3665 (0.4603) loss_oracle 0.0483 (0.0741) acc 78.1250 (76.4174) lr 6.9098e-04 eta 0:17:33
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,454
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 87.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,979
* accuracy: 81.5%
* error: 18.5%
* macro_f1: 78.4%
******* Domain a best val acc:      87.8%, epoch: 27 *******
******* Domain a best val test acc: 81.6%, epoch: 27 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [33/50] batch [20/288] time 0.196 (0.205) data 0.000 (0.013) loss 2.9528 (2.7417) teacher_loss 2.9215 (2.6707) loss_zs_kd 0.4999 (0.4944) loss_oracle 0.0312 (0.0711) acc 78.1250 (75.9375) lr 6.3188e-04 eta 0:17:39
epoch [33/50] batch [40/288] time 0.193 (0.199) data 0.000 (0.007) loss 3.2561 (2.8120) teacher_loss 3.2090 (2.7381) loss_zs_kd 0.4325 (0.4853) loss_oracle 0.0471 (0.0739) acc 68.7500 (75.0000) lr 6.3188e-04 eta 0:17:04
epoch [33/50] batch [60/288] time 0.191 (0.195) data 0.000 (0.005) loss 3.0690 (2.7749) teacher_loss 2.9780 (2.6991) loss_zs_kd 0.5253 (0.4859) loss_oracle 0.0910 (0.0758) acc 68.7500 (75.3125) lr 6.3188e-04 eta 0:16:41
epoch [33/50] batch [80/288] time 0.185 (0.194) data 0.000 (0.003) loss 2.2105 (2.7821) teacher_loss 2.1122 (2.7064) loss_zs_kd 0.3630 (0.4739) loss_oracle 0.0983 (0.0757) acc 75.0000 (75.3516) lr 6.3188e-04 eta 0:16:28
epoch [33/50] batch [100/288] time 0.194 (0.194) data 0.000 (0.003) loss 2.7474 (2.7262) teacher_loss 2.6695 (2.6521) loss_zs_kd 0.4491 (0.4784) loss_oracle 0.0779 (0.0741) acc 78.1250 (75.7812) lr 6.3188e-04 eta 0:16:26
epoch [33/50] batch [120/288] time 0.196 (0.194) data 0.000 (0.002) loss 4.4420 (2.7299) teacher_loss 4.3943 (2.6570) loss_zs_kd 0.5287 (0.4773) loss_oracle 0.0477 (0.0729) acc 62.5000 (76.0417) lr 6.3188e-04 eta 0:16:22
epoch [33/50] batch [140/288] time 0.196 (0.194) data 0.000 (0.002) loss 3.2709 (2.7237) teacher_loss 3.2275 (2.6510) loss_zs_kd 0.5233 (0.4765) loss_oracle 0.0433 (0.0727) acc 71.8750 (76.1384) lr 6.3188e-04 eta 0:16:18
epoch [33/50] batch [160/288] time 0.191 (0.194) data 0.000 (0.002) loss 3.2491 (2.7091) teacher_loss 3.1732 (2.6358) loss_zs_kd 0.6285 (0.4750) loss_oracle 0.0759 (0.0733) acc 68.7500 (76.1719) lr 6.3188e-04 eta 0:16:14
epoch [33/50] batch [180/288] time 0.195 (0.194) data 0.000 (0.002) loss 3.0775 (2.7121) teacher_loss 3.0008 (2.6381) loss_zs_kd 0.4835 (0.4731) loss_oracle 0.0766 (0.0740) acc 78.1250 (76.3194) lr 6.3188e-04 eta 0:16:10
epoch [33/50] batch [200/288] time 0.477 (0.198) data 0.000 (0.002) loss 3.0100 (2.7005) teacher_loss 2.9440 (2.6262) loss_zs_kd 0.3692 (0.4712) loss_oracle 0.0660 (0.0743) acc 68.7500 (76.2500) lr 6.3188e-04 eta 0:16:28
epoch [33/50] batch [220/288] time 0.186 (0.201) data 0.000 (0.001) loss 3.1530 (2.6979) teacher_loss 3.0916 (2.6238) loss_zs_kd 0.1777 (0.4691) loss_oracle 0.0615 (0.0741) acc 68.7500 (76.1932) lr 6.3188e-04 eta 0:16:37
epoch [33/50] batch [240/288] time 0.191 (0.200) data 0.000 (0.001) loss 2.0902 (2.6684) teacher_loss 2.0225 (2.5943) loss_zs_kd 0.4798 (0.4701) loss_oracle 0.0677 (0.0741) acc 78.1250 (76.4193) lr 6.3188e-04 eta 0:16:29
epoch [33/50] batch [260/288] time 0.195 (0.200) data 0.000 (0.001) loss 2.5431 (2.6831) teacher_loss 2.4639 (2.6097) loss_zs_kd 0.2936 (0.4707) loss_oracle 0.0793 (0.0734) acc 84.3750 (76.3101) lr 6.3188e-04 eta 0:16:23
epoch [33/50] batch [280/288] time 0.206 (0.199) data 0.000 (0.001) loss 2.8782 (2.6879) teacher_loss 2.8304 (2.6143) loss_zs_kd 0.3463 (0.4717) loss_oracle 0.0478 (0.0736) acc 71.8750 (76.2388) lr 6.3188e-04 eta 0:16:17
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,458
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 87.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,986
* accuracy: 81.8%
* error: 18.2%
* macro_f1: 78.2%
******* Domain a best val acc:      87.8%, epoch: 27 *******
******* Domain a best val test acc: 81.6%, epoch: 27 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [34/50] batch [20/288] time 0.195 (0.211) data 0.000 (0.015) loss 2.9518 (2.4935) teacher_loss 2.8957 (2.4218) loss_zs_kd 0.5121 (0.4662) loss_oracle 0.0561 (0.0717) acc 71.8750 (77.9688) lr 5.7422e-04 eta 0:17:06
epoch [34/50] batch [40/288] time 0.194 (0.202) data 0.000 (0.008) loss 2.4268 (2.6391) teacher_loss 2.3139 (2.5693) loss_zs_kd 0.2947 (0.4713) loss_oracle 0.1128 (0.0699) acc 81.2500 (76.9531) lr 5.7422e-04 eta 0:16:18
epoch [34/50] batch [60/288] time 0.195 (0.199) data 0.000 (0.005) loss 2.3645 (2.6541) teacher_loss 2.2734 (2.5837) loss_zs_kd 0.2804 (0.4693) loss_oracle 0.0910 (0.0704) acc 78.1250 (76.6667) lr 5.7422e-04 eta 0:16:04
epoch [34/50] batch [80/288] time 0.193 (0.198) data 0.000 (0.004) loss 2.1922 (2.6427) teacher_loss 2.1351 (2.5739) loss_zs_kd 0.5446 (0.4676) loss_oracle 0.0571 (0.0688) acc 81.2500 (76.8359) lr 5.7422e-04 eta 0:15:54
epoch [34/50] batch [100/288] time 0.192 (0.197) data 0.000 (0.003) loss 3.0588 (2.5899) teacher_loss 2.9873 (2.5202) loss_zs_kd 0.6077 (0.4682) loss_oracle 0.0715 (0.0697) acc 75.0000 (77.3438) lr 5.7422e-04 eta 0:15:45
epoch [34/50] batch [120/288] time 0.188 (0.197) data 0.000 (0.003) loss 2.7714 (2.6274) teacher_loss 2.6560 (2.5569) loss_zs_kd 0.3656 (0.4656) loss_oracle 0.1154 (0.0705) acc 68.7500 (76.8490) lr 5.7422e-04 eta 0:15:38
epoch [34/50] batch [140/288] time 0.188 (0.196) data 0.000 (0.002) loss 2.8161 (2.6152) teacher_loss 2.6983 (2.5440) loss_zs_kd 0.5102 (0.4682) loss_oracle 0.1177 (0.0713) acc 71.8750 (76.9420) lr 5.7422e-04 eta 0:15:33
epoch [34/50] batch [160/288] time 0.196 (0.195) data 0.000 (0.002) loss 1.5091 (2.6024) teacher_loss 1.4996 (2.5311) loss_zs_kd 0.4660 (0.4722) loss_oracle 0.0094 (0.0714) acc 87.5000 (76.9727) lr 5.7422e-04 eta 0:15:24
epoch [34/50] batch [180/288] time 0.196 (0.195) data 0.000 (0.002) loss 2.1531 (2.6259) teacher_loss 2.0620 (2.5550) loss_zs_kd 0.5531 (0.4774) loss_oracle 0.0911 (0.0709) acc 78.1250 (76.7708) lr 5.7422e-04 eta 0:15:20
epoch [34/50] batch [200/288] time 0.450 (0.194) data 0.000 (0.002) loss 4.0682 (2.6754) teacher_loss 3.9751 (2.6037) loss_zs_kd 0.5565 (0.4754) loss_oracle 0.0931 (0.0716) acc 68.7500 (76.3281) lr 5.7422e-04 eta 0:15:09
epoch [34/50] batch [220/288] time 0.082 (0.203) data 0.000 (0.002) loss 2.8824 (2.6778) teacher_loss 2.8130 (2.6058) loss_zs_kd 0.4395 (0.4769) loss_oracle 0.0693 (0.0720) acc 68.7500 (76.4347) lr 5.7422e-04 eta 0:15:48
epoch [34/50] batch [240/288] time 0.092 (0.193) data 0.000 (0.001) loss 2.4941 (2.6903) teacher_loss 2.3985 (2.6183) loss_zs_kd 0.5308 (0.4773) loss_oracle 0.0955 (0.0720) acc 68.7500 (76.2891) lr 5.7422e-04 eta 0:14:59
epoch [34/50] batch [260/288] time 0.083 (0.185) data 0.000 (0.001) loss 2.3873 (2.6829) teacher_loss 2.3346 (2.6106) loss_zs_kd 0.4795 (0.4753) loss_oracle 0.0527 (0.0723) acc 87.5000 (76.3221) lr 5.7422e-04 eta 0:14:19
epoch [34/50] batch [280/288] time 0.085 (0.178) data 0.000 (0.001) loss 3.1955 (2.6722) teacher_loss 3.0818 (2.5996) loss_zs_kd 0.6158 (0.4719) loss_oracle 0.1137 (0.0726) acc 71.8750 (76.3839) lr 5.7422e-04 eta 0:13:43
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,451
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 87.1%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,978
* accuracy: 81.5%
* error: 18.5%
* macro_f1: 78.0%
******* Domain a best val acc:      87.8%, epoch: 27 *******
******* Domain a best val test acc: 81.6%, epoch: 27 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [35/50] batch [20/288] time 0.091 (0.112) data 0.000 (0.014) loss 3.4080 (2.7502) teacher_loss 3.3603 (2.6785) loss_zs_kd 0.4844 (0.4923) loss_oracle 0.0477 (0.0716) acc 68.7500 (75.6250) lr 5.1825e-04 eta 0:08:33
epoch [35/50] batch [40/288] time 0.100 (0.103) data 0.000 (0.007) loss 3.3387 (2.9084) teacher_loss 3.2694 (2.8361) loss_zs_kd 0.3826 (0.4886) loss_oracle 0.0692 (0.0723) acc 71.8750 (74.5312) lr 5.1825e-04 eta 0:07:51
epoch [35/50] batch [60/288] time 0.091 (0.101) data 0.000 (0.005) loss 4.2341 (2.8449) teacher_loss 4.1310 (2.7689) loss_zs_kd 0.4149 (0.4764) loss_oracle 0.1032 (0.0761) acc 71.8750 (75.4167) lr 5.1825e-04 eta 0:07:37
epoch [35/50] batch [80/288] time 0.097 (0.099) data 0.000 (0.004) loss 2.4845 (2.8505) teacher_loss 2.4007 (2.7749) loss_zs_kd 0.5515 (0.4740) loss_oracle 0.0838 (0.0756) acc 75.0000 (75.0391) lr 5.1825e-04 eta 0:07:30
epoch [35/50] batch [100/288] time 0.095 (0.099) data 0.000 (0.003) loss 3.0897 (2.8284) teacher_loss 3.0449 (2.7537) loss_zs_kd 0.4313 (0.4789) loss_oracle 0.0449 (0.0747) acc 71.8750 (75.2500) lr 5.1825e-04 eta 0:07:26
epoch [35/50] batch [120/288] time 0.110 (0.099) data 0.000 (0.002) loss 2.0766 (2.7882) teacher_loss 2.0086 (2.7134) loss_zs_kd 0.6169 (0.4805) loss_oracle 0.0680 (0.0749) acc 84.3750 (75.6510) lr 5.1825e-04 eta 0:07:22
epoch [35/50] batch [140/288] time 0.103 (0.099) data 0.001 (0.002) loss 1.4828 (2.7837) teacher_loss 1.4091 (2.7081) loss_zs_kd 0.2504 (0.4762) loss_oracle 0.0738 (0.0756) acc 84.3750 (75.8036) lr 5.1825e-04 eta 0:07:21
epoch [35/50] batch [160/288] time 0.100 (0.101) data 0.000 (0.002) loss 2.1077 (2.7626) teacher_loss 2.0259 (2.6875) loss_zs_kd 0.4207 (0.4774) loss_oracle 0.0818 (0.0751) acc 78.1250 (75.9375) lr 5.1825e-04 eta 0:07:27
epoch [35/50] batch [180/288] time 0.098 (0.102) data 0.000 (0.002) loss 1.9176 (2.7182) teacher_loss 1.8503 (2.6423) loss_zs_kd 0.2979 (0.4721) loss_oracle 0.0672 (0.0759) acc 78.1250 (76.2153) lr 5.1825e-04 eta 0:07:31
epoch [35/50] batch [200/288] time 0.100 (0.102) data 0.001 (0.002) loss 1.5521 (2.6996) teacher_loss 1.5116 (2.6233) loss_zs_kd 0.5051 (0.4755) loss_oracle 0.0405 (0.0763) acc 81.2500 (76.3281) lr 5.1825e-04 eta 0:07:29
epoch [35/50] batch [220/288] time 0.100 (0.102) data 0.000 (0.001) loss 1.7618 (2.6824) teacher_loss 1.6734 (2.6064) loss_zs_kd 0.3242 (0.4755) loss_oracle 0.0883 (0.0760) acc 81.2500 (76.3778) lr 5.1825e-04 eta 0:07:27
epoch [35/50] batch [240/288] time 0.105 (0.102) data 0.001 (0.001) loss 1.4401 (2.6722) teacher_loss 1.3370 (2.5965) loss_zs_kd 0.5559 (0.4752) loss_oracle 0.1030 (0.0757) acc 84.3750 (76.4583) lr 5.1825e-04 eta 0:07:26
epoch [35/50] batch [260/288] time 0.107 (0.102) data 0.001 (0.001) loss 1.6814 (2.6348) teacher_loss 1.6169 (2.5591) loss_zs_kd 0.3785 (0.4760) loss_oracle 0.0645 (0.0757) acc 87.5000 (76.7428) lr 5.1825e-04 eta 0:07:24
epoch [35/50] batch [280/288] time 0.106 (0.102) data 0.000 (0.001) loss 2.7003 (2.6454) teacher_loss 2.6525 (2.5702) loss_zs_kd 0.3719 (0.4803) loss_oracle 0.0478 (0.0752) acc 75.0000 (76.6629) lr 5.1825e-04 eta 0:07:23
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,448
* accuracy: 87.5%
* error: 12.5%
* macro_f1: 87.0%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,970
* accuracy: 81.2%
* error: 18.8%
* macro_f1: 77.6%
******* Domain a best val acc:      87.8%, epoch: 27 *******
******* Domain a best val test acc: 81.6%, epoch: 27 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [36/50] batch [20/288] time 0.095 (0.116) data 0.000 (0.017) loss 3.7863 (2.5131) teacher_loss 3.6880 (2.4313) loss_zs_kd 0.4368 (0.4895) loss_oracle 0.0983 (0.0818) acc 65.6250 (78.7500) lr 4.6417e-04 eta 0:08:18
epoch [36/50] batch [40/288] time 0.094 (0.108) data 0.000 (0.009) loss 2.2823 (2.6470) teacher_loss 2.2347 (2.5677) loss_zs_kd 0.4006 (0.5097) loss_oracle 0.0476 (0.0793) acc 81.2500 (77.4219) lr 4.6417e-04 eta 0:07:41
epoch [36/50] batch [60/288] time 0.102 (0.105) data 0.000 (0.006) loss 1.9725 (2.6744) teacher_loss 1.9060 (2.5983) loss_zs_kd 0.6159 (0.5089) loss_oracle 0.0665 (0.0760) acc 87.5000 (76.9271) lr 4.6417e-04 eta 0:07:26
epoch [36/50] batch [80/288] time 0.092 (0.103) data 0.000 (0.004) loss 2.6901 (2.6248) teacher_loss 2.6279 (2.5498) loss_zs_kd 0.4468 (0.4960) loss_oracle 0.0622 (0.0750) acc 75.0000 (76.8750) lr 4.6417e-04 eta 0:07:17
epoch [36/50] batch [100/288] time 0.103 (0.102) data 0.000 (0.004) loss 3.2104 (2.5463) teacher_loss 3.1193 (2.4717) loss_zs_kd 0.3679 (0.4969) loss_oracle 0.0911 (0.0746) acc 75.0000 (77.6250) lr 4.6417e-04 eta 0:07:12
epoch [36/50] batch [120/288] time 0.089 (0.103) data 0.000 (0.003) loss 1.7851 (2.5677) teacher_loss 1.7190 (2.4937) loss_zs_kd 0.5092 (0.4882) loss_oracle 0.0661 (0.0741) acc 84.3750 (77.7083) lr 4.6417e-04 eta 0:07:14
epoch [36/50] batch [140/288] time 0.133 (0.103) data 0.000 (0.003) loss 2.5274 (2.5940) teacher_loss 2.4892 (2.5194) loss_zs_kd 0.4250 (0.4897) loss_oracle 0.0383 (0.0746) acc 75.0000 (77.2768) lr 4.6417e-04 eta 0:07:09
epoch [36/50] batch [160/288] time 0.093 (0.103) data 0.000 (0.002) loss 2.4234 (2.5645) teacher_loss 2.3562 (2.4902) loss_zs_kd 0.3806 (0.4951) loss_oracle 0.0672 (0.0743) acc 75.0000 (77.4609) lr 4.6417e-04 eta 0:07:08
epoch [36/50] batch [180/288] time 0.092 (0.102) data 0.000 (0.002) loss 3.8013 (2.5857) teacher_loss 3.7079 (2.5112) loss_zs_kd 0.4364 (0.4957) loss_oracle 0.0934 (0.0745) acc 71.8750 (77.3785) lr 4.6417e-04 eta 0:07:03
epoch [36/50] batch [200/288] time 0.083 (0.101) data 0.000 (0.002) loss 2.7173 (2.6103) teacher_loss 2.6551 (2.5351) loss_zs_kd 0.4499 (0.4942) loss_oracle 0.0622 (0.0752) acc 68.7500 (77.1719) lr 4.6417e-04 eta 0:06:55
epoch [36/50] batch [220/288] time 0.095 (0.100) data 0.000 (0.002) loss 2.9294 (2.6108) teacher_loss 2.8534 (2.5359) loss_zs_kd 0.4953 (0.4906) loss_oracle 0.0761 (0.0749) acc 65.6250 (76.9886) lr 4.6417e-04 eta 0:06:49
epoch [36/50] batch [240/288] time 0.101 (0.100) data 0.000 (0.002) loss 2.2928 (2.6030) teacher_loss 2.1872 (2.5281) loss_zs_kd 0.4538 (0.4878) loss_oracle 0.1056 (0.0748) acc 68.7500 (76.8229) lr 4.6417e-04 eta 0:06:46
epoch [36/50] batch [260/288] time 0.093 (0.100) data 0.000 (0.002) loss 1.6833 (2.5913) teacher_loss 1.5984 (2.5168) loss_zs_kd 0.4724 (0.4849) loss_oracle 0.0849 (0.0745) acc 87.5000 (76.8870) lr 4.6417e-04 eta 0:06:44
epoch [36/50] batch [280/288] time 0.085 (0.099) data 0.000 (0.001) loss 2.7736 (2.6039) teacher_loss 2.6559 (2.5294) loss_zs_kd 0.4108 (0.4891) loss_oracle 0.1178 (0.0745) acc 78.1250 (76.7522) lr 4.6417e-04 eta 0:06:40
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,455
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 87.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,987
* accuracy: 81.9%
* error: 18.1%
* macro_f1: 78.5%
******* Domain a best val acc:      87.8%, epoch: 27 *******
******* Domain a best val test acc: 81.6%, epoch: 27 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [37/50] batch [20/288] time 0.106 (0.115) data 0.000 (0.014) loss 2.6066 (2.5914) teacher_loss 2.5256 (2.5197) loss_zs_kd 0.4167 (0.5262) loss_oracle 0.0810 (0.0717) acc 75.0000 (78.1250) lr 4.1221e-04 eta 0:07:39
epoch [37/50] batch [40/288] time 0.098 (0.107) data 0.000 (0.007) loss 2.7258 (2.7044) teacher_loss 2.6370 (2.6296) loss_zs_kd 0.4909 (0.5027) loss_oracle 0.0888 (0.0748) acc 71.8750 (76.6406) lr 4.1221e-04 eta 0:07:07
epoch [37/50] batch [60/288] time 0.094 (0.104) data 0.001 (0.005) loss 2.2598 (2.6642) teacher_loss 2.1585 (2.5900) loss_zs_kd 0.5598 (0.4843) loss_oracle 0.1013 (0.0742) acc 81.2500 (76.8229) lr 4.1221e-04 eta 0:06:52
epoch [37/50] batch [80/288] time 0.101 (0.103) data 0.000 (0.004) loss 1.8830 (2.6133) teacher_loss 1.8331 (2.5377) loss_zs_kd 0.3347 (0.4898) loss_oracle 0.0499 (0.0756) acc 84.3750 (77.3828) lr 4.1221e-04 eta 0:06:46
epoch [37/50] batch [100/288] time 0.165 (0.104) data 0.001 (0.003) loss 2.1961 (2.5574) teacher_loss 2.1194 (2.4823) loss_zs_kd 0.4037 (0.4873) loss_oracle 0.0766 (0.0751) acc 81.2500 (77.9062) lr 4.1221e-04 eta 0:06:50
epoch [37/50] batch [120/288] time 0.125 (0.104) data 0.000 (0.003) loss 3.5106 (2.6124) teacher_loss 3.4764 (2.5372) loss_zs_kd 0.3389 (0.4949) loss_oracle 0.0342 (0.0752) acc 68.7500 (77.1875) lr 4.1221e-04 eta 0:06:47
epoch [37/50] batch [140/288] time 0.096 (0.104) data 0.000 (0.002) loss 2.6046 (2.6276) teacher_loss 2.5213 (2.5528) loss_zs_kd 0.4026 (0.4970) loss_oracle 0.0833 (0.0748) acc 75.0000 (76.8750) lr 4.1221e-04 eta 0:06:45
epoch [37/50] batch [160/288] time 0.100 (0.104) data 0.000 (0.002) loss 2.2733 (2.6134) teacher_loss 2.1706 (2.5392) loss_zs_kd 0.4145 (0.4970) loss_oracle 0.1028 (0.0742) acc 84.3750 (77.0703) lr 4.1221e-04 eta 0:06:41
epoch [37/50] batch [180/288] time 0.101 (0.103) data 0.000 (0.002) loss 2.4337 (2.5791) teacher_loss 2.3599 (2.5042) loss_zs_kd 0.5129 (0.4955) loss_oracle 0.0739 (0.0749) acc 81.2500 (77.4306) lr 4.1221e-04 eta 0:06:37
epoch [37/50] batch [200/288] time 0.096 (0.103) data 0.000 (0.002) loss 2.3074 (2.5903) teacher_loss 2.2502 (2.5152) loss_zs_kd 0.3753 (0.4985) loss_oracle 0.0572 (0.0751) acc 78.1250 (77.4219) lr 4.1221e-04 eta 0:06:34
epoch [37/50] batch [220/288] time 0.101 (0.103) data 0.001 (0.002) loss 3.2236 (2.5992) teacher_loss 3.1534 (2.5241) loss_zs_kd 0.5889 (0.4977) loss_oracle 0.0702 (0.0751) acc 71.8750 (77.2727) lr 4.1221e-04 eta 0:06:32
epoch [37/50] batch [240/288] time 0.111 (0.103) data 0.000 (0.001) loss 1.7362 (2.5970) teacher_loss 1.7030 (2.5219) loss_zs_kd 0.4751 (0.4956) loss_oracle 0.0333 (0.0751) acc 84.3750 (77.2005) lr 4.1221e-04 eta 0:06:28
epoch [37/50] batch [260/288] time 0.097 (0.102) data 0.000 (0.001) loss 3.1850 (2.6113) teacher_loss 3.1044 (2.5362) loss_zs_kd 0.5529 (0.4948) loss_oracle 0.0806 (0.0751) acc 78.1250 (77.0192) lr 4.1221e-04 eta 0:06:26
epoch [37/50] batch [280/288] time 0.103 (0.102) data 0.000 (0.001) loss 1.5918 (2.5940) teacher_loss 1.5058 (2.5183) loss_zs_kd 0.5452 (0.4968) loss_oracle 0.0860 (0.0757) acc 90.6250 (77.2098) lr 4.1221e-04 eta 0:06:23
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,436
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 86.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,974
* accuracy: 81.3%
* error: 18.7%
* macro_f1: 77.8%
******* Domain a best val acc:      87.8%, epoch: 27 *******
******* Domain a best val test acc: 81.6%, epoch: 27 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [38/50] batch [20/288] time 0.099 (0.120) data 0.000 (0.014) loss 3.4056 (2.7760) teacher_loss 3.3450 (2.6999) loss_zs_kd 0.3378 (0.5296) loss_oracle 0.0606 (0.0761) acc 68.7500 (75.4688) lr 3.6258e-04 eta 0:07:26
epoch [38/50] batch [40/288] time 0.110 (0.109) data 0.000 (0.007) loss 0.6911 (2.5562) teacher_loss 0.5971 (2.4777) loss_zs_kd 0.7282 (0.5160) loss_oracle 0.0940 (0.0784) acc 93.7500 (77.3438) lr 3.6258e-04 eta 0:06:42
epoch [38/50] batch [60/288] time 0.107 (0.106) data 0.000 (0.005) loss 1.8133 (2.5773) teacher_loss 1.7417 (2.4988) loss_zs_kd 0.5145 (0.5147) loss_oracle 0.0716 (0.0785) acc 81.2500 (77.0833) lr 3.6258e-04 eta 0:06:28
epoch [38/50] batch [80/288] time 0.101 (0.106) data 0.000 (0.004) loss 2.5614 (2.6104) teacher_loss 2.4704 (2.5321) loss_zs_kd 0.3604 (0.5130) loss_oracle 0.0910 (0.0783) acc 84.3750 (76.9922) lr 3.6258e-04 eta 0:06:30
epoch [38/50] batch [100/288] time 0.107 (0.107) data 0.000 (0.003) loss 2.5987 (2.6614) teacher_loss 2.5012 (2.5841) loss_zs_kd 0.4194 (0.5057) loss_oracle 0.0974 (0.0773) acc 81.2500 (76.6250) lr 3.6258e-04 eta 0:06:30
epoch [38/50] batch [120/288] time 0.099 (0.106) data 0.000 (0.003) loss 4.2145 (2.6899) teacher_loss 4.1617 (2.6122) loss_zs_kd 0.5292 (0.5015) loss_oracle 0.0528 (0.0776) acc 65.6250 (76.2760) lr 3.6258e-04 eta 0:06:24
epoch [38/50] batch [140/288] time 0.097 (0.105) data 0.000 (0.002) loss 1.9774 (2.7033) teacher_loss 1.8913 (2.6254) loss_zs_kd 0.4353 (0.5046) loss_oracle 0.0861 (0.0779) acc 81.2500 (75.9375) lr 3.6258e-04 eta 0:06:18
epoch [38/50] batch [160/288] time 0.106 (0.105) data 0.000 (0.002) loss 1.5247 (2.7223) teacher_loss 1.4473 (2.6456) loss_zs_kd 0.3721 (0.5098) loss_oracle 0.0774 (0.0767) acc 87.5000 (75.6836) lr 3.6258e-04 eta 0:06:15
epoch [38/50] batch [180/288] time 0.096 (0.104) data 0.000 (0.002) loss 2.8853 (2.7403) teacher_loss 2.7832 (2.6645) loss_zs_kd 0.7234 (0.5170) loss_oracle 0.1020 (0.0759) acc 78.1250 (75.5382) lr 3.6258e-04 eta 0:06:10
epoch [38/50] batch [200/288] time 0.100 (0.104) data 0.000 (0.002) loss 2.1918 (2.7063) teacher_loss 2.1266 (2.6307) loss_zs_kd 0.3234 (0.5129) loss_oracle 0.0652 (0.0757) acc 68.7500 (75.8438) lr 3.6258e-04 eta 0:06:07
epoch [38/50] batch [220/288] time 0.099 (0.103) data 0.000 (0.002) loss 1.5426 (2.6998) teacher_loss 1.4350 (2.6245) loss_zs_kd 0.4982 (0.5106) loss_oracle 0.1077 (0.0753) acc 84.3750 (75.9375) lr 3.6258e-04 eta 0:06:04
epoch [38/50] batch [240/288] time 0.094 (0.103) data 0.000 (0.001) loss 1.1967 (2.7003) teacher_loss 1.0865 (2.6249) loss_zs_kd 0.5466 (0.5126) loss_oracle 0.1102 (0.0754) acc 87.5000 (75.9766) lr 3.6258e-04 eta 0:06:00
epoch [38/50] batch [260/288] time 0.099 (0.103) data 0.000 (0.001) loss 2.7659 (2.6983) teacher_loss 2.6625 (2.6222) loss_zs_kd 0.3418 (0.5097) loss_oracle 0.1034 (0.0761) acc 81.2500 (76.0337) lr 3.6258e-04 eta 0:05:58
epoch [38/50] batch [280/288] time 0.099 (0.103) data 0.000 (0.001) loss 1.8467 (2.6849) teacher_loss 1.7607 (2.6086) loss_zs_kd 0.3467 (0.5063) loss_oracle 0.0860 (0.0763) acc 90.6250 (76.1607) lr 3.6258e-04 eta 0:05:55
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,456
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 87.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,980
* accuracy: 81.6%
* error: 18.4%
* macro_f1: 78.1%
******* Domain a best val acc:      87.8%, epoch: 27 *******
******* Domain a best val test acc: 81.6%, epoch: 27 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [39/50] batch [20/288] time 0.107 (0.136) data 0.000 (0.017) loss 2.3763 (2.5306) teacher_loss 2.2916 (2.4571) loss_zs_kd 0.3965 (0.4572) loss_oracle 0.0847 (0.0735) acc 81.2500 (76.4062) lr 3.1545e-04 eta 0:07:46
epoch [39/50] batch [40/288] time 0.111 (0.128) data 0.000 (0.009) loss 3.0792 (2.5936) teacher_loss 3.0032 (2.5215) loss_zs_kd 0.5832 (0.4912) loss_oracle 0.0760 (0.0721) acc 68.7500 (76.2500) lr 3.1545e-04 eta 0:07:17
epoch [39/50] batch [60/288] time 0.104 (0.123) data 0.001 (0.006) loss 2.5592 (2.6018) teacher_loss 2.4826 (2.5311) loss_zs_kd 0.3876 (0.4852) loss_oracle 0.0766 (0.0707) acc 71.8750 (75.7292) lr 3.1545e-04 eta 0:06:58
epoch [39/50] batch [80/288] time 0.103 (0.118) data 0.000 (0.004) loss 3.8956 (2.6083) teacher_loss 3.8406 (2.5367) loss_zs_kd 0.4204 (0.4834) loss_oracle 0.0549 (0.0716) acc 59.3750 (75.9375) lr 3.1545e-04 eta 0:06:39
epoch [39/50] batch [100/288] time 0.115 (0.115) data 0.000 (0.004) loss 2.8530 (2.6376) teacher_loss 2.8148 (2.5661) loss_zs_kd 0.4450 (0.4858) loss_oracle 0.0383 (0.0715) acc 75.0000 (75.8125) lr 3.1545e-04 eta 0:06:25
epoch [39/50] batch [120/288] time 0.097 (0.113) data 0.000 (0.003) loss 2.0164 (2.6622) teacher_loss 1.9376 (2.5902) loss_zs_kd 0.3879 (0.4853) loss_oracle 0.0788 (0.0720) acc 87.5000 (75.7031) lr 3.1545e-04 eta 0:06:15
epoch [39/50] batch [140/288] time 0.102 (0.111) data 0.000 (0.003) loss 3.4444 (2.6636) teacher_loss 3.3750 (2.5924) loss_zs_kd 0.5164 (0.4949) loss_oracle 0.0694 (0.0711) acc 59.3750 (75.7366) lr 3.1545e-04 eta 0:06:08
epoch [39/50] batch [160/288] time 0.094 (0.110) data 0.000 (0.002) loss 3.0206 (2.6486) teacher_loss 2.9326 (2.5785) loss_zs_kd 0.5988 (0.4955) loss_oracle 0.0880 (0.0701) acc 78.1250 (76.0938) lr 3.1545e-04 eta 0:06:01
epoch [39/50] batch [180/288] time 0.102 (0.109) data 0.001 (0.002) loss 1.7038 (2.6319) teacher_loss 1.6517 (2.5616) loss_zs_kd 0.5737 (0.4913) loss_oracle 0.0522 (0.0703) acc 84.3750 (76.3368) lr 3.1545e-04 eta 0:05:56
epoch [39/50] batch [200/288] time 0.097 (0.108) data 0.000 (0.002) loss 2.1934 (2.6620) teacher_loss 2.0785 (2.5906) loss_zs_kd 0.5532 (0.4930) loss_oracle 0.1149 (0.0714) acc 81.2500 (76.0000) lr 3.1545e-04 eta 0:05:51
epoch [39/50] batch [220/288] time 0.104 (0.107) data 0.000 (0.002) loss 3.4408 (2.6618) teacher_loss 3.3786 (2.5907) loss_zs_kd 0.4669 (0.4891) loss_oracle 0.0623 (0.0711) acc 71.8750 (76.0511) lr 3.1545e-04 eta 0:05:47
epoch [39/50] batch [240/288] time 0.103 (0.107) data 0.000 (0.002) loss 2.3620 (2.6686) teacher_loss 2.2571 (2.5970) loss_zs_kd 0.5951 (0.4940) loss_oracle 0.1049 (0.0716) acc 68.7500 (75.8333) lr 3.1545e-04 eta 0:05:43
epoch [39/50] batch [260/288] time 0.096 (0.106) data 0.000 (0.002) loss 1.4065 (2.6634) teacher_loss 1.3166 (2.5919) loss_zs_kd 0.2744 (0.4912) loss_oracle 0.0898 (0.0715) acc 90.6250 (75.8774) lr 3.1545e-04 eta 0:05:39
epoch [39/50] batch [280/288] time 0.102 (0.106) data 0.001 (0.001) loss 2.1401 (2.6550) teacher_loss 2.0585 (2.5838) loss_zs_kd 0.3772 (0.4890) loss_oracle 0.0817 (0.0712) acc 75.0000 (75.9710) lr 3.1545e-04 eta 0:05:36
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,460
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 87.3%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,985
* accuracy: 81.8%
* error: 18.2%
* macro_f1: 78.4%
******* Domain a best val acc:      87.8%, epoch: 39 *******
******* Domain a best val test acc: 81.8%, epoch: 39 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [40/50] batch [20/288] time 0.116 (0.134) data 0.001 (0.017) loss 2.5908 (2.5935) teacher_loss 2.5003 (2.5117) loss_zs_kd 0.4318 (0.4616) loss_oracle 0.0906 (0.0818) acc 75.0000 (76.7188) lr 2.7103e-04 eta 0:07:00
epoch [40/50] batch [40/288] time 0.090 (0.114) data 0.000 (0.009) loss 3.5708 (2.6973) teacher_loss 3.4785 (2.6192) loss_zs_kd 0.5584 (0.4910) loss_oracle 0.0923 (0.0782) acc 71.8750 (75.4688) lr 2.7103e-04 eta 0:05:57
epoch [40/50] batch [60/288] time 0.091 (0.107) data 0.000 (0.006) loss 2.2446 (2.6287) teacher_loss 2.1204 (2.5520) loss_zs_kd 0.4757 (0.4795) loss_oracle 0.1242 (0.0766) acc 75.0000 (76.1458) lr 2.7103e-04 eta 0:05:33
epoch [40/50] batch [80/288] time 0.103 (0.105) data 0.001 (0.004) loss 3.5017 (2.6435) teacher_loss 3.4177 (2.5675) loss_zs_kd 0.5550 (0.4898) loss_oracle 0.0840 (0.0760) acc 68.7500 (76.1328) lr 2.7103e-04 eta 0:05:23
epoch [40/50] batch [100/288] time 0.099 (0.103) data 0.000 (0.004) loss 2.6995 (2.6563) teacher_loss 2.6330 (2.5806) loss_zs_kd 0.3952 (0.4847) loss_oracle 0.0665 (0.0758) acc 71.8750 (76.1562) lr 2.7103e-04 eta 0:05:17
epoch [40/50] batch [120/288] time 0.101 (0.102) data 0.000 (0.003) loss 2.4550 (2.6419) teacher_loss 2.3885 (2.5680) loss_zs_kd 0.6092 (0.4886) loss_oracle 0.0665 (0.0740) acc 81.2500 (76.1979) lr 2.7103e-04 eta 0:05:11
epoch [40/50] batch [140/288] time 0.104 (0.102) data 0.000 (0.003) loss 3.0888 (2.6556) teacher_loss 2.9971 (2.5805) loss_zs_kd 0.4814 (0.4887) loss_oracle 0.0918 (0.0751) acc 68.7500 (76.1384) lr 2.7103e-04 eta 0:05:07
epoch [40/50] batch [160/288] time 0.099 (0.101) data 0.000 (0.002) loss 3.7412 (2.6599) teacher_loss 3.6695 (2.5860) loss_zs_kd 0.5614 (0.4929) loss_oracle 0.0717 (0.0738) acc 78.1250 (76.2500) lr 2.7103e-04 eta 0:05:04
epoch [40/50] batch [180/288] time 0.091 (0.101) data 0.000 (0.002) loss 2.0528 (2.6869) teacher_loss 1.9649 (2.6127) loss_zs_kd 0.4419 (0.4920) loss_oracle 0.0879 (0.0742) acc 84.3750 (76.1458) lr 2.7103e-04 eta 0:05:00
epoch [40/50] batch [200/288] time 0.105 (0.100) data 0.001 (0.002) loss 2.2800 (2.6653) teacher_loss 2.1928 (2.5917) loss_zs_kd 0.6661 (0.4884) loss_oracle 0.0872 (0.0736) acc 71.8750 (76.2031) lr 2.7103e-04 eta 0:04:58
epoch [40/50] batch [220/288] time 0.106 (0.101) data 0.000 (0.002) loss 2.5517 (2.6640) teacher_loss 2.4994 (2.5910) loss_zs_kd 0.2853 (0.4897) loss_oracle 0.0523 (0.0730) acc 78.1250 (76.3352) lr 2.7103e-04 eta 0:04:57
epoch [40/50] batch [240/288] time 0.098 (0.101) data 0.000 (0.002) loss 1.6733 (2.6377) teacher_loss 1.6089 (2.5654) loss_zs_kd 0.5573 (0.4884) loss_oracle 0.0644 (0.0723) acc 87.5000 (76.6276) lr 2.7103e-04 eta 0:04:54
epoch [40/50] batch [260/288] time 0.089 (0.101) data 0.000 (0.002) loss 3.2391 (2.6206) teacher_loss 3.1580 (2.5479) loss_zs_kd 0.7017 (0.4931) loss_oracle 0.0811 (0.0727) acc 68.7500 (76.8029) lr 2.7103e-04 eta 0:04:52
epoch [40/50] batch [280/288] time 0.085 (0.100) data 0.000 (0.001) loss 2.8552 (2.6112) teacher_loss 2.7735 (2.5378) loss_zs_kd 0.7204 (0.4930) loss_oracle 0.0817 (0.0734) acc 78.1250 (76.9196) lr 2.7103e-04 eta 0:04:48
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,460
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 87.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,976
* accuracy: 81.4%
* error: 18.6%
* macro_f1: 77.9%
******* Domain a best val acc:      87.8%, epoch: 39 *******
******* Domain a best val test acc: 81.8%, epoch: 39 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [41/50] batch [20/288] time 0.107 (0.126) data 0.000 (0.015) loss 2.0280 (2.6186) teacher_loss 1.9636 (2.5398) loss_zs_kd 0.5129 (0.5550) loss_oracle 0.0644 (0.0788) acc 78.1250 (76.2500) lr 2.2949e-04 eta 0:05:59
epoch [41/50] batch [40/288] time 0.105 (0.114) data 0.000 (0.008) loss 2.3744 (2.6498) teacher_loss 2.2722 (2.5721) loss_zs_kd 0.5907 (0.5108) loss_oracle 0.1022 (0.0777) acc 78.1250 (75.5469) lr 2.2949e-04 eta 0:05:23
epoch [41/50] batch [60/288] time 0.100 (0.110) data 0.000 (0.005) loss 3.3827 (2.6955) teacher_loss 3.3584 (2.6195) loss_zs_kd 0.4571 (0.4895) loss_oracle 0.0243 (0.0760) acc 81.2500 (75.2604) lr 2.2949e-04 eta 0:05:10
epoch [41/50] batch [80/288] time 0.096 (0.108) data 0.000 (0.004) loss 1.4660 (2.6913) teacher_loss 1.3893 (2.6152) loss_zs_kd 0.3662 (0.4813) loss_oracle 0.0767 (0.0762) acc 84.3750 (75.8594) lr 2.2949e-04 eta 0:05:01
epoch [41/50] batch [100/288] time 0.101 (0.106) data 0.000 (0.003) loss 4.0438 (2.6948) teacher_loss 3.9142 (2.6189) loss_zs_kd 0.6550 (0.4921) loss_oracle 0.1296 (0.0759) acc 71.8750 (75.6875) lr 2.2949e-04 eta 0:04:54
epoch [41/50] batch [120/288] time 0.101 (0.105) data 0.000 (0.003) loss 3.7793 (2.6903) teacher_loss 3.7316 (2.6149) loss_zs_kd 0.6319 (0.4943) loss_oracle 0.0477 (0.0754) acc 71.8750 (75.8073) lr 2.2949e-04 eta 0:04:49
epoch [41/50] batch [140/288] time 0.110 (0.104) data 0.000 (0.002) loss 3.4931 (2.6686) teacher_loss 3.4504 (2.5932) loss_zs_kd 0.3833 (0.4943) loss_oracle 0.0427 (0.0753) acc 65.6250 (76.0045) lr 2.2949e-04 eta 0:04:45
epoch [41/50] batch [160/288] time 0.096 (0.104) data 0.000 (0.002) loss 1.9024 (2.6367) teacher_loss 1.7997 (2.5613) loss_zs_kd 0.2880 (0.4916) loss_oracle 0.1028 (0.0754) acc 81.2500 (76.2305) lr 2.2949e-04 eta 0:04:42
epoch [41/50] batch [180/288] time 0.099 (0.103) data 0.001 (0.002) loss 4.6773 (2.6289) teacher_loss 4.5985 (2.5539) loss_zs_kd 0.8599 (0.4946) loss_oracle 0.0788 (0.0750) acc 62.5000 (76.2847) lr 2.2949e-04 eta 0:04:39
epoch [41/50] batch [200/288] time 0.100 (0.103) data 0.000 (0.002) loss 2.7355 (2.6341) teacher_loss 2.6346 (2.5590) loss_zs_kd 0.5208 (0.4975) loss_oracle 0.1009 (0.0751) acc 68.7500 (76.2344) lr 2.2949e-04 eta 0:04:36
epoch [41/50] batch [220/288] time 0.100 (0.103) data 0.000 (0.002) loss 2.8236 (2.6306) teacher_loss 2.7953 (2.5556) loss_zs_kd 0.6489 (0.5005) loss_oracle 0.0283 (0.0751) acc 78.1250 (76.3068) lr 2.2949e-04 eta 0:04:34
epoch [41/50] batch [240/288] time 0.094 (0.102) data 0.000 (0.001) loss 2.7131 (2.6301) teacher_loss 2.6242 (2.5550) loss_zs_kd 0.5573 (0.4960) loss_oracle 0.0889 (0.0751) acc 81.2500 (76.4323) lr 2.2949e-04 eta 0:04:30
epoch [41/50] batch [260/288] time 0.100 (0.102) data 0.000 (0.001) loss 4.2728 (2.6341) teacher_loss 4.2019 (2.5590) loss_zs_kd 0.5454 (0.4955) loss_oracle 0.0708 (0.0752) acc 65.6250 (76.4183) lr 2.2949e-04 eta 0:04:27
epoch [41/50] batch [280/288] time 0.083 (0.101) data 0.000 (0.001) loss 2.3065 (2.6340) teacher_loss 2.2185 (2.5588) loss_zs_kd 0.4654 (0.4952) loss_oracle 0.0880 (0.0752) acc 81.2500 (76.4174) lr 2.2949e-04 eta 0:04:23
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,456
* accuracy: 87.7%
* error: 12.3%
* macro_f1: 87.1%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,972
* accuracy: 81.3%
* error: 18.7%
* macro_f1: 77.6%
******* Domain a best val acc:      87.8%, epoch: 39 *******
******* Domain a best val test acc: 81.8%, epoch: 39 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [42/50] batch [20/288] time 0.098 (0.121) data 0.000 (0.016) loss 2.1214 (2.5957) teacher_loss 2.0448 (2.5275) loss_zs_kd 0.3616 (0.4542) loss_oracle 0.0766 (0.0681) acc 81.2500 (77.8125) lr 1.9098e-04 eta 0:05:12
epoch [42/50] batch [40/288] time 0.098 (0.112) data 0.000 (0.008) loss 2.6092 (2.6260) teacher_loss 2.5305 (2.5554) loss_zs_kd 0.4507 (0.4687) loss_oracle 0.0788 (0.0706) acc 75.0000 (76.5625) lr 1.9098e-04 eta 0:04:45
epoch [42/50] batch [60/288] time 0.105 (0.108) data 0.000 (0.006) loss 3.7704 (2.6743) teacher_loss 3.7038 (2.6029) loss_zs_kd 0.4370 (0.4897) loss_oracle 0.0666 (0.0714) acc 71.8750 (76.3542) lr 1.9098e-04 eta 0:04:34
epoch [42/50] batch [80/288] time 0.097 (0.106) data 0.000 (0.004) loss 1.7194 (2.6198) teacher_loss 1.6383 (2.5448) loss_zs_kd 0.5269 (0.4901) loss_oracle 0.0811 (0.0750) acc 81.2500 (76.9141) lr 1.9098e-04 eta 0:04:27
epoch [42/50] batch [100/288] time 0.101 (0.106) data 0.000 (0.004) loss 2.6915 (2.6586) teacher_loss 2.6039 (2.5839) loss_zs_kd 0.5577 (0.4948) loss_oracle 0.0876 (0.0747) acc 78.1250 (76.6875) lr 1.9098e-04 eta 0:04:23
epoch [42/50] batch [120/288] time 0.100 (0.105) data 0.000 (0.003) loss 1.9592 (2.5872) teacher_loss 1.8678 (2.5112) loss_zs_kd 0.4953 (0.4858) loss_oracle 0.0914 (0.0759) acc 75.0000 (77.2396) lr 1.9098e-04 eta 0:04:18
epoch [42/50] batch [140/288] time 0.106 (0.104) data 0.000 (0.003) loss 3.6693 (2.5734) teacher_loss 3.5707 (2.4980) loss_zs_kd 0.4975 (0.4931) loss_oracle 0.0986 (0.0754) acc 59.3750 (77.2768) lr 1.9098e-04 eta 0:04:15
epoch [42/50] batch [160/288] time 0.109 (0.104) data 0.000 (0.002) loss 2.7797 (2.5844) teacher_loss 2.7175 (2.5082) loss_zs_kd 0.6418 (0.4961) loss_oracle 0.0622 (0.0761) acc 75.0000 (77.0117) lr 1.9098e-04 eta 0:04:11
epoch [42/50] batch [180/288] time 0.097 (0.103) data 0.000 (0.002) loss 2.7674 (2.5776) teacher_loss 2.7074 (2.5022) loss_zs_kd 0.6284 (0.4973) loss_oracle 0.0600 (0.0754) acc 87.5000 (77.0833) lr 1.9098e-04 eta 0:04:08
epoch [42/50] batch [200/288] time 0.106 (0.103) data 0.000 (0.002) loss 2.6349 (2.5858) teacher_loss 2.4911 (2.5103) loss_zs_kd 0.4196 (0.4968) loss_oracle 0.1438 (0.0755) acc 81.2500 (77.2500) lr 1.9098e-04 eta 0:04:05
epoch [42/50] batch [220/288] time 0.099 (0.103) data 0.000 (0.002) loss 2.5704 (2.5967) teacher_loss 2.5415 (2.5211) loss_zs_kd 0.5012 (0.4985) loss_oracle 0.0288 (0.0756) acc 81.2500 (77.1165) lr 1.9098e-04 eta 0:04:03
epoch [42/50] batch [240/288] time 0.098 (0.103) data 0.000 (0.002) loss 2.6527 (2.5975) teacher_loss 2.5760 (2.5221) loss_zs_kd 0.4535 (0.4955) loss_oracle 0.0766 (0.0755) acc 81.2500 (77.1745) lr 1.9098e-04 eta 0:04:01
epoch [42/50] batch [260/288] time 0.096 (0.102) data 0.000 (0.001) loss 3.1598 (2.6152) teacher_loss 3.0644 (2.5395) loss_zs_kd 0.6638 (0.4961) loss_oracle 0.0954 (0.0757) acc 75.0000 (76.9471) lr 1.9098e-04 eta 0:03:58
epoch [42/50] batch [280/288] time 0.164 (0.103) data 0.001 (0.001) loss 2.2483 (2.6021) teacher_loss 2.1484 (2.5262) loss_zs_kd 0.3786 (0.4923) loss_oracle 0.0999 (0.0759) acc 78.1250 (77.0759) lr 1.9098e-04 eta 0:03:57
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,464
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 87.3%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,972
* accuracy: 81.3%
* error: 18.7%
* macro_f1: 77.9%
******* Domain a best val acc:      87.9%, epoch: 42 *******
******* Domain a best val test acc: 81.3%, epoch: 42 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [43/50] batch [20/288] time 0.093 (0.111) data 0.000 (0.013) loss 3.8899 (2.8533) teacher_loss 3.7903 (2.7704) loss_zs_kd 0.4245 (0.4913) loss_oracle 0.0997 (0.0829) acc 65.6250 (75.0000) lr 1.5567e-04 eta 0:04:12
epoch [43/50] batch [40/288] time 0.098 (0.102) data 0.000 (0.007) loss 3.2689 (2.7584) teacher_loss 3.1852 (2.6852) loss_zs_kd 0.4987 (0.4931) loss_oracle 0.0838 (0.0732) acc 59.3750 (75.2344) lr 1.5567e-04 eta 0:03:51
epoch [43/50] batch [60/288] time 0.094 (0.100) data 0.000 (0.004) loss 5.1736 (2.8281) teacher_loss 5.0536 (2.7530) loss_zs_kd 0.5661 (0.5123) loss_oracle 0.1200 (0.0750) acc 59.3750 (74.8438) lr 1.5567e-04 eta 0:03:43
epoch [43/50] batch [80/288] time 0.094 (0.099) data 0.000 (0.003) loss 1.5964 (2.6778) teacher_loss 1.5367 (2.6018) loss_zs_kd 0.2344 (0.4856) loss_oracle 0.0596 (0.0760) acc 90.6250 (76.2500) lr 1.5567e-04 eta 0:03:39
epoch [43/50] batch [100/288] time 0.091 (0.098) data 0.000 (0.003) loss 3.7993 (2.6952) teacher_loss 3.7703 (2.6195) loss_zs_kd 0.5891 (0.4767) loss_oracle 0.0289 (0.0757) acc 62.5000 (75.7188) lr 1.5567e-04 eta 0:03:35
epoch [43/50] batch [120/288] time 0.092 (0.097) data 0.000 (0.002) loss 2.0681 (2.6901) teacher_loss 1.9820 (2.6147) loss_zs_kd 0.4389 (0.4856) loss_oracle 0.0861 (0.0754) acc 84.3750 (76.0938) lr 1.5567e-04 eta 0:03:32
epoch [43/50] batch [140/288] time 0.095 (0.097) data 0.000 (0.002) loss 1.6537 (2.6686) teacher_loss 1.6033 (2.5942) loss_zs_kd 0.6083 (0.4907) loss_oracle 0.0504 (0.0744) acc 87.5000 (76.1830) lr 1.5567e-04 eta 0:03:30
epoch [43/50] batch [160/288] time 0.092 (0.097) data 0.000 (0.002) loss 2.5736 (2.6495) teacher_loss 2.4920 (2.5754) loss_zs_kd 0.4043 (0.4912) loss_oracle 0.0816 (0.0742) acc 75.0000 (76.2500) lr 1.5567e-04 eta 0:03:27
epoch [43/50] batch [180/288] time 0.089 (0.096) data 0.000 (0.002) loss 2.2709 (2.6250) teacher_loss 2.2131 (2.5507) loss_zs_kd 0.3319 (0.4885) loss_oracle 0.0578 (0.0743) acc 78.1250 (76.4410) lr 1.5567e-04 eta 0:03:24
epoch [43/50] batch [200/288] time 0.098 (0.096) data 0.000 (0.001) loss 2.4099 (2.6315) teacher_loss 2.3716 (2.5578) loss_zs_kd 0.5834 (0.4872) loss_oracle 0.0383 (0.0737) acc 84.3750 (76.4062) lr 1.5567e-04 eta 0:03:22
epoch [43/50] batch [220/288] time 0.100 (0.096) data 0.000 (0.001) loss 2.7485 (2.6347) teacher_loss 2.7107 (2.5611) loss_zs_kd 0.5907 (0.4869) loss_oracle 0.0377 (0.0736) acc 75.0000 (76.5341) lr 1.5567e-04 eta 0:03:20
epoch [43/50] batch [240/288] time 0.100 (0.096) data 0.000 (0.001) loss 2.4593 (2.6171) teacher_loss 2.4166 (2.5436) loss_zs_kd 0.6838 (0.4885) loss_oracle 0.0427 (0.0735) acc 81.2500 (76.5755) lr 1.5567e-04 eta 0:03:17
epoch [43/50] batch [260/288] time 0.092 (0.095) data 0.000 (0.001) loss 2.2786 (2.6175) teacher_loss 2.1853 (2.5441) loss_zs_kd 0.5296 (0.4910) loss_oracle 0.0933 (0.0734) acc 78.1250 (76.6226) lr 1.5567e-04 eta 0:03:14
epoch [43/50] batch [280/288] time 0.081 (0.095) data 0.000 (0.001) loss 2.2814 (2.6207) teacher_loss 2.1952 (2.5470) loss_zs_kd 0.4892 (0.4916) loss_oracle 0.0862 (0.0736) acc 84.3750 (76.7299) lr 1.5567e-04 eta 0:03:13
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,462
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 87.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,977
* accuracy: 81.5%
* error: 18.5%
* macro_f1: 78.0%
******* Domain a best val acc:      87.9%, epoch: 42 *******
******* Domain a best val test acc: 81.3%, epoch: 42 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [44/50] batch [20/288] time 0.082 (0.105) data 0.000 (0.013) loss 3.3529 (2.5811) teacher_loss 3.2983 (2.5144) loss_zs_kd 0.4119 (0.5020) loss_oracle 0.0546 (0.0666) acc 65.6250 (75.6250) lr 1.2369e-04 eta 0:03:29
epoch [44/50] batch [40/288] time 0.093 (0.097) data 0.000 (0.006) loss 2.2403 (2.7329) teacher_loss 2.1540 (2.6640) loss_zs_kd 0.4894 (0.4924) loss_oracle 0.0864 (0.0689) acc 78.1250 (74.7656) lr 1.2369e-04 eta 0:03:12
epoch [44/50] batch [60/288] time 0.087 (0.095) data 0.000 (0.004) loss 2.4141 (2.6537) teacher_loss 2.3230 (2.5828) loss_zs_kd 0.3192 (0.5023) loss_oracle 0.0912 (0.0709) acc 78.1250 (75.9375) lr 1.2369e-04 eta 0:03:05
epoch [44/50] batch [80/288] time 0.090 (0.093) data 0.000 (0.003) loss 3.1808 (2.6574) teacher_loss 3.1048 (2.5851) loss_zs_kd 0.6189 (0.5022) loss_oracle 0.0760 (0.0722) acc 68.7500 (75.8984) lr 1.2369e-04 eta 0:03:00
epoch [44/50] batch [100/288] time 0.097 (0.093) data 0.000 (0.003) loss 2.8899 (2.6239) teacher_loss 2.8089 (2.5513) loss_zs_kd 0.4265 (0.4982) loss_oracle 0.0810 (0.0727) acc 71.8750 (76.2812) lr 1.2369e-04 eta 0:02:57
epoch [44/50] batch [120/288] time 0.100 (0.094) data 0.000 (0.002) loss 3.1336 (2.6122) teacher_loss 3.0621 (2.5392) loss_zs_kd 0.6263 (0.4935) loss_oracle 0.0716 (0.0730) acc 71.8750 (76.3542) lr 1.2369e-04 eta 0:02:57
epoch [44/50] batch [140/288] time 0.099 (0.094) data 0.001 (0.002) loss 2.3296 (2.6183) teacher_loss 2.2242 (2.5443) loss_zs_kd 0.5959 (0.4946) loss_oracle 0.1054 (0.0740) acc 87.5000 (76.5179) lr 1.2369e-04 eta 0:02:56
epoch [44/50] batch [160/288] time 0.098 (0.095) data 0.000 (0.002) loss 2.9184 (2.6334) teacher_loss 2.8297 (2.5596) loss_zs_kd 0.5844 (0.4967) loss_oracle 0.0887 (0.0739) acc 65.6250 (76.3672) lr 1.2369e-04 eta 0:02:56
epoch [44/50] batch [180/288] time 0.094 (0.095) data 0.000 (0.002) loss 2.5047 (2.6535) teacher_loss 2.4070 (2.5793) loss_zs_kd 0.3845 (0.4972) loss_oracle 0.0977 (0.0741) acc 84.3750 (76.3368) lr 1.2369e-04 eta 0:02:55
epoch [44/50] batch [200/288] time 0.092 (0.096) data 0.000 (0.001) loss 2.3463 (2.6507) teacher_loss 2.2816 (2.5758) loss_zs_kd 0.5062 (0.5026) loss_oracle 0.0647 (0.0749) acc 78.1250 (76.3438) lr 1.2369e-04 eta 0:02:54
epoch [44/50] batch [220/288] time 0.107 (0.097) data 0.000 (0.001) loss 2.4498 (2.6625) teacher_loss 2.3543 (2.5874) loss_zs_kd 0.4781 (0.5020) loss_oracle 0.0955 (0.0751) acc 75.0000 (76.2500) lr 1.2369e-04 eta 0:02:54
epoch [44/50] batch [240/288] time 0.101 (0.097) data 0.000 (0.001) loss 2.7597 (2.6960) teacher_loss 2.6760 (2.6212) loss_zs_kd 0.4977 (0.5008) loss_oracle 0.0837 (0.0748) acc 71.8750 (75.9245) lr 1.2369e-04 eta 0:02:52
epoch [44/50] batch [260/288] time 0.087 (0.098) data 0.000 (0.001) loss 3.8496 (2.6842) teacher_loss 3.7630 (2.6093) loss_zs_kd 0.3991 (0.4977) loss_oracle 0.0866 (0.0749) acc 68.7500 (75.9375) lr 1.2369e-04 eta 0:02:52
epoch [44/50] batch [280/288] time 0.102 (0.099) data 0.000 (0.001) loss 3.7050 (2.6807) teacher_loss 3.6378 (2.6063) loss_zs_kd 0.3628 (0.4992) loss_oracle 0.0672 (0.0744) acc 65.6250 (75.8259) lr 1.2369e-04 eta 0:02:51
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,463
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 87.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,971
* accuracy: 81.2%
* error: 18.8%
* macro_f1: 77.8%
******* Domain a best val acc:      87.9%, epoch: 42 *******
******* Domain a best val test acc: 81.3%, epoch: 42 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [45/50] batch [20/288] time 0.106 (0.132) data 0.000 (0.016) loss 3.5282 (2.6310) teacher_loss 3.4472 (2.5548) loss_zs_kd 0.2968 (0.4979) loss_oracle 0.0810 (0.0762) acc 65.6250 (77.0312) lr 9.5173e-05 eta 0:03:44
epoch [45/50] batch [40/288] time 0.107 (0.116) data 0.000 (0.008) loss 2.1986 (2.4546) teacher_loss 2.1345 (2.3777) loss_zs_kd 0.2682 (0.4963) loss_oracle 0.0641 (0.0769) acc 75.0000 (78.2031) lr 9.5173e-05 eta 0:03:16
epoch [45/50] batch [60/288] time 0.096 (0.110) data 0.000 (0.005) loss 1.4564 (2.4836) teacher_loss 1.3566 (2.4109) loss_zs_kd 0.4925 (0.4779) loss_oracle 0.0998 (0.0727) acc 84.3750 (77.8646) lr 9.5173e-05 eta 0:03:03
epoch [45/50] batch [80/288] time 0.100 (0.107) data 0.001 (0.004) loss 1.7237 (2.5204) teacher_loss 1.6732 (2.4497) loss_zs_kd 0.4468 (0.4757) loss_oracle 0.0505 (0.0707) acc 81.2500 (77.1484) lr 9.5173e-05 eta 0:02:55
epoch [45/50] batch [100/288] time 0.094 (0.105) data 0.000 (0.003) loss 4.2153 (2.5080) teacher_loss 4.1291 (2.4377) loss_zs_kd 0.5137 (0.4762) loss_oracle 0.0862 (0.0703) acc 53.1250 (77.4688) lr 9.5173e-05 eta 0:02:50
epoch [45/50] batch [120/288] time 0.102 (0.103) data 0.000 (0.003) loss 2.1620 (2.5341) teacher_loss 2.0953 (2.4617) loss_zs_kd 0.4489 (0.4819) loss_oracle 0.0666 (0.0724) acc 81.2500 (77.0052) lr 9.5173e-05 eta 0:02:46
epoch [45/50] batch [140/288] time 0.090 (0.103) data 0.000 (0.002) loss 0.6227 (2.5436) teacher_loss 0.5439 (2.4708) loss_zs_kd 0.4761 (0.4807) loss_oracle 0.0789 (0.0728) acc 100.0000 (77.0089) lr 9.5173e-05 eta 0:02:42
epoch [45/50] batch [160/288] time 0.093 (0.102) data 0.000 (0.002) loss 2.3843 (2.5532) teacher_loss 2.3294 (2.4804) loss_zs_kd 0.5578 (0.4861) loss_oracle 0.0550 (0.0728) acc 78.1250 (76.9141) lr 9.5173e-05 eta 0:02:40
epoch [45/50] batch [180/288] time 0.090 (0.101) data 0.000 (0.002) loss 2.6679 (2.5606) teacher_loss 2.5935 (2.4888) loss_zs_kd 0.6897 (0.4890) loss_oracle 0.0744 (0.0718) acc 78.1250 (76.7535) lr 9.5173e-05 eta 0:02:36
epoch [45/50] batch [200/288] time 0.100 (0.101) data 0.000 (0.002) loss 1.3664 (2.5696) teacher_loss 1.2681 (2.4977) loss_zs_kd 0.3533 (0.4914) loss_oracle 0.0983 (0.0718) acc 90.6250 (76.8281) lr 9.5173e-05 eta 0:02:34
epoch [45/50] batch [220/288] time 0.099 (0.101) data 0.000 (0.002) loss 2.0794 (2.5608) teacher_loss 1.9843 (2.4888) loss_zs_kd 0.6033 (0.4919) loss_oracle 0.0951 (0.0720) acc 84.3750 (77.0312) lr 9.5173e-05 eta 0:02:32
epoch [45/50] batch [240/288] time 0.121 (0.102) data 0.000 (0.002) loss 2.3462 (2.5381) teacher_loss 2.2840 (2.4658) loss_zs_kd 0.3923 (0.4891) loss_oracle 0.0622 (0.0723) acc 78.1250 (77.3828) lr 9.5173e-05 eta 0:02:31
epoch [45/50] batch [260/288] time 0.096 (0.102) data 0.000 (0.001) loss 3.3071 (2.5304) teacher_loss 3.2760 (2.4581) loss_zs_kd 0.7352 (0.4924) loss_oracle 0.0312 (0.0723) acc 68.7500 (77.5000) lr 9.5173e-05 eta 0:02:29
epoch [45/50] batch [280/288] time 0.086 (0.101) data 0.000 (0.001) loss 2.4776 (2.5629) teacher_loss 2.4155 (2.4902) loss_zs_kd 0.5508 (0.4934) loss_oracle 0.0622 (0.0726) acc 81.2500 (77.3326) lr 9.5173e-05 eta 0:02:26
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,469
* accuracy: 88.1%
* error: 11.9%
* macro_f1: 87.4%
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,969
* accuracy: 81.1%
* error: 18.9%
* macro_f1: 77.6%
******* Domain a best val acc:      88.1%, epoch: 45 *******
******* Domain a best val test acc: 81.1%, epoch: 45 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [46/50] batch [20/288] time 0.102 (0.116) data 0.000 (0.014) loss 2.2545 (2.6180) teacher_loss 2.1924 (2.5410) loss_zs_kd 0.4955 (0.4867) loss_oracle 0.0621 (0.0770) acc 78.1250 (77.0312) lr 7.0224e-05 eta 0:02:44
epoch [46/50] batch [40/288] time 0.094 (0.106) data 0.000 (0.007) loss 3.5295 (2.7110) teacher_loss 3.4864 (2.6336) loss_zs_kd 0.5129 (0.5077) loss_oracle 0.0432 (0.0774) acc 71.8750 (76.4844) lr 7.0224e-05 eta 0:02:28
epoch [46/50] batch [60/288] time 0.094 (0.102) data 0.001 (0.005) loss 2.4344 (2.6845) teacher_loss 2.3339 (2.6080) loss_zs_kd 0.5233 (0.5240) loss_oracle 0.1005 (0.0765) acc 81.2500 (76.2500) lr 7.0224e-05 eta 0:02:20
epoch [46/50] batch [80/288] time 0.098 (0.100) data 0.000 (0.004) loss 2.3580 (2.6109) teacher_loss 2.2885 (2.5330) loss_zs_kd 0.3774 (0.5108) loss_oracle 0.0694 (0.0780) acc 78.1250 (76.9141) lr 7.0224e-05 eta 0:02:16
epoch [46/50] batch [100/288] time 0.100 (0.100) data 0.000 (0.003) loss 2.0023 (2.6019) teacher_loss 1.9276 (2.5263) loss_zs_kd 0.6521 (0.5099) loss_oracle 0.0747 (0.0756) acc 84.3750 (77.0625) lr 7.0224e-05 eta 0:02:13
epoch [46/50] batch [120/288] time 0.090 (0.099) data 0.000 (0.003) loss 1.4759 (2.6020) teacher_loss 1.3921 (2.5270) loss_zs_kd 0.4669 (0.5092) loss_oracle 0.0838 (0.0751) acc 78.1250 (76.9531) lr 7.0224e-05 eta 0:02:10
epoch [46/50] batch [140/288] time 0.099 (0.098) data 0.000 (0.002) loss 2.4253 (2.5990) teacher_loss 2.3488 (2.5234) loss_zs_kd 0.4614 (0.5052) loss_oracle 0.0765 (0.0756) acc 78.1250 (76.9196) lr 7.0224e-05 eta 0:02:07
epoch [46/50] batch [160/288] time 0.099 (0.098) data 0.000 (0.002) loss 2.5668 (2.5884) teacher_loss 2.5141 (2.5134) loss_zs_kd 0.6064 (0.5033) loss_oracle 0.0527 (0.0750) acc 78.1250 (76.9141) lr 7.0224e-05 eta 0:02:05
epoch [46/50] batch [180/288] time 0.102 (0.098) data 0.000 (0.002) loss 2.8752 (2.6209) teacher_loss 2.8181 (2.5453) loss_zs_kd 0.6657 (0.5054) loss_oracle 0.0571 (0.0756) acc 81.2500 (76.6146) lr 7.0224e-05 eta 0:02:03
epoch [46/50] batch [200/288] time 0.094 (0.098) data 0.000 (0.002) loss 3.4748 (2.6567) teacher_loss 3.3988 (2.5811) loss_zs_kd 0.3128 (0.5025) loss_oracle 0.0759 (0.0756) acc 68.7500 (76.2188) lr 7.0224e-05 eta 0:02:01
epoch [46/50] batch [220/288] time 0.106 (0.099) data 0.000 (0.001) loss 2.4973 (2.6606) teacher_loss 2.3990 (2.5855) loss_zs_kd 0.3960 (0.4983) loss_oracle 0.0983 (0.0751) acc 78.1250 (76.1506) lr 7.0224e-05 eta 0:02:00
epoch [46/50] batch [240/288] time 0.102 (0.100) data 0.000 (0.001) loss 2.5753 (2.6614) teacher_loss 2.4798 (2.5867) loss_zs_kd 0.4985 (0.5013) loss_oracle 0.0955 (0.0748) acc 75.0000 (76.0417) lr 7.0224e-05 eta 0:01:59
epoch [46/50] batch [260/288] time 0.095 (0.100) data 0.001 (0.001) loss 2.2802 (2.6431) teacher_loss 2.1961 (2.5684) loss_zs_kd 0.3264 (0.5023) loss_oracle 0.0841 (0.0747) acc 87.5000 (76.2861) lr 7.0224e-05 eta 0:01:58
epoch [46/50] batch [280/288] time 0.101 (0.100) data 0.000 (0.001) loss 3.2609 (2.6476) teacher_loss 3.1510 (2.5726) loss_zs_kd 0.4953 (0.5034) loss_oracle 0.1099 (0.0750) acc 71.8750 (76.3393) lr 7.0224e-05 eta 0:01:56
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,467
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 87.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,966
* accuracy: 81.0%
* error: 19.0%
* macro_f1: 77.5%
******* Domain a best val acc:      88.1%, epoch: 45 *******
******* Domain a best val test acc: 81.1%, epoch: 45 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [47/50] batch [20/288] time 0.091 (0.113) data 0.000 (0.015) loss 3.5587 (2.6690) teacher_loss 3.4871 (2.5917) loss_zs_kd 0.4966 (0.5135) loss_oracle 0.0716 (0.0773) acc 65.6250 (78.2812) lr 4.8943e-05 eta 0:02:08
epoch [47/50] batch [40/288] time 0.100 (0.105) data 0.000 (0.008) loss 3.4718 (2.6749) teacher_loss 3.3969 (2.5977) loss_zs_kd 0.4015 (0.4741) loss_oracle 0.0750 (0.0772) acc 75.0000 (77.4219) lr 4.8943e-05 eta 0:01:56
epoch [47/50] batch [60/288] time 0.098 (0.102) data 0.000 (0.005) loss 5.2027 (2.6937) teacher_loss 5.1382 (2.6184) loss_zs_kd 0.4217 (0.4732) loss_oracle 0.0645 (0.0753) acc 65.6250 (77.4479) lr 4.8943e-05 eta 0:01:51
epoch [47/50] batch [80/288] time 0.105 (0.101) data 0.000 (0.004) loss 2.8194 (2.6701) teacher_loss 2.7194 (2.5960) loss_zs_kd 0.3767 (0.4862) loss_oracle 0.0999 (0.0741) acc 75.0000 (77.4219) lr 4.8943e-05 eta 0:01:47
epoch [47/50] batch [100/288] time 0.092 (0.099) data 0.000 (0.003) loss 1.7073 (2.6095) teacher_loss 1.5951 (2.5365) loss_zs_kd 0.3241 (0.4925) loss_oracle 0.1122 (0.0730) acc 81.2500 (77.3125) lr 4.8943e-05 eta 0:01:44
epoch [47/50] batch [120/288] time 0.106 (0.099) data 0.000 (0.003) loss 3.9272 (2.5779) teacher_loss 3.8751 (2.5053) loss_zs_kd 0.3802 (0.4951) loss_oracle 0.0521 (0.0726) acc 68.7500 (77.1354) lr 4.8943e-05 eta 0:01:42
epoch [47/50] batch [140/288] time 0.101 (0.099) data 0.000 (0.002) loss 2.0883 (2.5884) teacher_loss 2.0123 (2.5151) loss_zs_kd 0.7142 (0.5021) loss_oracle 0.0760 (0.0733) acc 75.0000 (77.0982) lr 4.8943e-05 eta 0:01:40
epoch [47/50] batch [160/288] time 0.105 (0.099) data 0.000 (0.002) loss 2.3056 (2.6082) teacher_loss 2.2077 (2.5339) loss_zs_kd 0.8255 (0.5072) loss_oracle 0.0980 (0.0743) acc 75.0000 (76.6992) lr 4.8943e-05 eta 0:01:38
epoch [47/50] batch [180/288] time 0.101 (0.099) data 0.000 (0.002) loss 2.5607 (2.6074) teacher_loss 2.4646 (2.5328) loss_zs_kd 0.5462 (0.5082) loss_oracle 0.0961 (0.0745) acc 71.8750 (76.4583) lr 4.8943e-05 eta 0:01:35
epoch [47/50] batch [200/288] time 0.182 (0.100) data 0.001 (0.002) loss 0.9007 (2.6083) teacher_loss 0.8487 (2.5331) loss_zs_kd 0.3685 (0.5029) loss_oracle 0.0520 (0.0752) acc 90.6250 (76.5781) lr 4.8943e-05 eta 0:01:35
epoch [47/50] batch [220/288] time 0.090 (0.100) data 0.000 (0.002) loss 1.9326 (2.6132) teacher_loss 1.8465 (2.5385) loss_zs_kd 0.5198 (0.5038) loss_oracle 0.0861 (0.0747) acc 78.1250 (76.5057) lr 4.8943e-05 eta 0:01:33
epoch [47/50] batch [240/288] time 0.106 (0.100) data 0.000 (0.001) loss 1.5672 (2.6038) teacher_loss 1.4907 (2.5296) loss_zs_kd 0.4419 (0.5040) loss_oracle 0.0766 (0.0742) acc 87.5000 (76.6406) lr 4.8943e-05 eta 0:01:31
epoch [47/50] batch [260/288] time 0.091 (0.100) data 0.000 (0.001) loss 3.8294 (2.6073) teacher_loss 3.7432 (2.5334) loss_zs_kd 0.6637 (0.5055) loss_oracle 0.0863 (0.0739) acc 71.8750 (76.6707) lr 4.8943e-05 eta 0:01:29
epoch [47/50] batch [280/288] time 0.086 (0.099) data 0.000 (0.001) loss 1.5676 (2.6215) teacher_loss 1.4717 (2.5476) loss_zs_kd 0.5139 (0.5074) loss_oracle 0.0960 (0.0740) acc 93.7500 (76.5290) lr 4.8943e-05 eta 0:01:26
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,468
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 87.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,968
* accuracy: 81.1%
* error: 18.9%
* macro_f1: 77.6%
******* Domain a best val acc:      88.1%, epoch: 45 *******
******* Domain a best val test acc: 81.1%, epoch: 45 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [48/50] batch [20/288] time 0.095 (0.116) data 0.000 (0.015) loss 2.9711 (2.8300) teacher_loss 2.8916 (2.7632) loss_zs_kd 0.5080 (0.5166) loss_oracle 0.0795 (0.0668) acc 71.8750 (73.9062) lr 3.1417e-05 eta 0:01:38
epoch [48/50] batch [40/288] time 0.097 (0.109) data 0.000 (0.007) loss 3.8632 (2.7527) teacher_loss 3.7586 (2.6822) loss_zs_kd 0.5996 (0.5135) loss_oracle 0.1046 (0.0705) acc 71.8750 (75.3125) lr 3.1417e-05 eta 0:01:30
epoch [48/50] batch [60/288] time 0.099 (0.106) data 0.001 (0.005) loss 2.8175 (2.6573) teacher_loss 2.7214 (2.5865) loss_zs_kd 0.4914 (0.5325) loss_oracle 0.0961 (0.0708) acc 68.7500 (76.0938) lr 3.1417e-05 eta 0:01:25
epoch [48/50] batch [80/288] time 0.099 (0.105) data 0.000 (0.004) loss 3.0677 (2.6417) teacher_loss 3.0010 (2.5702) loss_zs_kd 0.5226 (0.5166) loss_oracle 0.0667 (0.0716) acc 75.0000 (76.4062) lr 3.1417e-05 eta 0:01:22
epoch [48/50] batch [100/288] time 0.101 (0.104) data 0.000 (0.003) loss 2.6473 (2.6229) teacher_loss 2.5662 (2.5503) loss_zs_kd 0.4026 (0.5097) loss_oracle 0.0811 (0.0725) acc 78.1250 (76.5938) lr 3.1417e-05 eta 0:01:19
epoch [48/50] batch [120/288] time 0.106 (0.104) data 0.000 (0.003) loss 2.4540 (2.6216) teacher_loss 2.3629 (2.5467) loss_zs_kd 0.6393 (0.5061) loss_oracle 0.0911 (0.0749) acc 75.0000 (76.5885) lr 3.1417e-05 eta 0:01:17
epoch [48/50] batch [140/288] time 0.101 (0.103) data 0.001 (0.002) loss 2.4682 (2.6282) teacher_loss 2.3604 (2.5523) loss_zs_kd 0.5697 (0.5073) loss_oracle 0.1078 (0.0759) acc 81.2500 (76.5625) lr 3.1417e-05 eta 0:01:14
epoch [48/50] batch [160/288] time 0.106 (0.105) data 0.000 (0.002) loss 2.4180 (2.6477) teacher_loss 2.3391 (2.5721) loss_zs_kd 0.3956 (0.5101) loss_oracle 0.0789 (0.0756) acc 75.0000 (76.1914) lr 3.1417e-05 eta 0:01:14
epoch [48/50] batch [180/288] time 0.095 (0.106) data 0.000 (0.002) loss 2.2875 (2.6075) teacher_loss 2.1925 (2.5322) loss_zs_kd 0.4974 (0.5147) loss_oracle 0.0950 (0.0753) acc 84.3750 (76.7361) lr 3.1417e-05 eta 0:01:12
epoch [48/50] batch [200/288] time 0.100 (0.105) data 0.000 (0.002) loss 1.7771 (2.6051) teacher_loss 1.6650 (2.5296) loss_zs_kd 0.5619 (0.5154) loss_oracle 0.1120 (0.0755) acc 84.3750 (76.9219) lr 3.1417e-05 eta 0:01:09
epoch [48/50] batch [220/288] time 0.101 (0.105) data 0.000 (0.002) loss 2.1475 (2.5944) teacher_loss 2.0954 (2.5194) loss_zs_kd 0.6281 (0.5151) loss_oracle 0.0521 (0.0750) acc 81.2500 (77.1449) lr 3.1417e-05 eta 0:01:07
epoch [48/50] batch [240/288] time 0.099 (0.104) data 0.000 (0.002) loss 2.7090 (2.5949) teacher_loss 2.6418 (2.5201) loss_zs_kd 0.4594 (0.5159) loss_oracle 0.0673 (0.0749) acc 81.2500 (77.1094) lr 3.1417e-05 eta 0:01:05
epoch [48/50] batch [260/288] time 0.090 (0.104) data 0.001 (0.001) loss 2.6369 (2.5960) teacher_loss 2.5441 (2.5215) loss_zs_kd 0.4875 (0.5138) loss_oracle 0.0928 (0.0745) acc 75.0000 (77.0192) lr 3.1417e-05 eta 0:01:02
epoch [48/50] batch [280/288] time 0.103 (0.104) data 0.000 (0.001) loss 2.3013 (2.5896) teacher_loss 2.2148 (2.5153) loss_zs_kd 0.5407 (0.5137) loss_oracle 0.0865 (0.0743) acc 78.1250 (76.9978) lr 3.1417e-05 eta 0:01:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,464
* accuracy: 87.9%
* error: 12.1%
* macro_f1: 87.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,966
* accuracy: 81.0%
* error: 19.0%
* macro_f1: 77.5%
******* Domain a best val acc:      88.1%, epoch: 45 *******
******* Domain a best val test acc: 81.1%, epoch: 45 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [49/50] batch [20/288] time 0.092 (0.113) data 0.000 (0.014) loss 2.8920 (2.4624) teacher_loss 2.8205 (2.3954) loss_zs_kd 0.4445 (0.5235) loss_oracle 0.0715 (0.0670) acc 71.8750 (78.2812) lr 1.7713e-05 eta 0:01:03
epoch [49/50] batch [40/288] time 0.099 (0.105) data 0.000 (0.007) loss 3.8608 (2.5324) teacher_loss 3.7969 (2.4659) loss_zs_kd 0.5988 (0.5109) loss_oracle 0.0639 (0.0665) acc 65.6250 (77.5781) lr 1.7713e-05 eta 0:00:56
epoch [49/50] batch [60/288] time 0.095 (0.102) data 0.000 (0.005) loss 1.6322 (2.5165) teacher_loss 1.5606 (2.4466) loss_zs_kd 0.3841 (0.5225) loss_oracle 0.0716 (0.0699) acc 90.6250 (77.9688) lr 1.7713e-05 eta 0:00:52
epoch [49/50] batch [80/288] time 0.091 (0.101) data 0.000 (0.004) loss 2.6363 (2.4797) teacher_loss 2.5742 (2.4093) loss_zs_kd 0.4231 (0.5231) loss_oracle 0.0622 (0.0704) acc 78.1250 (78.5547) lr 1.7713e-05 eta 0:00:49
epoch [49/50] batch [100/288] time 0.101 (0.100) data 0.000 (0.003) loss 1.6608 (2.5202) teacher_loss 1.6015 (2.4501) loss_zs_kd 0.4514 (0.5261) loss_oracle 0.0594 (0.0700) acc 87.5000 (77.8750) lr 1.7713e-05 eta 0:00:47
epoch [49/50] batch [120/288] time 0.091 (0.099) data 0.000 (0.003) loss 3.5289 (2.5483) teacher_loss 3.4388 (2.4781) loss_zs_kd 0.4362 (0.5186) loss_oracle 0.0901 (0.0701) acc 75.0000 (77.4479) lr 1.7713e-05 eta 0:00:45
epoch [49/50] batch [140/288] time 0.097 (0.100) data 0.000 (0.002) loss 3.4178 (2.5477) teacher_loss 3.3506 (2.4766) loss_zs_kd 0.6599 (0.5210) loss_oracle 0.0672 (0.0711) acc 71.8750 (77.4777) lr 1.7713e-05 eta 0:00:43
epoch [49/50] batch [160/288] time 0.090 (0.100) data 0.000 (0.002) loss 2.2394 (2.5364) teacher_loss 2.1939 (2.4644) loss_zs_kd 0.4849 (0.5164) loss_oracle 0.0455 (0.0720) acc 87.5000 (77.7539) lr 1.7713e-05 eta 0:00:41
epoch [49/50] batch [180/288] time 0.093 (0.099) data 0.000 (0.002) loss 2.9979 (2.5249) teacher_loss 2.9552 (2.4535) loss_zs_kd 0.7245 (0.5142) loss_oracle 0.0427 (0.0714) acc 71.8750 (77.8125) lr 1.7713e-05 eta 0:00:39
epoch [49/50] batch [200/288] time 0.096 (0.099) data 0.000 (0.002) loss 4.1926 (2.5574) teacher_loss 4.1327 (2.4858) loss_zs_kd 0.4319 (0.5123) loss_oracle 0.0599 (0.0716) acc 71.8750 (77.6250) lr 1.7713e-05 eta 0:00:37
epoch [49/50] batch [220/288] time 0.093 (0.098) data 0.000 (0.002) loss 3.2420 (2.5739) teacher_loss 3.1531 (2.5016) loss_zs_kd 0.5565 (0.5131) loss_oracle 0.0890 (0.0723) acc 71.8750 (77.5568) lr 1.7713e-05 eta 0:00:35
epoch [49/50] batch [240/288] time 0.090 (0.098) data 0.000 (0.001) loss 1.9926 (2.5706) teacher_loss 1.8927 (2.4977) loss_zs_kd 0.4783 (0.5117) loss_oracle 0.0999 (0.0729) acc 84.3750 (77.5260) lr 1.7713e-05 eta 0:00:32
epoch [49/50] batch [260/288] time 0.115 (0.098) data 0.000 (0.001) loss 2.7331 (2.5528) teacher_loss 2.6710 (2.4793) loss_zs_kd 0.6076 (0.5079) loss_oracle 0.0622 (0.0735) acc 81.2500 (77.7043) lr 1.7713e-05 eta 0:00:31
epoch [49/50] batch [280/288] time 0.085 (0.098) data 0.000 (0.001) loss 1.5365 (2.5376) teacher_loss 1.4311 (2.4642) loss_zs_kd 0.3477 (0.5045) loss_oracle 0.1054 (0.0733) acc 84.3750 (77.7009) lr 1.7713e-05 eta 0:00:29
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,466
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 87.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,966
* accuracy: 81.0%
* error: 19.0%
* macro_f1: 77.5%
******* Domain a best val acc:      88.1%, epoch: 45 *******
******* Domain a best val test acc: 81.1%, epoch: 45 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
epoch [50/50] batch [20/288] time 0.127 (0.138) data 0.000 (0.015) loss 3.1864 (2.3148) teacher_loss 3.1185 (2.2468) loss_zs_kd 0.3965 (0.5073) loss_oracle 0.0678 (0.0680) acc 68.7500 (77.5000) lr 7.8853e-06 eta 0:00:36
epoch [50/50] batch [40/288] time 0.102 (0.131) data 0.000 (0.008) loss 2.6114 (2.5468) teacher_loss 2.5375 (2.4746) loss_zs_kd 0.6834 (0.5127) loss_oracle 0.0739 (0.0722) acc 78.1250 (76.7188) lr 7.8853e-06 eta 0:00:32
epoch [50/50] batch [60/288] time 0.111 (0.122) data 0.000 (0.005) loss 2.4508 (2.5549) teacher_loss 2.4031 (2.4824) loss_zs_kd 0.4736 (0.4976) loss_oracle 0.0478 (0.0725) acc 81.2500 (77.3438) lr 7.8853e-06 eta 0:00:27
epoch [50/50] batch [80/288] time 0.099 (0.117) data 0.000 (0.004) loss 1.8982 (2.5856) teacher_loss 1.8172 (2.5142) loss_zs_kd 0.4737 (0.4999) loss_oracle 0.0810 (0.0714) acc 81.2500 (76.8750) lr 7.8853e-06 eta 0:00:24
epoch [50/50] batch [100/288] time 0.098 (0.113) data 0.000 (0.003) loss 3.1087 (2.6281) teacher_loss 3.0487 (2.5578) loss_zs_kd 0.5648 (0.5039) loss_oracle 0.0600 (0.0703) acc 78.1250 (76.3438) lr 7.8853e-06 eta 0:00:21
epoch [50/50] batch [120/288] time 0.086 (0.113) data 0.000 (0.003) loss 2.4921 (2.6261) teacher_loss 2.4010 (2.5555) loss_zs_kd 0.4903 (0.5033) loss_oracle 0.0911 (0.0705) acc 71.8750 (76.3802) lr 7.8853e-06 eta 0:00:18
epoch [50/50] batch [140/288] time 0.097 (0.111) data 0.000 (0.002) loss 3.0278 (2.6009) teacher_loss 2.9705 (2.5290) loss_zs_kd 0.5487 (0.5129) loss_oracle 0.0572 (0.0719) acc 75.0000 (76.5402) lr 7.8853e-06 eta 0:00:16
epoch [50/50] batch [160/288] time 0.099 (0.109) data 0.000 (0.002) loss 2.4843 (2.5701) teacher_loss 2.3836 (2.4986) loss_zs_kd 0.3345 (0.5068) loss_oracle 0.1007 (0.0715) acc 78.1250 (76.8164) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [180/288] time 0.093 (0.108) data 0.000 (0.002) loss 2.8663 (2.6092) teacher_loss 2.7886 (2.5381) loss_zs_kd 0.6183 (0.5103) loss_oracle 0.0776 (0.0712) acc 75.0000 (76.6493) lr 7.8853e-06 eta 0:00:11
epoch [50/50] batch [200/288] time 0.094 (0.107) data 0.000 (0.002) loss 3.2049 (2.6203) teacher_loss 3.1332 (2.5486) loss_zs_kd 0.3839 (0.5096) loss_oracle 0.0717 (0.0717) acc 65.6250 (76.6250) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [220/288] time 0.097 (0.106) data 0.000 (0.002) loss 1.8171 (2.6131) teacher_loss 1.7787 (2.5412) loss_zs_kd 0.3888 (0.5107) loss_oracle 0.0383 (0.0720) acc 78.1250 (76.6477) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [240/288] time 0.095 (0.105) data 0.000 (0.001) loss 2.4229 (2.6393) teacher_loss 2.3393 (2.5665) loss_zs_kd 0.3885 (0.5091) loss_oracle 0.0836 (0.0728) acc 78.1250 (76.6406) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [260/288] time 0.093 (0.105) data 0.000 (0.001) loss 1.9297 (2.6561) teacher_loss 1.8524 (2.5834) loss_zs_kd 0.4869 (0.5107) loss_oracle 0.0773 (0.0728) acc 75.0000 (76.4062) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [280/288] time 0.086 (0.104) data 0.000 (0.001) loss 3.5964 (2.6472) teacher_loss 3.4814 (2.5739) loss_zs_kd 0.5462 (0.5101) loss_oracle 0.1150 (0.0733) acc 59.3750 (76.4844) lr 7.8853e-06 eta 0:00:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,465
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 87.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,966
* accuracy: 81.0%
* error: 19.0%
* macro_f1: 77.5%
******* Domain a best val acc:      88.1%, epoch: 45 *******
******* Domain a best val test acc: 81.1%, epoch: 45 *******
******* Domain a best test acc:     83.1%, epoch: 5 *******
Checkpoint saved to icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model.pth.tar-50
Finish the whole training
Elapsed: 0:46:13
[Info] Hyperparameters saved to: icml/multi-dg/oracle/12_justnegativece/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/hyperparameters.json
