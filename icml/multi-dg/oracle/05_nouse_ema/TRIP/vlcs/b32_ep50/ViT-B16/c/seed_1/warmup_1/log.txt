Loading trainer: TRIP
Loading dataset: SPG_VLCS
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------------------------
Dataset    SPG_VLCS
Source     ['labelme', 'pascal', 'sun']
Target     ['caltech']
# classes  5
# train_x  6,519
# val      2,795
# test     1,415
---------  ----------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,195
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/oracle/05_nouse_ema/TRIP/vlcs/b32_ep50/ViT-B16/c/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/203] time 0.146 (0.152) data 0.000 (0.019) loss 0.8152 (0.9969) ce_loss 0.6226 (0.7933) teacher_loss 0.6224 (0.7936) loss_zs_kd 0.0002 (0.0001) loss_oracle 0.1927 (0.2032) acc 68.7500 (70.9375) kd_loss 0.9106 (0.9609) lr 1.0000e-05 eta 0:25:36
epoch [1/50] batch [40/203] time 0.130 (0.127) data 0.000 (0.009) loss 0.9801 (0.9606) ce_loss 0.7842 (0.7590) teacher_loss 0.7817 (0.7592) loss_zs_kd 0.0011 (0.0004) loss_oracle 0.1979 (0.2012) acc 75.0000 (72.2656) kd_loss 0.9346 (0.9509) lr 1.0000e-05 eta 0:21:26
epoch [1/50] batch [60/203] time 0.105 (0.117) data 0.000 (0.006) loss 0.9871 (0.9596) ce_loss 0.7793 (0.7569) teacher_loss 0.7875 (0.7573) loss_zs_kd 0.0033 (0.0009) loss_oracle 0.1979 (0.2018) acc 75.0000 (73.2812) kd_loss 0.9442 (0.9544) lr 1.0000e-05 eta 0:19:43
epoch [1/50] batch [80/203] time 0.126 (0.118) data 0.000 (0.005) loss 0.8873 (0.9308) ce_loss 0.6860 (0.7273) teacher_loss 0.6836 (0.7278) loss_zs_kd 0.0049 (0.0018) loss_oracle 0.2013 (0.2022) acc 71.8750 (74.1797) kd_loss 0.9427 (0.9561) lr 1.0000e-05 eta 0:19:47
epoch [1/50] batch [100/203] time 0.079 (0.120) data 0.000 (0.004) loss 1.0950 (0.9377) ce_loss 0.8818 (0.7331) teacher_loss 0.8842 (0.7334) loss_zs_kd 0.0065 (0.0026) loss_oracle 0.2075 (0.2030) acc 71.8750 (73.9688) kd_loss 0.9819 (0.9596) lr 1.0000e-05 eta 0:20:02
epoch [1/50] batch [120/203] time 0.131 (0.120) data 0.000 (0.003) loss 1.0674 (0.9376) ce_loss 0.8599 (0.7321) teacher_loss 0.8597 (0.7325) loss_zs_kd 0.0079 (0.0033) loss_oracle 0.2037 (0.2034) acc 65.6250 (73.4375) kd_loss 0.9614 (0.9618) lr 1.0000e-05 eta 0:20:07
epoch [1/50] batch [140/203] time 0.111 (0.117) data 0.000 (0.003) loss 0.7925 (0.9320) ce_loss 0.5898 (0.7263) teacher_loss 0.5900 (0.7266) loss_zs_kd 0.0080 (0.0040) loss_oracle 0.1984 (0.2033) acc 78.1250 (73.7500) kd_loss 0.9394 (0.9619) lr 1.0000e-05 eta 0:19:33
epoch [1/50] batch [160/203] time 0.083 (0.115) data 0.000 (0.003) loss 0.8134 (0.9308) ce_loss 0.6162 (0.7252) teacher_loss 0.6157 (0.7255) loss_zs_kd 0.0065 (0.0047) loss_oracle 0.1944 (0.2030) acc 78.1250 (73.8867) kd_loss 0.9204 (0.9603) lr 1.0000e-05 eta 0:19:09
epoch [1/50] batch [180/203] time 0.118 (0.114) data 0.000 (0.002) loss 1.0716 (0.9389) ce_loss 0.8647 (0.7331) teacher_loss 0.8657 (0.7334) loss_zs_kd 0.0090 (0.0051) loss_oracle 0.2014 (0.2030) acc 65.6250 (73.7326) kd_loss 0.9534 (0.9602) lr 1.0000e-05 eta 0:18:54
epoch [1/50] batch [200/203] time 0.124 (0.112) data 0.000 (0.002) loss 0.8729 (0.9315) ce_loss 0.6484 (0.7253) teacher_loss 0.6504 (0.7256) loss_zs_kd 0.0124 (0.0055) loss_oracle 0.2163 (0.2031) acc 75.0000 (73.9375) kd_loss 1.0215 (0.9610) lr 1.0000e-05 eta 0:18:36
Evaluate on the *val* set
=> result
* total: 2,795
* correct: 2,213
* accuracy: 79.2%
* error: 20.8%
* macro_f1: 82.9%
Checkpoint saved to icml/multi-dg/oracle/05_nouse_ema/TRIP/vlcs/b32_ep50/ViT-B16/c/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 1,415
* correct: 1,415
* accuracy: 100.0%
* error: 0.0%
* macro_f1: 100.0%
Checkpoint saved to icml/multi-dg/oracle/05_nouse_ema/TRIP/vlcs/b32_ep50/ViT-B16/c/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain c best val acc:      79.2%, epoch: 1 *******
******* Domain c best val test acc: 100.0%, epoch: 1 *******
******* Domain c best test acc:     100.0%, epoch: 1 *******
[TensorBoard] Initialized writer in: icml/multi-dg/oracle/05_nouse_ema/TRIP/vlcs/b32_ep50/ViT-B16/c/seed_1/warmup_1/ana
epoch [2/50] batch [20/203] time 0.077 (0.126) data 0.000 (0.018) loss 1.1250 (1.0093) ce_loss 0.8540 (0.6740) teacher_loss 0.8148 (0.6736) loss_zs_kd 0.0499 (0.0629) loss_oracle 0.2852 (0.3043) acc 68.7500 (75.7812) kd_loss 1.0579 (0.9885) lr 2.0000e-03 eta 0:20:54
