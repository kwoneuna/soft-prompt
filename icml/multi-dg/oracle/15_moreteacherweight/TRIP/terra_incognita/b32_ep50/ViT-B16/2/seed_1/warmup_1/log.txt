Loading trainer: TRIP
Loading dataset: SPG_TerraIncognita
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------------------------------------------
Dataset    SPG_TerraIncognita
Source     ['location_100', 'location_43', 'location_46']
Target     ['location_38']
# classes  10
# train_x  10,216
# val      4,378
# test     9,736
---------  ----------------------------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
alpha_logit                                        1
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,196
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/oracle/15_moreteacherweight/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/319] time 0.102 (0.161) data 0.000 (0.043) loss 1.7248 (2.4768) teacher_loss 1.2533 (2.0156) loss_zs_kd 0.0002 (0.0001) loss_oracle 0.4715 (0.4612) acc 59.3750 (36.8750) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:42:47
epoch [1/50] batch [40/319] time 0.087 (0.125) data 0.000 (0.022) loss 2.0202 (2.3647) teacher_loss 1.5765 (1.9122) loss_zs_kd 0.0015 (0.0003) loss_oracle 0.4437 (0.4526) acc 56.2500 (39.7656) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:33:15
epoch [1/50] batch [60/319] time 0.085 (0.113) data 0.000 (0.015) loss 2.6332 (2.3952) teacher_loss 2.1709 (1.9402) loss_zs_kd 0.0025 (0.0007) loss_oracle 0.4623 (0.4551) acc 31.2500 (39.2188) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:29:54
epoch [1/50] batch [80/319] time 0.081 (0.105) data 0.000 (0.011) loss 2.4219 (2.3759) teacher_loss 1.9708 (1.9213) loss_zs_kd 0.0039 (0.0015) loss_oracle 0.4511 (0.4545) acc 43.7500 (38.8672) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:27:51
epoch [1/50] batch [100/319] time 0.079 (0.101) data 0.000 (0.009) loss 2.6596 (2.3361) teacher_loss 2.1713 (1.8827) loss_zs_kd 0.0078 (0.0025) loss_oracle 0.4882 (0.4534) acc 25.0000 (39.5000) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:26:37
epoch [1/50] batch [120/319] time 0.081 (0.097) data 0.000 (0.008) loss 2.3000 (2.3335) teacher_loss 1.8702 (1.8808) loss_zs_kd 0.0061 (0.0034) loss_oracle 0.4298 (0.4527) acc 40.6250 (39.6615) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:25:37
epoch [1/50] batch [140/319] time 0.077 (0.094) data 0.000 (0.006) loss 2.4497 (2.3323) teacher_loss 2.0524 (1.8808) loss_zs_kd 0.0101 (0.0044) loss_oracle 0.3974 (0.4516) acc 37.5000 (39.5982) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:24:50
epoch [1/50] batch [160/319] time 0.080 (0.093) data 0.000 (0.006) loss 2.2695 (2.3274) teacher_loss 1.8273 (1.8767) loss_zs_kd 0.0133 (0.0057) loss_oracle 0.4421 (0.4507) acc 34.3750 (39.7266) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:24:24
epoch [1/50] batch [180/319] time 0.062 (0.091) data 0.000 (0.005) loss 2.5948 (2.3233) teacher_loss 2.1766 (1.8734) loss_zs_kd 0.0235 (0.0072) loss_oracle 0.4182 (0.4499) acc 28.1250 (39.6701) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:23:58
epoch [1/50] batch [200/319] time 0.076 (0.090) data 0.000 (0.005) loss 2.4659 (2.3186) teacher_loss 1.9905 (1.8688) loss_zs_kd 0.0253 (0.0088) loss_oracle 0.4754 (0.4498) acc 31.2500 (39.6562) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:23:35
epoch [1/50] batch [220/319] time 0.080 (0.089) data 0.000 (0.004) loss 2.1743 (2.3244) teacher_loss 1.7133 (1.8745) loss_zs_kd 0.0316 (0.0106) loss_oracle 0.4610 (0.4499) acc 46.8750 (39.2614) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:23:21
epoch [1/50] batch [240/319] time 0.078 (0.088) data 0.000 (0.004) loss 2.3696 (2.3319) teacher_loss 1.9335 (1.8810) loss_zs_kd 0.0447 (0.0126) loss_oracle 0.4361 (0.4509) acc 28.1250 (38.9714) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:23:05
epoch [1/50] batch [260/319] time 0.079 (0.087) data 0.000 (0.004) loss 2.4153 (2.3292) teacher_loss 1.9787 (1.8782) loss_zs_kd 0.0406 (0.0146) loss_oracle 0.4366 (0.4510) acc 40.6250 (38.9062) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:22:51
epoch [1/50] batch [280/319] time 0.079 (0.087) data 0.000 (0.003) loss 2.5499 (2.3254) teacher_loss 2.0855 (1.8743) loss_zs_kd 0.0593 (0.0169) loss_oracle 0.4643 (0.4511) acc 31.2500 (38.9286) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:22:42
epoch [1/50] batch [300/319] time 0.093 (0.087) data 0.001 (0.003) loss 2.7309 (2.3212) teacher_loss 2.2804 (1.8701) loss_zs_kd 0.0588 (0.0194) loss_oracle 0.4505 (0.4511) acc 31.2500 (38.9271) alaph_mean 0.2500 (0.2500) alpha_val 0.2500 (0.2500) lr 1.0000e-05 eta 0:22:35
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,775
* accuracy: 40.5%
* error: 59.5%
* macro_f1: 26.1%
Checkpoint saved to icml/multi-dg/oracle/15_moreteacherweight/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 3,192
* accuracy: 32.8%
* error: 67.2%
* macro_f1: 12.2%
Checkpoint saved to icml/multi-dg/oracle/15_moreteacherweight/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      40.5%, epoch: 1 *******
******* Domain 2 best val test acc: 32.8%, epoch: 1 *******
******* Domain 2 best test acc:     32.8%, epoch: 1 *******
epoch [2/50] batch [20/319] time 0.082 (0.117) data 0.000 (0.036) loss 2.2275 (2.1826) teacher_loss 1.4569 (1.5851) loss_zs_kd 1.0952 (0.4036) loss_oracle 0.7706 (0.5975) acc 62.5000 (43.1250) alaph_mean 0.2501 (0.2500) alpha_val 0.2501 (0.2500) lr 2.0000e-03 eta 0:30:33
epoch [2/50] batch [40/319] time 0.087 (0.101) data 0.000 (0.018) loss 2.5825 (2.2169) teacher_loss 1.8957 (1.5407) loss_zs_kd 1.0098 (0.6772) loss_oracle 0.6868 (0.6762) acc 31.2500 (47.0312) alaph_mean 0.2509 (0.2503) alpha_val 0.2509 (0.2503) lr 2.0000e-03 eta 0:26:18
epoch [2/50] batch [60/319] time 0.080 (0.096) data 0.000 (0.012) loss 2.1047 (2.1999) teacher_loss 1.2582 (1.4950) loss_zs_kd 2.0859 (0.9916) loss_oracle 0.8465 (0.7049) acc 56.2500 (49.5833) alaph_mean 0.2516 (0.2506) alpha_val 0.2516 (0.2506) lr 2.0000e-03 eta 0:24:54
epoch [2/50] batch [80/319] time 0.082 (0.093) data 0.000 (0.009) loss 2.0034 (2.1769) teacher_loss 1.3573 (1.4730) loss_zs_kd 1.9284 (1.0964) loss_oracle 0.6461 (0.7039) acc 53.1250 (50.2344) alaph_mean 0.2530 (0.2510) alpha_val 0.2530 (0.2510) lr 2.0000e-03 eta 0:23:59
epoch [2/50] batch [100/319] time 0.081 (0.091) data 0.000 (0.007) loss 1.9756 (2.1506) teacher_loss 1.2102 (1.4466) loss_zs_kd 2.2704 (1.3010) loss_oracle 0.7655 (0.7040) acc 62.5000 (51.0625) alaph_mean 0.2543 (0.2516) alpha_val 0.2543 (0.2516) lr 2.0000e-03 eta 0:23:40
epoch [2/50] batch [120/319] time 0.068 (0.089) data 0.000 (0.006) loss 1.7502 (2.1278) teacher_loss 1.0618 (1.4166) loss_zs_kd 2.5713 (1.5758) loss_oracle 0.6883 (0.7112) acc 68.7500 (52.2656) alaph_mean 0.2556 (0.2521) alpha_val 0.2556 (0.2521) lr 2.0000e-03 eta 0:22:59
epoch [2/50] batch [140/319] time 0.089 (0.088) data 0.000 (0.005) loss 2.0465 (2.1047) teacher_loss 1.2547 (1.3883) loss_zs_kd 2.0852 (1.7266) loss_oracle 0.7918 (0.7164) acc 56.2500 (53.1920) alaph_mean 0.2568 (0.2527) alpha_val 0.2568 (0.2527) lr 2.0000e-03 eta 0:22:47
epoch [2/50] batch [160/319] time 0.086 (0.088) data 0.000 (0.005) loss 1.8502 (2.0841) teacher_loss 1.1104 (1.3612) loss_zs_kd 3.0763 (1.8589) loss_oracle 0.7398 (0.7228) acc 68.7500 (54.3750) alaph_mean 0.2582 (0.2533) alpha_val 0.2582 (0.2533) lr 2.0000e-03 eta 0:22:40
epoch [2/50] batch [180/319] time 0.092 (0.088) data 0.000 (0.004) loss 2.0689 (2.0649) teacher_loss 1.3063 (1.3359) loss_zs_kd 4.2106 (2.0142) loss_oracle 0.7626 (0.7290) acc 50.0000 (55.9201) alaph_mean 0.2597 (0.2539) alpha_val 0.2597 (0.2539) lr 2.0000e-03 eta 0:22:34
epoch [2/50] batch [200/319] time 0.087 (0.089) data 0.000 (0.004) loss 1.7698 (2.0549) teacher_loss 0.8660 (1.3146) loss_zs_kd 4.1624 (2.2240) loss_oracle 0.9038 (0.7402) acc 71.8750 (56.8906) alaph_mean 0.2617 (0.2546) alpha_val 0.2617 (0.2546) lr 2.0000e-03 eta 0:22:45
epoch [2/50] batch [220/319] time 0.068 (0.087) data 0.000 (0.003) loss 2.4131 (2.0491) teacher_loss 1.6618 (1.3022) loss_zs_kd 3.5000 (2.3845) loss_oracle 0.7513 (0.7469) acc 46.8750 (57.9403) alaph_mean 0.2641 (0.2554) alpha_val 0.2641 (0.2554) lr 2.0000e-03 eta 0:22:22
epoch [2/50] batch [240/319] time 0.080 (0.086) data 0.000 (0.003) loss 1.8057 (2.0352) teacher_loss 1.0178 (1.2871) loss_zs_kd 3.6076 (2.4961) loss_oracle 0.7879 (0.7481) acc 62.5000 (58.7630) alaph_mean 0.2664 (0.2562) alpha_val 0.2664 (0.2562) lr 2.0000e-03 eta 0:22:11
epoch [2/50] batch [260/319] time 0.076 (0.086) data 0.000 (0.003) loss 1.9052 (2.0199) teacher_loss 1.1254 (1.2658) loss_zs_kd 4.0089 (2.6244) loss_oracle 0.7797 (0.7541) acc 68.7500 (59.6995) alaph_mean 0.2682 (0.2571) alpha_val 0.2682 (0.2571) lr 2.0000e-03 eta 0:22:07
epoch [2/50] batch [280/319] time 0.070 (0.086) data 0.000 (0.003) loss 1.8097 (2.0148) teacher_loss 1.1348 (1.2581) loss_zs_kd 3.7249 (2.7118) loss_oracle 0.6749 (0.7566) acc 71.8750 (60.4129) alaph_mean 0.2708 (0.2580) alpha_val 0.2708 (0.2580) lr 2.0000e-03 eta 0:21:55
epoch [2/50] batch [300/319] time 0.067 (0.085) data 0.000 (0.003) loss 1.8727 (2.0096) teacher_loss 1.0649 (1.2504) loss_zs_kd 3.8340 (2.8145) loss_oracle 0.8079 (0.7593) acc 65.6250 (60.9271) alaph_mean 0.2733 (0.2589) alpha_val 0.2733 (0.2589) lr 2.0000e-03 eta 0:21:37
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,213
* accuracy: 27.7%
* error: 72.3%
* macro_f1: 29.9%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 3,693
* accuracy: 37.9%
* error: 62.1%
* macro_f1: 21.1%
Checkpoint saved to icml/multi-dg/oracle/15_moreteacherweight/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      40.5%, epoch: 1 *******
******* Domain 2 best val test acc: 32.8%, epoch: 1 *******
******* Domain 2 best test acc:     37.9%, epoch: 2 *******
epoch [3/50] batch [20/319] time 0.067 (0.097) data 0.000 (0.027) loss 1.5977 (1.7623) teacher_loss 0.8058 (0.9641) loss_zs_kd 5.0575 (4.6897) loss_oracle 0.7919 (0.7982) acc 78.1250 (77.9688) alaph_mean 0.2776 (0.2766) alpha_val 0.2776 (0.2766) lr 1.9980e-03 eta 0:24:42
epoch [3/50] batch [40/319] time 0.060 (0.081) data 0.000 (0.014) loss 1.8260 (1.7760) teacher_loss 0.9593 (0.9749) loss_zs_kd 5.6878 (4.8729) loss_oracle 0.8666 (0.8010) acc 78.1250 (77.4219) alaph_mean 0.2802 (0.2778) alpha_val 0.2802 (0.2778) lr 1.9980e-03 eta 0:20:30
epoch [3/50] batch [60/319] time 0.078 (0.076) data 0.001 (0.009) loss 1.5440 (1.7778) teacher_loss 0.6468 (0.9744) loss_zs_kd 4.3795 (4.9704) loss_oracle 0.8972 (0.8034) acc 90.6250 (77.3438) alaph_mean 0.2825 (0.2790) alpha_val 0.2825 (0.2790) lr 1.9980e-03 eta 0:19:18
epoch [3/50] batch [80/319] time 0.083 (0.078) data 0.000 (0.007) loss 1.7082 (1.7961) teacher_loss 1.0077 (1.0016) loss_zs_kd 4.7432 (4.8266) loss_oracle 0.7006 (0.7946) acc 84.3750 (76.6016) alaph_mean 0.2849 (0.2802) alpha_val 0.2849 (0.2802) lr 1.9980e-03 eta 0:19:51
epoch [3/50] batch [100/319] time 0.084 (0.079) data 0.000 (0.006) loss 1.6911 (1.8080) teacher_loss 0.9048 (1.0219) loss_zs_kd 5.6460 (4.9531) loss_oracle 0.7863 (0.7861) acc 75.0000 (75.6562) alaph_mean 0.2876 (0.2814) alpha_val 0.2876 (0.2814) lr 1.9980e-03 eta 0:20:08
epoch [3/50] batch [120/319] time 0.086 (0.079) data 0.000 (0.005) loss 1.9407 (1.7989) teacher_loss 1.1716 (1.0204) loss_zs_kd 4.9768 (5.0028) loss_oracle 0.7691 (0.7785) acc 62.5000 (75.8073) alaph_mean 0.2900 (0.2826) alpha_val 0.2900 (0.2826) lr 1.9980e-03 eta 0:20:01
epoch [3/50] batch [140/319] time 0.071 (0.080) data 0.000 (0.004) loss 1.5775 (1.7901) teacher_loss 0.8624 (1.0166) loss_zs_kd 5.5247 (5.0348) loss_oracle 0.7151 (0.7735) acc 87.5000 (76.3170) alaph_mean 0.2925 (0.2839) alpha_val 0.2925 (0.2839) lr 1.9980e-03 eta 0:20:17
epoch [3/50] batch [160/319] time 0.084 (0.080) data 0.000 (0.004) loss 1.6838 (1.7810) teacher_loss 0.9781 (1.0098) loss_zs_kd 5.3012 (5.0876) loss_oracle 0.7057 (0.7713) acc 75.0000 (76.3672) alaph_mean 0.2949 (0.2851) alpha_val 0.2949 (0.2851) lr 1.9980e-03 eta 0:20:15
epoch [3/50] batch [180/319] time 0.075 (0.081) data 0.000 (0.003) loss 1.8433 (1.7753) teacher_loss 1.0628 (1.0027) loss_zs_kd 5.3889 (5.1189) loss_oracle 0.7805 (0.7726) acc 75.0000 (76.2847) alaph_mean 0.2969 (0.2863) alpha_val 0.2969 (0.2863) lr 1.9980e-03 eta 0:20:19
epoch [3/50] batch [200/319] time 0.080 (0.081) data 0.000 (0.003) loss 1.6821 (1.7666) teacher_loss 0.8253 (0.9858) loss_zs_kd 4.9273 (5.0656) loss_oracle 0.8568 (0.7808) acc 78.1250 (76.5156) alaph_mean 0.2987 (0.2875) alpha_val 0.2987 (0.2875) lr 1.9980e-03 eta 0:20:19
epoch [3/50] batch [220/319] time 0.079 (0.081) data 0.000 (0.003) loss 1.7778 (1.7651) teacher_loss 1.0670 (0.9810) loss_zs_kd 4.1588 (5.0051) loss_oracle 0.7108 (0.7841) acc 81.2500 (76.5767) alaph_mean 0.3005 (0.2886) alpha_val 0.3005 (0.2886) lr 1.9980e-03 eta 0:20:17
epoch [3/50] batch [240/319] time 0.086 (0.081) data 0.000 (0.002) loss 2.1265 (1.7640) teacher_loss 1.4282 (0.9846) loss_zs_kd 4.5157 (4.9487) loss_oracle 0.6983 (0.7794) acc 68.7500 (76.6016) alaph_mean 0.3030 (0.2897) alpha_val 0.3030 (0.2897) lr 1.9980e-03 eta 0:20:22
epoch [3/50] batch [260/319] time 0.087 (0.081) data 0.000 (0.002) loss 1.6548 (1.7650) teacher_loss 0.9901 (0.9916) loss_zs_kd 3.9277 (4.8686) loss_oracle 0.6647 (0.7734) acc 78.1250 (76.3101) alaph_mean 0.3055 (0.2908) alpha_val 0.3055 (0.2908) lr 1.9980e-03 eta 0:20:26
epoch [3/50] batch [280/319] time 0.076 (0.082) data 0.000 (0.002) loss 1.6526 (1.7603) teacher_loss 0.8169 (0.9888) loss_zs_kd 4.8633 (4.8290) loss_oracle 0.8358 (0.7714) acc 78.1250 (76.1830) alaph_mean 0.3074 (0.2919) alpha_val 0.3074 (0.2919) lr 1.9980e-03 eta 0:20:25
epoch [3/50] batch [300/319] time 0.078 (0.082) data 0.000 (0.002) loss 1.4742 (1.7564) teacher_loss 0.6299 (0.9852) loss_zs_kd 5.5570 (4.8488) loss_oracle 0.8443 (0.7712) acc 84.3750 (76.3125) alaph_mean 0.3094 (0.2930) alpha_val 0.3094 (0.2930) lr 1.9980e-03 eta 0:20:28
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,146
* accuracy: 26.2%
* error: 73.8%
* macro_f1: 24.3%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 3,215
* accuracy: 33.0%
* error: 67.0%
* macro_f1: 16.0%
******* Domain 2 best val acc:      40.5%, epoch: 1 *******
******* Domain 2 best val test acc: 32.8%, epoch: 1 *******
******* Domain 2 best test acc:     37.9%, epoch: 2 *******
epoch [4/50] batch [20/319] time 0.075 (0.103) data 0.000 (0.026) loss 1.7252 (1.6334) teacher_loss 0.8709 (0.8205) loss_zs_kd 4.5564 (5.0575) loss_oracle 0.8543 (0.8129) acc 84.3750 (79.2188) alaph_mean 0.3126 (0.3119) alpha_val 0.3126 (0.3119) lr 1.9921e-03 eta 0:25:36
epoch [4/50] batch [40/319] time 0.077 (0.091) data 0.000 (0.013) loss 1.6255 (1.6646) teacher_loss 0.9532 (0.8884) loss_zs_kd 3.5853 (4.8479) loss_oracle 0.6723 (0.7761) acc 71.8750 (77.5781) alaph_mean 0.3145 (0.3127) alpha_val 0.3145 (0.3127) lr 1.9921e-03 eta 0:22:41
epoch [4/50] batch [60/319] time 0.082 (0.087) data 0.000 (0.009) loss 1.8624 (1.6551) teacher_loss 1.1170 (0.8943) loss_zs_kd 4.8138 (4.7010) loss_oracle 0.7455 (0.7608) acc 65.6250 (78.2812) alaph_mean 0.3163 (0.3137) alpha_val 0.3163 (0.3137) lr 1.9921e-03 eta 0:21:38
epoch [4/50] batch [80/319] time 0.080 (0.086) data 0.000 (0.007) loss 1.7970 (1.6556) teacher_loss 1.0266 (0.8971) loss_zs_kd 5.1301 (4.7956) loss_oracle 0.7704 (0.7585) acc 78.1250 (78.2812) alaph_mean 0.3182 (0.3146) alpha_val 0.3182 (0.3146) lr 1.9921e-03 eta 0:21:21
epoch [4/50] batch [100/319] time 0.085 (0.085) data 0.000 (0.005) loss 1.7296 (1.6551) teacher_loss 1.0332 (0.9022) loss_zs_kd 4.9797 (4.8585) loss_oracle 0.6964 (0.7530) acc 71.8750 (78.1250) alaph_mean 0.3202 (0.3155) alpha_val 0.3202 (0.3155) lr 1.9921e-03 eta 0:21:10
epoch [4/50] batch [120/319] time 0.080 (0.085) data 0.000 (0.005) loss 1.5343 (1.6537) teacher_loss 0.8075 (0.9033) loss_zs_kd 5.7226 (4.9631) loss_oracle 0.7268 (0.7504) acc 90.6250 (77.9948) alaph_mean 0.3220 (0.3165) alpha_val 0.3220 (0.3165) lr 1.9921e-03 eta 0:21:05
epoch [4/50] batch [140/319] time 0.079 (0.085) data 0.000 (0.004) loss 1.6603 (1.6550) teacher_loss 0.8853 (0.9029) loss_zs_kd 4.3896 (4.9381) loss_oracle 0.7750 (0.7522) acc 78.1250 (77.9911) alaph_mean 0.3239 (0.3174) alpha_val 0.3239 (0.3174) lr 1.9921e-03 eta 0:21:02
epoch [4/50] batch [160/319] time 0.078 (0.085) data 0.000 (0.004) loss 1.8736 (1.6515) teacher_loss 1.1895 (0.8983) loss_zs_kd 4.6324 (4.9007) loss_oracle 0.6841 (0.7532) acc 68.7500 (78.0859) alaph_mean 0.3255 (0.3183) alpha_val 0.3255 (0.3183) lr 1.9921e-03 eta 0:20:54
epoch [4/50] batch [180/319] time 0.089 (0.085) data 0.000 (0.003) loss 1.6486 (1.6526) teacher_loss 0.9604 (0.9066) loss_zs_kd 4.2324 (4.8761) loss_oracle 0.6881 (0.7460) acc 71.8750 (77.7083) alaph_mean 0.3273 (0.3192) alpha_val 0.3273 (0.3192) lr 1.9921e-03 eta 0:20:51
epoch [4/50] batch [200/319] time 0.083 (0.086) data 0.000 (0.003) loss 1.6346 (1.6524) teacher_loss 0.9163 (0.9122) loss_zs_kd 4.6799 (4.8465) loss_oracle 0.7182 (0.7402) acc 81.2500 (77.5000) alaph_mean 0.3289 (0.3201) alpha_val 0.3289 (0.3201) lr 1.9921e-03 eta 0:21:12
epoch [4/50] batch [220/319] time 0.081 (0.086) data 0.000 (0.003) loss 1.7966 (1.6656) teacher_loss 1.1771 (0.9334) loss_zs_kd 3.7639 (4.7746) loss_oracle 0.6195 (0.7322) acc 68.7500 (76.2784) alaph_mean 0.3312 (0.3210) alpha_val 0.3312 (0.3210) lr 1.9921e-03 eta 0:21:07
epoch [4/50] batch [240/319] time 0.075 (0.085) data 0.000 (0.002) loss 1.8966 (1.6772) teacher_loss 1.1357 (0.9492) loss_zs_kd 3.6616 (4.6724) loss_oracle 0.7609 (0.7280) acc 62.5000 (75.2734) alaph_mean 0.3333 (0.3220) alpha_val 0.3333 (0.3220) lr 1.9921e-03 eta 0:20:58
epoch [4/50] batch [260/319] time 0.096 (0.085) data 0.000 (0.002) loss 1.8312 (1.6894) teacher_loss 1.0960 (0.9631) loss_zs_kd 3.2445 (4.5903) loss_oracle 0.7352 (0.7263) acc 71.8750 (74.9519) alaph_mean 0.3355 (0.3229) alpha_val 0.3355 (0.3229) lr 1.9921e-03 eta 0:20:54
epoch [4/50] batch [280/319] time 0.097 (0.085) data 0.000 (0.002) loss 1.7530 (1.6917) teacher_loss 1.0280 (0.9667) loss_zs_kd 3.5349 (4.5198) loss_oracle 0.7250 (0.7250) acc 75.0000 (74.6875) alaph_mean 0.3374 (0.3239) alpha_val 0.3374 (0.3239) lr 1.9921e-03 eta 0:20:52
epoch [4/50] batch [300/319] time 0.076 (0.085) data 0.000 (0.002) loss 1.5557 (1.6916) teacher_loss 0.8986 (0.9692) loss_zs_kd 3.6625 (4.4781) loss_oracle 0.6572 (0.7225) acc 78.1250 (74.5312) alaph_mean 0.3392 (0.3249) alpha_val 0.3392 (0.3249) lr 1.9921e-03 eta 0:20:49
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,880
* accuracy: 42.9%
* error: 57.1%
* macro_f1: 31.7%
Checkpoint saved to icml/multi-dg/oracle/15_moreteacherweight/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 977
* accuracy: 10.0%
* error: 90.0%
* macro_f1: 11.0%
******* Domain 2 best val acc:      42.9%, epoch: 4 *******
******* Domain 2 best val test acc: 10.0%, epoch: 4 *******
******* Domain 2 best test acc:     37.9%, epoch: 2 *******
epoch [5/50] batch [20/319] time 0.089 (0.110) data 0.000 (0.026) loss 1.8046 (1.7069) teacher_loss 1.1597 (1.0191) loss_zs_kd 3.9624 (4.1182) loss_oracle 0.6449 (0.6878) acc 78.1250 (72.3438) alaph_mean 0.3427 (0.3418) alpha_val 0.3427 (0.3418) lr 1.9823e-03 eta 0:26:56
epoch [5/50] batch [40/319] time 0.083 (0.094) data 0.000 (0.013) loss 1.4048 (1.6959) teacher_loss 0.7617 (1.0077) loss_zs_kd 4.0966 (4.0880) loss_oracle 0.6431 (0.6882) acc 87.5000 (72.8125) alaph_mean 0.3445 (0.3427) alpha_val 0.3445 (0.3427) lr 1.9823e-03 eta 0:22:49
epoch [5/50] batch [60/319] time 0.086 (0.091) data 0.001 (0.009) loss 1.4826 (1.6929) teacher_loss 0.8245 (1.0098) loss_zs_kd 4.0022 (4.0799) loss_oracle 0.6581 (0.6832) acc 84.3750 (73.9583) alaph_mean 0.3466 (0.3437) alpha_val 0.3466 (0.3437) lr 1.9823e-03 eta 0:22:03
epoch [5/50] batch [80/319] time 0.085 (0.089) data 0.000 (0.007) loss 1.4351 (1.6786) teacher_loss 0.8351 (1.0009) loss_zs_kd 4.1704 (4.0937) loss_oracle 0.6000 (0.6776) acc 81.2500 (74.6094) alaph_mean 0.3486 (0.3447) alpha_val 0.3486 (0.3447) lr 1.9823e-03 eta 0:21:42
epoch [5/50] batch [100/319] time 0.091 (0.089) data 0.000 (0.005) loss 1.6853 (1.6643) teacher_loss 1.0861 (0.9841) loss_zs_kd 4.5066 (4.1542) loss_oracle 0.5992 (0.6802) acc 71.8750 (75.3438) alaph_mean 0.3504 (0.3456) alpha_val 0.3504 (0.3456) lr 1.9823e-03 eta 0:21:32
epoch [5/50] batch [120/319] time 0.085 (0.088) data 0.000 (0.005) loss 1.5938 (1.6711) teacher_loss 0.9646 (0.9914) loss_zs_kd 4.3990 (4.1951) loss_oracle 0.6293 (0.6798) acc 68.7500 (74.8177) alaph_mean 0.3521 (0.3466) alpha_val 0.3521 (0.3466) lr 1.9823e-03 eta 0:21:22
epoch [5/50] batch [140/319] time 0.102 (0.088) data 0.000 (0.004) loss 2.0015 (1.6660) teacher_loss 1.2856 (0.9954) loss_zs_kd 4.3184 (4.2159) loss_oracle 0.7159 (0.6705) acc 46.8750 (74.2634) alaph_mean 0.3537 (0.3475) alpha_val 0.3537 (0.3475) lr 1.9823e-03 eta 0:21:19
epoch [5/50] batch [160/319] time 0.091 (0.088) data 0.000 (0.004) loss 1.7591 (1.6554) teacher_loss 0.9575 (0.9826) loss_zs_kd 4.3552 (4.2346) loss_oracle 0.8016 (0.6728) acc 75.0000 (74.3750) alaph_mean 0.3546 (0.3483) alpha_val 0.3546 (0.3483) lr 1.9823e-03 eta 0:21:12
epoch [5/50] batch [180/319] time 0.074 (0.088) data 0.000 (0.003) loss 1.3710 (1.6478) teacher_loss 0.5519 (0.9615) loss_zs_kd 4.5563 (4.2770) loss_oracle 0.8191 (0.6862) acc 90.6250 (75.1562) alaph_mean 0.3555 (0.3491) alpha_val 0.3555 (0.3491) lr 1.9823e-03 eta 0:21:08
epoch [5/50] batch [200/319] time 0.084 (0.087) data 0.000 (0.003) loss 1.5589 (1.6487) teacher_loss 0.8891 (0.9558) loss_zs_kd 4.3522 (4.2701) loss_oracle 0.6699 (0.6929) acc 75.0000 (75.5938) alaph_mean 0.3566 (0.3498) alpha_val 0.3566 (0.3498) lr 1.9823e-03 eta 0:20:59
epoch [5/50] batch [220/319] time 0.088 (0.087) data 0.000 (0.003) loss 1.4692 (1.6540) teacher_loss 0.7896 (0.9580) loss_zs_kd 4.7627 (4.2962) loss_oracle 0.6796 (0.6959) acc 75.0000 (75.4545) alaph_mean 0.3579 (0.3505) alpha_val 0.3579 (0.3505) lr 1.9823e-03 eta 0:20:56
epoch [5/50] batch [240/319] time 0.089 (0.087) data 0.000 (0.002) loss 1.4655 (1.6567) teacher_loss 0.7896 (0.9643) loss_zs_kd 4.8974 (4.3410) loss_oracle 0.6760 (0.6924) acc 81.2500 (75.4948) alaph_mean 0.3595 (0.3512) alpha_val 0.3595 (0.3512) lr 1.9823e-03 eta 0:20:53
epoch [5/50] batch [260/319] time 0.078 (0.087) data 0.000 (0.002) loss 1.7589 (1.6540) teacher_loss 1.0238 (0.9583) loss_zs_kd 5.6431 (4.4182) loss_oracle 0.7351 (0.6957) acc 75.0000 (75.7452) alaph_mean 0.3607 (0.3518) alpha_val 0.3607 (0.3518) lr 1.9823e-03 eta 0:20:49
epoch [5/50] batch [280/319] time 0.091 (0.086) data 0.000 (0.002) loss 1.3110 (1.6494) teacher_loss 0.5216 (0.9485) loss_zs_kd 5.1545 (4.4741) loss_oracle 0.7894 (0.7009) acc 87.5000 (75.9598) alaph_mean 0.3617 (0.3525) alpha_val 0.3617 (0.3525) lr 1.9823e-03 eta 0:20:44
epoch [5/50] batch [300/319] time 0.083 (0.086) data 0.000 (0.002) loss 1.5813 (1.6449) teacher_loss 0.8543 (0.9377) loss_zs_kd 4.7435 (4.5042) loss_oracle 0.7270 (0.7072) acc 71.8750 (76.3854) alaph_mean 0.3629 (0.3532) alpha_val 0.3629 (0.3532) lr 1.9823e-03 eta 0:20:38
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,357
* accuracy: 31.0%
* error: 69.0%
* macro_f1: 32.2%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 2,845
* accuracy: 29.2%
* error: 70.8%
* macro_f1: 16.4%
******* Domain 2 best val acc:      42.9%, epoch: 4 *******
******* Domain 2 best val test acc: 10.0%, epoch: 4 *******
******* Domain 2 best test acc:     37.9%, epoch: 2 *******
epoch [6/50] batch [20/319] time 0.085 (0.113) data 0.000 (0.025) loss 1.7399 (1.6069) teacher_loss 0.9602 (0.8406) loss_zs_kd 4.6955 (4.5622) loss_oracle 0.7798 (0.7663) acc 78.1250 (80.6250) alaph_mean 0.3651 (0.3646) alpha_val 0.3651 (0.3646) lr 1.9686e-03 eta 0:27:02
epoch [6/50] batch [40/319] time 0.086 (0.097) data 0.000 (0.013) loss 1.6142 (1.5664) teacher_loss 0.8900 (0.8097) loss_zs_kd 4.7959 (4.6396) loss_oracle 0.7241 (0.7567) acc 84.3750 (80.7031) alaph_mean 0.3661 (0.3651) alpha_val 0.3661 (0.3651) lr 1.9686e-03 eta 0:23:12
epoch [6/50] batch [60/319] time 0.075 (0.091) data 0.000 (0.009) loss 1.6259 (1.5979) teacher_loss 0.9143 (0.8385) loss_zs_kd 4.5000 (4.7071) loss_oracle 0.7115 (0.7595) acc 81.2500 (79.6354) alaph_mean 0.3674 (0.3657) alpha_val 0.3674 (0.3657) lr 1.9686e-03 eta 0:21:36
epoch [6/50] batch [80/319] time 0.080 (0.088) data 0.000 (0.006) loss 1.6878 (1.5974) teacher_loss 0.8959 (0.8448) loss_zs_kd 4.7125 (4.7185) loss_oracle 0.7919 (0.7526) acc 84.3750 (80.1562) alaph_mean 0.3686 (0.3662) alpha_val 0.3686 (0.3662) lr 1.9686e-03 eta 0:20:54
epoch [6/50] batch [100/319] time 0.079 (0.086) data 0.000 (0.005) loss 1.4386 (1.5964) teacher_loss 0.5781 (0.8316) loss_zs_kd 5.3059 (4.7376) loss_oracle 0.8605 (0.7648) acc 84.3750 (80.3438) alaph_mean 0.3696 (0.3668) alpha_val 0.3696 (0.3668) lr 1.9686e-03 eta 0:20:30
epoch [6/50] batch [120/319] time 0.082 (0.086) data 0.000 (0.004) loss 1.5460 (1.5931) teacher_loss 0.7112 (0.8121) loss_zs_kd 5.5254 (4.7203) loss_oracle 0.8348 (0.7811) acc 90.6250 (80.7552) alaph_mean 0.3704 (0.3673) alpha_val 0.3704 (0.3673) lr 1.9686e-03 eta 0:20:27
epoch [6/50] batch [140/319] time 0.073 (0.086) data 0.000 (0.004) loss 1.6974 (1.5905) teacher_loss 0.7937 (0.7976) loss_zs_kd 5.5242 (4.8196) loss_oracle 0.9037 (0.7929) acc 71.8750 (81.1161) alaph_mean 0.3714 (0.3679) alpha_val 0.3714 (0.3679) lr 1.9686e-03 eta 0:20:21
epoch [6/50] batch [160/319] time 0.093 (0.088) data 0.000 (0.003) loss 1.6982 (1.5846) teacher_loss 0.8051 (0.7811) loss_zs_kd 5.4139 (4.9268) loss_oracle 0.8931 (0.8035) acc 78.1250 (81.2500) alaph_mean 0.3723 (0.3684) alpha_val 0.3723 (0.3684) lr 1.9686e-03 eta 0:20:50
epoch [6/50] batch [180/319] time 0.092 (0.088) data 0.000 (0.003) loss 1.6666 (1.5801) teacher_loss 0.8072 (0.7690) loss_zs_kd 5.6663 (5.0026) loss_oracle 0.8594 (0.8111) acc 78.1250 (81.4757) alaph_mean 0.3731 (0.3688) alpha_val 0.3731 (0.3688) lr 1.9686e-03 eta 0:20:43
epoch [6/50] batch [200/319] time 0.077 (0.087) data 0.000 (0.003) loss 1.6464 (1.5792) teacher_loss 0.8356 (0.7630) loss_zs_kd 4.6625 (5.0354) loss_oracle 0.8108 (0.8162) acc 84.3750 (81.5781) alaph_mean 0.3739 (0.3693) alpha_val 0.3739 (0.3693) lr 1.9686e-03 eta 0:20:32
epoch [6/50] batch [220/319] time 0.088 (0.087) data 0.000 (0.003) loss 1.7524 (1.5788) teacher_loss 0.9840 (0.7619) loss_zs_kd 4.9540 (5.0427) loss_oracle 0.7685 (0.8169) acc 81.2500 (81.6193) alaph_mean 0.3749 (0.3698) alpha_val 0.3749 (0.3698) lr 1.9686e-03 eta 0:20:25
epoch [6/50] batch [240/319] time 0.090 (0.087) data 0.000 (0.002) loss 1.7949 (1.5853) teacher_loss 1.0142 (0.7683) loss_zs_kd 4.4131 (5.0106) loss_oracle 0.7806 (0.8170) acc 75.0000 (81.3932) alaph_mean 0.3761 (0.3703) alpha_val 0.3761 (0.3703) lr 1.9686e-03 eta 0:20:22
epoch [6/50] batch [260/319] time 0.079 (0.086) data 0.000 (0.002) loss 1.3091 (1.5818) teacher_loss 0.3864 (0.7634) loss_zs_kd 4.8753 (4.9973) loss_oracle 0.9227 (0.8184) acc 87.5000 (81.6707) alaph_mean 0.3772 (0.3708) alpha_val 0.3772 (0.3708) lr 1.9686e-03 eta 0:20:13
epoch [6/50] batch [280/319] time 0.074 (0.086) data 0.000 (0.002) loss 1.6730 (1.5835) teacher_loss 0.7978 (0.7611) loss_zs_kd 4.8157 (4.9777) loss_oracle 0.8752 (0.8225) acc 81.2500 (81.7299) alaph_mean 0.3782 (0.3713) alpha_val 0.3782 (0.3713) lr 1.9686e-03 eta 0:20:03
epoch [6/50] batch [300/319] time 0.094 (0.085) data 0.000 (0.002) loss 1.5530 (1.5841) teacher_loss 0.6529 (0.7564) loss_zs_kd 5.2275 (4.9497) loss_oracle 0.9000 (0.8277) acc 81.2500 (81.7812) alaph_mean 0.3791 (0.3717) alpha_val 0.3791 (0.3717) lr 1.9686e-03 eta 0:20:00
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,052
* accuracy: 46.9%
* error: 53.1%
* macro_f1: 42.1%
Checkpoint saved to icml/multi-dg/oracle/15_moreteacherweight/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 4,731
* accuracy: 48.6%
* error: 51.4%
* macro_f1: 21.1%
Checkpoint saved to icml/multi-dg/oracle/15_moreteacherweight/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      46.9%, epoch: 6 *******
******* Domain 2 best val test acc: 48.6%, epoch: 6 *******
******* Domain 2 best test acc:     48.6%, epoch: 6 *******
epoch [7/50] batch [20/319] time 0.088 (0.109) data 0.000 (0.024) loss 1.2259 (1.5483) teacher_loss 0.5100 (0.7937) loss_zs_kd 4.1272 (4.0872) loss_oracle 0.7159 (0.7546) acc 90.6250 (82.5000) alaph_mean 0.3811 (0.3805) alpha_val 0.3811 (0.3805) lr 1.9511e-03 eta 0:25:30
epoch [7/50] batch [40/319] time 0.084 (0.097) data 0.000 (0.012) loss 1.8331 (1.5664) teacher_loss 0.9931 (0.7847) loss_zs_kd 5.0070 (4.2510) loss_oracle 0.8400 (0.7817) acc 68.7500 (80.0000) alaph_mean 0.3817 (0.3810) alpha_val 0.3817 (0.3810) lr 1.9511e-03 eta 0:22:37
epoch [7/50] batch [60/319] time 0.086 (0.092) data 0.001 (0.008) loss 1.5549 (1.5573) teacher_loss 0.7185 (0.7601) loss_zs_kd 4.1117 (4.3011) loss_oracle 0.8364 (0.7972) acc 78.1250 (80.8333) alaph_mean 0.3825 (0.3814) alpha_val 0.3825 (0.3814) lr 1.9511e-03 eta 0:21:20
epoch [7/50] batch [80/319] time 0.084 (0.090) data 0.000 (0.006) loss 1.6660 (1.5651) teacher_loss 0.8441 (0.7665) loss_zs_kd 4.0939 (4.2798) loss_oracle 0.8219 (0.7986) acc 78.1250 (81.1328) alaph_mean 0.3832 (0.3817) alpha_val 0.3832 (0.3817) lr 1.9511e-03 eta 0:20:58
epoch [7/50] batch [100/319] time 0.084 (0.089) data 0.000 (0.005) loss 1.7414 (1.5817) teacher_loss 1.0253 (0.7868) loss_zs_kd 3.7757 (4.2077) loss_oracle 0.7161 (0.7949) acc 78.1250 (81.2500) alaph_mean 0.3842 (0.3821) alpha_val 0.3842 (0.3821) lr 1.9511e-03 eta 0:20:43
epoch [7/50] batch [120/319] time 0.082 (0.089) data 0.000 (0.004) loss 1.7706 (1.5693) teacher_loss 0.9269 (0.7756) loss_zs_kd 4.2046 (4.1987) loss_oracle 0.8437 (0.7938) acc 75.0000 (81.9010) alaph_mean 0.3850 (0.3826) alpha_val 0.3850 (0.3826) lr 1.9511e-03 eta 0:20:34
epoch [7/50] batch [140/319] time 0.083 (0.089) data 0.001 (0.004) loss 1.4483 (1.5629) teacher_loss 0.6280 (0.7620) loss_zs_kd 4.8721 (4.2108) loss_oracle 0.8203 (0.8009) acc 90.6250 (82.5000) alaph_mean 0.3857 (0.3830) alpha_val 0.3857 (0.3830) lr 1.9511e-03 eta 0:20:31
epoch [7/50] batch [160/319] time 0.077 (0.088) data 0.000 (0.003) loss 1.4229 (1.5632) teacher_loss 0.5356 (0.7570) loss_zs_kd 4.7010 (4.2691) loss_oracle 0.8873 (0.8062) acc 90.6250 (82.3633) alaph_mean 0.3866 (0.3834) alpha_val 0.3866 (0.3834) lr 1.9511e-03 eta 0:20:22
epoch [7/50] batch [180/319] time 0.084 (0.087) data 0.000 (0.003) loss 1.5561 (1.5652) teacher_loss 0.7664 (0.7569) loss_zs_kd 4.7520 (4.3058) loss_oracle 0.7897 (0.8083) acc 81.2500 (82.2569) alaph_mean 0.3875 (0.3838) alpha_val 0.3875 (0.3838) lr 1.9511e-03 eta 0:20:07
epoch [7/50] batch [200/319] time 0.089 (0.087) data 0.000 (0.003) loss 1.9306 (1.5608) teacher_loss 1.2518 (0.7552) loss_zs_kd 4.2077 (4.2873) loss_oracle 0.6789 (0.8056) acc 68.7500 (82.5938) alaph_mean 0.3885 (0.3842) alpha_val 0.3885 (0.3842) lr 1.9511e-03 eta 0:20:06
epoch [7/50] batch [220/319] time 0.084 (0.087) data 0.000 (0.002) loss 1.6195 (1.5547) teacher_loss 0.8269 (0.7515) loss_zs_kd 3.9739 (4.2856) loss_oracle 0.7925 (0.8032) acc 81.2500 (82.8835) alaph_mean 0.3894 (0.3846) alpha_val 0.3894 (0.3846) lr 1.9511e-03 eta 0:20:01
epoch [7/50] batch [240/319] time 0.086 (0.087) data 0.001 (0.002) loss 1.5959 (1.5548) teacher_loss 0.9067 (0.7549) loss_zs_kd 4.4650 (4.2918) loss_oracle 0.6893 (0.7999) acc 81.2500 (82.9036) alaph_mean 0.3905 (0.3851) alpha_val 0.3905 (0.3851) lr 1.9511e-03 eta 0:19:57
epoch [7/50] batch [260/319] time 0.076 (0.087) data 0.001 (0.002) loss 1.5168 (1.5584) teacher_loss 0.8152 (0.7645) loss_zs_kd 4.0321 (4.2894) loss_oracle 0.7016 (0.7939) acc 87.5000 (82.6322) alaph_mean 0.3917 (0.3855) alpha_val 0.3917 (0.3855) lr 1.9511e-03 eta 0:19:51
epoch [7/50] batch [280/319] time 0.076 (0.086) data 0.000 (0.002) loss 1.3436 (1.5567) teacher_loss 0.6170 (0.7679) loss_zs_kd 4.4924 (4.2821) loss_oracle 0.7267 (0.7888) acc 87.5000 (82.3884) alaph_mean 0.3926 (0.3860) alpha_val 0.3926 (0.3860) lr 1.9511e-03 eta 0:19:47
epoch [7/50] batch [300/319] time 0.093 (0.086) data 0.000 (0.002) loss 1.5440 (1.5544) teacher_loss 0.8448 (0.7714) loss_zs_kd 4.7723 (4.2930) loss_oracle 0.6992 (0.7829) acc 78.1250 (82.1979) alaph_mean 0.3933 (0.3865) alpha_val 0.3933 (0.3865) lr 1.9511e-03 eta 0:19:45
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,443
* accuracy: 33.0%
* error: 67.0%
* macro_f1: 32.1%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 3,614
* accuracy: 37.1%
* error: 62.9%
* macro_f1: 18.9%
******* Domain 2 best val acc:      46.9%, epoch: 6 *******
******* Domain 2 best val test acc: 48.6%, epoch: 6 *******
******* Domain 2 best test acc:     48.6%, epoch: 6 *******
epoch [8/50] batch [20/319] time 0.099 (0.117) data 0.000 (0.029) loss 1.7707 (1.6708) teacher_loss 1.0084 (0.8784) loss_zs_kd 5.1237 (5.2515) loss_oracle 0.7623 (0.7924) acc 68.7500 (76.8750) alaph_mean 0.3948 (0.3944) alpha_val 0.3948 (0.3944) lr 1.9298e-03 eta 0:26:45
epoch [8/50] batch [40/319] time 0.093 (0.102) data 0.000 (0.015) loss 1.3264 (1.6189) teacher_loss 0.4987 (0.8292) loss_zs_kd 5.3530 (5.2271) loss_oracle 0.8277 (0.7896) acc 90.6250 (78.5156) alaph_mean 0.3957 (0.3949) alpha_val 0.3957 (0.3949) lr 1.9298e-03 eta 0:23:17
epoch [8/50] batch [60/319] time 0.088 (0.096) data 0.001 (0.010) loss 1.4314 (1.5998) teacher_loss 0.7183 (0.8083) loss_zs_kd 6.0804 (5.4138) loss_oracle 0.7131 (0.7915) acc 90.6250 (79.8958) alaph_mean 0.3966 (0.3953) alpha_val 0.3966 (0.3953) lr 1.9298e-03 eta 0:21:51
epoch [8/50] batch [80/319] time 0.085 (0.093) data 0.000 (0.007) loss 1.5098 (1.5683) teacher_loss 0.7377 (0.7839) loss_zs_kd 5.4232 (5.4843) loss_oracle 0.7721 (0.7845) acc 78.1250 (80.7812) alaph_mean 0.3973 (0.3957) alpha_val 0.3973 (0.3957) lr 1.9298e-03 eta 0:21:12
epoch [8/50] batch [100/319] time 0.087 (0.094) data 0.000 (0.006) loss 1.4250 (1.5736) teacher_loss 0.6247 (0.7913) loss_zs_kd 5.2918 (5.4424) loss_oracle 0.8003 (0.7823) acc 87.5000 (80.8438) alaph_mean 0.3982 (0.3961) alpha_val 0.3982 (0.3961) lr 1.9298e-03 eta 0:21:25
epoch [8/50] batch [120/319] time 0.093 (0.093) data 0.001 (0.005) loss 1.5861 (1.5565) teacher_loss 0.8234 (0.7750) loss_zs_kd 5.3847 (5.3624) loss_oracle 0.7627 (0.7815) acc 87.5000 (81.3802) alaph_mean 0.3990 (0.3966) alpha_val 0.3990 (0.3966) lr 1.9298e-03 eta 0:21:04
epoch [8/50] batch [140/319] time 0.085 (0.092) data 0.000 (0.004) loss 1.2821 (1.5554) teacher_loss 0.4018 (0.7708) loss_zs_kd 6.0612 (5.3704) loss_oracle 0.8802 (0.7846) acc 93.7500 (81.2054) alaph_mean 0.3998 (0.3970) alpha_val 0.3998 (0.3970) lr 1.9298e-03 eta 0:20:46
epoch [8/50] batch [160/319] time 0.083 (0.091) data 0.000 (0.004) loss 1.6722 (1.5563) teacher_loss 0.8685 (0.7688) loss_zs_kd 4.5422 (5.3587) loss_oracle 0.8037 (0.7875) acc 71.8750 (81.3672) alaph_mean 0.4006 (0.3974) alpha_val 0.4006 (0.3974) lr 1.9298e-03 eta 0:20:33
epoch [8/50] batch [180/319] time 0.080 (0.090) data 0.000 (0.003) loss 1.6160 (1.5558) teacher_loss 0.7438 (0.7621) loss_zs_kd 5.5122 (5.3604) loss_oracle 0.8722 (0.7937) acc 78.1250 (81.3542) alaph_mean 0.4013 (0.3978) alpha_val 0.4013 (0.3978) lr 1.9298e-03 eta 0:20:20
epoch [8/50] batch [200/319] time 0.085 (0.089) data 0.000 (0.003) loss 1.6073 (1.5480) teacher_loss 0.7520 (0.7487) loss_zs_kd 5.2857 (5.3582) loss_oracle 0.8553 (0.7993) acc 78.1250 (81.5469) alaph_mean 0.4020 (0.3982) alpha_val 0.4020 (0.3982) lr 1.9298e-03 eta 0:20:07
epoch [8/50] batch [220/319] time 0.077 (0.088) data 0.000 (0.003) loss 1.3095 (1.5445) teacher_loss 0.4799 (0.7421) loss_zs_kd 5.7249 (5.3575) loss_oracle 0.8296 (0.8024) acc 90.6250 (81.6903) alaph_mean 0.4028 (0.3985) alpha_val 0.4028 (0.3985) lr 1.9298e-03 eta 0:19:51
epoch [8/50] batch [240/319] time 0.087 (0.088) data 0.000 (0.003) loss 1.7210 (1.5409) teacher_loss 0.9091 (0.7347) loss_zs_kd 5.0567 (5.3990) loss_oracle 0.8119 (0.8062) acc 75.0000 (81.9271) alaph_mean 0.4035 (0.3989) alpha_val 0.4035 (0.3989) lr 1.9298e-03 eta 0:19:40
epoch [8/50] batch [260/319] time 0.078 (0.087) data 0.000 (0.002) loss 1.6496 (1.5368) teacher_loss 0.8220 (0.7303) loss_zs_kd 5.2612 (5.4065) loss_oracle 0.8276 (0.8065) acc 71.8750 (82.1875) alaph_mean 0.4043 (0.3993) alpha_val 0.4043 (0.3993) lr 1.9298e-03 eta 0:19:28
epoch [8/50] batch [280/319] time 0.071 (0.086) data 0.000 (0.002) loss 1.5210 (1.5346) teacher_loss 0.6831 (0.7265) loss_zs_kd 5.5949 (5.4096) loss_oracle 0.8380 (0.8080) acc 84.3750 (82.1875) alaph_mean 0.4050 (0.3997) alpha_val 0.4050 (0.3997) lr 1.9298e-03 eta 0:19:20
epoch [8/50] batch [300/319] time 0.081 (0.086) data 0.000 (0.002) loss 1.7229 (1.5366) teacher_loss 0.9422 (0.7280) loss_zs_kd 5.5888 (5.4230) loss_oracle 0.7808 (0.8086) acc 78.1250 (82.1667) alaph_mean 0.4058 (0.4001) alpha_val 0.4058 (0.4001) lr 1.9298e-03 eta 0:19:11
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,193
* accuracy: 27.2%
* error: 72.8%
* macro_f1: 25.1%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 2,997
* accuracy: 30.8%
* error: 69.2%
* macro_f1: 16.4%
******* Domain 2 best val acc:      46.9%, epoch: 6 *******
******* Domain 2 best val test acc: 48.6%, epoch: 6 *******
******* Domain 2 best test acc:     48.6%, epoch: 6 *******
epoch [9/50] batch [20/319] time 0.072 (0.112) data 0.000 (0.031) loss 1.2696 (1.4566) teacher_loss 0.4346 (0.6250) loss_zs_kd 5.5133 (5.6834) loss_oracle 0.8351 (0.8315) acc 90.6250 (85.3125) alaph_mean 0.4073 (0.4070) alpha_val 0.4073 (0.4070) lr 1.9048e-03 eta 0:24:54
epoch [9/50] batch [40/319] time 0.072 (0.097) data 0.000 (0.016) loss 1.4053 (1.4431) teacher_loss 0.4707 (0.5968) loss_zs_kd 5.9147 (5.7755) loss_oracle 0.9346 (0.8463) acc 87.5000 (86.2500) alaph_mean 0.4079 (0.4073) alpha_val 0.4079 (0.4073) lr 1.9048e-03 eta 0:21:41
epoch [9/50] batch [60/319] time 0.079 (0.090) data 0.000 (0.011) loss 1.7505 (1.4788) teacher_loss 0.9463 (0.6295) loss_zs_kd 5.6850 (5.9031) loss_oracle 0.8041 (0.8494) acc 75.0000 (84.8958) alaph_mean 0.4084 (0.4076) alpha_val 0.4084 (0.4076) lr 1.9048e-03 eta 0:20:06
epoch [9/50] batch [80/319] time 0.079 (0.087) data 0.000 (0.008) loss 1.5185 (1.4917) teacher_loss 0.6760 (0.6387) loss_zs_kd 5.7174 (5.8481) loss_oracle 0.8425 (0.8529) acc 81.2500 (84.3750) alaph_mean 0.4091 (0.4079) alpha_val 0.4091 (0.4079) lr 1.9048e-03 eta 0:19:21
epoch [9/50] batch [100/319] time 0.086 (0.086) data 0.000 (0.006) loss 1.5173 (1.4982) teacher_loss 0.5831 (0.6417) loss_zs_kd 5.8063 (5.7466) loss_oracle 0.9342 (0.8565) acc 81.2500 (84.1250) alaph_mean 0.4097 (0.4082) alpha_val 0.4097 (0.4082) lr 1.9048e-03 eta 0:19:02
epoch [9/50] batch [120/319] time 0.076 (0.085) data 0.000 (0.005) loss 1.6579 (1.5069) teacher_loss 1.0019 (0.6601) loss_zs_kd 5.8455 (5.7352) loss_oracle 0.6560 (0.8468) acc 84.3750 (84.0365) alaph_mean 0.4103 (0.4085) alpha_val 0.4103 (0.4085) lr 1.9048e-03 eta 0:18:51
epoch [9/50] batch [140/319] time 0.079 (0.085) data 0.000 (0.005) loss 1.7596 (1.5104) teacher_loss 0.9777 (0.6681) loss_zs_kd 5.8270 (5.7674) loss_oracle 0.7818 (0.8422) acc 71.8750 (83.5938) alaph_mean 0.4110 (0.4088) alpha_val 0.4110 (0.4088) lr 1.9048e-03 eta 0:18:41
epoch [9/50] batch [160/319] time 0.087 (0.084) data 0.000 (0.004) loss 1.4749 (1.5016) teacher_loss 0.7133 (0.6617) loss_zs_kd 6.4959 (5.8084) loss_oracle 0.7616 (0.8399) acc 84.3750 (83.7305) alaph_mean 0.4116 (0.4091) alpha_val 0.4116 (0.4091) lr 1.9048e-03 eta 0:18:31
epoch [9/50] batch [180/319] time 0.084 (0.084) data 0.000 (0.004) loss 1.3378 (1.4967) teacher_loss 0.4506 (0.6589) loss_zs_kd 6.6794 (5.8489) loss_oracle 0.8872 (0.8379) acc 96.8750 (83.9931) alaph_mean 0.4123 (0.4094) alpha_val 0.4123 (0.4094) lr 1.9048e-03 eta 0:18:30
epoch [9/50] batch [200/319] time 0.087 (0.084) data 0.000 (0.003) loss 1.4680 (1.4946) teacher_loss 0.7213 (0.6611) loss_zs_kd 5.7619 (5.8383) loss_oracle 0.7467 (0.8334) acc 84.3750 (84.0781) alaph_mean 0.4130 (0.4098) alpha_val 0.4130 (0.4098) lr 1.9048e-03 eta 0:18:29
epoch [9/50] batch [220/319] time 0.082 (0.084) data 0.000 (0.003) loss 1.5135 (1.4928) teacher_loss 0.7454 (0.6624) loss_zs_kd 5.8720 (5.8411) loss_oracle 0.7680 (0.8304) acc 71.8750 (84.1051) alaph_mean 0.4137 (0.4101) alpha_val 0.4137 (0.4101) lr 1.9048e-03 eta 0:18:28
epoch [9/50] batch [240/319] time 0.084 (0.084) data 0.000 (0.003) loss 1.5272 (1.4905) teacher_loss 0.7551 (0.6623) loss_zs_kd 6.2020 (5.8506) loss_oracle 0.7721 (0.8283) acc 75.0000 (83.9062) alaph_mean 0.4144 (0.4104) alpha_val 0.4144 (0.4104) lr 1.9048e-03 eta 0:18:28
epoch [9/50] batch [260/319] time 0.078 (0.084) data 0.000 (0.003) loss 1.5217 (1.4912) teacher_loss 0.7307 (0.6660) loss_zs_kd 5.0172 (5.8367) loss_oracle 0.7910 (0.8253) acc 78.1250 (83.9543) alaph_mean 0.4152 (0.4108) alpha_val 0.4152 (0.4108) lr 1.9048e-03 eta 0:18:21
epoch [9/50] batch [280/319] time 0.077 (0.084) data 0.000 (0.002) loss 1.3714 (1.4975) teacher_loss 0.5719 (0.6740) loss_zs_kd 5.1886 (5.8224) loss_oracle 0.7995 (0.8235) acc 84.3750 (83.7277) alaph_mean 0.4159 (0.4111) alpha_val 0.4159 (0.4111) lr 1.9048e-03 eta 0:18:18
epoch [9/50] batch [300/319] time 0.074 (0.085) data 0.000 (0.002) loss 1.5167 (1.4974) teacher_loss 0.6855 (0.6755) loss_zs_kd 5.5481 (5.8145) loss_oracle 0.8313 (0.8219) acc 78.1250 (83.7500) alaph_mean 0.4167 (0.4115) alpha_val 0.4167 (0.4115) lr 1.9048e-03 eta 0:18:30
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,874
* accuracy: 42.8%
* error: 57.2%
* macro_f1: 31.3%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 2,993
* accuracy: 30.7%
* error: 69.3%
* macro_f1: 18.3%
******* Domain 2 best val acc:      46.9%, epoch: 6 *******
******* Domain 2 best val test acc: 48.6%, epoch: 6 *******
******* Domain 2 best test acc:     48.6%, epoch: 6 *******
epoch [10/50] batch [20/319] time 0.085 (0.113) data 0.000 (0.030) loss 1.7202 (1.5763) teacher_loss 0.9961 (0.8819) loss_zs_kd 4.6305 (4.4800) loss_oracle 0.7241 (0.6944) acc 75.0000 (79.5312) alaph_mean 0.4181 (0.4178) alpha_val 0.4181 (0.4178) lr 1.8763e-03 eta 0:24:36
epoch [10/50] batch [40/319] time 0.056 (0.096) data 0.000 (0.015) loss 1.5785 (1.5964) teacher_loss 0.8129 (0.8764) loss_zs_kd 4.4407 (4.5602) loss_oracle 0.7656 (0.7200) acc 78.1250 (78.1250) alaph_mean 0.4188 (0.4181) alpha_val 0.4188 (0.4181) lr 1.8763e-03 eta 0:20:54
epoch [10/50] batch [60/319] time 0.089 (0.091) data 0.001 (0.010) loss 2.2766 (1.5977) teacher_loss 1.5732 (0.8683) loss_zs_kd 4.7751 (4.5811) loss_oracle 0.7033 (0.7294) acc 50.0000 (77.8125) alaph_mean 0.4194 (0.4185) alpha_val 0.4194 (0.4185) lr 1.8763e-03 eta 0:19:46
epoch [10/50] batch [80/319] time 0.091 (0.090) data 0.000 (0.008) loss 1.5485 (1.6128) teacher_loss 0.5656 (0.8519) loss_zs_kd 5.0802 (4.6076) loss_oracle 0.9829 (0.7608) acc 75.0000 (76.9922) alaph_mean 0.4199 (0.4188) alpha_val 0.4199 (0.4188) lr 1.8763e-03 eta 0:19:29
epoch [10/50] batch [100/319] time 0.087 (0.092) data 0.000 (0.006) loss 1.6498 (1.6416) teacher_loss 0.7333 (0.8554) loss_zs_kd 5.2220 (4.6978) loss_oracle 0.9165 (0.7862) acc 78.1250 (76.0000) alaph_mean 0.4204 (0.4190) alpha_val 0.4204 (0.4190) lr 1.8763e-03 eta 0:19:51
epoch [10/50] batch [120/319] time 0.085 (0.090) data 0.000 (0.005) loss 1.5626 (1.6462) teacher_loss 0.6238 (0.8407) loss_zs_kd 4.6870 (4.7434) loss_oracle 0.9388 (0.8056) acc 81.2500 (76.1198) alaph_mean 0.4209 (0.4193) alpha_val 0.4209 (0.4193) lr 1.8763e-03 eta 0:19:28
epoch [10/50] batch [140/319] time 0.080 (0.088) data 0.000 (0.005) loss 1.5276 (1.6439) teacher_loss 0.6494 (0.8286) loss_zs_kd 4.9063 (4.7614) loss_oracle 0.8782 (0.8153) acc 71.8750 (76.1607) alaph_mean 0.4213 (0.4196) alpha_val 0.4213 (0.4196) lr 1.8763e-03 eta 0:19:04
epoch [10/50] batch [160/319] time 0.083 (0.088) data 0.000 (0.004) loss 1.7262 (1.6439) teacher_loss 0.9301 (0.8202) loss_zs_kd 4.3478 (4.7670) loss_oracle 0.7960 (0.8236) acc 71.8750 (76.3086) alaph_mean 0.4218 (0.4198) alpha_val 0.4218 (0.4198) lr 1.8763e-03 eta 0:18:52
epoch [10/50] batch [180/319] time 0.077 (0.087) data 0.000 (0.004) loss 1.4855 (1.6533) teacher_loss 0.5277 (0.8270) loss_zs_kd 3.8273 (4.7042) loss_oracle 0.9578 (0.8263) acc 84.3750 (76.2326) alaph_mean 0.4223 (0.4201) alpha_val 0.4223 (0.4201) lr 1.8763e-03 eta 0:18:42
epoch [10/50] batch [200/319] time 0.087 (0.086) data 0.000 (0.003) loss 1.5405 (1.6548) teacher_loss 0.6460 (0.8256) loss_zs_kd 3.7199 (4.5951) loss_oracle 0.8945 (0.8291) acc 84.3750 (76.1406) alaph_mean 0.4228 (0.4203) alpha_val 0.4228 (0.4203) lr 1.8763e-03 eta 0:18:33
epoch [10/50] batch [220/319] time 0.085 (0.086) data 0.000 (0.003) loss 1.4357 (1.6536) teacher_loss 0.5904 (0.8243) loss_zs_kd 3.8545 (4.5146) loss_oracle 0.8453 (0.8292) acc 84.3750 (76.0653) alaph_mean 0.4233 (0.4206) alpha_val 0.4233 (0.4206) lr 1.8763e-03 eta 0:18:28
epoch [10/50] batch [240/319] time 0.076 (0.086) data 0.000 (0.003) loss 1.7060 (1.6523) teacher_loss 0.9416 (0.8228) loss_zs_kd 4.1967 (4.4914) loss_oracle 0.7644 (0.8296) acc 75.0000 (76.1589) alaph_mean 0.4237 (0.4208) alpha_val 0.4237 (0.4208) lr 1.8763e-03 eta 0:18:21
epoch [10/50] batch [260/319] time 0.074 (0.085) data 0.000 (0.003) loss 1.8076 (1.6532) teacher_loss 1.0711 (0.8237) loss_zs_kd 4.7668 (4.4935) loss_oracle 0.7365 (0.8295) acc 59.3750 (76.1058) alaph_mean 0.4242 (0.4210) alpha_val 0.4242 (0.4210) lr 1.8763e-03 eta 0:18:14
epoch [10/50] batch [280/319] time 0.080 (0.085) data 0.000 (0.002) loss 1.6654 (1.6511) teacher_loss 0.8493 (0.8220) loss_zs_kd 3.6851 (4.4570) loss_oracle 0.8161 (0.8292) acc 78.1250 (76.1161) alaph_mean 0.4247 (0.4213) alpha_val 0.4247 (0.4213) lr 1.8763e-03 eta 0:18:07
epoch [10/50] batch [300/319] time 0.083 (0.085) data 0.000 (0.002) loss 1.7313 (1.6482) teacher_loss 0.9386 (0.8223) loss_zs_kd 3.8892 (4.3798) loss_oracle 0.7927 (0.8259) acc 71.8750 (76.0729) alaph_mean 0.4252 (0.4215) alpha_val 0.4252 (0.4215) lr 1.8763e-03 eta 0:18:04
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,996
* accuracy: 45.6%
* error: 54.4%
* macro_f1: 39.7%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 3,526
* accuracy: 36.2%
* error: 63.8%
* macro_f1: 20.7%
******* Domain 2 best val acc:      46.9%, epoch: 6 *******
******* Domain 2 best val test acc: 48.6%, epoch: 6 *******
******* Domain 2 best test acc:     48.6%, epoch: 6 *******
epoch [11/50] batch [20/319] time 0.082 (0.107) data 0.000 (0.024) loss 1.4410 (1.5401) teacher_loss 0.6608 (0.7197) loss_zs_kd 5.1297 (4.6885) loss_oracle 0.7802 (0.8204) acc 87.5000 (80.1562) alaph_mean 0.4259 (0.4257) alpha_val 0.4259 (0.4257) lr 1.8443e-03 eta 0:22:41
epoch [11/50] batch [40/319] time 0.087 (0.095) data 0.000 (0.012) loss 1.4719 (1.5456) teacher_loss 0.6619 (0.7475) loss_zs_kd 4.3960 (4.7177) loss_oracle 0.8100 (0.7981) acc 81.2500 (79.1406) alaph_mean 0.4263 (0.4259) alpha_val 0.4263 (0.4259) lr 1.8443e-03 eta 0:20:11
epoch [11/50] batch [60/319] time 0.077 (0.091) data 0.001 (0.008) loss 1.8133 (1.5554) teacher_loss 1.1383 (0.7670) loss_zs_kd 4.3949 (4.6254) loss_oracle 0.6749 (0.7885) acc 68.7500 (78.3854) alaph_mean 0.4267 (0.4261) alpha_val 0.4267 (0.4261) lr 1.8443e-03 eta 0:19:12
epoch [11/50] batch [80/319] time 0.085 (0.088) data 0.000 (0.006) loss 1.6307 (1.5618) teacher_loss 0.8347 (0.7789) loss_zs_kd 4.4407 (4.5451) loss_oracle 0.7960 (0.7829) acc 78.1250 (78.4375) alaph_mean 0.4272 (0.4263) alpha_val 0.4272 (0.4263) lr 1.8443e-03 eta 0:18:41
epoch [11/50] batch [100/319] time 0.080 (0.087) data 0.000 (0.005) loss 1.6466 (1.5764) teacher_loss 0.8262 (0.7952) loss_zs_kd 4.0025 (4.4731) loss_oracle 0.8205 (0.7812) acc 71.8750 (77.7188) alaph_mean 0.4277 (0.4266) alpha_val 0.4277 (0.4266) lr 1.8443e-03 eta 0:18:20
epoch [11/50] batch [120/319] time 0.083 (0.086) data 0.000 (0.004) loss 1.5240 (1.5689) teacher_loss 0.8435 (0.7950) loss_zs_kd 4.1832 (4.4179) loss_oracle 0.6805 (0.7739) acc 84.3750 (78.2031) alaph_mean 0.4282 (0.4268) alpha_val 0.4282 (0.4268) lr 1.8443e-03 eta 0:18:10
epoch [11/50] batch [140/319] time 0.086 (0.085) data 0.000 (0.004) loss 1.9449 (1.5620) teacher_loss 1.1675 (0.7930) loss_zs_kd 4.5289 (4.4104) loss_oracle 0.7774 (0.7690) acc 62.5000 (78.7500) alaph_mean 0.4285 (0.4270) alpha_val 0.4285 (0.4270) lr 1.8443e-03 eta 0:17:58
epoch [11/50] batch [160/319] time 0.086 (0.085) data 0.000 (0.003) loss 1.2078 (1.5591) teacher_loss 0.3938 (0.7831) loss_zs_kd 4.6407 (4.4405) loss_oracle 0.8140 (0.7760) acc 90.6250 (79.1406) alaph_mean 0.4289 (0.4272) alpha_val 0.4289 (0.4272) lr 1.8443e-03 eta 0:17:56
epoch [11/50] batch [180/319] time 0.081 (0.085) data 0.000 (0.003) loss 1.5869 (1.5516) teacher_loss 0.8022 (0.7719) loss_zs_kd 4.2078 (4.4523) loss_oracle 0.7847 (0.7798) acc 87.5000 (79.5139) alaph_mean 0.4293 (0.4274) alpha_val 0.4293 (0.4274) lr 1.8443e-03 eta 0:17:48
epoch [11/50] batch [200/319] time 0.086 (0.085) data 0.000 (0.003) loss 1.5032 (1.5417) teacher_loss 0.7185 (0.7574) loss_zs_kd 4.6764 (4.4752) loss_oracle 0.7847 (0.7843) acc 78.1250 (79.9375) alaph_mean 0.4297 (0.4277) alpha_val 0.4297 (0.4277) lr 1.8443e-03 eta 0:17:46
epoch [11/50] batch [220/319] time 0.084 (0.085) data 0.000 (0.002) loss 1.5445 (1.5329) teacher_loss 0.7284 (0.7434) loss_zs_kd 4.9566 (4.5178) loss_oracle 0.8161 (0.7895) acc 81.2500 (80.2273) alaph_mean 0.4300 (0.4279) alpha_val 0.4300 (0.4279) lr 1.8443e-03 eta 0:17:43
epoch [11/50] batch [240/319] time 0.088 (0.085) data 0.001 (0.002) loss 1.3809 (1.5244) teacher_loss 0.4842 (0.7317) loss_zs_kd 4.7726 (4.5193) loss_oracle 0.8967 (0.7927) acc 87.5000 (80.6771) alaph_mean 0.4303 (0.4280) alpha_val 0.4303 (0.4280) lr 1.8443e-03 eta 0:17:41
epoch [11/50] batch [260/319] time 0.088 (0.085) data 0.000 (0.002) loss 1.3484 (1.5170) teacher_loss 0.5557 (0.7204) loss_zs_kd 4.4012 (4.4906) loss_oracle 0.7927 (0.7966) acc 84.3750 (81.0577) alaph_mean 0.4307 (0.4282) alpha_val 0.4307 (0.4282) lr 1.8443e-03 eta 0:17:40
epoch [11/50] batch [280/319] time 0.080 (0.085) data 0.000 (0.002) loss 1.5512 (1.5129) teacher_loss 0.7148 (0.7117) loss_zs_kd 4.1369 (4.4918) loss_oracle 0.8364 (0.8012) acc 84.3750 (81.3170) alaph_mean 0.4311 (0.4284) alpha_val 0.4311 (0.4284) lr 1.8443e-03 eta 0:17:38
epoch [11/50] batch [300/319] time 0.077 (0.086) data 0.000 (0.002) loss 1.8985 (1.5150) teacher_loss 1.0795 (0.7127) loss_zs_kd 4.4544 (4.4823) loss_oracle 0.8190 (0.8023) acc 65.6250 (81.3125) alaph_mean 0.4315 (0.4286) alpha_val 0.4315 (0.4286) lr 1.8443e-03 eta 0:17:47
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,868
* accuracy: 42.7%
* error: 57.3%
* macro_f1: 34.6%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 1,762
* accuracy: 18.1%
* error: 81.9%
* macro_f1: 14.9%
******* Domain 2 best val acc:      46.9%, epoch: 6 *******
******* Domain 2 best val test acc: 48.6%, epoch: 6 *******
******* Domain 2 best test acc:     48.6%, epoch: 6 *******
epoch [12/50] batch [20/319] time 0.084 (0.119) data 0.000 (0.034) loss 1.5638 (1.4802) teacher_loss 0.7826 (0.6443) loss_zs_kd 4.0723 (4.3491) loss_oracle 0.7811 (0.8359) acc 84.3750 (83.2812) alaph_mean 0.4322 (0.4320) alpha_val 0.4322 (0.4320) lr 1.8090e-03 eta 0:24:42
epoch [12/50] batch [40/319] time 0.083 (0.100) data 0.000 (0.017) loss 1.4018 (1.4928) teacher_loss 0.6099 (0.6559) loss_zs_kd 4.0184 (4.2602) loss_oracle 0.7919 (0.8369) acc 78.1250 (82.1875) alaph_mean 0.4325 (0.4322) alpha_val 0.4325 (0.4322) lr 1.8090e-03 eta 0:20:36
epoch [12/50] batch [60/319] time 0.070 (0.092) data 0.000 (0.011) loss 1.5944 (1.5106) teacher_loss 0.8412 (0.6851) loss_zs_kd 3.6153 (4.1729) loss_oracle 0.7531 (0.8255) acc 87.5000 (81.7188) alaph_mean 0.4330 (0.4324) alpha_val 0.4330 (0.4324) lr 1.8090e-03 eta 0:19:03
epoch [12/50] batch [80/319] time 0.082 (0.090) data 0.000 (0.009) loss 1.3302 (1.5138) teacher_loss 0.6620 (0.7148) loss_zs_kd 3.5971 (4.0098) loss_oracle 0.6682 (0.7990) acc 93.7500 (81.6797) alaph_mean 0.4334 (0.4326) alpha_val 0.4334 (0.4326) lr 1.8090e-03 eta 0:18:37
epoch [12/50] batch [100/319] time 0.077 (0.092) data 0.000 (0.007) loss 1.4776 (1.5208) teacher_loss 0.7649 (0.7477) loss_zs_kd 4.0395 (3.9527) loss_oracle 0.7128 (0.7731) acc 81.2500 (81.4688) alaph_mean 0.4339 (0.4328) alpha_val 0.4339 (0.4328) lr 1.8090e-03 eta 0:18:59
epoch [12/50] batch [120/319] time 0.083 (0.091) data 0.000 (0.006) loss 1.6528 (1.5298) teacher_loss 0.9290 (0.7675) loss_zs_kd 4.0943 (3.9652) loss_oracle 0.7238 (0.7623) acc 75.0000 (81.0156) alaph_mean 0.4344 (0.4330) alpha_val 0.4344 (0.4330) lr 1.8090e-03 eta 0:18:42
epoch [12/50] batch [140/319] time 0.076 (0.089) data 0.000 (0.005) loss 1.5594 (1.5180) teacher_loss 0.7661 (0.7651) loss_zs_kd 4.7339 (4.0449) loss_oracle 0.7933 (0.7529) acc 90.6250 (81.6071) alaph_mean 0.4349 (0.4333) alpha_val 0.4349 (0.4333) lr 1.8090e-03 eta 0:18:19
epoch [12/50] batch [160/319] time 0.084 (0.088) data 0.000 (0.004) loss 1.5069 (1.5131) teacher_loss 0.6626 (0.7585) loss_zs_kd 4.6825 (4.0880) loss_oracle 0.8443 (0.7546) acc 81.2500 (81.9531) alaph_mean 0.4354 (0.4335) alpha_val 0.4354 (0.4335) lr 1.8090e-03 eta 0:18:05
epoch [12/50] batch [180/319] time 0.079 (0.087) data 0.000 (0.004) loss 1.4877 (1.5119) teacher_loss 0.6650 (0.7473) loss_zs_kd 4.1194 (4.1267) loss_oracle 0.8228 (0.7647) acc 90.6250 (82.3438) alaph_mean 0.4358 (0.4337) alpha_val 0.4358 (0.4337) lr 1.8090e-03 eta 0:17:51
epoch [12/50] batch [200/319] time 0.089 (0.087) data 0.000 (0.004) loss 1.5329 (1.5074) teacher_loss 0.7684 (0.7382) loss_zs_kd 4.2635 (4.1125) loss_oracle 0.7645 (0.7692) acc 81.2500 (82.5156) alaph_mean 0.4361 (0.4340) alpha_val 0.4361 (0.4340) lr 1.8090e-03 eta 0:17:45
epoch [12/50] batch [220/319] time 0.087 (0.086) data 0.000 (0.003) loss 1.3175 (1.4981) teacher_loss 0.5320 (0.7273) loss_zs_kd 4.0543 (4.1341) loss_oracle 0.7856 (0.7708) acc 90.6250 (82.8125) alaph_mean 0.4365 (0.4342) alpha_val 0.4365 (0.4342) lr 1.8090e-03 eta 0:17:36
epoch [12/50] batch [240/319] time 0.073 (0.086) data 0.000 (0.003) loss 1.4160 (1.4966) teacher_loss 0.6898 (0.7240) loss_zs_kd 4.9730 (4.1829) loss_oracle 0.7263 (0.7727) acc 81.2500 (82.7734) alaph_mean 0.4368 (0.4344) alpha_val 0.4368 (0.4344) lr 1.8090e-03 eta 0:17:31
epoch [12/50] batch [260/319] time 0.092 (0.086) data 0.000 (0.003) loss 1.4660 (1.4898) teacher_loss 0.6646 (0.7163) loss_zs_kd 4.7960 (4.2376) loss_oracle 0.8014 (0.7735) acc 84.3750 (82.8726) alaph_mean 0.4372 (0.4346) alpha_val 0.4372 (0.4346) lr 1.8090e-03 eta 0:17:21
epoch [12/50] batch [280/319] time 0.086 (0.085) data 0.000 (0.003) loss 1.1963 (1.4855) teacher_loss 0.4189 (0.7078) loss_zs_kd 4.7784 (4.2642) loss_oracle 0.7774 (0.7777) acc 96.8750 (82.9464) alaph_mean 0.4375 (0.4348) alpha_val 0.4375 (0.4348) lr 1.8090e-03 eta 0:17:14
epoch [12/50] batch [300/319] time 0.084 (0.085) data 0.000 (0.003) loss 1.3521 (1.4812) teacher_loss 0.4305 (0.6989) loss_zs_kd 4.7375 (4.2965) loss_oracle 0.9216 (0.7823) acc 100.0000 (83.0729) alaph_mean 0.4378 (0.4350) alpha_val 0.4378 (0.4350) lr 1.8090e-03 eta 0:17:13
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,732
* accuracy: 39.6%
* error: 60.4%
* macro_f1: 32.3%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 1,198
* accuracy: 12.3%
* error: 87.7%
* macro_f1: 11.9%
******* Domain 2 best val acc:      46.9%, epoch: 6 *******
******* Domain 2 best val test acc: 48.6%, epoch: 6 *******
******* Domain 2 best test acc:     48.6%, epoch: 6 *******
epoch [13/50] batch [20/319] time 0.076 (0.115) data 0.000 (0.028) loss 1.5304 (1.5372) teacher_loss 0.6318 (0.6983) loss_zs_kd 4.3315 (4.5888) loss_oracle 0.8986 (0.8389) acc 81.2500 (83.7500) alaph_mean 0.4384 (0.4383) alpha_val 0.4384 (0.4383) lr 1.7705e-03 eta 0:23:16
epoch [13/50] batch [40/319] time 0.079 (0.098) data 0.000 (0.014) loss 1.5628 (1.4756) teacher_loss 0.7008 (0.6394) loss_zs_kd 3.6130 (4.3712) loss_oracle 0.8620 (0.8361) acc 78.1250 (83.6719) alaph_mean 0.4387 (0.4384) alpha_val 0.4387 (0.4384) lr 1.7705e-03 eta 0:19:49
epoch [13/50] batch [60/319] time 0.080 (0.093) data 0.001 (0.010) loss 1.7316 (1.4975) teacher_loss 0.8494 (0.6593) loss_zs_kd 3.9048 (4.2288) loss_oracle 0.8822 (0.8381) acc 65.6250 (83.0208) alaph_mean 0.4391 (0.4386) alpha_val 0.4391 (0.4386) lr 1.7705e-03 eta 0:18:44
epoch [13/50] batch [80/319] time 0.075 (0.090) data 0.000 (0.007) loss 1.2860 (1.4927) teacher_loss 0.4911 (0.6605) loss_zs_kd 4.0466 (4.2117) loss_oracle 0.7950 (0.8322) acc 87.5000 (83.3594) alaph_mean 0.4394 (0.4387) alpha_val 0.4394 (0.4387) lr 1.7705e-03 eta 0:18:01
epoch [13/50] batch [100/319] time 0.076 (0.088) data 0.000 (0.006) loss 1.4198 (1.4860) teacher_loss 0.4803 (0.6545) loss_zs_kd 4.6781 (4.1744) loss_oracle 0.9395 (0.8315) acc 81.2500 (83.7812) alaph_mean 0.4397 (0.4389) alpha_val 0.4397 (0.4389) lr 1.7705e-03 eta 0:17:34
epoch [13/50] batch [120/319] time 0.085 (0.087) data 0.000 (0.005) loss 1.5178 (1.4833) teacher_loss 0.6995 (0.6583) loss_zs_kd 4.0438 (4.1585) loss_oracle 0.8183 (0.8250) acc 87.5000 (83.4896) alaph_mean 0.4400 (0.4391) alpha_val 0.4400 (0.4391) lr 1.7705e-03 eta 0:17:20
epoch [13/50] batch [140/319] time 0.086 (0.086) data 0.000 (0.004) loss 1.7665 (1.4771) teacher_loss 0.9377 (0.6552) loss_zs_kd 4.2998 (4.1447) loss_oracle 0.8289 (0.8219) acc 75.0000 (83.5714) alaph_mean 0.4403 (0.4392) alpha_val 0.4403 (0.4392) lr 1.7705e-03 eta 0:17:16
epoch [13/50] batch [160/319] time 0.080 (0.086) data 0.000 (0.004) loss 1.5420 (1.4734) teacher_loss 0.7131 (0.6503) loss_zs_kd 4.9925 (4.1998) loss_oracle 0.8288 (0.8232) acc 84.3750 (83.5742) alaph_mean 0.4406 (0.4394) alpha_val 0.4406 (0.4394) lr 1.7705e-03 eta 0:17:12
epoch [13/50] batch [180/319] time 0.078 (0.085) data 0.000 (0.003) loss 1.4568 (1.4760) teacher_loss 0.6597 (0.6529) loss_zs_kd 4.7979 (4.2938) loss_oracle 0.7971 (0.8230) acc 84.3750 (83.3681) alaph_mean 0.4408 (0.4395) alpha_val 0.4408 (0.4395) lr 1.7705e-03 eta 0:16:59
epoch [13/50] batch [200/319] time 0.088 (0.085) data 0.000 (0.003) loss 1.6944 (1.4773) teacher_loss 0.8107 (0.6511) loss_zs_kd 5.3915 (4.3947) loss_oracle 0.8837 (0.8262) acc 81.2500 (83.4062) alaph_mean 0.4410 (0.4397) alpha_val 0.4410 (0.4397) lr 1.7705e-03 eta 0:16:53
epoch [13/50] batch [220/319] time 0.082 (0.085) data 0.000 (0.003) loss 1.4255 (1.4712) teacher_loss 0.5647 (0.6430) loss_zs_kd 5.1350 (4.4678) loss_oracle 0.8608 (0.8281) acc 84.3750 (83.6222) alaph_mean 0.4413 (0.4398) alpha_val 0.4413 (0.4398) lr 1.7705e-03 eta 0:16:52
epoch [13/50] batch [240/319] time 0.082 (0.085) data 0.000 (0.003) loss 1.6408 (1.4727) teacher_loss 0.7420 (0.6434) loss_zs_kd 5.1858 (4.5166) loss_oracle 0.8989 (0.8293) acc 78.1250 (83.4766) alaph_mean 0.4415 (0.4399) alpha_val 0.4415 (0.4399) lr 1.7705e-03 eta 0:16:50
epoch [13/50] batch [260/319] time 0.081 (0.085) data 0.000 (0.002) loss 1.3511 (1.4706) teacher_loss 0.5693 (0.6413) loss_zs_kd 5.2177 (4.5532) loss_oracle 0.7818 (0.8293) acc 78.1250 (83.5096) alaph_mean 0.4418 (0.4401) alpha_val 0.4418 (0.4401) lr 1.7705e-03 eta 0:16:46
epoch [13/50] batch [280/319] time 0.088 (0.085) data 0.000 (0.002) loss 1.3866 (1.4683) teacher_loss 0.5500 (0.6406) loss_zs_kd 4.3122 (4.5614) loss_oracle 0.8366 (0.8276) acc 78.1250 (83.6830) alaph_mean 0.4421 (0.4402) alpha_val 0.4421 (0.4402) lr 1.7705e-03 eta 0:16:45
epoch [13/50] batch [300/319] time 0.087 (0.086) data 0.000 (0.002) loss 1.4690 (1.4681) teacher_loss 0.6519 (0.6430) loss_zs_kd 4.7069 (4.5566) loss_oracle 0.8171 (0.8252) acc 90.6250 (83.7188) alaph_mean 0.4424 (0.4403) alpha_val 0.4424 (0.4403) lr 1.7705e-03 eta 0:16:53
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,020
* accuracy: 46.1%
* error: 53.9%
* macro_f1: 34.6%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 1,117
* accuracy: 11.5%
* error: 88.5%
* macro_f1: 12.2%
******* Domain 2 best val acc:      46.9%, epoch: 6 *******
******* Domain 2 best val test acc: 48.6%, epoch: 6 *******
******* Domain 2 best test acc:     48.6%, epoch: 6 *******
epoch [14/50] batch [20/319] time 0.069 (0.114) data 0.000 (0.031) loss 1.3392 (1.4993) teacher_loss 0.6368 (0.7409) loss_zs_kd 4.2532 (4.1986) loss_oracle 0.7025 (0.7583) acc 84.3750 (81.4062) alaph_mean 0.4431 (0.4429) alpha_val 0.4431 (0.4429) lr 1.7290e-03 eta 0:22:28
epoch [14/50] batch [40/319] time 0.080 (0.097) data 0.000 (0.016) loss 1.5181 (1.4883) teacher_loss 0.7260 (0.7026) loss_zs_kd 4.0045 (4.3022) loss_oracle 0.7922 (0.7858) acc 81.2500 (82.3438) alaph_mean 0.4434 (0.4431) alpha_val 0.4434 (0.4431) lr 1.7290e-03 eta 0:19:00
epoch [14/50] batch [60/319] time 0.074 (0.092) data 0.001 (0.011) loss 1.3623 (1.4833) teacher_loss 0.4850 (0.6919) loss_zs_kd 4.3402 (4.3258) loss_oracle 0.8773 (0.7914) acc 84.3750 (82.6562) alaph_mean 0.4437 (0.4433) alpha_val 0.4437 (0.4433) lr 1.7290e-03 eta 0:18:02
epoch [14/50] batch [80/319] time 0.076 (0.094) data 0.000 (0.008) loss 1.5012 (1.4755) teacher_loss 0.7318 (0.6779) loss_zs_kd 4.6210 (4.3189) loss_oracle 0.7693 (0.7976) acc 81.2500 (83.2422) alaph_mean 0.4440 (0.4434) alpha_val 0.4440 (0.4434) lr 1.7290e-03 eta 0:18:20
epoch [14/50] batch [100/319] time 0.087 (0.091) data 0.000 (0.007) loss 1.3092 (1.4775) teacher_loss 0.4636 (0.6811) loss_zs_kd 3.8941 (4.2860) loss_oracle 0.8456 (0.7964) acc 90.6250 (83.2812) alaph_mean 0.4443 (0.4436) alpha_val 0.4443 (0.4436) lr 1.7290e-03 eta 0:17:48
epoch [14/50] batch [120/319] time 0.082 (0.089) data 0.000 (0.005) loss 1.3859 (1.4722) teacher_loss 0.5537 (0.6743) loss_zs_kd 4.5339 (4.2527) loss_oracle 0.8322 (0.7979) acc 87.5000 (83.4896) alaph_mean 0.4445 (0.4437) alpha_val 0.4445 (0.4437) lr 1.7290e-03 eta 0:17:23
epoch [14/50] batch [140/319] time 0.079 (0.088) data 0.000 (0.005) loss 1.7263 (1.4742) teacher_loss 0.8183 (0.6706) loss_zs_kd 5.1398 (4.3278) loss_oracle 0.9080 (0.8036) acc 81.2500 (83.6161) alaph_mean 0.4448 (0.4438) alpha_val 0.4448 (0.4438) lr 1.7290e-03 eta 0:17:07
epoch [14/50] batch [160/319] time 0.084 (0.088) data 0.000 (0.004) loss 1.5950 (1.4789) teacher_loss 0.8565 (0.6765) loss_zs_kd 4.3063 (4.3651) loss_oracle 0.7385 (0.8025) acc 87.5000 (83.4961) alaph_mean 0.4451 (0.4440) alpha_val 0.4451 (0.4440) lr 1.7290e-03 eta 0:17:00
epoch [14/50] batch [180/319] time 0.079 (0.087) data 0.000 (0.004) loss 1.4049 (1.4775) teacher_loss 0.5310 (0.6709) loss_zs_kd 4.3220 (4.3515) loss_oracle 0.8740 (0.8066) acc 87.5000 (83.4896) alaph_mean 0.4453 (0.4441) alpha_val 0.4453 (0.4441) lr 1.7290e-03 eta 0:16:48
epoch [14/50] batch [200/319] time 0.084 (0.086) data 0.000 (0.003) loss 1.5074 (1.4711) teacher_loss 0.6978 (0.6639) loss_zs_kd 3.9691 (4.3406) loss_oracle 0.8096 (0.8072) acc 81.2500 (83.8750) alaph_mean 0.4456 (0.4442) alpha_val 0.4456 (0.4442) lr 1.7290e-03 eta 0:16:41
epoch [14/50] batch [220/319] time 0.071 (0.086) data 0.000 (0.003) loss 1.3251 (1.4701) teacher_loss 0.4256 (0.6621) loss_zs_kd 4.0215 (4.2962) loss_oracle 0.8995 (0.8079) acc 90.6250 (83.9062) alaph_mean 0.4458 (0.4444) alpha_val 0.4458 (0.4444) lr 1.7290e-03 eta 0:16:30
epoch [14/50] batch [240/319] time 0.081 (0.085) data 0.000 (0.003) loss 1.4865 (1.4663) teacher_loss 0.6715 (0.6586) loss_zs_kd 4.2464 (4.2747) loss_oracle 0.8151 (0.8077) acc 90.6250 (84.0625) alaph_mean 0.4460 (0.4445) alpha_val 0.4460 (0.4445) lr 1.7290e-03 eta 0:16:20
epoch [14/50] batch [260/319] time 0.071 (0.084) data 0.000 (0.003) loss 1.3298 (1.4638) teacher_loss 0.5143 (0.6557) loss_zs_kd 4.3237 (4.2892) loss_oracle 0.8155 (0.8081) acc 87.5000 (84.3029) alaph_mean 0.4462 (0.4446) alpha_val 0.4462 (0.4446) lr 1.7290e-03 eta 0:16:11
epoch [14/50] batch [280/319] time 0.069 (0.084) data 0.000 (0.003) loss 1.2962 (1.4613) teacher_loss 0.4972 (0.6516) loss_zs_kd 4.5371 (4.2932) loss_oracle 0.7990 (0.8096) acc 87.5000 (84.3750) alaph_mean 0.4465 (0.4447) alpha_val 0.4465 (0.4447) lr 1.7290e-03 eta 0:16:02
epoch [14/50] batch [300/319] time 0.076 (0.083) data 0.000 (0.002) loss 1.5549 (1.4612) teacher_loss 0.6428 (0.6503) loss_zs_kd 4.0074 (4.2963) loss_oracle 0.9122 (0.8110) acc 81.2500 (84.3229) alaph_mean 0.4467 (0.4449) alpha_val 0.4467 (0.4449) lr 1.7290e-03 eta 0:15:53
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,785
* accuracy: 40.8%
* error: 59.2%
* macro_f1: 32.1%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 1,254
* accuracy: 12.9%
* error: 87.1%
* macro_f1: 12.1%
******* Domain 2 best val acc:      46.9%, epoch: 6 *******
******* Domain 2 best val test acc: 48.6%, epoch: 6 *******
******* Domain 2 best test acc:     48.6%, epoch: 6 *******
epoch [15/50] batch [20/319] time 0.077 (0.107) data 0.000 (0.026) loss 1.7481 (1.4728) teacher_loss 0.9892 (0.6619) loss_zs_kd 4.6682 (4.6268) loss_oracle 0.7589 (0.8109) acc 68.7500 (81.4062) alaph_mean 0.4471 (0.4470) alpha_val 0.4471 (0.4470) lr 1.6845e-03 eta 0:20:30
epoch [15/50] batch [40/319] time 0.082 (0.095) data 0.000 (0.013) loss 1.2661 (1.4462) teacher_loss 0.4341 (0.6208) loss_zs_kd 5.0366 (4.7074) loss_oracle 0.8320 (0.8254) acc 96.8750 (83.4375) alaph_mean 0.4473 (0.4471) alpha_val 0.4473 (0.4471) lr 1.6845e-03 eta 0:18:09
epoch [15/50] batch [60/319] time 0.077 (0.091) data 0.000 (0.009) loss 1.5987 (1.4628) teacher_loss 0.7523 (0.6462) loss_zs_kd 4.6205 (4.6462) loss_oracle 0.8464 (0.8166) acc 81.2500 (83.5938) alaph_mean 0.4475 (0.4472) alpha_val 0.4475 (0.4472) lr 1.6845e-03 eta 0:17:15
epoch [15/50] batch [80/319] time 0.085 (0.088) data 0.000 (0.007) loss 1.2373 (1.4633) teacher_loss 0.6605 (0.6736) loss_zs_kd 3.8203 (4.5064) loss_oracle 0.5767 (0.7897) acc 93.7500 (83.3594) alaph_mean 0.4478 (0.4473) alpha_val 0.4478 (0.4473) lr 1.6845e-03 eta 0:16:48
epoch [15/50] batch [100/319] time 0.072 (0.087) data 0.000 (0.005) loss 1.4053 (1.4605) teacher_loss 0.7448 (0.6908) loss_zs_kd 4.9535 (4.4654) loss_oracle 0.6605 (0.7698) acc 84.3750 (83.6875) alaph_mean 0.4481 (0.4474) alpha_val 0.4481 (0.4474) lr 1.6845e-03 eta 0:16:27
epoch [15/50] batch [120/319] time 0.075 (0.086) data 0.000 (0.005) loss 1.5371 (1.4670) teacher_loss 0.8048 (0.7031) loss_zs_kd 4.8131 (4.5080) loss_oracle 0.7323 (0.7639) acc 87.5000 (83.4375) alaph_mean 0.4484 (0.4476) alpha_val 0.4484 (0.4476) lr 1.6845e-03 eta 0:16:14
epoch [15/50] batch [140/319] time 0.085 (0.086) data 0.000 (0.004) loss 1.5762 (1.4694) teacher_loss 0.7625 (0.7035) loss_zs_kd 4.7142 (4.5308) loss_oracle 0.8137 (0.7659) acc 84.3750 (83.4821) alaph_mean 0.4487 (0.4477) alpha_val 0.4487 (0.4477) lr 1.6845e-03 eta 0:16:11
epoch [15/50] batch [160/319] time 0.081 (0.085) data 0.000 (0.004) loss 1.2983 (1.4612) teacher_loss 0.5117 (0.6891) loss_zs_kd 4.8272 (4.5572) loss_oracle 0.7866 (0.7721) acc 87.5000 (83.5742) alaph_mean 0.4489 (0.4479) alpha_val 0.4489 (0.4479) lr 1.6845e-03 eta 0:15:59
epoch [15/50] batch [180/319] time 0.091 (0.084) data 0.000 (0.003) loss 1.4782 (1.4546) teacher_loss 0.6138 (0.6757) loss_zs_kd 4.6126 (4.6097) loss_oracle 0.8644 (0.7789) acc 84.3750 (83.7500) alaph_mean 0.4491 (0.4480) alpha_val 0.4491 (0.4480) lr 1.6845e-03 eta 0:15:54
epoch [15/50] batch [200/319] time 0.083 (0.085) data 0.000 (0.003) loss 1.7166 (1.4629) teacher_loss 0.8947 (0.6819) loss_zs_kd 4.8345 (4.6069) loss_oracle 0.8219 (0.7810) acc 84.3750 (83.6875) alaph_mean 0.4494 (0.4481) alpha_val 0.4494 (0.4481) lr 1.6845e-03 eta 0:15:54
epoch [15/50] batch [220/319] time 0.088 (0.085) data 0.000 (0.003) loss 1.4965 (1.4661) teacher_loss 0.8475 (0.6820) loss_zs_kd 4.2450 (4.6034) loss_oracle 0.6490 (0.7842) acc 81.2500 (83.4517) alaph_mean 0.4496 (0.4482) alpha_val 0.4496 (0.4482) lr 1.6845e-03 eta 0:15:54
epoch [15/50] batch [240/319] time 0.084 (0.085) data 0.000 (0.002) loss 1.3633 (1.4674) teacher_loss 0.5832 (0.6827) loss_zs_kd 4.8176 (4.5892) loss_oracle 0.7801 (0.7848) acc 84.3750 (83.5417) alaph_mean 0.4499 (0.4484) alpha_val 0.4499 (0.4484) lr 1.6845e-03 eta 0:15:50
epoch [15/50] batch [260/319] time 0.075 (0.084) data 0.000 (0.002) loss 1.4124 (1.4747) teacher_loss 0.6722 (0.6900) loss_zs_kd 4.1544 (4.5795) loss_oracle 0.7402 (0.7847) acc 84.3750 (83.3534) alaph_mean 0.4501 (0.4485) alpha_val 0.4501 (0.4485) lr 1.6845e-03 eta 0:15:47
epoch [15/50] batch [280/319] time 0.133 (0.085) data 0.001 (0.002) loss 1.4087 (1.4768) teacher_loss 0.5605 (0.6914) loss_zs_kd 4.2707 (4.5556) loss_oracle 0.8482 (0.7854) acc 87.5000 (83.4375) alaph_mean 0.4504 (0.4486) alpha_val 0.4504 (0.4486) lr 1.6845e-03 eta 0:15:55
epoch [15/50] batch [300/319] time 0.090 (0.085) data 0.000 (0.002) loss 1.5574 (1.4711) teacher_loss 0.8198 (0.6862) loss_zs_kd 4.2112 (4.5213) loss_oracle 0.7376 (0.7850) acc 81.2500 (83.5729) alaph_mean 0.4506 (0.4487) alpha_val 0.4506 (0.4487) lr 1.6845e-03 eta 0:15:55
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,890
* accuracy: 43.2%
* error: 56.8%
* macro_f1: 34.2%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 1,464
* accuracy: 15.0%
* error: 85.0%
* macro_f1: 11.2%
******* Domain 2 best val acc:      46.9%, epoch: 6 *******
******* Domain 2 best val test acc: 48.6%, epoch: 6 *******
******* Domain 2 best test acc:     48.6%, epoch: 6 *******
epoch [16/50] batch [20/319] time 0.073 (0.105) data 0.000 (0.026) loss 1.2880 (1.4843) teacher_loss 0.4451 (0.6773) loss_zs_kd 4.9007 (4.6798) loss_oracle 0.8430 (0.8071) acc 84.3750 (85.3125) alaph_mean 0.4510 (0.4509) alpha_val 0.4510 (0.4509) lr 1.6374e-03 eta 0:19:29
epoch [16/50] batch [40/319] time 0.092 (0.093) data 0.000 (0.013) loss 1.6300 (1.4692) teacher_loss 0.7887 (0.6694) loss_zs_kd 5.0735 (4.6804) loss_oracle 0.8414 (0.7998) acc 75.0000 (84.1406) alaph_mean 0.4512 (0.4510) alpha_val 0.4512 (0.4510) lr 1.6374e-03 eta 0:17:19
epoch [16/50] batch [60/319] time 0.085 (0.089) data 0.000 (0.009) loss 1.1851 (1.4677) teacher_loss 0.3626 (0.6715) loss_zs_kd 4.3162 (4.6186) loss_oracle 0.8226 (0.7962) acc 87.5000 (83.8021) alaph_mean 0.4514 (0.4511) alpha_val 0.4514 (0.4511) lr 1.6374e-03 eta 0:16:30
epoch [16/50] batch [80/319] time 0.084 (0.088) data 0.000 (0.007) loss 1.2555 (1.4671) teacher_loss 0.4317 (0.6663) loss_zs_kd 5.0799 (4.6218) loss_oracle 0.8238 (0.8008) acc 84.3750 (84.1016) alaph_mean 0.4516 (0.4512) alpha_val 0.4516 (0.4512) lr 1.6374e-03 eta 0:16:15
epoch [16/50] batch [100/319] time 0.077 (0.088) data 0.000 (0.005) loss 1.5181 (1.4689) teacher_loss 0.7271 (0.6635) loss_zs_kd 4.6957 (4.6373) loss_oracle 0.7910 (0.8054) acc 84.3750 (83.8438) alaph_mean 0.4518 (0.4513) alpha_val 0.4518 (0.4513) lr 1.6374e-03 eta 0:16:19
epoch [16/50] batch [120/319] time 0.086 (0.088) data 0.000 (0.004) loss 1.2343 (1.4547) teacher_loss 0.3726 (0.6425) loss_zs_kd 4.5395 (4.6521) loss_oracle 0.8617 (0.8123) acc 90.6250 (84.2708) alaph_mean 0.4520 (0.4514) alpha_val 0.4520 (0.4514) lr 1.6374e-03 eta 0:16:12
epoch [16/50] batch [140/319] time 0.100 (0.088) data 0.000 (0.004) loss 1.3600 (1.4697) teacher_loss 0.5730 (0.6546) loss_zs_kd 4.8289 (4.6391) loss_oracle 0.7869 (0.8151) acc 87.5000 (83.5938) alaph_mean 0.4522 (0.4515) alpha_val 0.4522 (0.4515) lr 1.6374e-03 eta 0:16:06
epoch [16/50] batch [160/319] time 0.077 (0.087) data 0.000 (0.003) loss 1.3109 (1.4739) teacher_loss 0.5191 (0.6611) loss_zs_kd 4.5981 (4.6425) loss_oracle 0.7919 (0.8127) acc 87.5000 (83.3594) alaph_mean 0.4524 (0.4516) alpha_val 0.4524 (0.4516) lr 1.6374e-03 eta 0:15:57
epoch [16/50] batch [180/319] time 0.093 (0.087) data 0.000 (0.003) loss 1.6174 (1.4671) teacher_loss 0.8561 (0.6550) loss_zs_kd 5.0438 (4.6476) loss_oracle 0.7612 (0.8121) acc 87.5000 (83.6806) alaph_mean 0.4526 (0.4517) alpha_val 0.4526 (0.4517) lr 1.6374e-03 eta 0:15:55
epoch [16/50] batch [200/319] time 0.075 (0.086) data 0.000 (0.003) loss 1.3729 (1.4645) teacher_loss 0.4584 (0.6487) loss_zs_kd 4.8565 (4.6560) loss_oracle 0.9144 (0.8158) acc 93.7500 (83.8438) alaph_mean 0.4528 (0.4518) alpha_val 0.4528 (0.4518) lr 1.6374e-03 eta 0:15:46
epoch [16/50] batch [220/319] time 0.083 (0.086) data 0.000 (0.003) loss 1.3769 (1.4651) teacher_loss 0.5251 (0.6449) loss_zs_kd 4.2921 (4.6634) loss_oracle 0.8518 (0.8202) acc 84.3750 (83.7784) alaph_mean 0.4529 (0.4519) alpha_val 0.4529 (0.4519) lr 1.6374e-03 eta 0:15:44
epoch [16/50] batch [240/319] time 0.083 (0.086) data 0.000 (0.002) loss 1.2554 (1.4618) teacher_loss 0.3991 (0.6394) loss_zs_kd 5.0055 (4.6545) loss_oracle 0.8563 (0.8223) acc 96.8750 (83.9844) alaph_mean 0.4530 (0.4520) alpha_val 0.4530 (0.4520) lr 1.6374e-03 eta 0:15:40
epoch [16/50] batch [260/319] time 0.083 (0.086) data 0.000 (0.002) loss 1.2640 (1.4615) teacher_loss 0.4580 (0.6384) loss_zs_kd 4.5085 (4.6340) loss_oracle 0.8060 (0.8231) acc 87.5000 (84.1466) alaph_mean 0.4532 (0.4521) alpha_val 0.4532 (0.4521) lr 1.6374e-03 eta 0:15:36
epoch [16/50] batch [280/319] time 0.081 (0.086) data 0.000 (0.002) loss 1.7395 (1.4639) teacher_loss 0.9723 (0.6405) loss_zs_kd 4.2531 (4.6179) loss_oracle 0.7672 (0.8234) acc 65.6250 (84.0402) alaph_mean 0.4534 (0.4522) alpha_val 0.4534 (0.4522) lr 1.6374e-03 eta 0:15:33
epoch [16/50] batch [300/319] time 0.075 (0.085) data 0.000 (0.002) loss 1.7200 (1.4637) teacher_loss 0.9065 (0.6404) loss_zs_kd 4.8274 (4.6037) loss_oracle 0.8136 (0.8233) acc 81.2500 (84.0000) alaph_mean 0.4535 (0.4522) alpha_val 0.4535 (0.4522) lr 1.6374e-03 eta 0:15:26
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,124
* accuracy: 48.5%
* error: 51.5%
* macro_f1: 32.9%
Checkpoint saved to icml/multi-dg/oracle/15_moreteacherweight/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 1,248
* accuracy: 12.8%
* error: 87.2%
* macro_f1: 12.5%
******* Domain 2 best val acc:      48.5%, epoch: 16 *******
******* Domain 2 best val test acc: 12.8%, epoch: 16 *******
******* Domain 2 best test acc:     48.6%, epoch: 6 *******
epoch [17/50] batch [20/319] time 0.090 (0.114) data 0.000 (0.030) loss 1.3996 (1.4296) teacher_loss 0.6013 (0.6368) loss_zs_kd 4.1154 (4.0666) loss_oracle 0.7983 (0.7929) acc 81.2500 (83.9062) alaph_mean 0.4539 (0.4538) alpha_val 0.4539 (0.4538) lr 1.5878e-03 eta 0:20:35
epoch [17/50] batch [40/319] time 0.080 (0.098) data 0.000 (0.015) loss 1.3903 (1.4353) teacher_loss 0.5242 (0.6394) loss_zs_kd 4.5535 (4.2009) loss_oracle 0.8661 (0.7958) acc 90.6250 (84.2969) alaph_mean 0.4541 (0.4539) alpha_val 0.4541 (0.4539) lr 1.5878e-03 eta 0:17:40
