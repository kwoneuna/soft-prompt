Loading trainer: TRIP
Loading dataset: SPG_OfficeHome
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------------------------------
Dataset    SPG_OfficeHome
Source     ['clipart', 'product', 'real_world']
Target     ['art']
# classes  65
# train_x  9,222
# val      3,939
# test     2,427
---------  ------------------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
alpha_logit                                        1
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  256
gate.mlp.2.bias                                    2
Total trainable params: 70,019
[Info] Hyperparameters saved to: icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/hyperparameters.json
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/288] time 0.078 (0.141) data 0.000 (0.023) loss 1.7229 (1.5338) teacher_loss 1.5280 (1.2950) loss_zs_kd 0.0000 (0.0000) loss_oracle 0.0013 (0.0017) acc 56.2500 (66.2500) lr 1.0000e-05 eta 0:33:44
epoch [1/50] batch [40/288] time 0.081 (0.111) data 0.000 (0.012) loss 1.1510 (1.4922) teacher_loss 1.0177 (1.2660) loss_zs_kd 0.0001 (0.0001) loss_oracle 0.0033 (0.0025) acc 71.8750 (66.6406) lr 1.0000e-05 eta 0:26:37
epoch [1/50] batch [60/288] time 0.078 (0.101) data 0.000 (0.008) loss 1.2294 (1.4920) teacher_loss 1.0078 (1.2607) loss_zs_kd 0.0004 (0.0002) loss_oracle 0.0073 (0.0036) acc 68.7500 (67.2396) lr 1.0000e-05 eta 0:24:02
epoch [1/50] batch [80/288] time 0.082 (0.094) data 0.000 (0.006) loss 1.7303 (1.4615) teacher_loss 1.5150 (1.2339) loss_zs_kd 0.0005 (0.0003) loss_oracle 0.0164 (0.0056) acc 68.7500 (68.0859) lr 1.0000e-05 eta 0:22:32
epoch [1/50] batch [100/288] time 0.068 (0.092) data 0.000 (0.005) loss 1.7500 (1.4425) teacher_loss 1.4170 (1.2121) loss_zs_kd 0.0020 (0.0005) loss_oracle 0.0123 (0.0068) acc 68.7500 (68.5312) lr 1.0000e-05 eta 0:21:51
epoch [1/50] batch [120/288] time 0.083 (0.090) data 0.000 (0.004) loss 1.5127 (1.4634) teacher_loss 1.3728 (1.2300) loss_zs_kd 0.0024 (0.0008) loss_oracle 0.0150 (0.0079) acc 59.3750 (68.1771) lr 1.0000e-05 eta 0:21:27
epoch [1/50] batch [140/288] time 0.088 (0.089) data 0.000 (0.004) loss 1.3936 (1.4464) teacher_loss 1.1672 (1.2139) loss_zs_kd 0.0019 (0.0011) loss_oracle 0.0108 (0.0091) acc 68.7500 (68.6161) lr 1.0000e-05 eta 0:21:10
epoch [1/50] batch [160/288] time 0.081 (0.088) data 0.000 (0.003) loss 1.2595 (1.4438) teacher_loss 0.9419 (1.2123) loss_zs_kd 0.0028 (0.0014) loss_oracle 0.0222 (0.0102) acc 78.1250 (68.8086) lr 1.0000e-05 eta 0:20:57
epoch [1/50] batch [180/288] time 0.074 (0.087) data 0.000 (0.003) loss 1.5764 (1.4398) teacher_loss 1.2998 (1.2058) loss_zs_kd 0.0034 (0.0017) loss_oracle 0.0126 (0.0109) acc 65.6250 (68.7847) lr 1.0000e-05 eta 0:20:36
epoch [1/50] batch [200/288] time 0.080 (0.086) data 0.000 (0.003) loss 1.3197 (1.4372) teacher_loss 1.0014 (1.2009) loss_zs_kd 0.0047 (0.0020) loss_oracle 0.0208 (0.0111) acc 78.1250 (69.0000) lr 1.0000e-05 eta 0:20:22
epoch [1/50] batch [220/288] time 0.065 (0.085) data 0.000 (0.002) loss 1.3908 (1.4438) teacher_loss 1.1572 (1.2051) loss_zs_kd 0.0051 (0.0023) loss_oracle 0.0231 (0.0113) acc 68.7500 (68.7642) lr 1.0000e-05 eta 0:20:07
epoch [1/50] batch [240/288] time 0.078 (0.084) data 0.000 (0.002) loss 1.0581 (1.4392) teacher_loss 0.8658 (1.2017) loss_zs_kd 0.0043 (0.0025) loss_oracle 0.0271 (0.0127) acc 75.0000 (68.8802) lr 1.0000e-05 eta 0:19:52
epoch [1/50] batch [260/288] time 0.069 (0.083) data 0.001 (0.002) loss 1.4052 (1.4441) teacher_loss 1.1281 (1.2070) loss_zs_kd 0.0051 (0.0028) loss_oracle 0.0222 (0.0134) acc 71.8750 (68.8341) lr 1.0000e-05 eta 0:19:37
epoch [1/50] batch [280/288] time 0.081 (0.083) data 0.000 (0.002) loss 1.0868 (1.4448) teacher_loss 0.8584 (1.2076) loss_zs_kd 0.0039 (0.0030) loss_oracle 0.0154 (0.0134) acc 81.2500 (68.7835) lr 1.0000e-05 eta 0:19:28
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,278
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 82.3%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,969
* accuracy: 81.1%
* error: 18.9%
* macro_f1: 76.9%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      83.2%, epoch: 1 *******
******* Domain a best val test acc: 81.1%, epoch: 1 *******
******* Domain a best test acc:     81.1%, epoch: 1 *******
epoch [2/50] batch [20/288] time 0.079 (0.102) data 0.000 (0.017) loss 1.9233 (1.7038) teacher_loss 1.3354 (1.1185) loss_zs_kd 0.0842 (0.0590) loss_oracle 0.5182 (0.4390) acc 65.6250 (70.1562) lr 2.0000e-03 eta 0:23:50
epoch [2/50] batch [40/288] time 0.069 (0.089) data 0.000 (0.009) loss 2.0492 (1.8438) teacher_loss 1.1720 (1.1506) loss_zs_kd 0.0920 (0.0652) loss_oracle 0.6719 (0.5405) acc 78.1250 (70.2344) lr 2.0000e-03 eta 0:20:58
epoch [2/50] batch [60/288] time 0.079 (0.085) data 0.000 (0.006) loss 1.7854 (1.8861) teacher_loss 1.0757 (1.1612) loss_zs_kd 0.0614 (0.0647) loss_oracle 0.5705 (0.5744) acc 71.8750 (69.7396) lr 2.0000e-03 eta 0:19:58
epoch [2/50] batch [80/288] time 0.083 (0.083) data 0.000 (0.005) loss 1.9277 (1.8652) teacher_loss 1.2141 (1.1525) loss_zs_kd 0.1047 (0.0644) loss_oracle 0.6227 (0.5734) acc 65.6250 (69.7656) lr 2.0000e-03 eta 0:19:29
epoch [2/50] batch [100/288] time 0.081 (0.083) data 0.000 (0.004) loss 1.7662 (1.8428) teacher_loss 1.1198 (1.1419) loss_zs_kd 0.0572 (0.0642) loss_oracle 0.4551 (0.5678) acc 68.7500 (69.8750) lr 2.0000e-03 eta 0:19:21
epoch [2/50] batch [120/288] time 0.081 (0.082) data 0.000 (0.003) loss 1.9561 (1.8238) teacher_loss 1.1770 (1.1314) loss_zs_kd 0.0668 (0.0633) loss_oracle 0.5882 (0.5633) acc 78.1250 (70.2083) lr 2.0000e-03 eta 0:19:13
epoch [2/50] batch [140/288] time 0.084 (0.083) data 0.000 (0.003) loss 1.8833 (1.7967) teacher_loss 1.2020 (1.1121) loss_zs_kd 0.0786 (0.0613) loss_oracle 0.5366 (0.5609) acc 68.7500 (70.8036) lr 2.0000e-03 eta 0:19:21
epoch [2/50] batch [160/288] time 0.083 (0.083) data 0.000 (0.002) loss 2.1410 (1.7741) teacher_loss 1.5220 (1.0921) loss_zs_kd 0.0672 (0.0608) loss_oracle 0.5915 (0.5629) acc 59.3750 (71.3281) lr 2.0000e-03 eta 0:19:16
epoch [2/50] batch [180/288] time 0.082 (0.083) data 0.000 (0.002) loss 2.1589 (1.7789) teacher_loss 1.5600 (1.1030) loss_zs_kd 0.0573 (0.0603) loss_oracle 0.5556 (0.5617) acc 65.6250 (71.1806) lr 2.0000e-03 eta 0:19:15
epoch [2/50] batch [200/288] time 0.089 (0.083) data 0.001 (0.002) loss 1.4266 (1.7742) teacher_loss 0.8327 (1.1038) loss_zs_kd 0.0465 (0.0606) loss_oracle 0.5661 (0.5579) acc 71.8750 (71.2812) lr 2.0000e-03 eta 0:19:13
epoch [2/50] batch [220/288] time 0.082 (0.083) data 0.000 (0.002) loss 1.8375 (1.7784) teacher_loss 1.1310 (1.1086) loss_zs_kd 0.0868 (0.0603) loss_oracle 0.5670 (0.5583) acc 75.0000 (71.0795) lr 2.0000e-03 eta 0:19:09
epoch [2/50] batch [240/288] time 0.085 (0.083) data 0.000 (0.002) loss 1.5944 (1.7636) teacher_loss 1.0110 (1.0969) loss_zs_kd 0.0560 (0.0606) loss_oracle 0.5350 (0.5568) acc 75.0000 (71.4453) lr 2.0000e-03 eta 0:19:05
epoch [2/50] batch [260/288] time 0.082 (0.083) data 0.000 (0.002) loss 2.6451 (1.7657) teacher_loss 1.9833 (1.1035) loss_zs_kd 0.0808 (0.0604) loss_oracle 0.5667 (0.5559) acc 46.8750 (71.2260) lr 2.0000e-03 eta 0:19:03
epoch [2/50] batch [280/288] time 0.082 (0.082) data 0.000 (0.002) loss 1.6716 (1.7562) teacher_loss 1.1770 (1.0976) loss_zs_kd 0.0376 (0.0596) loss_oracle 0.5312 (0.5542) acc 62.5000 (71.3281) lr 2.0000e-03 eta 0:18:58
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,358
* accuracy: 85.3%
* error: 14.7%
* macro_f1: 84.5%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.5%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      85.3%, epoch: 2 *******
******* Domain a best val test acc: 83.1%, epoch: 2 *******
******* Domain a best test acc:     83.1%, epoch: 2 *******
epoch [3/50] batch [20/288] time 0.084 (0.107) data 0.000 (0.016) loss 1.2799 (1.6341) teacher_loss 0.7864 (0.9967) loss_zs_kd 0.0368 (0.0595) loss_oracle 0.5546 (0.5375) acc 78.1250 (73.9062) lr 1.9980e-03 eta 0:24:32
epoch [3/50] batch [40/288] time 0.087 (0.097) data 0.000 (0.008) loss 1.7765 (1.6887) teacher_loss 1.0694 (1.0412) loss_zs_kd 0.0505 (0.0591) loss_oracle 0.5441 (0.5450) acc 71.8750 (72.4219) lr 1.9980e-03 eta 0:22:23
epoch [3/50] batch [60/288] time 0.086 (0.094) data 0.000 (0.006) loss 1.0612 (1.6926) teacher_loss 0.4762 (1.0470) loss_zs_kd 0.0602 (0.0593) loss_oracle 0.5072 (0.5365) acc 87.5000 (71.9792) lr 1.9980e-03 eta 0:21:32
epoch [3/50] batch [80/288] time 0.087 (0.092) data 0.000 (0.004) loss 1.6084 (1.6881) teacher_loss 1.0078 (1.0565) loss_zs_kd 0.0413 (0.0574) loss_oracle 0.4860 (0.5265) acc 78.1250 (72.2656) lr 1.9980e-03 eta 0:20:59
epoch [3/50] batch [100/288] time 0.084 (0.089) data 0.000 (0.003) loss 1.3760 (1.6677) teacher_loss 0.8604 (1.0465) loss_zs_kd 0.0437 (0.0563) loss_oracle 0.4518 (0.5211) acc 78.1250 (72.3750) lr 1.9980e-03 eta 0:20:26
epoch [3/50] batch [120/288] time 0.082 (0.088) data 0.000 (0.003) loss 1.5107 (1.6638) teacher_loss 0.9032 (1.0486) loss_zs_kd 0.0744 (0.0578) loss_oracle 0.4953 (0.5151) acc 84.3750 (72.3958) lr 1.9980e-03 eta 0:20:10
epoch [3/50] batch [140/288] time 0.089 (0.088) data 0.000 (0.003) loss 1.6560 (1.6555) teacher_loss 1.0246 (1.0464) loss_zs_kd 0.0436 (0.0577) loss_oracle 0.5463 (0.5110) acc 75.0000 (72.3438) lr 1.9980e-03 eta 0:20:01
epoch [3/50] batch [160/288] time 0.081 (0.088) data 0.000 (0.002) loss 2.2950 (1.6806) teacher_loss 1.6057 (1.0674) loss_zs_kd 0.0922 (0.0581) loss_oracle 0.5254 (0.5124) acc 59.3750 (71.5625) lr 1.9980e-03 eta 0:19:55
epoch [3/50] batch [180/288] time 0.084 (0.087) data 0.000 (0.002) loss 2.0620 (1.7013) teacher_loss 1.3756 (1.0810) loss_zs_kd 0.0937 (0.0583) loss_oracle 0.5146 (0.5135) acc 62.5000 (71.1632) lr 1.9980e-03 eta 0:19:45
epoch [3/50] batch [200/288] time 0.083 (0.086) data 0.000 (0.002) loss 1.5685 (1.7011) teacher_loss 1.0413 (1.0773) loss_zs_kd 0.0430 (0.0586) loss_oracle 0.4835 (0.5144) acc 71.8750 (71.3750) lr 1.9980e-03 eta 0:19:38
epoch [3/50] batch [220/288] time 0.081 (0.086) data 0.000 (0.002) loss 2.2521 (1.7058) teacher_loss 1.5195 (1.0810) loss_zs_kd 0.1061 (0.0587) loss_oracle 0.4957 (0.5138) acc 59.3750 (71.1932) lr 1.9980e-03 eta 0:19:31
epoch [3/50] batch [240/288] time 0.087 (0.086) data 0.002 (0.002) loss 1.7764 (1.6951) teacher_loss 1.1148 (1.0717) loss_zs_kd 0.0906 (0.0596) loss_oracle 0.4842 (0.5120) acc 71.8750 (71.3281) lr 1.9980e-03 eta 0:19:25
epoch [3/50] batch [260/288] time 0.086 (0.086) data 0.001 (0.002) loss 1.7203 (1.7002) teacher_loss 1.0962 (1.0740) loss_zs_kd 0.0534 (0.0604) loss_oracle 0.5056 (0.5113) acc 71.8750 (71.3101) lr 1.9980e-03 eta 0:19:21
epoch [3/50] batch [280/288] time 0.081 (0.085) data 0.000 (0.001) loss 1.4601 (1.6964) teacher_loss 0.8683 (1.0692) loss_zs_kd 0.0445 (0.0607) loss_oracle 0.4671 (0.5099) acc 71.8750 (71.4844) lr 1.9980e-03 eta 0:19:16
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,362
* accuracy: 85.4%
* error: 14.6%
* macro_f1: 84.6%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,026
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 79.7%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      85.4%, epoch: 3 *******
******* Domain a best val test acc: 83.5%, epoch: 3 *******
******* Domain a best test acc:     83.5%, epoch: 3 *******
epoch [4/50] batch [20/288] time 0.074 (0.110) data 0.000 (0.013) loss 1.4514 (1.6370) teacher_loss 0.8014 (1.0017) loss_zs_kd 0.0341 (0.0714) loss_oracle 0.4532 (0.4755) acc 81.2500 (74.0625) lr 1.9921e-03 eta 0:24:40
epoch [4/50] batch [40/288] time 0.072 (0.093) data 0.000 (0.007) loss 1.5169 (1.6199) teacher_loss 0.9527 (1.0111) loss_zs_kd 0.0635 (0.0637) loss_oracle 0.4609 (0.4718) acc 78.1250 (73.2812) lr 1.9921e-03 eta 0:20:55
epoch [4/50] batch [60/288] time 0.074 (0.088) data 0.000 (0.005) loss 1.3365 (1.5728) teacher_loss 0.7095 (0.9762) loss_zs_kd 0.0725 (0.0644) loss_oracle 0.4731 (0.4699) acc 81.2500 (73.8542) lr 1.9921e-03 eta 0:19:39
epoch [4/50] batch [80/288] time 0.063 (0.084) data 0.000 (0.004) loss 1.4587 (1.5974) teacher_loss 0.9089 (1.0049) loss_zs_kd 0.0498 (0.0637) loss_oracle 0.4011 (0.4662) acc 71.8750 (72.9688) lr 1.9921e-03 eta 0:18:49
epoch [4/50] batch [100/288] time 0.064 (0.080) data 0.000 (0.003) loss 1.9474 (1.6044) teacher_loss 1.3465 (1.0162) loss_zs_kd 0.0732 (0.0623) loss_oracle 0.4520 (0.4607) acc 65.6250 (72.8125) lr 1.9921e-03 eta 0:17:55
epoch [4/50] batch [120/288] time 0.062 (0.078) data 0.000 (0.002) loss 1.5888 (1.6121) teacher_loss 1.0106 (1.0274) loss_zs_kd 0.1325 (0.0644) loss_oracle 0.4629 (0.4586) acc 75.0000 (72.5521) lr 1.9921e-03 eta 0:17:22
epoch [4/50] batch [140/288] time 0.081 (0.076) data 0.000 (0.002) loss 1.9442 (1.6256) teacher_loss 1.2350 (1.0337) loss_zs_kd 0.0654 (0.0660) loss_oracle 0.4565 (0.4591) acc 71.8750 (72.4107) lr 1.9921e-03 eta 0:16:57
epoch [4/50] batch [160/288] time 0.065 (0.075) data 0.000 (0.002) loss 2.0614 (1.6407) teacher_loss 1.3851 (1.0485) loss_zs_kd 0.0692 (0.0668) loss_oracle 0.4595 (0.4586) acc 62.5000 (72.1875) lr 1.9921e-03 eta 0:16:46
epoch [4/50] batch [180/288] time 0.068 (0.075) data 0.000 (0.002) loss 1.5474 (1.6338) teacher_loss 1.0179 (1.0404) loss_zs_kd 0.0552 (0.0670) loss_oracle 0.4343 (0.4557) acc 71.8750 (72.4653) lr 1.9921e-03 eta 0:16:37
epoch [4/50] batch [200/288] time 0.063 (0.074) data 0.000 (0.002) loss 1.2751 (1.6270) teacher_loss 0.7117 (1.0354) loss_zs_kd 0.0568 (0.0664) loss_oracle 0.3699 (0.4520) acc 81.2500 (72.4844) lr 1.9921e-03 eta 0:16:24
epoch [4/50] batch [220/288] time 0.079 (0.073) data 0.000 (0.001) loss 1.3953 (1.6270) teacher_loss 0.7558 (1.0373) loss_zs_kd 0.0728 (0.0656) loss_oracle 0.4471 (0.4497) acc 81.2500 (72.4432) lr 1.9921e-03 eta 0:16:13
epoch [4/50] batch [240/288] time 0.062 (0.072) data 0.000 (0.001) loss 1.7700 (1.6353) teacher_loss 1.2977 (1.0455) loss_zs_kd 0.0568 (0.0663) loss_oracle 0.4236 (0.4470) acc 65.6250 (72.2526) lr 1.9921e-03 eta 0:16:03
epoch [4/50] batch [260/288] time 0.074 (0.072) data 0.000 (0.001) loss 1.6687 (1.6391) teacher_loss 1.0568 (1.0496) loss_zs_kd 0.0431 (0.0652) loss_oracle 0.4276 (0.4458) acc 68.7500 (72.1274) lr 1.9921e-03 eta 0:15:56
epoch [4/50] batch [280/288] time 0.081 (0.072) data 0.000 (0.001) loss 1.5648 (1.6428) teacher_loss 1.0024 (1.0538) loss_zs_kd 0.0384 (0.0648) loss_oracle 0.4242 (0.4441) acc 75.0000 (72.1652) lr 1.9921e-03 eta 0:15:55
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,361
* accuracy: 85.3%
* error: 14.7%
* macro_f1: 84.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.7%
******* Domain a best val acc:      85.4%, epoch: 3 *******
******* Domain a best val test acc: 83.5%, epoch: 3 *******
******* Domain a best test acc:     83.5%, epoch: 3 *******
epoch [5/50] batch [20/288] time 0.065 (0.096) data 0.000 (0.016) loss 1.3553 (1.5457) teacher_loss 0.7817 (0.9783) loss_zs_kd 0.0848 (0.0688) loss_oracle 0.4313 (0.4245) acc 78.1250 (72.9688) lr 1.9823e-03 eta 0:21:16
epoch [5/50] batch [40/288] time 0.078 (0.085) data 0.000 (0.008) loss 1.7391 (1.6137) teacher_loss 1.0911 (1.0454) loss_zs_kd 0.0599 (0.0664) loss_oracle 0.5316 (0.4215) acc 71.8750 (72.6562) lr 1.9823e-03 eta 0:18:45
epoch [5/50] batch [60/288] time 0.065 (0.081) data 0.000 (0.005) loss 1.5870 (1.6665) teacher_loss 0.9466 (1.0842) loss_zs_kd 0.0876 (0.0669) loss_oracle 0.4490 (0.4394) acc 78.1250 (71.1458) lr 1.9823e-03 eta 0:17:53
epoch [5/50] batch [80/288] time 0.080 (0.079) data 0.000 (0.004) loss 1.6521 (1.6589) teacher_loss 1.1780 (1.0795) loss_zs_kd 0.0637 (0.0654) loss_oracle 0.3449 (0.4371) acc 75.0000 (71.2500) lr 1.9823e-03 eta 0:17:18
epoch [5/50] batch [100/288] time 0.119 (0.079) data 0.001 (0.003) loss 1.5715 (1.6627) teacher_loss 1.0666 (1.0845) loss_zs_kd 0.0679 (0.0648) loss_oracle 0.4356 (0.4372) acc 78.1250 (71.0625) lr 1.9823e-03 eta 0:17:22
epoch [5/50] batch [120/288] time 0.067 (0.081) data 0.000 (0.003) loss 1.5513 (1.6652) teacher_loss 0.9753 (1.0927) loss_zs_kd 0.0784 (0.0637) loss_oracle 0.3664 (0.4336) acc 68.7500 (70.7292) lr 1.9823e-03 eta 0:17:38
epoch [5/50] batch [140/288] time 0.065 (0.079) data 0.000 (0.003) loss 1.6037 (1.6598) teacher_loss 1.0090 (1.0871) loss_zs_kd 0.0659 (0.0650) loss_oracle 0.3784 (0.4297) acc 75.0000 (70.9821) lr 1.9823e-03 eta 0:17:17
epoch [5/50] batch [160/288] time 0.069 (0.078) data 0.000 (0.002) loss 1.4405 (1.6481) teacher_loss 0.8750 (1.0801) loss_zs_kd 0.0462 (0.0642) loss_oracle 0.4144 (0.4262) acc 75.0000 (71.2305) lr 1.9823e-03 eta 0:17:05
epoch [5/50] batch [180/288] time 0.067 (0.077) data 0.001 (0.002) loss 1.8790 (1.6467) teacher_loss 1.3184 (1.0773) loss_zs_kd 0.0557 (0.0639) loss_oracle 0.4226 (0.4262) acc 56.2500 (71.3368) lr 1.9823e-03 eta 0:16:52
epoch [5/50] batch [200/288] time 0.074 (0.077) data 0.001 (0.002) loss 1.3320 (1.6424) teacher_loss 0.6464 (1.0723) loss_zs_kd 0.0754 (0.0640) loss_oracle 0.4029 (0.4236) acc 87.5000 (71.5156) lr 1.9823e-03 eta 0:16:39
epoch [5/50] batch [220/288] time 0.064 (0.076) data 0.000 (0.002) loss 1.9230 (1.6405) teacher_loss 1.3051 (1.0692) loss_zs_kd 0.0586 (0.0643) loss_oracle 0.4038 (0.4228) acc 65.6250 (71.5057) lr 1.9823e-03 eta 0:16:33
epoch [5/50] batch [240/288] time 0.069 (0.076) data 0.000 (0.002) loss 1.6988 (1.6420) teacher_loss 1.1502 (1.0698) loss_zs_kd 0.0713 (0.0642) loss_oracle 0.3475 (0.4203) acc 68.7500 (71.5234) lr 1.9823e-03 eta 0:16:26
epoch [5/50] batch [260/288] time 0.065 (0.076) data 0.000 (0.001) loss 1.6137 (1.6406) teacher_loss 1.0398 (1.0707) loss_zs_kd 0.0519 (0.0640) loss_oracle 0.3808 (0.4193) acc 68.7500 (71.5385) lr 1.9823e-03 eta 0:16:27
epoch [5/50] batch [280/288] time 0.081 (0.076) data 0.000 (0.001) loss 1.5395 (1.6414) teacher_loss 0.9858 (1.0692) loss_zs_kd 0.0546 (0.0647) loss_oracle 0.4018 (0.4189) acc 75.0000 (71.6295) lr 1.9823e-03 eta 0:16:22
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,386
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.3%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,013
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.4%
******* Domain a best val acc:      86.0%, epoch: 5 *******
******* Domain a best val test acc: 82.9%, epoch: 5 *******
******* Domain a best test acc:     83.5%, epoch: 3 *******
epoch [6/50] batch [20/288] time 0.082 (0.098) data 0.000 (0.020) loss 1.1931 (1.5368) teacher_loss 0.7193 (0.9727) loss_zs_kd 0.0392 (0.0650) loss_oracle 0.4291 (0.4106) acc 78.1250 (75.6250) lr 1.9686e-03 eta 0:21:11
epoch [6/50] batch [40/288] time 0.070 (0.086) data 0.000 (0.010) loss 1.5861 (1.5673) teacher_loss 1.0688 (1.0053) loss_zs_kd 0.0654 (0.0640) loss_oracle 0.3320 (0.4000) acc 75.0000 (72.9688) lr 1.9686e-03 eta 0:18:35
epoch [6/50] batch [60/288] time 0.085 (0.083) data 0.000 (0.007) loss 1.5449 (1.6280) teacher_loss 1.1006 (1.0609) loss_zs_kd 0.0539 (0.0600) loss_oracle 0.3403 (0.3981) acc 65.6250 (71.4583) lr 1.9686e-03 eta 0:17:48
epoch [6/50] batch [80/288] time 0.071 (0.080) data 0.000 (0.005) loss 1.5005 (1.6274) teacher_loss 0.7840 (1.0597) loss_zs_kd 0.0426 (0.0598) loss_oracle 0.3945 (0.4013) acc 78.1250 (70.9375) lr 1.9686e-03 eta 0:17:16
epoch [6/50] batch [100/288] time 0.081 (0.079) data 0.000 (0.004) loss 2.0355 (1.6287) teacher_loss 1.4278 (1.0636) loss_zs_kd 0.0549 (0.0601) loss_oracle 0.4076 (0.4002) acc 65.6250 (70.8750) lr 1.9686e-03 eta 0:17:01
epoch [6/50] batch [120/288] time 0.073 (0.078) data 0.000 (0.004) loss 1.2843 (1.6215) teacher_loss 0.8024 (1.0564) loss_zs_kd 0.0451 (0.0616) loss_oracle 0.4260 (0.4009) acc 81.2500 (71.2240) lr 1.9686e-03 eta 0:16:43
epoch [6/50] batch [140/288] time 0.081 (0.078) data 0.000 (0.003) loss 1.2583 (1.6289) teacher_loss 0.8067 (1.0624) loss_zs_kd 0.0417 (0.0622) loss_oracle 0.3570 (0.3992) acc 81.2500 (71.2500) lr 1.9686e-03 eta 0:16:35
epoch [6/50] batch [160/288] time 0.066 (0.077) data 0.000 (0.003) loss 1.6289 (1.6287) teacher_loss 1.0636 (1.0610) loss_zs_kd 0.0588 (0.0629) loss_oracle 0.3885 (0.3999) acc 68.7500 (71.4453) lr 1.9686e-03 eta 0:16:19
epoch [6/50] batch [180/288] time 0.082 (0.078) data 0.000 (0.002) loss 1.7584 (1.6234) teacher_loss 1.1223 (1.0533) loss_zs_kd 0.0753 (0.0625) loss_oracle 0.4194 (0.3999) acc 71.8750 (71.7361) lr 1.9686e-03 eta 0:16:43
epoch [6/50] batch [200/288] time 0.082 (0.078) data 0.000 (0.002) loss 1.6167 (1.6320) teacher_loss 0.9237 (1.0598) loss_zs_kd 0.1753 (0.0647) loss_oracle 0.3940 (0.3996) acc 75.0000 (71.6719) lr 1.9686e-03 eta 0:16:33
epoch [6/50] batch [220/288] time 0.082 (0.078) data 0.000 (0.002) loss 1.9868 (1.6265) teacher_loss 1.3632 (1.0582) loss_zs_kd 0.0925 (0.0651) loss_oracle 0.3787 (0.3994) acc 65.6250 (71.8182) lr 1.9686e-03 eta 0:16:38
epoch [6/50] batch [240/288] time 0.083 (0.079) data 0.001 (0.002) loss 1.8193 (1.6275) teacher_loss 1.2610 (1.0598) loss_zs_kd 0.0745 (0.0651) loss_oracle 0.4315 (0.3992) acc 62.5000 (71.7578) lr 1.9686e-03 eta 0:16:41
epoch [6/50] batch [260/288] time 0.087 (0.079) data 0.001 (0.002) loss 1.9394 (1.6203) teacher_loss 1.2848 (1.0532) loss_zs_kd 0.0934 (0.0648) loss_oracle 0.4066 (0.3986) acc 65.6250 (72.0072) lr 1.9686e-03 eta 0:16:43
epoch [6/50] batch [280/288] time 0.081 (0.079) data 0.000 (0.002) loss 1.3708 (1.6160) teacher_loss 0.8173 (1.0454) loss_zs_kd 0.0732 (0.0648) loss_oracle 0.3763 (0.3983) acc 81.2500 (72.2991) lr 1.9686e-03 eta 0:16:44
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,387
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.2%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.7%
******* Domain a best val acc:      86.0%, epoch: 6 *******
******* Domain a best val test acc: 83.1%, epoch: 6 *******
******* Domain a best test acc:     83.5%, epoch: 3 *******
epoch [7/50] batch [20/288] time 0.089 (0.090) data 0.000 (0.013) loss 2.1603 (1.6912) teacher_loss 1.5345 (1.1468) loss_zs_kd 0.0866 (0.0679) loss_oracle 0.3902 (0.3905) acc 62.5000 (69.5312) lr 1.9511e-03 eta 0:18:53
epoch [7/50] batch [40/288] time 0.087 (0.087) data 0.000 (0.007) loss 2.2529 (1.6884) teacher_loss 1.6948 (1.1420) loss_zs_kd 0.0658 (0.0696) loss_oracle 0.3413 (0.3905) acc 59.3750 (69.7656) lr 1.9511e-03 eta 0:18:14
epoch [7/50] batch [60/288] time 0.088 (0.087) data 0.000 (0.005) loss 1.7543 (1.6489) teacher_loss 1.2329 (1.0933) loss_zs_kd 0.0657 (0.0719) loss_oracle 0.4020 (0.3903) acc 71.8750 (70.9375) lr 1.9511e-03 eta 0:18:13
epoch [7/50] batch [80/288] time 0.086 (0.087) data 0.000 (0.003) loss 1.4008 (1.6446) teacher_loss 0.8721 (1.0803) loss_zs_kd 0.0588 (0.0721) loss_oracle 0.4100 (0.3912) acc 71.8750 (71.0938) lr 1.9511e-03 eta 0:18:10
epoch [7/50] batch [100/288] time 0.086 (0.086) data 0.000 (0.003) loss 1.6066 (1.6353) teacher_loss 1.1921 (1.0759) loss_zs_kd 0.0458 (0.0700) loss_oracle 0.3431 (0.3883) acc 71.8750 (71.2500) lr 1.9511e-03 eta 0:18:07
epoch [7/50] batch [120/288] time 0.086 (0.087) data 0.000 (0.002) loss 2.5556 (1.6346) teacher_loss 2.0372 (1.0796) loss_zs_kd 0.0851 (0.0687) loss_oracle 0.3760 (0.3873) acc 46.8750 (71.0677) lr 1.9511e-03 eta 0:18:06
epoch [7/50] batch [140/288] time 0.067 (0.086) data 0.000 (0.002) loss 1.4054 (1.6268) teacher_loss 0.7371 (1.0662) loss_zs_kd 0.0413 (0.0685) loss_oracle 0.3774 (0.3881) acc 87.5000 (71.5402) lr 1.9511e-03 eta 0:18:01
epoch [7/50] batch [160/288] time 0.087 (0.086) data 0.000 (0.002) loss 1.3551 (1.6164) teacher_loss 0.9408 (1.0521) loss_zs_kd 0.0501 (0.0681) loss_oracle 0.3907 (0.3891) acc 75.0000 (71.8164) lr 1.9511e-03 eta 0:17:55
epoch [7/50] batch [180/288] time 0.087 (0.087) data 0.000 (0.002) loss 1.8432 (1.6144) teacher_loss 1.3274 (1.0517) loss_zs_kd 0.0858 (0.0669) loss_oracle 0.3899 (0.3886) acc 65.6250 (71.9792) lr 1.9511e-03 eta 0:18:01
epoch [7/50] batch [200/288] time 0.137 (0.087) data 0.001 (0.002) loss 1.7133 (1.6097) teacher_loss 1.0479 (1.0489) loss_zs_kd 0.0391 (0.0661) loss_oracle 0.3757 (0.3881) acc 71.8750 (72.0156) lr 1.9511e-03 eta 0:18:08
epoch [7/50] batch [220/288] time 0.087 (0.088) data 0.000 (0.001) loss 1.6232 (1.6163) teacher_loss 1.0586 (1.0545) loss_zs_kd 0.0492 (0.0662) loss_oracle 0.3816 (0.3870) acc 78.1250 (72.0312) lr 1.9511e-03 eta 0:18:17
epoch [7/50] batch [240/288] time 0.086 (0.088) data 0.000 (0.001) loss 1.6812 (1.6193) teacher_loss 1.0817 (1.0554) loss_zs_kd 0.0493 (0.0659) loss_oracle 0.3859 (0.3860) acc 71.8750 (72.0703) lr 1.9511e-03 eta 0:18:12
epoch [7/50] batch [260/288] time 0.087 (0.087) data 0.000 (0.001) loss 1.4185 (1.6185) teacher_loss 0.7471 (1.0564) loss_zs_kd 0.0809 (0.0658) loss_oracle 0.3394 (0.3838) acc 87.5000 (72.0433) lr 1.9511e-03 eta 0:18:05
epoch [7/50] batch [280/288] time 0.075 (0.087) data 0.000 (0.001) loss 1.2007 (1.6242) teacher_loss 0.6473 (1.0598) loss_zs_kd 0.0593 (0.0655) loss_oracle 0.3812 (0.3829) acc 84.3750 (71.9196) lr 1.9511e-03 eta 0:17:58
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,382
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.1%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,029
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.1%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.0%, epoch: 6 *******
******* Domain a best val test acc: 83.1%, epoch: 6 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [8/50] batch [20/288] time 0.147 (0.172) data 0.001 (0.018) loss 1.4437 (1.4883) teacher_loss 0.9387 (0.9397) loss_zs_kd 0.0542 (0.0608) loss_oracle 0.3546 (0.3730) acc 78.1250 (76.0938) lr 1.9298e-03 eta 0:35:26
epoch [8/50] batch [40/288] time 0.149 (0.161) data 0.000 (0.009) loss 1.3242 (1.5473) teacher_loss 0.6676 (0.9839) loss_zs_kd 0.0523 (0.0617) loss_oracle 0.3699 (0.3792) acc 87.5000 (74.0625) lr 1.9298e-03 eta 0:33:12
epoch [8/50] batch [60/288] time 0.145 (0.157) data 0.000 (0.006) loss 1.6529 (1.5422) teacher_loss 0.9812 (0.9861) loss_zs_kd 0.0913 (0.0651) loss_oracle 0.3509 (0.3777) acc 71.8750 (74.1667) lr 1.9298e-03 eta 0:32:15
epoch [8/50] batch [80/288] time 0.142 (0.155) data 0.000 (0.005) loss 1.4938 (1.5864) teacher_loss 0.9236 (1.0311) loss_zs_kd 0.0441 (0.0666) loss_oracle 0.3297 (0.3754) acc 68.7500 (72.5391) lr 1.9298e-03 eta 0:31:42
epoch [8/50] batch [100/288] time 0.145 (0.153) data 0.000 (0.004) loss 1.5445 (1.5921) teacher_loss 0.9753 (1.0334) loss_zs_kd 0.0610 (0.0670) loss_oracle 0.3463 (0.3728) acc 84.3750 (72.8438) lr 1.9298e-03 eta 0:31:20
epoch [8/50] batch [120/288] time 0.167 (0.152) data 0.000 (0.003) loss 1.0896 (1.5769) teacher_loss 0.6484 (1.0180) loss_zs_kd 0.0555 (0.0661) loss_oracle 0.3579 (0.3729) acc 90.6250 (73.2031) lr 1.9298e-03 eta 0:31:06
epoch [8/50] batch [140/288] time 0.151 (0.152) data 0.000 (0.003) loss 1.4992 (1.5829) teacher_loss 1.0699 (1.0204) loss_zs_kd 0.0512 (0.0667) loss_oracle 0.3731 (0.3719) acc 68.7500 (72.8571) lr 1.9298e-03 eta 0:30:55
epoch [8/50] batch [160/288] time 0.149 (0.151) data 0.000 (0.002) loss 1.5946 (1.5985) teacher_loss 1.1843 (1.0381) loss_zs_kd 0.0617 (0.0671) loss_oracle 0.3476 (0.3711) acc 68.7500 (72.2266) lr 1.9298e-03 eta 0:30:45
epoch [8/50] batch [180/288] time 0.147 (0.150) data 0.000 (0.002) loss 2.3646 (1.5995) teacher_loss 1.8661 (1.0388) loss_zs_kd 0.0946 (0.0671) loss_oracle 0.3606 (0.3718) acc 53.1250 (72.1181) lr 1.9298e-03 eta 0:30:36
epoch [8/50] batch [200/288] time 0.146 (0.150) data 0.000 (0.002) loss 1.3663 (1.5884) teacher_loss 0.8421 (1.0295) loss_zs_kd 0.0836 (0.0673) loss_oracle 0.3722 (0.3721) acc 81.2500 (72.2812) lr 1.9298e-03 eta 0:30:28
epoch [8/50] batch [220/288] time 0.148 (0.150) data 0.000 (0.002) loss 1.9211 (1.5975) teacher_loss 1.3020 (1.0378) loss_zs_kd 0.1099 (0.0676) loss_oracle 0.3781 (0.3729) acc 68.7500 (72.1165) lr 1.9298e-03 eta 0:30:22
epoch [8/50] batch [240/288] time 0.149 (0.150) data 0.000 (0.002) loss 1.5029 (1.5869) teacher_loss 0.9919 (1.0288) loss_zs_kd 0.0617 (0.0670) loss_oracle 0.3728 (0.3728) acc 75.0000 (72.4089) lr 1.9298e-03 eta 0:30:15
epoch [8/50] batch [260/288] time 0.084 (0.147) data 0.000 (0.002) loss 1.7782 (1.5947) teacher_loss 1.1321 (1.0330) loss_zs_kd 0.0546 (0.0675) loss_oracle 0.3838 (0.3728) acc 68.7500 (72.3438) lr 1.9298e-03 eta 0:29:47
epoch [8/50] batch [280/288] time 0.072 (0.152) data 0.000 (0.001) loss 1.6535 (1.5902) teacher_loss 1.0730 (1.0305) loss_zs_kd 0.0690 (0.0678) loss_oracle 0.3629 (0.3724) acc 71.8750 (72.5000) lr 1.9298e-03 eta 0:30:38
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,383
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.1%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,029
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.1%
******* Domain a best val acc:      86.0%, epoch: 6 *******
******* Domain a best val test acc: 83.1%, epoch: 6 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [9/50] batch [20/288] time 0.135 (0.164) data 0.000 (0.016) loss 1.8825 (1.6430) teacher_loss 1.3303 (1.0760) loss_zs_kd 0.0745 (0.0778) loss_oracle 0.4058 (0.3640) acc 59.3750 (70.6250) lr 1.9048e-03 eta 0:32:59
epoch [9/50] batch [40/288] time 0.148 (0.155) data 0.000 (0.008) loss 1.0920 (1.6360) teacher_loss 0.6929 (1.0694) loss_zs_kd 0.0737 (0.0834) loss_oracle 0.3359 (0.3675) acc 81.2500 (70.8594) lr 1.9048e-03 eta 0:31:04
epoch [9/50] batch [60/288] time 0.144 (0.152) data 0.000 (0.005) loss 1.5738 (1.6033) teacher_loss 0.9167 (1.0515) loss_zs_kd 0.0699 (0.0775) loss_oracle 0.3788 (0.3664) acc 78.1250 (71.7708) lr 1.9048e-03 eta 0:30:24
epoch [9/50] batch [80/288] time 0.148 (0.150) data 0.000 (0.004) loss 2.0126 (1.6084) teacher_loss 1.4129 (1.0552) loss_zs_kd 0.0543 (0.0749) loss_oracle 0.3696 (0.3693) acc 59.3750 (71.8750) lr 1.9048e-03 eta 0:30:04
epoch [9/50] batch [100/288] time 0.147 (0.149) data 0.000 (0.003) loss 2.1353 (1.6202) teacher_loss 1.5820 (1.0635) loss_zs_kd 0.0710 (0.0740) loss_oracle 0.3609 (0.3700) acc 62.5000 (71.8125) lr 1.9048e-03 eta 0:29:52
epoch [9/50] batch [120/288] time 0.148 (0.149) data 0.000 (0.003) loss 1.7498 (1.6167) teacher_loss 1.2252 (1.0610) loss_zs_kd 0.0940 (0.0718) loss_oracle 0.3304 (0.3702) acc 65.6250 (71.8229) lr 1.9048e-03 eta 0:29:45
epoch [9/50] batch [140/288] time 0.146 (0.149) data 0.000 (0.002) loss 1.8690 (1.6096) teacher_loss 1.1575 (1.0482) loss_zs_kd 0.0732 (0.0712) loss_oracle 0.4004 (0.3709) acc 75.0000 (72.2991) lr 1.9048e-03 eta 0:29:36
epoch [9/50] batch [160/288] time 0.147 (0.148) data 0.000 (0.002) loss 1.1292 (1.6006) teacher_loss 0.6273 (1.0405) loss_zs_kd 0.0456 (0.0711) loss_oracle 0.3903 (0.3718) acc 71.8750 (72.5000) lr 1.9048e-03 eta 0:29:28
epoch [9/50] batch [180/288] time 0.143 (0.148) data 0.000 (0.002) loss 1.9206 (1.5948) teacher_loss 1.3362 (1.0342) loss_zs_kd 0.0587 (0.0705) loss_oracle 0.3751 (0.3714) acc 68.7500 (72.6562) lr 1.9048e-03 eta 0:29:20
epoch [9/50] batch [200/288] time 0.141 (0.147) data 0.000 (0.002) loss 2.1552 (1.6125) teacher_loss 1.5559 (1.0490) loss_zs_kd 0.0580 (0.0711) loss_oracle 0.3550 (0.3694) acc 68.7500 (72.5469) lr 1.9048e-03 eta 0:29:13
epoch [9/50] batch [220/288] time 0.144 (0.147) data 0.000 (0.002) loss 1.4921 (1.6051) teacher_loss 0.7992 (1.0422) loss_zs_kd 0.0895 (0.0709) loss_oracle 0.3881 (0.3698) acc 71.8750 (72.7273) lr 1.9048e-03 eta 0:29:06
epoch [9/50] batch [240/288] time 0.144 (0.147) data 0.000 (0.002) loss 1.2294 (1.6004) teacher_loss 0.8054 (1.0404) loss_zs_kd 0.0723 (0.0703) loss_oracle 0.3396 (0.3689) acc 71.8750 (72.6432) lr 1.9048e-03 eta 0:29:01
epoch [9/50] batch [260/288] time 0.146 (0.147) data 0.000 (0.001) loss 1.6112 (1.6092) teacher_loss 1.0957 (1.0484) loss_zs_kd 0.0589 (0.0705) loss_oracle 0.3493 (0.3682) acc 59.3750 (72.4159) lr 1.9048e-03 eta 0:28:57
epoch [9/50] batch [280/288] time 0.397 (0.149) data 0.000 (0.001) loss 1.7202 (1.6031) teacher_loss 1.1394 (1.0444) loss_zs_kd 0.0734 (0.0699) loss_oracle 0.3564 (0.3673) acc 68.7500 (72.5223) lr 1.9048e-03 eta 0:29:22
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,386
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.7%
******* Domain a best val acc:      86.0%, epoch: 6 *******
******* Domain a best val test acc: 83.1%, epoch: 6 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [10/50] batch [20/288] time 0.145 (0.160) data 0.000 (0.012) loss 1.5480 (1.5079) teacher_loss 0.9291 (0.9641) loss_zs_kd 0.0499 (0.0580) loss_oracle 0.3370 (0.3602) acc 71.8750 (75.1562) lr 1.8763e-03 eta 0:31:22
epoch [10/50] batch [40/288] time 0.147 (0.153) data 0.000 (0.006) loss 1.1493 (1.5437) teacher_loss 0.7319 (0.9955) loss_zs_kd 0.0475 (0.0637) loss_oracle 0.3598 (0.3611) acc 81.2500 (73.8281) lr 1.8763e-03 eta 0:29:59
epoch [10/50] batch [60/288] time 0.146 (0.151) data 0.000 (0.004) loss 1.0202 (1.5413) teacher_loss 0.5099 (0.9816) loss_zs_kd 0.1094 (0.0685) loss_oracle 0.3507 (0.3618) acc 87.5000 (73.9062) lr 1.8763e-03 eta 0:29:30
epoch [10/50] batch [80/288] time 0.148 (0.150) data 0.000 (0.003) loss 1.5315 (1.5286) teacher_loss 0.9631 (0.9724) loss_zs_kd 0.0690 (0.0692) loss_oracle 0.3591 (0.3614) acc 68.7500 (74.0625) lr 1.8763e-03 eta 0:29:15
epoch [10/50] batch [100/288] time 0.147 (0.149) data 0.000 (0.003) loss 1.2619 (1.5185) teacher_loss 0.7418 (0.9568) loss_zs_kd 0.1023 (0.0706) loss_oracle 0.3602 (0.3627) acc 68.7500 (74.1562) lr 1.8763e-03 eta 0:29:05
epoch [10/50] batch [120/288] time 0.148 (0.149) data 0.000 (0.002) loss 1.9489 (1.5334) teacher_loss 1.5185 (0.9639) loss_zs_kd 0.0647 (0.0722) loss_oracle 0.3673 (0.3632) acc 68.7500 (74.0365) lr 1.8763e-03 eta 0:28:58
epoch [10/50] batch [140/288] time 0.144 (0.149) data 0.000 (0.002) loss 2.1339 (1.5496) teacher_loss 1.5266 (0.9758) loss_zs_kd 0.0874 (0.0715) loss_oracle 0.3492 (0.3625) acc 62.5000 (73.8170) lr 1.8763e-03 eta 0:29:02
epoch [10/50] batch [160/288] time 0.150 (0.149) data 0.000 (0.002) loss 1.2707 (1.5466) teacher_loss 0.7133 (0.9762) loss_zs_kd 0.0974 (0.0710) loss_oracle 0.3456 (0.3635) acc 81.2500 (74.0039) lr 1.8763e-03 eta 0:28:55
epoch [10/50] batch [180/288] time 0.146 (0.149) data 0.000 (0.001) loss 1.5196 (1.5517) teacher_loss 1.0230 (0.9797) loss_zs_kd 0.0639 (0.0706) loss_oracle 0.3319 (0.3648) acc 78.1250 (74.0799) lr 1.8763e-03 eta 0:28:48
epoch [10/50] batch [200/288] time 0.147 (0.148) data 0.000 (0.001) loss 1.6526 (1.5604) teacher_loss 1.1734 (0.9848) loss_zs_kd 0.0665 (0.0702) loss_oracle 0.3606 (0.3666) acc 71.8750 (73.9531) lr 1.8763e-03 eta 0:28:41
epoch [10/50] batch [220/288] time 0.145 (0.148) data 0.001 (0.001) loss 1.8247 (1.5805) teacher_loss 1.1433 (1.0025) loss_zs_kd 0.0757 (0.0708) loss_oracle 0.3619 (0.3673) acc 65.6250 (73.4659) lr 1.8763e-03 eta 0:28:36
epoch [10/50] batch [240/288] time 0.148 (0.148) data 0.000 (0.001) loss 1.4734 (1.5800) teacher_loss 0.7574 (1.0012) loss_zs_kd 0.1103 (0.0706) loss_oracle 0.3298 (0.3664) acc 78.1250 (73.4896) lr 1.8763e-03 eta 0:28:30
epoch [10/50] batch [260/288] time 0.146 (0.148) data 0.000 (0.001) loss 1.6323 (1.5968) teacher_loss 1.0560 (1.0210) loss_zs_kd 0.0683 (0.0704) loss_oracle 0.3463 (0.3658) acc 78.1250 (72.9928) lr 1.8763e-03 eta 0:28:25
epoch [10/50] batch [280/288] time 0.344 (0.151) data 0.000 (0.001) loss 1.2485 (1.5930) teacher_loss 0.6764 (1.0181) loss_zs_kd 0.0760 (0.0706) loss_oracle 0.3798 (0.3653) acc 84.3750 (73.0915) lr 1.8763e-03 eta 0:28:56
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,391
* accuracy: 86.1%
* error: 13.9%
* macro_f1: 85.3%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.7%
******* Domain a best val acc:      86.1%, epoch: 10 *******
******* Domain a best val test acc: 83.3%, epoch: 10 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [11/50] batch [20/288] time 0.142 (0.169) data 0.000 (0.015) loss 1.7554 (1.6339) teacher_loss 1.1302 (1.0571) loss_zs_kd 0.0879 (0.0673) loss_oracle 0.3877 (0.3584) acc 68.7500 (74.2188) lr 1.8443e-03 eta 0:32:21
epoch [11/50] batch [40/288] time 0.147 (0.158) data 0.000 (0.008) loss 1.0928 (1.6381) teacher_loss 0.4417 (1.0528) loss_zs_kd 0.0457 (0.0661) loss_oracle 0.3595 (0.3595) acc 84.3750 (73.9062) lr 1.8443e-03 eta 0:30:17
epoch [11/50] batch [60/288] time 0.146 (0.154) data 0.000 (0.005) loss 1.2250 (1.6296) teacher_loss 0.6260 (1.0430) loss_zs_kd 0.0557 (0.0673) loss_oracle 0.3514 (0.3595) acc 87.5000 (73.8542) lr 1.8443e-03 eta 0:29:29
epoch [11/50] batch [80/288] time 0.149 (0.153) data 0.000 (0.004) loss 1.5949 (1.5934) teacher_loss 1.1182 (1.0142) loss_zs_kd 0.0599 (0.0690) loss_oracle 0.3613 (0.3592) acc 78.1250 (74.5312) lr 1.8443e-03 eta 0:29:04
epoch [11/50] batch [100/288] time 0.146 (0.151) data 0.000 (0.003) loss 1.5308 (1.6026) teacher_loss 0.9360 (1.0205) loss_zs_kd 0.0999 (0.0687) loss_oracle 0.3592 (0.3584) acc 71.8750 (74.3750) lr 1.8443e-03 eta 0:28:49
epoch [11/50] batch [120/288] time 0.146 (0.151) data 0.000 (0.003) loss 1.9577 (1.6301) teacher_loss 1.3113 (1.0463) loss_zs_kd 0.0915 (0.0699) loss_oracle 0.3568 (0.3569) acc 65.6250 (73.5938) lr 1.8443e-03 eta 0:28:36
epoch [11/50] batch [140/288] time 0.146 (0.150) data 0.000 (0.002) loss 1.9241 (1.6237) teacher_loss 1.2621 (1.0407) loss_zs_kd 0.1275 (0.0718) loss_oracle 0.3900 (0.3565) acc 59.3750 (73.3482) lr 1.8443e-03 eta 0:28:27
epoch [11/50] batch [160/288] time 0.145 (0.150) data 0.000 (0.002) loss 1.8216 (1.6390) teacher_loss 1.0798 (1.0534) loss_zs_kd 0.0409 (0.0721) loss_oracle 0.3796 (0.3565) acc 78.1250 (73.0273) lr 1.8443e-03 eta 0:28:19
epoch [11/50] batch [180/288] time 0.146 (0.149) data 0.000 (0.002) loss 1.1717 (1.6152) teacher_loss 0.6364 (1.0326) loss_zs_kd 0.0445 (0.0712) loss_oracle 0.3384 (0.3561) acc 84.3750 (73.5764) lr 1.8443e-03 eta 0:28:12
epoch [11/50] batch [200/288] time 0.147 (0.149) data 0.000 (0.002) loss 1.0449 (1.6192) teacher_loss 0.4082 (1.0352) loss_zs_kd 0.1128 (0.0712) loss_oracle 0.3613 (0.3556) acc 90.6250 (73.4219) lr 1.8443e-03 eta 0:28:06
epoch [11/50] batch [220/288] time 0.145 (0.149) data 0.000 (0.002) loss 1.4613 (1.6311) teacher_loss 0.8434 (1.0455) loss_zs_kd 0.0670 (0.0711) loss_oracle 0.3533 (0.3555) acc 75.0000 (73.1392) lr 1.8443e-03 eta 0:28:00
epoch [11/50] batch [240/288] time 0.139 (0.149) data 0.000 (0.001) loss 1.5719 (1.6237) teacher_loss 0.9894 (1.0402) loss_zs_kd 0.0449 (0.0705) loss_oracle 0.3451 (0.3553) acc 78.1250 (73.3203) lr 1.8443e-03 eta 0:27:56
epoch [11/50] batch [260/288] time 0.144 (0.148) data 0.000 (0.001) loss 1.3099 (1.6096) teacher_loss 0.8737 (1.0284) loss_zs_kd 0.0276 (0.0702) loss_oracle 0.3367 (0.3553) acc 71.8750 (73.5577) lr 1.8443e-03 eta 0:27:50
epoch [11/50] batch [280/288] time 0.067 (0.153) data 0.000 (0.001) loss 1.7078 (1.6107) teacher_loss 1.1903 (1.0310) loss_zs_kd 0.0571 (0.0697) loss_oracle 0.3523 (0.3551) acc 68.7500 (73.3929) lr 1.8443e-03 eta 0:28:40
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,384
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,015
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.2%
******* Domain a best val acc:      86.1%, epoch: 10 *******
******* Domain a best val test acc: 83.3%, epoch: 10 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [12/50] batch [20/288] time 0.089 (0.109) data 0.001 (0.015) loss 1.7522 (1.5653) teacher_loss 1.1212 (0.9907) loss_zs_kd 0.1065 (0.0801) loss_oracle 0.3996 (0.3509) acc 68.7500 (72.1875) lr 1.8090e-03 eta 0:20:21
epoch [12/50] batch [40/288] time 0.085 (0.099) data 0.001 (0.008) loss 2.0553 (1.5756) teacher_loss 1.4343 (1.0041) loss_zs_kd 0.0765 (0.0742) loss_oracle 0.3526 (0.3536) acc 62.5000 (72.8125) lr 1.8090e-03 eta 0:18:24
epoch [12/50] batch [60/288] time 0.081 (0.093) data 0.000 (0.005) loss 1.3257 (1.5627) teacher_loss 0.7320 (1.0007) loss_zs_kd 0.0508 (0.0685) loss_oracle 0.3270 (0.3510) acc 78.1250 (72.9167) lr 1.8090e-03 eta 0:17:19
epoch [12/50] batch [80/288] time 0.084 (0.091) data 0.000 (0.004) loss 2.1478 (1.5874) teacher_loss 1.4800 (1.0259) loss_zs_kd 0.0957 (0.0702) loss_oracle 0.3925 (0.3524) acc 62.5000 (72.5781) lr 1.8090e-03 eta 0:16:50
epoch [12/50] batch [100/288] time 0.082 (0.089) data 0.000 (0.003) loss 1.3352 (1.5849) teacher_loss 0.6100 (1.0234) loss_zs_kd 0.0536 (0.0706) loss_oracle 0.3399 (0.3512) acc 81.2500 (72.8125) lr 1.8090e-03 eta 0:16:33
epoch [12/50] batch [120/288] time 0.085 (0.088) data 0.000 (0.003) loss 1.3142 (1.5723) teacher_loss 0.8576 (1.0144) loss_zs_kd 0.0663 (0.0699) loss_oracle 0.3293 (0.3506) acc 75.0000 (73.0469) lr 1.8090e-03 eta 0:16:22
epoch [12/50] batch [140/288] time 0.088 (0.088) data 0.001 (0.002) loss 1.6964 (1.5800) teacher_loss 1.0363 (1.0189) loss_zs_kd 0.0500 (0.0684) loss_oracle 0.3804 (0.3505) acc 78.1250 (72.9688) lr 1.8090e-03 eta 0:16:17
epoch [12/50] batch [160/288] time 0.084 (0.088) data 0.000 (0.002) loss 1.2050 (1.5719) teacher_loss 0.7169 (1.0100) loss_zs_kd 0.0494 (0.0688) loss_oracle 0.3456 (0.3508) acc 81.2500 (73.2422) lr 1.8090e-03 eta 0:16:12
epoch [12/50] batch [180/288] time 0.086 (0.088) data 0.000 (0.002) loss 1.6781 (1.5721) teacher_loss 1.1298 (1.0059) loss_zs_kd 0.0864 (0.0691) loss_oracle 0.3626 (0.3505) acc 65.6250 (73.2292) lr 1.8090e-03 eta 0:16:11
epoch [12/50] batch [200/288] time 0.137 (0.088) data 0.000 (0.002) loss 1.6843 (1.5736) teacher_loss 1.0257 (1.0027) loss_zs_kd 0.0961 (0.0697) loss_oracle 0.3564 (0.3499) acc 68.7500 (73.3594) lr 1.8090e-03 eta 0:16:10
epoch [12/50] batch [220/288] time 0.085 (0.089) data 0.000 (0.002) loss 1.5170 (1.5766) teacher_loss 0.9775 (1.0045) loss_zs_kd 0.0983 (0.0697) loss_oracle 0.3505 (0.3494) acc 68.7500 (73.2528) lr 1.8090e-03 eta 0:16:16
epoch [12/50] batch [240/288] time 0.085 (0.088) data 0.000 (0.002) loss 1.1435 (1.5798) teacher_loss 0.5627 (1.0062) loss_zs_kd 0.0737 (0.0700) loss_oracle 0.3505 (0.3492) acc 84.3750 (73.3854) lr 1.8090e-03 eta 0:16:10
epoch [12/50] batch [260/288] time 0.085 (0.088) data 0.000 (0.001) loss 1.4934 (1.5821) teacher_loss 0.9194 (1.0088) loss_zs_kd 0.0785 (0.0707) loss_oracle 0.3431 (0.3490) acc 78.1250 (73.3413) lr 1.8090e-03 eta 0:16:05
epoch [12/50] batch [280/288] time 0.086 (0.088) data 0.000 (0.001) loss 1.8516 (1.5797) teacher_loss 1.1557 (1.0055) loss_zs_kd 0.0695 (0.0711) loss_oracle 0.3680 (0.3490) acc 78.1250 (73.4933) lr 1.8090e-03 eta 0:16:01
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,385
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.5%
******* Domain a best val acc:      86.1%, epoch: 10 *******
******* Domain a best val test acc: 83.3%, epoch: 10 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [13/50] batch [20/288] time 0.082 (0.100) data 0.000 (0.014) loss 1.8178 (1.5294) teacher_loss 1.2327 (0.9667) loss_zs_kd 0.0650 (0.0728) loss_oracle 0.3362 (0.3506) acc 68.7500 (74.2188) lr 1.7705e-03 eta 0:18:14
epoch [13/50] batch [40/288] time 0.081 (0.091) data 0.000 (0.007) loss 2.0351 (1.6419) teacher_loss 1.4287 (1.0781) loss_zs_kd 0.0547 (0.0707) loss_oracle 0.3287 (0.3496) acc 62.5000 (71.0938) lr 1.7705e-03 eta 0:16:37
epoch [13/50] batch [60/288] time 0.086 (0.088) data 0.001 (0.005) loss 2.1073 (1.6357) teacher_loss 1.5263 (1.0703) loss_zs_kd 0.0428 (0.0705) loss_oracle 0.3267 (0.3475) acc 68.7500 (71.8750) lr 1.7705e-03 eta 0:16:00
epoch [13/50] batch [80/288] time 0.086 (0.087) data 0.000 (0.004) loss 1.4977 (1.6329) teacher_loss 0.9749 (1.0671) loss_zs_kd 0.0623 (0.0690) loss_oracle 0.3375 (0.3485) acc 75.0000 (72.2266) lr 1.7705e-03 eta 0:15:46
epoch [13/50] batch [100/288] time 0.082 (0.086) data 0.000 (0.003) loss 1.2950 (1.6222) teacher_loss 0.7597 (1.0557) loss_zs_kd 0.0895 (0.0686) loss_oracle 0.3471 (0.3474) acc 78.1250 (72.2812) lr 1.7705e-03 eta 0:15:36
epoch [13/50] batch [120/288] time 0.082 (0.086) data 0.000 (0.002) loss 1.6791 (1.6091) teacher_loss 1.1162 (1.0442) loss_zs_kd 0.0642 (0.0685) loss_oracle 0.3464 (0.3469) acc 75.0000 (72.4740) lr 1.7705e-03 eta 0:15:28
epoch [13/50] batch [140/288] time 0.084 (0.085) data 0.000 (0.002) loss 2.0771 (1.6033) teacher_loss 1.3923 (1.0353) loss_zs_kd 0.0630 (0.0697) loss_oracle 0.3419 (0.3463) acc 65.6250 (72.6786) lr 1.7705e-03 eta 0:15:21
epoch [13/50] batch [160/288] time 0.081 (0.085) data 0.000 (0.002) loss 1.4579 (1.6024) teacher_loss 0.7963 (1.0298) loss_zs_kd 0.0889 (0.0704) loss_oracle 0.3299 (0.3460) acc 81.2500 (72.9102) lr 1.7705e-03 eta 0:15:16
epoch [13/50] batch [180/288] time 0.090 (0.085) data 0.000 (0.002) loss 1.6441 (1.5987) teacher_loss 0.9860 (1.0250) loss_zs_kd 0.0677 (0.0710) loss_oracle 0.3439 (0.3456) acc 75.0000 (73.1076) lr 1.7705e-03 eta 0:15:14
epoch [13/50] batch [200/288] time 0.084 (0.085) data 0.000 (0.002) loss 1.1816 (1.5948) teacher_loss 0.6265 (1.0196) loss_zs_kd 0.0601 (0.0710) loss_oracle 0.3640 (0.3462) acc 78.1250 (73.2031) lr 1.7705e-03 eta 0:15:11
epoch [13/50] batch [220/288] time 0.086 (0.085) data 0.000 (0.002) loss 1.4866 (1.5955) teacher_loss 0.8196 (1.0167) loss_zs_kd 0.0681 (0.0714) loss_oracle 0.3214 (0.3467) acc 75.0000 (73.1960) lr 1.7705e-03 eta 0:15:12
epoch [13/50] batch [240/288] time 0.087 (0.086) data 0.000 (0.001) loss 1.2977 (1.5859) teacher_loss 0.8990 (1.0102) loss_zs_kd 0.0561 (0.0716) loss_oracle 0.3357 (0.3461) acc 75.0000 (73.3203) lr 1.7705e-03 eta 0:15:25
epoch [13/50] batch [260/288] time 0.087 (0.086) data 0.000 (0.001) loss 1.4515 (1.5892) teacher_loss 0.8075 (1.0149) loss_zs_kd 0.0744 (0.0713) loss_oracle 0.3435 (0.3460) acc 78.1250 (73.1010) lr 1.7705e-03 eta 0:15:22
epoch [13/50] batch [280/288] time 0.088 (0.086) data 0.000 (0.001) loss 1.1745 (1.5919) teacher_loss 0.6437 (1.0155) loss_zs_kd 0.0600 (0.0710) loss_oracle 0.3797 (0.3454) acc 84.3750 (73.0022) lr 1.7705e-03 eta 0:15:19
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,383
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.6%
******* Domain a best val acc:      86.1%, epoch: 10 *******
******* Domain a best val test acc: 83.3%, epoch: 10 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [14/50] batch [20/288] time 0.085 (0.108) data 0.001 (0.019) loss 1.5170 (1.5355) teacher_loss 0.9516 (0.9535) loss_zs_kd 0.0660 (0.0674) loss_oracle 0.3028 (0.3448) acc 71.8750 (72.6562) lr 1.7290e-03 eta 0:19:05
epoch [14/50] batch [40/288] time 0.082 (0.096) data 0.000 (0.009) loss 1.2100 (1.5900) teacher_loss 0.6831 (1.0338) loss_zs_kd 0.0450 (0.0662) loss_oracle 0.3167 (0.3399) acc 75.0000 (71.2500) lr 1.7290e-03 eta 0:16:54
epoch [14/50] batch [60/288] time 0.082 (0.091) data 0.000 (0.006) loss 1.7813 (1.5896) teacher_loss 1.4059 (1.0365) loss_zs_kd 0.0664 (0.0684) loss_oracle 0.3598 (0.3386) acc 71.8750 (71.5104) lr 1.7290e-03 eta 0:16:06
epoch [14/50] batch [80/288] time 0.088 (0.089) data 0.001 (0.005) loss 1.1985 (1.6141) teacher_loss 0.7421 (1.0605) loss_zs_kd 0.0589 (0.0702) loss_oracle 0.3398 (0.3381) acc 78.1250 (71.0938) lr 1.7290e-03 eta 0:15:44
epoch [14/50] batch [100/288] time 0.082 (0.088) data 0.000 (0.004) loss 1.4862 (1.6009) teacher_loss 1.1067 (1.0453) loss_zs_kd 0.0923 (0.0719) loss_oracle 0.3654 (0.3370) acc 62.5000 (71.5000) lr 1.7290e-03 eta 0:15:29
epoch [14/50] batch [120/288] time 0.082 (0.087) data 0.000 (0.003) loss 1.7123 (1.5928) teacher_loss 1.2426 (1.0364) loss_zs_kd 0.0702 (0.0712) loss_oracle 0.3087 (0.3374) acc 65.6250 (71.6667) lr 1.7290e-03 eta 0:15:18
epoch [14/50] batch [140/288] time 0.086 (0.086) data 0.000 (0.003) loss 2.0952 (1.6131) teacher_loss 1.4875 (1.0514) loss_zs_kd 0.0774 (0.0728) loss_oracle 0.3435 (0.3376) acc 56.2500 (71.2054) lr 1.7290e-03 eta 0:15:09
epoch [14/50] batch [160/288] time 0.086 (0.086) data 0.001 (0.003) loss 1.9681 (1.6106) teacher_loss 1.2714 (1.0484) loss_zs_kd 0.0675 (0.0728) loss_oracle 0.3378 (0.3373) acc 62.5000 (71.3672) lr 1.7290e-03 eta 0:15:03
epoch [14/50] batch [180/288] time 0.088 (0.086) data 0.001 (0.002) loss 2.2478 (1.6100) teacher_loss 1.7426 (1.0453) loss_zs_kd 0.0842 (0.0727) loss_oracle 0.3333 (0.3370) acc 56.2500 (71.6146) lr 1.7290e-03 eta 0:15:00
epoch [14/50] batch [200/288] time 0.085 (0.086) data 0.000 (0.002) loss 1.7036 (1.6037) teacher_loss 1.2094 (1.0392) loss_zs_kd 0.0920 (0.0733) loss_oracle 0.3232 (0.3370) acc 71.8750 (71.7969) lr 1.7290e-03 eta 0:14:56
epoch [14/50] batch [220/288] time 0.069 (0.086) data 0.000 (0.002) loss 1.5649 (1.6051) teacher_loss 0.9116 (1.0384) loss_zs_kd 0.0681 (0.0725) loss_oracle 0.3306 (0.3379) acc 65.6250 (71.8750) lr 1.7290e-03 eta 0:14:55
epoch [14/50] batch [240/288] time 0.085 (0.086) data 0.000 (0.002) loss 1.8414 (1.6090) teacher_loss 1.2963 (1.0423) loss_zs_kd 0.1184 (0.0723) loss_oracle 0.3407 (0.3378) acc 62.5000 (71.8880) lr 1.7290e-03 eta 0:14:53
epoch [14/50] batch [260/288] time 0.090 (0.086) data 0.000 (0.002) loss 1.6833 (1.6065) teacher_loss 1.0053 (1.0386) loss_zs_kd 0.0534 (0.0722) loss_oracle 0.3288 (0.3385) acc 75.0000 (72.0433) lr 1.7290e-03 eta 0:14:50
epoch [14/50] batch [280/288] time 0.088 (0.087) data 0.000 (0.002) loss 1.9352 (1.6052) teacher_loss 1.3667 (1.0374) loss_zs_kd 0.0860 (0.0721) loss_oracle 0.3572 (0.3387) acc 65.6250 (72.1205) lr 1.7290e-03 eta 0:14:58
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,392
* accuracy: 86.1%
* error: 13.9%
* macro_f1: 85.5%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.7%
******* Domain a best val acc:      86.1%, epoch: 14 *******
******* Domain a best val test acc: 83.3%, epoch: 14 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [15/50] batch [20/288] time 0.086 (0.104) data 0.000 (0.019) loss 1.4385 (1.4998) teacher_loss 0.8323 (0.9311) loss_zs_kd 0.0908 (0.0653) loss_oracle 0.3791 (0.3383) acc 81.2500 (75.6250) lr 1.6845e-03 eta 0:17:52
epoch [15/50] batch [40/288] time 0.085 (0.094) data 0.000 (0.010) loss 1.9041 (1.5367) teacher_loss 1.4154 (0.9511) loss_zs_kd 0.0539 (0.0685) loss_oracle 0.3356 (0.3378) acc 65.6250 (75.3906) lr 1.6845e-03 eta 0:16:10
epoch [15/50] batch [60/288] time 0.085 (0.091) data 0.000 (0.006) loss 1.1611 (1.5430) teacher_loss 0.6691 (0.9737) loss_zs_kd 0.0587 (0.0677) loss_oracle 0.3246 (0.3358) acc 78.1250 (74.6875) lr 1.6845e-03 eta 0:15:36
epoch [15/50] batch [80/288] time 0.085 (0.089) data 0.000 (0.005) loss 1.5212 (1.5484) teacher_loss 0.9310 (0.9831) loss_zs_kd 0.1061 (0.0684) loss_oracle 0.3415 (0.3356) acc 78.1250 (74.2969) lr 1.6845e-03 eta 0:15:19
epoch [15/50] batch [100/288] time 0.084 (0.088) data 0.000 (0.004) loss 1.1298 (1.5393) teacher_loss 0.5760 (0.9734) loss_zs_kd 0.0791 (0.0676) loss_oracle 0.3837 (0.3366) acc 84.3750 (74.0625) lr 1.6845e-03 eta 0:15:07
epoch [15/50] batch [120/288] time 0.085 (0.088) data 0.000 (0.003) loss 1.6631 (1.5626) teacher_loss 1.0491 (0.9978) loss_zs_kd 0.0915 (0.0679) loss_oracle 0.3561 (0.3365) acc 78.1250 (73.4115) lr 1.6845e-03 eta 0:15:00
epoch [15/50] batch [140/288] time 0.083 (0.087) data 0.000 (0.003) loss 2.0472 (1.5627) teacher_loss 1.3667 (0.9970) loss_zs_kd 0.0964 (0.0687) loss_oracle 0.3674 (0.3375) acc 59.3750 (73.3705) lr 1.6845e-03 eta 0:14:54
epoch [15/50] batch [160/288] time 0.085 (0.087) data 0.000 (0.003) loss 1.6032 (1.5667) teacher_loss 0.9525 (1.0025) loss_zs_kd 0.0752 (0.0689) loss_oracle 0.3348 (0.3380) acc 75.0000 (73.3789) lr 1.6845e-03 eta 0:14:51
epoch [15/50] batch [180/288] time 0.085 (0.087) data 0.000 (0.002) loss 1.7444 (1.5749) teacher_loss 1.2699 (1.0094) loss_zs_kd 0.0584 (0.0690) loss_oracle 0.3378 (0.3374) acc 68.7500 (73.1597) lr 1.6845e-03 eta 0:14:47
epoch [15/50] batch [200/288] time 0.085 (0.087) data 0.000 (0.002) loss 1.5757 (1.5762) teacher_loss 0.8367 (1.0084) loss_zs_kd 0.0832 (0.0689) loss_oracle 0.3249 (0.3377) acc 78.1250 (73.1719) lr 1.6845e-03 eta 0:14:44
epoch [15/50] batch [220/288] time 0.087 (0.087) data 0.000 (0.002) loss 1.7154 (1.5722) teacher_loss 1.1365 (1.0049) loss_zs_kd 0.0983 (0.0691) loss_oracle 0.3813 (0.3379) acc 75.0000 (73.2670) lr 1.6845e-03 eta 0:14:42
epoch [15/50] batch [240/288] time 0.086 (0.087) data 0.000 (0.002) loss 1.5111 (1.5810) teacher_loss 1.0009 (1.0119) loss_zs_kd 0.0573 (0.0699) loss_oracle 0.3587 (0.3380) acc 71.8750 (73.1771) lr 1.6845e-03 eta 0:14:42
epoch [15/50] batch [260/288] time 0.087 (0.087) data 0.000 (0.002) loss 1.4449 (1.5829) teacher_loss 1.0153 (1.0148) loss_zs_kd 0.0781 (0.0702) loss_oracle 0.3347 (0.3377) acc 71.8750 (73.1971) lr 1.6845e-03 eta 0:14:43
epoch [15/50] batch [280/288] time 0.116 (0.087) data 0.001 (0.002) loss 1.6479 (1.5835) teacher_loss 1.1938 (1.0163) loss_zs_kd 0.0491 (0.0704) loss_oracle 0.3075 (0.3373) acc 68.7500 (73.2254) lr 1.6845e-03 eta 0:14:42
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,394
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.4%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.7%
******* Domain a best val acc:      86.2%, epoch: 15 *******
******* Domain a best val test acc: 83.4%, epoch: 15 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [16/50] batch [20/288] time 0.086 (0.104) data 0.001 (0.014) loss 1.4771 (1.5664) teacher_loss 0.7588 (1.0011) loss_zs_kd 0.0517 (0.0636) loss_oracle 0.3221 (0.3345) acc 78.1250 (72.9688) lr 1.6374e-03 eta 0:17:23
epoch [16/50] batch [40/288] time 0.085 (0.095) data 0.000 (0.007) loss 1.5654 (1.5795) teacher_loss 0.9543 (1.0202) loss_zs_kd 0.0774 (0.0701) loss_oracle 0.3263 (0.3341) acc 71.8750 (72.8125) lr 1.6374e-03 eta 0:15:49
epoch [16/50] batch [60/288] time 0.085 (0.091) data 0.000 (0.005) loss 1.9177 (1.5913) teacher_loss 1.3659 (1.0097) loss_zs_kd 0.0761 (0.0695) loss_oracle 0.3310 (0.3347) acc 62.5000 (72.8125) lr 1.6374e-03 eta 0:15:16
epoch [16/50] batch [80/288] time 0.082 (0.090) data 0.000 (0.004) loss 1.6646 (1.6057) teacher_loss 1.1840 (1.0175) loss_zs_kd 0.1120 (0.0728) loss_oracle 0.3241 (0.3360) acc 65.6250 (72.8125) lr 1.6374e-03 eta 0:14:58
epoch [16/50] batch [100/288] time 0.086 (0.089) data 0.000 (0.003) loss 1.4868 (1.6022) teacher_loss 0.9808 (1.0102) loss_zs_kd 0.0590 (0.0726) loss_oracle 0.3623 (0.3363) acc 68.7500 (72.8750) lr 1.6374e-03 eta 0:14:49
epoch [16/50] batch [120/288] time 0.086 (0.089) data 0.000 (0.003) loss 2.0409 (1.6016) teacher_loss 1.3585 (1.0146) loss_zs_kd 0.0912 (0.0729) loss_oracle 0.3066 (0.3349) acc 65.6250 (72.9427) lr 1.6374e-03 eta 0:14:42
epoch [16/50] batch [140/288] time 0.086 (0.088) data 0.000 (0.002) loss 1.5872 (1.6104) teacher_loss 1.0437 (1.0221) loss_zs_kd 0.0872 (0.0739) loss_oracle 0.3658 (0.3366) acc 62.5000 (72.6562) lr 1.6374e-03 eta 0:14:37
epoch [16/50] batch [160/288] time 0.085 (0.088) data 0.000 (0.002) loss 1.3178 (1.5960) teacher_loss 0.7388 (1.0131) loss_zs_kd 0.0755 (0.0736) loss_oracle 0.3293 (0.3365) acc 78.1250 (72.9688) lr 1.6374e-03 eta 0:14:31
epoch [16/50] batch [180/288] time 0.082 (0.088) data 0.000 (0.002) loss 1.5625 (1.5842) teacher_loss 1.0518 (1.0081) loss_zs_kd 0.0739 (0.0728) loss_oracle 0.2996 (0.3357) acc 71.8750 (73.1250) lr 1.6374e-03 eta 0:14:26
epoch [16/50] batch [200/288] time 0.083 (0.087) data 0.000 (0.002) loss 1.6868 (1.5894) teacher_loss 1.2351 (1.0136) loss_zs_kd 0.0433 (0.0729) loss_oracle 0.3351 (0.3342) acc 75.0000 (72.8906) lr 1.6374e-03 eta 0:14:21
epoch [16/50] batch [220/288] time 0.082 (0.087) data 0.000 (0.002) loss 1.2123 (1.5833) teacher_loss 0.8501 (1.0117) loss_zs_kd 0.0684 (0.0723) loss_oracle 0.2975 (0.3332) acc 75.0000 (73.0398) lr 1.6374e-03 eta 0:14:16
epoch [16/50] batch [240/288] time 0.123 (0.087) data 0.001 (0.001) loss 1.9827 (1.5880) teacher_loss 1.4091 (1.0191) loss_zs_kd 0.0671 (0.0721) loss_oracle 0.3366 (0.3324) acc 65.6250 (72.8906) lr 1.6374e-03 eta 0:14:12
epoch [16/50] batch [260/288] time 0.085 (0.087) data 0.000 (0.001) loss 1.3151 (1.5831) teacher_loss 0.8162 (1.0165) loss_zs_kd 0.0716 (0.0722) loss_oracle 0.3652 (0.3325) acc 75.0000 (72.8245) lr 1.6374e-03 eta 0:14:09
epoch [16/50] batch [280/288] time 0.086 (0.086) data 0.000 (0.001) loss 1.3405 (1.5832) teacher_loss 0.6872 (1.0165) loss_zs_kd 0.0564 (0.0732) loss_oracle 0.3326 (0.3325) acc 81.2500 (72.8460) lr 1.6374e-03 eta 0:14:07
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,389
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.4%
******* Domain a best val acc:      86.2%, epoch: 15 *******
******* Domain a best val test acc: 83.4%, epoch: 15 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [17/50] batch [20/288] time 0.084 (0.107) data 0.000 (0.019) loss 1.2575 (1.5497) teacher_loss 0.7311 (0.9735) loss_zs_kd 0.0341 (0.0740) loss_oracle 0.3320 (0.3350) acc 81.2500 (72.9688) lr 1.5878e-03 eta 0:17:21
epoch [17/50] batch [40/288] time 0.086 (0.096) data 0.000 (0.009) loss 1.6678 (1.5335) teacher_loss 0.9992 (0.9574) loss_zs_kd 0.0661 (0.0725) loss_oracle 0.3272 (0.3330) acc 71.8750 (74.0625) lr 1.5878e-03 eta 0:15:33
epoch [17/50] batch [60/288] time 0.086 (0.091) data 0.000 (0.006) loss 1.3492 (1.5400) teacher_loss 0.8774 (0.9629) loss_zs_kd 0.0633 (0.0712) loss_oracle 0.3115 (0.3329) acc 75.0000 (74.0625) lr 1.5878e-03 eta 0:14:43
epoch [17/50] batch [80/288] time 0.081 (0.088) data 0.001 (0.005) loss 1.7100 (1.5656) teacher_loss 1.2064 (0.9881) loss_zs_kd 0.0517 (0.0705) loss_oracle 0.3494 (0.3338) acc 68.7500 (73.1250) lr 1.5878e-03 eta 0:14:15
epoch [17/50] batch [100/288] time 0.084 (0.086) data 0.000 (0.004) loss 1.4395 (1.5670) teacher_loss 0.9296 (0.9970) loss_zs_kd 0.0836 (0.0709) loss_oracle 0.3297 (0.3329) acc 75.0000 (73.2500) lr 1.5878e-03 eta 0:13:56
epoch [17/50] batch [120/288] time 0.085 (0.085) data 0.000 (0.003) loss 1.6533 (1.5803) teacher_loss 1.0924 (1.0082) loss_zs_kd 0.0515 (0.0712) loss_oracle 0.3307 (0.3328) acc 68.7500 (73.0208) lr 1.5878e-03 eta 0:13:43
epoch [17/50] batch [140/288] time 0.084 (0.085) data 0.000 (0.003) loss 1.7205 (1.5887) teacher_loss 1.0846 (1.0182) loss_zs_kd 0.0632 (0.0711) loss_oracle 0.3376 (0.3341) acc 68.7500 (72.8795) lr 1.5878e-03 eta 0:13:41
epoch [17/50] batch [160/288] time 0.085 (0.085) data 0.000 (0.003) loss 1.6796 (1.5848) teacher_loss 1.1123 (1.0141) loss_zs_kd 0.0772 (0.0712) loss_oracle 0.3849 (0.3338) acc 65.6250 (72.9688) lr 1.5878e-03 eta 0:13:39
epoch [17/50] batch [180/288] time 0.088 (0.085) data 0.000 (0.002) loss 1.4116 (1.5806) teacher_loss 0.8955 (1.0094) loss_zs_kd 0.0875 (0.0718) loss_oracle 0.3408 (0.3338) acc 78.1250 (73.1424) lr 1.5878e-03 eta 0:13:38
epoch [17/50] batch [200/288] time 0.085 (0.085) data 0.000 (0.002) loss 1.4867 (1.5821) teacher_loss 0.9926 (1.0107) loss_zs_kd 0.0966 (0.0712) loss_oracle 0.3333 (0.3331) acc 75.0000 (72.9688) lr 1.5878e-03 eta 0:13:36
epoch [17/50] batch [220/288] time 0.085 (0.085) data 0.000 (0.002) loss 1.3841 (1.5854) teacher_loss 0.7689 (1.0148) loss_zs_kd 0.0558 (0.0712) loss_oracle 0.3854 (0.3321) acc 84.3750 (73.0256) lr 1.5878e-03 eta 0:13:34
epoch [17/50] batch [240/288] time 0.088 (0.085) data 0.001 (0.002) loss 1.4259 (1.5789) teacher_loss 0.7799 (1.0086) loss_zs_kd 0.0627 (0.0706) loss_oracle 0.3627 (0.3325) acc 75.0000 (73.1250) lr 1.5878e-03 eta 0:13:33
epoch [17/50] batch [260/288] time 0.091 (0.085) data 0.000 (0.002) loss 2.1129 (1.5844) teacher_loss 1.4048 (1.0110) loss_zs_kd 0.0827 (0.0709) loss_oracle 0.3271 (0.3327) acc 56.2500 (72.9928) lr 1.5878e-03 eta 0:13:32
epoch [17/50] batch [280/288] time 0.089 (0.085) data 0.001 (0.002) loss 1.1187 (1.5863) teacher_loss 0.6201 (1.0119) loss_zs_kd 0.0746 (0.0713) loss_oracle 0.3333 (0.3325) acc 81.2500 (72.9911) lr 1.5878e-03 eta 0:13:32
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,395
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,028
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.0%
******* Domain a best val acc:      86.2%, epoch: 17 *******
******* Domain a best val test acc: 83.6%, epoch: 17 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [18/50] batch [20/288] time 0.081 (0.095) data 0.000 (0.013) loss 2.0371 (1.5361) teacher_loss 1.2437 (0.9796) loss_zs_kd 0.0704 (0.0672) loss_oracle 0.3107 (0.3282) acc 71.8750 (74.5312) lr 1.5358e-03 eta 0:14:57
epoch [18/50] batch [40/288] time 0.082 (0.089) data 0.000 (0.006) loss 1.4870 (1.5219) teacher_loss 0.8557 (0.9443) loss_zs_kd 0.0610 (0.0689) loss_oracle 0.3547 (0.3304) acc 78.1250 (75.1562) lr 1.5358e-03 eta 0:14:01
epoch [18/50] batch [60/288] time 0.082 (0.087) data 0.000 (0.004) loss 2.1132 (1.5467) teacher_loss 1.4653 (0.9632) loss_zs_kd 0.0957 (0.0708) loss_oracle 0.2990 (0.3303) acc 56.2500 (74.1146) lr 1.5358e-03 eta 0:13:39
epoch [18/50] batch [80/288] time 0.080 (0.086) data 0.000 (0.003) loss 1.7135 (1.5470) teacher_loss 1.0775 (0.9690) loss_zs_kd 0.0565 (0.0714) loss_oracle 0.3120 (0.3292) acc 71.8750 (74.3359) lr 1.5358e-03 eta 0:13:27
epoch [18/50] batch [100/288] time 0.080 (0.085) data 0.000 (0.003) loss 1.6420 (1.5483) teacher_loss 0.9956 (0.9722) loss_zs_kd 0.0571 (0.0742) loss_oracle 0.3545 (0.3319) acc 71.8750 (74.0938) lr 1.5358e-03 eta 0:13:17
epoch [18/50] batch [120/288] time 0.084 (0.084) data 0.000 (0.002) loss 1.8443 (1.5569) teacher_loss 1.1722 (0.9764) loss_zs_kd 0.0979 (0.0742) loss_oracle 0.3334 (0.3347) acc 68.7500 (73.7760) lr 1.5358e-03 eta 0:13:11
epoch [18/50] batch [140/288] time 0.085 (0.084) data 0.000 (0.002) loss 1.9206 (1.5701) teacher_loss 1.3290 (0.9950) loss_zs_kd 0.0777 (0.0733) loss_oracle 0.3371 (0.3372) acc 71.8750 (73.1920) lr 1.5358e-03 eta 0:13:08
epoch [18/50] batch [160/288] time 0.085 (0.084) data 0.000 (0.002) loss 1.3985 (1.5643) teacher_loss 0.8030 (0.9933) loss_zs_kd 0.0586 (0.0726) loss_oracle 0.3147 (0.3363) acc 81.2500 (73.3594) lr 1.5358e-03 eta 0:13:08
epoch [18/50] batch [180/288] time 0.090 (0.085) data 0.000 (0.002) loss 1.2271 (1.5491) teacher_loss 0.7422 (0.9844) loss_zs_kd 0.0778 (0.0727) loss_oracle 0.3351 (0.3349) acc 75.0000 (73.6979) lr 1.5358e-03 eta 0:13:09
epoch [18/50] batch [200/288] time 0.086 (0.085) data 0.000 (0.002) loss 1.4277 (1.5558) teacher_loss 0.8035 (0.9893) loss_zs_kd 0.0867 (0.0733) loss_oracle 0.3172 (0.3345) acc 78.1250 (73.6406) lr 1.5358e-03 eta 0:13:09
epoch [18/50] batch [220/288] time 0.085 (0.085) data 0.000 (0.001) loss 2.0663 (1.5511) teacher_loss 1.3558 (0.9810) loss_zs_kd 0.0867 (0.0731) loss_oracle 0.3331 (0.3348) acc 62.5000 (73.8778) lr 1.5358e-03 eta 0:13:08
epoch [18/50] batch [240/288] time 0.087 (0.085) data 0.000 (0.001) loss 1.5564 (1.5600) teacher_loss 1.0511 (0.9907) loss_zs_kd 0.0632 (0.0732) loss_oracle 0.3083 (0.3346) acc 62.5000 (73.4635) lr 1.5358e-03 eta 0:13:07
epoch [18/50] batch [260/288] time 0.086 (0.085) data 0.000 (0.001) loss 1.6612 (1.5553) teacher_loss 1.2267 (0.9870) loss_zs_kd 0.0691 (0.0731) loss_oracle 0.3217 (0.3337) acc 65.6250 (73.4615) lr 1.5358e-03 eta 0:13:06
epoch [18/50] batch [280/288] time 0.086 (0.085) data 0.000 (0.001) loss 1.7768 (1.5550) teacher_loss 1.1118 (0.9874) loss_zs_kd 0.0784 (0.0736) loss_oracle 0.3404 (0.3329) acc 65.6250 (73.4598) lr 1.5358e-03 eta 0:13:06
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,404
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.9%
******* Domain a best val acc:      86.4%, epoch: 18 *******
******* Domain a best val test acc: 83.3%, epoch: 18 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [19/50] batch [20/288] time 0.067 (0.102) data 0.000 (0.022) loss 1.7244 (1.6112) teacher_loss 1.0081 (1.0271) loss_zs_kd 0.0603 (0.0740) loss_oracle 0.3007 (0.3377) acc 78.1250 (73.7500) lr 1.4818e-03 eta 0:15:41
epoch [19/50] batch [40/288] time 0.085 (0.090) data 0.000 (0.011) loss 1.4816 (1.6506) teacher_loss 1.0056 (1.0536) loss_zs_kd 0.0794 (0.0782) loss_oracle 0.3200 (0.3371) acc 71.8750 (71.0938) lr 1.4818e-03 eta 0:13:49
epoch [19/50] batch [60/288] time 0.099 (0.090) data 0.000 (0.008) loss 1.2789 (1.5774) teacher_loss 0.8569 (0.9962) loss_zs_kd 0.0511 (0.0748) loss_oracle 0.2909 (0.3344) acc 71.8750 (72.6562) lr 1.4818e-03 eta 0:13:40
epoch [19/50] batch [80/288] time 0.086 (0.089) data 0.001 (0.006) loss 1.7096 (1.5706) teacher_loss 1.1438 (0.9959) loss_zs_kd 0.0548 (0.0736) loss_oracle 0.3090 (0.3318) acc 68.7500 (72.9297) lr 1.4818e-03 eta 0:13:34
epoch [19/50] batch [100/288] time 0.099 (0.089) data 0.001 (0.005) loss 1.5264 (1.5749) teacher_loss 0.9954 (1.0037) loss_zs_kd 0.0529 (0.0722) loss_oracle 0.3423 (0.3314) acc 78.1250 (72.8750) lr 1.4818e-03 eta 0:13:29
epoch [19/50] batch [120/288] time 0.084 (0.088) data 0.000 (0.004) loss 1.6846 (1.5851) teacher_loss 1.1526 (1.0140) loss_zs_kd 0.0659 (0.0740) loss_oracle 0.2946 (0.3317) acc 65.6250 (72.7604) lr 1.4818e-03 eta 0:13:24
epoch [19/50] batch [140/288] time 0.087 (0.088) data 0.001 (0.003) loss 1.3563 (1.5848) teacher_loss 0.8551 (1.0125) loss_zs_kd 0.0542 (0.0739) loss_oracle 0.2818 (0.3317) acc 68.7500 (73.1027) lr 1.4818e-03 eta 0:13:19
epoch [19/50] batch [160/288] time 0.085 (0.088) data 0.000 (0.003) loss 1.6376 (1.5889) teacher_loss 1.0290 (1.0137) loss_zs_kd 0.1061 (0.0752) loss_oracle 0.3463 (0.3305) acc 78.1250 (73.0664) lr 1.4818e-03 eta 0:13:15
epoch [19/50] batch [180/288] time 0.087 (0.088) data 0.000 (0.003) loss 1.6828 (1.5990) teacher_loss 1.1346 (1.0161) loss_zs_kd 0.0539 (0.0752) loss_oracle 0.3045 (0.3315) acc 65.6250 (72.9514) lr 1.4818e-03 eta 0:13:13
epoch [19/50] batch [200/288] time 0.083 (0.087) data 0.000 (0.002) loss 1.2902 (1.5988) teacher_loss 0.7304 (1.0132) loss_zs_kd 0.0727 (0.0760) loss_oracle 0.3061 (0.3317) acc 81.2500 (73.0625) lr 1.4818e-03 eta 0:13:08
epoch [19/50] batch [220/288] time 0.097 (0.087) data 0.000 (0.002) loss 1.6939 (1.6120) teacher_loss 1.1031 (1.0230) loss_zs_kd 0.0822 (0.0765) loss_oracle 0.3272 (0.3316) acc 62.5000 (72.8835) lr 1.4818e-03 eta 0:13:04
epoch [19/50] batch [240/288] time 0.089 (0.087) data 0.000 (0.002) loss 1.0590 (1.6063) teacher_loss 0.5514 (1.0185) loss_zs_kd 0.0501 (0.0760) loss_oracle 0.3546 (0.3329) acc 90.6250 (73.0469) lr 1.4818e-03 eta 0:13:01
epoch [19/50] batch [260/288] time 0.085 (0.087) data 0.000 (0.002) loss 1.8209 (1.6147) teacher_loss 1.3807 (1.0281) loss_zs_kd 0.0596 (0.0757) loss_oracle 0.3030 (0.3332) acc 59.3750 (72.9808) lr 1.4818e-03 eta 0:12:59
epoch [19/50] batch [280/288] time 0.087 (0.087) data 0.001 (0.002) loss 1.4138 (1.6101) teacher_loss 0.8428 (1.0260) loss_zs_kd 0.0492 (0.0751) loss_oracle 0.3323 (0.3334) acc 81.2500 (73.0804) lr 1.4818e-03 eta 0:12:56
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,394
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.7%
******* Domain a best val acc:      86.4%, epoch: 18 *******
******* Domain a best val test acc: 83.3%, epoch: 18 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [20/50] batch [20/288] time 0.100 (0.109) data 0.001 (0.017) loss 1.8140 (1.5576) teacher_loss 1.1894 (0.9672) loss_zs_kd 0.0949 (0.0720) loss_oracle 0.3214 (0.3338) acc 65.6250 (73.7500) lr 1.4258e-03 eta 0:16:09
epoch [20/50] batch [40/288] time 0.076 (0.102) data 0.000 (0.009) loss 1.3625 (1.5850) teacher_loss 0.7938 (0.9947) loss_zs_kd 0.0890 (0.0775) loss_oracle 0.3759 (0.3298) acc 75.0000 (73.3594) lr 1.4258e-03 eta 0:15:02
epoch [20/50] batch [60/288] time 0.085 (0.096) data 0.000 (0.006) loss 1.9064 (1.5886) teacher_loss 1.3470 (1.0039) loss_zs_kd 0.0585 (0.0781) loss_oracle 0.3054 (0.3290) acc 65.6250 (73.4375) lr 1.4258e-03 eta 0:14:09
epoch [20/50] batch [80/288] time 0.085 (0.093) data 0.000 (0.004) loss 1.9131 (1.6001) teacher_loss 1.2939 (1.0088) loss_zs_kd 0.0561 (0.0770) loss_oracle 0.3734 (0.3302) acc 65.6250 (72.9297) lr 1.4258e-03 eta 0:13:42
epoch [20/50] batch [100/288] time 0.084 (0.091) data 0.000 (0.004) loss 1.6662 (1.6156) teacher_loss 1.1641 (1.0297) loss_zs_kd 0.0462 (0.0755) loss_oracle 0.2979 (0.3278) acc 65.6250 (72.3125) lr 1.4258e-03 eta 0:13:27
epoch [20/50] batch [120/288] time 0.083 (0.090) data 0.000 (0.003) loss 1.8515 (1.6213) teacher_loss 1.0636 (1.0367) loss_zs_kd 0.0702 (0.0756) loss_oracle 0.3550 (0.3282) acc 71.8750 (72.1354) lr 1.4258e-03 eta 0:13:15
epoch [20/50] batch [140/288] time 0.086 (0.090) data 0.000 (0.003) loss 1.1820 (1.6122) teacher_loss 0.6305 (1.0316) loss_zs_kd 0.0547 (0.0742) loss_oracle 0.2971 (0.3283) acc 84.3750 (72.4554) lr 1.4258e-03 eta 0:13:08
epoch [20/50] batch [160/288] time 0.079 (0.089) data 0.000 (0.002) loss 1.9067 (1.6129) teacher_loss 1.2711 (1.0327) loss_zs_kd 0.0697 (0.0740) loss_oracle 0.3565 (0.3303) acc 65.6250 (72.5000) lr 1.4258e-03 eta 0:12:58
epoch [20/50] batch [180/288] time 0.066 (0.087) data 0.000 (0.002) loss 1.3887 (1.6009) teacher_loss 0.7474 (1.0189) loss_zs_kd 0.0644 (0.0746) loss_oracle 0.3217 (0.3310) acc 75.0000 (72.9167) lr 1.4258e-03 eta 0:12:44
epoch [20/50] batch [200/288] time 0.086 (0.087) data 0.000 (0.002) loss 1.8842 (1.6034) teacher_loss 1.3345 (1.0209) loss_zs_kd 0.0757 (0.0745) loss_oracle 0.4168 (0.3311) acc 62.5000 (72.7500) lr 1.4258e-03 eta 0:12:37
epoch [20/50] batch [220/288] time 0.088 (0.087) data 0.000 (0.002) loss 1.3192 (1.6005) teacher_loss 0.9484 (1.0173) loss_zs_kd 0.0737 (0.0744) loss_oracle 0.3363 (0.3324) acc 75.0000 (72.8835) lr 1.4258e-03 eta 0:12:35
epoch [20/50] batch [240/288] time 0.087 (0.087) data 0.000 (0.002) loss 2.0368 (1.6006) teacher_loss 1.5706 (1.0190) loss_zs_kd 0.0692 (0.0743) loss_oracle 0.2847 (0.3323) acc 59.3750 (72.7865) lr 1.4258e-03 eta 0:12:33
epoch [20/50] batch [260/288] time 0.085 (0.087) data 0.000 (0.002) loss 1.4083 (1.5980) teacher_loss 0.8044 (1.0156) loss_zs_kd 0.0750 (0.0744) loss_oracle 0.3173 (0.3314) acc 78.1250 (72.9567) lr 1.4258e-03 eta 0:12:31
epoch [20/50] batch [280/288] time 0.085 (0.087) data 0.000 (0.001) loss 1.2932 (1.5947) teacher_loss 0.5664 (1.0124) loss_zs_kd 0.0882 (0.0742) loss_oracle 0.3336 (0.3311) acc 84.3750 (73.0580) lr 1.4258e-03 eta 0:12:29
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,396
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,019
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.5%
******* Domain a best val acc:      86.4%, epoch: 18 *******
******* Domain a best val test acc: 83.3%, epoch: 18 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [21/50] batch [20/288] time 0.081 (0.101) data 0.000 (0.018) loss 1.2423 (1.5461) teacher_loss 0.7656 (0.9911) loss_zs_kd 0.0673 (0.0822) loss_oracle 0.3543 (0.3343) acc 78.1250 (73.2812) lr 1.3681e-03 eta 0:14:26
epoch [21/50] batch [40/288] time 0.084 (0.092) data 0.000 (0.009) loss 1.7258 (1.5571) teacher_loss 0.9801 (0.9931) loss_zs_kd 0.0777 (0.0788) loss_oracle 0.3349 (0.3367) acc 68.7500 (73.2031) lr 1.3681e-03 eta 0:13:10
epoch [21/50] batch [60/288] time 0.077 (0.096) data 0.000 (0.006) loss 1.2884 (1.5772) teacher_loss 0.7242 (1.0133) loss_zs_kd 0.0875 (0.0754) loss_oracle 0.3012 (0.3366) acc 78.1250 (73.2812) lr 1.3681e-03 eta 0:13:39
epoch [21/50] batch [80/288] time 0.081 (0.091) data 0.000 (0.005) loss 1.3840 (1.5563) teacher_loss 0.7382 (0.9929) loss_zs_kd 0.0944 (0.0756) loss_oracle 0.3186 (0.3355) acc 84.3750 (74.0234) lr 1.3681e-03 eta 0:13:00
epoch [21/50] batch [100/288] time 0.087 (0.090) data 0.000 (0.004) loss 1.4641 (1.5679) teacher_loss 0.8892 (1.0061) loss_zs_kd 0.0936 (0.0734) loss_oracle 0.2854 (0.3347) acc 75.0000 (73.5625) lr 1.3681e-03 eta 0:12:47
epoch [21/50] batch [120/288] time 0.086 (0.089) data 0.000 (0.003) loss 1.5107 (1.5721) teacher_loss 0.8835 (1.0100) loss_zs_kd 0.0420 (0.0731) loss_oracle 0.3532 (0.3355) acc 84.3750 (73.3073) lr 1.3681e-03 eta 0:12:37
epoch [21/50] batch [140/288] time 0.071 (0.088) data 0.000 (0.003) loss 2.0896 (1.5770) teacher_loss 1.5665 (1.0138) loss_zs_kd 0.0672 (0.0731) loss_oracle 0.3257 (0.3349) acc 59.3750 (73.1920) lr 1.3681e-03 eta 0:12:28
epoch [21/50] batch [160/288] time 0.085 (0.087) data 0.000 (0.002) loss 1.5418 (1.5852) teacher_loss 1.0501 (1.0167) loss_zs_kd 0.0878 (0.0744) loss_oracle 0.3192 (0.3350) acc 71.8750 (73.1250) lr 1.3681e-03 eta 0:12:21
epoch [21/50] batch [180/288] time 0.090 (0.087) data 0.000 (0.002) loss 1.9382 (1.5802) teacher_loss 1.2609 (1.0097) loss_zs_kd 0.0751 (0.0765) loss_oracle 0.3429 (0.3342) acc 65.6250 (73.2465) lr 1.3681e-03 eta 0:12:18
epoch [21/50] batch [200/288] time 0.086 (0.087) data 0.000 (0.002) loss 1.9760 (1.5865) teacher_loss 1.5340 (1.0138) loss_zs_kd 0.1023 (0.0767) loss_oracle 0.3183 (0.3343) acc 62.5000 (73.0781) lr 1.3681e-03 eta 0:12:14
epoch [21/50] batch [220/288] time 0.085 (0.087) data 0.000 (0.002) loss 1.2023 (1.5866) teacher_loss 0.7517 (1.0132) loss_zs_kd 0.0568 (0.0763) loss_oracle 0.3563 (0.3344) acc 87.5000 (73.2670) lr 1.3681e-03 eta 0:12:10
epoch [21/50] batch [240/288] time 0.086 (0.087) data 0.000 (0.002) loss 1.7656 (1.5850) teacher_loss 1.1323 (1.0093) loss_zs_kd 0.0645 (0.0756) loss_oracle 0.3477 (0.3350) acc 71.8750 (73.4115) lr 1.3681e-03 eta 0:12:07
epoch [21/50] batch [260/288] time 0.084 (0.086) data 0.000 (0.002) loss 1.0156 (1.5849) teacher_loss 0.5018 (1.0086) loss_zs_kd 0.0748 (0.0750) loss_oracle 0.3310 (0.3349) acc 90.6250 (73.5216) lr 1.3681e-03 eta 0:12:03
epoch [21/50] batch [280/288] time 0.085 (0.086) data 0.000 (0.001) loss 1.7387 (1.5790) teacher_loss 1.0697 (1.0021) loss_zs_kd 0.0685 (0.0747) loss_oracle 0.3282 (0.3344) acc 71.8750 (73.6272) lr 1.3681e-03 eta 0:12:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,398
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.9%
******* Domain a best val acc:      86.4%, epoch: 18 *******
******* Domain a best val test acc: 83.3%, epoch: 18 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [22/50] batch [20/288] time 0.082 (0.096) data 0.000 (0.011) loss 1.4947 (1.7205) teacher_loss 0.8652 (1.1217) loss_zs_kd 0.0593 (0.0785) loss_oracle 0.3269 (0.3314) acc 75.0000 (70.4688) lr 1.3090e-03 eta 0:13:20
epoch [22/50] batch [40/288] time 0.076 (0.088) data 0.000 (0.006) loss 1.3264 (1.6254) teacher_loss 0.7333 (1.0365) loss_zs_kd 0.0589 (0.0747) loss_oracle 0.3104 (0.3338) acc 78.1250 (72.8125) lr 1.3090e-03 eta 0:12:13
epoch [22/50] batch [60/288] time 0.083 (0.086) data 0.000 (0.004) loss 1.2203 (1.5536) teacher_loss 0.6974 (0.9722) loss_zs_kd 0.0457 (0.0756) loss_oracle 0.3155 (0.3331) acc 81.2500 (73.9583) lr 1.3090e-03 eta 0:11:53
epoch [22/50] batch [80/288] time 0.082 (0.085) data 0.000 (0.003) loss 1.5186 (1.5538) teacher_loss 0.8949 (0.9763) loss_zs_kd 0.0968 (0.0743) loss_oracle 0.3182 (0.3320) acc 81.2500 (73.7500) lr 1.3090e-03 eta 0:11:44
epoch [22/50] batch [100/288] time 0.084 (0.085) data 0.000 (0.002) loss 1.5577 (1.5694) teacher_loss 0.8901 (0.9932) loss_zs_kd 0.0955 (0.0742) loss_oracle 0.3132 (0.3311) acc 75.0000 (73.3125) lr 1.3090e-03 eta 0:11:43
epoch [22/50] batch [120/288] time 0.079 (0.085) data 0.000 (0.002) loss 1.7754 (1.5783) teacher_loss 1.0403 (0.9981) loss_zs_kd 0.0790 (0.0759) loss_oracle 0.3311 (0.3304) acc 75.0000 (73.3594) lr 1.3090e-03 eta 0:11:39
epoch [22/50] batch [140/288] time 0.086 (0.085) data 0.000 (0.002) loss 1.2145 (1.5685) teacher_loss 0.6675 (0.9950) loss_zs_kd 0.0981 (0.0763) loss_oracle 0.3351 (0.3303) acc 84.3750 (73.5268) lr 1.3090e-03 eta 0:11:38
epoch [22/50] batch [160/288] time 0.085 (0.085) data 0.000 (0.002) loss 1.7398 (1.5791) teacher_loss 1.2377 (1.0032) loss_zs_kd 0.0789 (0.0765) loss_oracle 0.3210 (0.3304) acc 62.5000 (73.4570) lr 1.3090e-03 eta 0:11:39
epoch [22/50] batch [180/288] time 0.085 (0.085) data 0.000 (0.001) loss 1.5006 (1.5876) teacher_loss 0.9649 (1.0111) loss_zs_kd 0.0600 (0.0764) loss_oracle 0.3312 (0.3308) acc 75.0000 (73.2292) lr 1.3090e-03 eta 0:11:37
epoch [22/50] batch [200/288] time 0.084 (0.085) data 0.001 (0.001) loss 1.7292 (1.5881) teacher_loss 1.1492 (1.0139) loss_zs_kd 0.0613 (0.0763) loss_oracle 0.3579 (0.3307) acc 65.6250 (73.2500) lr 1.3090e-03 eta 0:11:35
epoch [22/50] batch [220/288] time 0.084 (0.085) data 0.000 (0.001) loss 1.2935 (1.5892) teacher_loss 0.6646 (1.0142) loss_zs_kd 0.0816 (0.0765) loss_oracle 0.3315 (0.3307) acc 81.2500 (73.2244) lr 1.3090e-03 eta 0:11:33
epoch [22/50] batch [240/288] time 0.084 (0.085) data 0.000 (0.001) loss 1.8977 (1.6009) teacher_loss 1.2041 (1.0256) loss_zs_kd 0.0639 (0.0762) loss_oracle 0.3310 (0.3306) acc 62.5000 (72.9688) lr 1.3090e-03 eta 0:11:31
epoch [22/50] batch [260/288] time 0.084 (0.085) data 0.000 (0.001) loss 1.3676 (1.5961) teacher_loss 0.8922 (1.0186) loss_zs_kd 0.0898 (0.0765) loss_oracle 0.3181 (0.3304) acc 78.1250 (73.0649) lr 1.3090e-03 eta 0:11:29
epoch [22/50] batch [280/288] time 0.087 (0.085) data 0.000 (0.001) loss 1.6379 (1.5980) teacher_loss 1.0511 (1.0223) loss_zs_kd 0.0943 (0.0758) loss_oracle 0.3345 (0.3300) acc 68.7500 (72.9911) lr 1.3090e-03 eta 0:11:27
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,397
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,019
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.4%
******* Domain a best val acc:      86.4%, epoch: 18 *******
******* Domain a best val test acc: 83.3%, epoch: 18 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [23/50] batch [20/288] time 0.080 (0.100) data 0.000 (0.013) loss 1.4136 (1.5959) teacher_loss 0.7954 (0.9835) loss_zs_kd 0.0464 (0.0763) loss_oracle 0.3505 (0.3310) acc 75.0000 (74.2188) lr 1.2487e-03 eta 0:13:21
epoch [23/50] batch [40/288] time 0.080 (0.091) data 0.000 (0.007) loss 1.2270 (1.5852) teacher_loss 0.4117 (0.9798) loss_zs_kd 0.0765 (0.0789) loss_oracle 0.4034 (0.3246) acc 93.7500 (73.7500) lr 1.2487e-03 eta 0:12:08
epoch [23/50] batch [60/288] time 0.081 (0.088) data 0.000 (0.005) loss 1.2485 (1.5597) teacher_loss 0.7248 (0.9658) loss_zs_kd 0.0783 (0.0765) loss_oracle 0.3043 (0.3248) acc 81.2500 (74.6875) lr 1.2487e-03 eta 0:11:42
epoch [23/50] batch [80/288] time 0.082 (0.087) data 0.000 (0.003) loss 1.7885 (1.5586) teacher_loss 1.1003 (0.9666) loss_zs_kd 0.1042 (0.0751) loss_oracle 0.3455 (0.3257) acc 78.1250 (74.7266) lr 1.2487e-03 eta 0:11:31
epoch [23/50] batch [100/288] time 0.087 (0.086) data 0.000 (0.003) loss 1.1474 (1.5676) teacher_loss 0.7715 (0.9764) loss_zs_kd 0.0787 (0.0747) loss_oracle 0.3188 (0.3265) acc 78.1250 (74.0625) lr 1.2487e-03 eta 0:11:23
epoch [23/50] batch [120/288] time 0.134 (0.086) data 0.001 (0.002) loss 1.4226 (1.5668) teacher_loss 0.8839 (0.9812) loss_zs_kd 0.0703 (0.0749) loss_oracle 0.3030 (0.3250) acc 78.1250 (74.0885) lr 1.2487e-03 eta 0:11:23
epoch [23/50] batch [140/288] time 0.085 (0.087) data 0.000 (0.002) loss 2.1981 (1.5889) teacher_loss 1.4268 (1.0006) loss_zs_kd 0.0648 (0.0751) loss_oracle 0.3128 (0.3261) acc 65.6250 (73.4152) lr 1.2487e-03 eta 0:11:28
epoch [23/50] batch [160/288] time 0.084 (0.086) data 0.000 (0.002) loss 1.4158 (1.5830) teacher_loss 0.7712 (0.9945) loss_zs_kd 0.0811 (0.0771) loss_oracle 0.3193 (0.3266) acc 78.1250 (73.6719) lr 1.2487e-03 eta 0:11:21
epoch [23/50] batch [180/288] time 0.087 (0.086) data 0.000 (0.002) loss 1.8165 (1.5852) teacher_loss 1.3068 (0.9966) loss_zs_kd 0.0811 (0.0771) loss_oracle 0.3129 (0.3264) acc 65.6250 (73.5069) lr 1.2487e-03 eta 0:11:16
epoch [23/50] batch [200/288] time 0.081 (0.085) data 0.000 (0.002) loss 1.4648 (1.5902) teacher_loss 0.9342 (1.0023) loss_zs_kd 0.0598 (0.0768) loss_oracle 0.3155 (0.3254) acc 75.0000 (73.4219) lr 1.2487e-03 eta 0:11:11
epoch [23/50] batch [220/288] time 0.087 (0.085) data 0.000 (0.001) loss 1.5692 (1.5803) teacher_loss 0.9695 (0.9929) loss_zs_kd 0.0862 (0.0768) loss_oracle 0.3070 (0.3250) acc 71.8750 (73.8636) lr 1.2487e-03 eta 0:11:07
epoch [23/50] batch [240/288] time 0.081 (0.085) data 0.000 (0.001) loss 1.5319 (1.5724) teacher_loss 1.0053 (0.9875) loss_zs_kd 0.0652 (0.0766) loss_oracle 0.3249 (0.3258) acc 75.0000 (73.9714) lr 1.2487e-03 eta 0:11:04
epoch [23/50] batch [260/288] time 0.087 (0.085) data 0.000 (0.001) loss 1.4434 (1.5751) teacher_loss 1.0497 (0.9919) loss_zs_kd 0.0491 (0.0766) loss_oracle 0.3051 (0.3257) acc 68.7500 (73.7981) lr 1.2487e-03 eta 0:11:00
epoch [23/50] batch [280/288] time 0.072 (0.084) data 0.000 (0.001) loss 2.0106 (1.5710) teacher_loss 1.3473 (0.9903) loss_zs_kd 0.0790 (0.0764) loss_oracle 0.3306 (0.3260) acc 65.6250 (73.7835) lr 1.2487e-03 eta 0:10:55
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,400
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,014
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.6%
******* Domain a best val acc:      86.4%, epoch: 18 *******
******* Domain a best val test acc: 83.3%, epoch: 18 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [24/50] batch [20/288] time 0.082 (0.103) data 0.000 (0.014) loss 1.6368 (1.5200) teacher_loss 0.9100 (0.9453) loss_zs_kd 0.1113 (0.0740) loss_oracle 0.3230 (0.3264) acc 81.2500 (75.0000) lr 1.1874e-03 eta 0:13:16
epoch [24/50] batch [40/288] time 0.082 (0.093) data 0.000 (0.007) loss 1.3067 (1.5551) teacher_loss 0.8532 (0.9949) loss_zs_kd 0.0568 (0.0729) loss_oracle 0.3230 (0.3280) acc 75.0000 (73.6719) lr 1.1874e-03 eta 0:12:00
epoch [24/50] batch [60/288] time 0.081 (0.089) data 0.000 (0.005) loss 1.5782 (1.5743) teacher_loss 0.9649 (1.0067) loss_zs_kd 0.0991 (0.0753) loss_oracle 0.3070 (0.3238) acc 71.8750 (73.3854) lr 1.1874e-03 eta 0:11:29
epoch [24/50] batch [80/288] time 0.084 (0.088) data 0.000 (0.004) loss 1.7697 (1.5980) teacher_loss 1.0826 (1.0237) loss_zs_kd 0.0621 (0.0726) loss_oracle 0.3388 (0.3244) acc 71.8750 (72.9297) lr 1.1874e-03 eta 0:11:16
epoch [24/50] batch [100/288] time 0.085 (0.087) data 0.000 (0.003) loss 1.4268 (1.5771) teacher_loss 0.7339 (0.9990) loss_zs_kd 0.0787 (0.0733) loss_oracle 0.3983 (0.3252) acc 81.2500 (73.4062) lr 1.1874e-03 eta 0:11:11
epoch [24/50] batch [120/288] time 0.085 (0.087) data 0.000 (0.003) loss 1.2864 (1.5684) teacher_loss 0.7596 (0.9903) loss_zs_kd 0.0801 (0.0737) loss_oracle 0.3431 (0.3261) acc 78.1250 (73.8542) lr 1.1874e-03 eta 0:11:06
epoch [24/50] batch [140/288] time 0.085 (0.087) data 0.000 (0.002) loss 1.8072 (1.5728) teacher_loss 1.2713 (0.9965) loss_zs_kd 0.0717 (0.0747) loss_oracle 0.3512 (0.3270) acc 65.6250 (73.6830) lr 1.1874e-03 eta 0:11:02
epoch [24/50] batch [160/288] time 0.154 (0.088) data 0.000 (0.002) loss 1.6143 (1.5774) teacher_loss 1.0921 (1.0024) loss_zs_kd 0.0869 (0.0751) loss_oracle 0.3237 (0.3276) acc 71.8750 (73.4961) lr 1.1874e-03 eta 0:11:07
epoch [24/50] batch [180/288] time 0.084 (0.088) data 0.000 (0.002) loss 1.7028 (1.5788) teacher_loss 1.0301 (1.0047) loss_zs_kd 0.1227 (0.0750) loss_oracle 0.3623 (0.3272) acc 65.6250 (73.5417) lr 1.1874e-03 eta 0:11:11
epoch [24/50] batch [200/288] time 0.086 (0.088) data 0.000 (0.002) loss 1.4038 (1.5774) teacher_loss 0.8358 (1.0062) loss_zs_kd 0.0873 (0.0746) loss_oracle 0.3715 (0.3265) acc 84.3750 (73.4062) lr 1.1874e-03 eta 0:11:06
epoch [24/50] batch [220/288] time 0.085 (0.088) data 0.000 (0.002) loss 1.4896 (1.5738) teacher_loss 0.9760 (1.0033) loss_zs_kd 0.0372 (0.0741) loss_oracle 0.3182 (0.3268) acc 71.8750 (73.4659) lr 1.1874e-03 eta 0:11:03
epoch [24/50] batch [240/288] time 0.089 (0.088) data 0.000 (0.001) loss 1.5604 (1.5763) teacher_loss 1.0896 (1.0053) loss_zs_kd 0.0674 (0.0746) loss_oracle 0.3347 (0.3264) acc 65.6250 (73.2161) lr 1.1874e-03 eta 0:10:59
epoch [24/50] batch [260/288] time 0.083 (0.087) data 0.000 (0.001) loss 1.6816 (1.5867) teacher_loss 1.0172 (1.0157) loss_zs_kd 0.1018 (0.0750) loss_oracle 0.3094 (0.3264) acc 68.7500 (72.9087) lr 1.1874e-03 eta 0:10:57
epoch [24/50] batch [280/288] time 0.085 (0.087) data 0.000 (0.001) loss 1.2321 (1.5881) teacher_loss 0.4791 (1.0135) loss_zs_kd 0.0904 (0.0756) loss_oracle 0.2980 (0.3272) acc 87.5000 (72.9576) lr 1.1874e-03 eta 0:10:54
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,400
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.7%
******* Domain a best val acc:      86.4%, epoch: 18 *******
******* Domain a best val test acc: 83.3%, epoch: 18 *******
******* Domain a best test acc:     83.6%, epoch: 7 *******
epoch [25/50] batch [20/288] time 0.089 (0.106) data 0.000 (0.015) loss 1.4537 (1.6286) teacher_loss 0.8459 (1.0371) loss_zs_kd 0.0655 (0.0786) loss_oracle 0.2965 (0.3282) acc 81.2500 (71.7188) lr 1.1253e-03 eta 0:13:11
epoch [25/50] batch [40/288] time 0.082 (0.095) data 0.000 (0.008) loss 1.9434 (1.6619) teacher_loss 1.2920 (1.0703) loss_zs_kd 0.0779 (0.0758) loss_oracle 0.3270 (0.3288) acc 62.5000 (70.8594) lr 1.1253e-03 eta 0:11:44
epoch [25/50] batch [60/288] time 0.072 (0.088) data 0.000 (0.005) loss 1.4164 (1.6108) teacher_loss 0.7279 (1.0174) loss_zs_kd 0.0859 (0.0756) loss_oracle 0.3036 (0.3267) acc 68.7500 (72.2396) lr 1.1253e-03 eta 0:10:57
epoch [25/50] batch [80/288] time 0.070 (0.087) data 0.000 (0.004) loss 1.4249 (1.6104) teacher_loss 0.8592 (1.0159) loss_zs_kd 0.0708 (0.0770) loss_oracle 0.3079 (0.3275) acc 75.0000 (72.0312) lr 1.1253e-03 eta 0:10:47
epoch [25/50] batch [100/288] time 0.087 (0.087) data 0.000 (0.003) loss 1.1711 (1.5970) teacher_loss 0.7101 (1.0029) loss_zs_kd 0.0715 (0.0770) loss_oracle 0.3135 (0.3281) acc 75.0000 (72.6562) lr 1.1253e-03 eta 0:10:46
epoch [25/50] batch [120/288] time 0.085 (0.087) data 0.000 (0.003) loss 1.6479 (1.5902) teacher_loss 1.0961 (0.9980) loss_zs_kd 0.0973 (0.0779) loss_oracle 0.3066 (0.3267) acc 75.0000 (72.9167) lr 1.1253e-03 eta 0:10:43
epoch [25/50] batch [140/288] time 0.086 (0.087) data 0.000 (0.002) loss 1.4505 (1.5911) teacher_loss 0.8655 (0.9996) loss_zs_kd 0.0715 (0.0774) loss_oracle 0.3811 (0.3273) acc 84.3750 (73.2366) lr 1.1253e-03 eta 0:10:41
epoch [25/50] batch [160/288] time 0.086 (0.087) data 0.000 (0.002) loss 0.9991 (1.5819) teacher_loss 0.4497 (0.9875) loss_zs_kd 0.0539 (0.0755) loss_oracle 0.3438 (0.3285) acc 84.3750 (73.4180) lr 1.1253e-03 eta 0:10:37
epoch [25/50] batch [180/288] time 0.084 (0.087) data 0.000 (0.002) loss 1.4427 (1.5833) teacher_loss 0.6797 (0.9852) loss_zs_kd 0.0722 (0.0760) loss_oracle 0.3212 (0.3278) acc 78.1250 (73.4722) lr 1.1253e-03 eta 0:10:35
epoch [25/50] batch [200/288] time 0.087 (0.089) data 0.000 (0.002) loss 1.5089 (1.5818) teacher_loss 0.9070 (0.9853) loss_zs_kd 0.0839 (0.0760) loss_oracle 0.3134 (0.3266) acc 75.0000 (73.5469) lr 1.1253e-03 eta 0:10:45
epoch [25/50] batch [220/288] time 0.086 (0.088) data 0.000 (0.002) loss 1.7883 (1.5898) teacher_loss 1.1449 (0.9926) loss_zs_kd 0.0769 (0.0766) loss_oracle 0.3377 (0.3268) acc 68.7500 (73.3665) lr 1.1253e-03 eta 0:10:42
epoch [25/50] batch [240/288] time 0.087 (0.088) data 0.000 (0.002) loss 2.0846 (1.5908) teacher_loss 1.5816 (0.9934) loss_zs_kd 0.0852 (0.0759) loss_oracle 0.3124 (0.3265) acc 59.3750 (73.3594) lr 1.1253e-03 eta 0:10:38
epoch [25/50] batch [260/288] time 0.087 (0.088) data 0.000 (0.001) loss 1.8655 (1.5984) teacher_loss 1.0237 (1.0017) loss_zs_kd 0.1059 (0.0757) loss_oracle 0.4352 (0.3270) acc 75.0000 (73.1611) lr 1.1253e-03 eta 0:10:35
epoch [25/50] batch [280/288] time 0.087 (0.088) data 0.000 (0.001) loss 1.1971 (1.5993) teacher_loss 0.5834 (1.0042) loss_zs_kd 0.0564 (0.0759) loss_oracle 0.3275 (0.3267) acc 84.3750 (73.0246) lr 1.1253e-03 eta 0:10:32
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,405
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,033
* accuracy: 83.8%
* error: 16.2%
* macro_f1: 80.1%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.4%, epoch: 25 *******
******* Domain a best val test acc: 83.8%, epoch: 25 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [26/50] batch [20/288] time 0.082 (0.106) data 0.000 (0.020) loss 2.1124 (1.6110) teacher_loss 1.3881 (1.0536) loss_zs_kd 0.1000 (0.0780) loss_oracle 0.2914 (0.3220) acc 68.7500 (71.8750) lr 1.0628e-03 eta 0:12:41
epoch [26/50] batch [40/288] time 0.082 (0.094) data 0.000 (0.010) loss 1.9778 (1.6086) teacher_loss 1.2405 (1.0338) loss_zs_kd 0.1017 (0.0760) loss_oracle 0.3464 (0.3245) acc 71.8750 (72.5000) lr 1.0628e-03 eta 0:11:15
epoch [26/50] batch [60/288] time 0.086 (0.090) data 0.000 (0.007) loss 1.6007 (1.6300) teacher_loss 1.0398 (1.0521) loss_zs_kd 0.1018 (0.0721) loss_oracle 0.3525 (0.3253) acc 75.0000 (72.1875) lr 1.0628e-03 eta 0:10:42
epoch [26/50] batch [80/288] time 0.087 (0.089) data 0.000 (0.005) loss 1.8070 (1.6116) teacher_loss 1.1883 (1.0251) loss_zs_kd 0.1250 (0.0738) loss_oracle 0.2982 (0.3265) acc 62.5000 (72.6562) lr 1.0628e-03 eta 0:10:33
epoch [26/50] batch [100/288] time 0.087 (0.088) data 0.000 (0.004) loss 1.4983 (1.5929) teacher_loss 0.9454 (1.0088) loss_zs_kd 0.0772 (0.0743) loss_oracle 0.3037 (0.3277) acc 71.8750 (72.8750) lr 1.0628e-03 eta 0:10:28
epoch [26/50] batch [120/288] time 0.086 (0.088) data 0.000 (0.004) loss 1.1066 (1.6090) teacher_loss 0.5835 (1.0229) loss_zs_kd 0.0621 (0.0742) loss_oracle 0.2903 (0.3276) acc 78.1250 (72.7083) lr 1.0628e-03 eta 0:10:23
epoch [26/50] batch [140/288] time 0.084 (0.088) data 0.000 (0.003) loss 1.5783 (1.5997) teacher_loss 0.8151 (1.0192) loss_zs_kd 0.0848 (0.0747) loss_oracle 0.3204 (0.3274) acc 81.2500 (72.7902) lr 1.0628e-03 eta 0:10:20
epoch [26/50] batch [160/288] time 0.086 (0.088) data 0.001 (0.003) loss 2.0218 (1.5964) teacher_loss 1.2036 (1.0171) loss_zs_kd 0.1240 (0.0751) loss_oracle 0.3446 (0.3281) acc 68.7500 (72.6367) lr 1.0628e-03 eta 0:10:16
epoch [26/50] batch [180/288] time 0.086 (0.087) data 0.000 (0.003) loss 1.2862 (1.5978) teacher_loss 0.7225 (1.0167) loss_zs_kd 0.0705 (0.0746) loss_oracle 0.3206 (0.3279) acc 81.2500 (72.6736) lr 1.0628e-03 eta 0:10:13
epoch [26/50] batch [200/288] time 0.121 (0.088) data 0.000 (0.002) loss 2.0348 (1.6025) teacher_loss 1.3575 (1.0208) loss_zs_kd 0.0703 (0.0746) loss_oracle 0.3195 (0.3278) acc 65.6250 (72.5625) lr 1.0628e-03 eta 0:10:13
epoch [26/50] batch [220/288] time 0.087 (0.088) data 0.000 (0.002) loss 1.6699 (1.6070) teacher_loss 1.1830 (1.0221) loss_zs_kd 0.0564 (0.0746) loss_oracle 0.3111 (0.3284) acc 50.0000 (72.4716) lr 1.0628e-03 eta 0:10:15
epoch [26/50] batch [240/288] time 0.087 (0.088) data 0.001 (0.002) loss 1.2871 (1.6016) teacher_loss 0.6996 (1.0172) loss_zs_kd 0.0695 (0.0739) loss_oracle 0.3278 (0.3281) acc 84.3750 (72.6432) lr 1.0628e-03 eta 0:10:13
epoch [26/50] batch [260/288] time 0.089 (0.088) data 0.001 (0.002) loss 1.9195 (1.5994) teacher_loss 1.3258 (1.0191) loss_zs_kd 0.0869 (0.0739) loss_oracle 0.3210 (0.3283) acc 71.8750 (72.6803) lr 1.0628e-03 eta 0:10:11
epoch [26/50] batch [280/288] time 0.088 (0.088) data 0.000 (0.002) loss 1.6159 (1.5957) teacher_loss 1.1405 (1.0176) loss_zs_kd 0.1172 (0.0745) loss_oracle 0.3531 (0.3281) acc 65.6250 (72.7790) lr 1.0628e-03 eta 0:10:09
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,400
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,016
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.4%
******* Domain a best val acc:      86.4%, epoch: 25 *******
******* Domain a best val test acc: 83.8%, epoch: 25 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [27/50] batch [20/288] time 0.075 (0.096) data 0.000 (0.016) loss 1.4455 (1.5694) teacher_loss 0.9752 (1.0027) loss_zs_kd 0.0700 (0.0776) loss_oracle 0.3065 (0.3277) acc 84.3750 (73.4375) lr 1.0000e-03 eta 0:10:58
epoch [27/50] batch [40/288] time 0.081 (0.089) data 0.000 (0.008) loss 1.1660 (1.5578) teacher_loss 0.7179 (0.9993) loss_zs_kd 0.0584 (0.0752) loss_oracle 0.3231 (0.3269) acc 78.1250 (73.5938) lr 1.0000e-03 eta 0:10:10
epoch [27/50] batch [60/288] time 0.084 (0.087) data 0.000 (0.005) loss 1.6326 (1.5846) teacher_loss 1.0401 (1.0034) loss_zs_kd 0.1277 (0.0770) loss_oracle 0.3291 (0.3273) acc 68.7500 (73.6458) lr 1.0000e-03 eta 0:09:56
epoch [27/50] batch [80/288] time 0.082 (0.086) data 0.000 (0.004) loss 1.3667 (1.5620) teacher_loss 0.8274 (0.9780) loss_zs_kd 0.0541 (0.0749) loss_oracle 0.3360 (0.3288) acc 75.0000 (74.2969) lr 1.0000e-03 eta 0:09:48
epoch [27/50] batch [100/288] time 0.082 (0.085) data 0.000 (0.003) loss 1.5551 (1.5540) teacher_loss 0.9199 (0.9700) loss_zs_kd 0.1084 (0.0749) loss_oracle 0.3281 (0.3288) acc 78.1250 (74.1250) lr 1.0000e-03 eta 0:09:41
epoch [27/50] batch [120/288] time 0.083 (0.085) data 0.000 (0.003) loss 1.6956 (1.5674) teacher_loss 1.1092 (0.9763) loss_zs_kd 0.0682 (0.0751) loss_oracle 0.3223 (0.3283) acc 68.7500 (73.8542) lr 1.0000e-03 eta 0:09:36
epoch [27/50] batch [140/288] time 0.081 (0.084) data 0.000 (0.002) loss 1.6435 (1.5580) teacher_loss 1.0527 (0.9705) loss_zs_kd 0.0666 (0.0755) loss_oracle 0.3336 (0.3279) acc 75.0000 (74.0848) lr 1.0000e-03 eta 0:09:31
epoch [27/50] batch [160/288] time 0.075 (0.084) data 0.000 (0.002) loss 1.7274 (1.5766) teacher_loss 1.1256 (0.9873) loss_zs_kd 0.0883 (0.0751) loss_oracle 0.3212 (0.3278) acc 68.7500 (73.6914) lr 1.0000e-03 eta 0:09:25
epoch [27/50] batch [180/288] time 0.081 (0.083) data 0.000 (0.002) loss 1.3953 (1.5736) teacher_loss 0.6947 (0.9853) loss_zs_kd 0.1131 (0.0760) loss_oracle 0.3244 (0.3276) acc 81.2500 (73.6111) lr 1.0000e-03 eta 0:09:21
epoch [27/50] batch [200/288] time 0.081 (0.083) data 0.000 (0.002) loss 1.6724 (1.5647) teacher_loss 1.0568 (0.9765) loss_zs_kd 0.0791 (0.0764) loss_oracle 0.3194 (0.3274) acc 75.0000 (73.9219) lr 1.0000e-03 eta 0:09:18
epoch [27/50] batch [220/288] time 0.082 (0.083) data 0.000 (0.002) loss 1.6677 (1.5614) teacher_loss 1.2319 (0.9752) loss_zs_kd 0.0551 (0.0764) loss_oracle 0.3232 (0.3268) acc 59.3750 (73.8778) lr 1.0000e-03 eta 0:09:14
epoch [27/50] batch [240/288] time 0.111 (0.083) data 0.000 (0.002) loss 1.3918 (1.5617) teacher_loss 0.9711 (0.9770) loss_zs_kd 0.0631 (0.0766) loss_oracle 0.3378 (0.3267) acc 78.1250 (74.0234) lr 1.0000e-03 eta 0:09:16
epoch [27/50] batch [260/288] time 0.082 (0.084) data 0.000 (0.001) loss 1.2254 (1.5663) teacher_loss 0.8464 (0.9815) loss_zs_kd 0.0581 (0.0770) loss_oracle 0.2942 (0.3270) acc 68.7500 (73.9784) lr 1.0000e-03 eta 0:09:15
epoch [27/50] batch [280/288] time 0.073 (0.083) data 0.000 (0.001) loss 1.4071 (1.5746) teacher_loss 0.8344 (0.9894) loss_zs_kd 0.0805 (0.0772) loss_oracle 0.3579 (0.3274) acc 78.1250 (73.6384) lr 1.0000e-03 eta 0:09:11
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,407
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.7%
******* Domain a best val acc:      86.5%, epoch: 27 *******
******* Domain a best val test acc: 83.2%, epoch: 27 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [28/50] batch [20/288] time 0.082 (0.098) data 0.000 (0.015) loss 1.6094 (1.6196) teacher_loss 1.1027 (1.0403) loss_zs_kd 0.0806 (0.0767) loss_oracle 0.3012 (0.3187) acc 71.8750 (72.9688) lr 9.3721e-04 eta 0:10:47
epoch [28/50] batch [40/288] time 0.066 (0.088) data 0.000 (0.008) loss 1.6504 (1.6175) teacher_loss 0.9933 (1.0429) loss_zs_kd 0.0642 (0.0775) loss_oracle 0.3452 (0.3212) acc 84.3750 (72.6562) lr 9.3721e-04 eta 0:09:38
epoch [28/50] batch [60/288] time 0.085 (0.085) data 0.000 (0.005) loss 1.7083 (1.6264) teacher_loss 1.1977 (1.0535) loss_zs_kd 0.0762 (0.0764) loss_oracle 0.3243 (0.3243) acc 71.8750 (73.1250) lr 9.3721e-04 eta 0:09:15
epoch [28/50] batch [80/288] time 0.082 (0.082) data 0.000 (0.004) loss 1.7568 (1.6340) teacher_loss 1.1435 (1.0583) loss_zs_kd 0.0550 (0.0769) loss_oracle 0.3192 (0.3251) acc 68.7500 (72.2656) lr 9.3721e-04 eta 0:08:56
epoch [28/50] batch [100/288] time 0.071 (0.079) data 0.000 (0.003) loss 1.9705 (1.6299) teacher_loss 1.3025 (1.0465) loss_zs_kd 0.0658 (0.0788) loss_oracle 0.3299 (0.3250) acc 62.5000 (72.6562) lr 9.3721e-04 eta 0:08:37
epoch [28/50] batch [120/288] time 0.063 (0.077) data 0.000 (0.003) loss 1.5486 (1.6157) teacher_loss 0.9508 (1.0307) loss_zs_kd 0.0956 (0.0783) loss_oracle 0.3200 (0.3260) acc 78.1250 (72.8906) lr 9.3721e-04 eta 0:08:22
epoch [28/50] batch [140/288] time 0.069 (0.076) data 0.000 (0.002) loss 1.7074 (1.6079) teacher_loss 1.1273 (1.0273) loss_zs_kd 0.0526 (0.0768) loss_oracle 0.3150 (0.3267) acc 71.8750 (72.9911) lr 9.3721e-04 eta 0:08:12
epoch [28/50] batch [160/288] time 0.067 (0.075) data 0.000 (0.002) loss 1.9732 (1.6158) teacher_loss 1.4534 (1.0332) loss_zs_kd 0.0628 (0.0767) loss_oracle 0.2894 (0.3266) acc 62.5000 (72.5391) lr 9.3721e-04 eta 0:08:06
epoch [28/50] batch [180/288] time 0.067 (0.075) data 0.000 (0.002) loss 1.4594 (1.6153) teacher_loss 0.8032 (1.0289) loss_zs_kd 0.0924 (0.0767) loss_oracle 0.3213 (0.3269) acc 81.2500 (72.6389) lr 9.3721e-04 eta 0:08:01
epoch [28/50] batch [200/288] time 0.063 (0.074) data 0.000 (0.002) loss 1.4014 (1.6140) teacher_loss 0.7911 (1.0275) loss_zs_kd 0.0671 (0.0768) loss_oracle 0.3231 (0.3264) acc 78.1250 (72.7188) lr 9.3721e-04 eta 0:07:57
epoch [28/50] batch [220/288] time 0.081 (0.075) data 0.000 (0.002) loss 1.4597 (1.6099) teacher_loss 0.8766 (1.0258) loss_zs_kd 0.0662 (0.0768) loss_oracle 0.3067 (0.3262) acc 78.1250 (72.8267) lr 9.3721e-04 eta 0:07:57
epoch [28/50] batch [240/288] time 0.083 (0.075) data 0.000 (0.001) loss 1.0907 (1.5999) teacher_loss 0.6688 (1.0202) loss_zs_kd 0.0713 (0.0766) loss_oracle 0.3114 (0.3259) acc 78.1250 (72.9557) lr 9.3721e-04 eta 0:08:00
epoch [28/50] batch [260/288] time 0.087 (0.076) data 0.001 (0.001) loss 1.8179 (1.6029) teacher_loss 1.2832 (1.0209) loss_zs_kd 0.0651 (0.0767) loss_oracle 0.3177 (0.3261) acc 62.5000 (72.8726) lr 9.3721e-04 eta 0:08:04
epoch [28/50] batch [280/288] time 0.082 (0.077) data 0.000 (0.001) loss 1.1342 (1.5973) teacher_loss 0.5528 (1.0176) loss_zs_kd 0.0450 (0.0763) loss_oracle 0.3164 (0.3257) acc 84.3750 (73.0804) lr 9.3721e-04 eta 0:08:06
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,397
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,027
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.1%
******* Domain a best val acc:      86.5%, epoch: 27 *******
******* Domain a best val test acc: 83.2%, epoch: 27 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [29/50] batch [20/288] time 0.067 (0.083) data 0.000 (0.011) loss 1.5143 (1.5166) teacher_loss 0.9094 (0.9475) loss_zs_kd 0.0788 (0.0747) loss_oracle 0.3178 (0.3206) acc 78.1250 (73.5938) lr 8.7467e-04 eta 0:08:46
epoch [29/50] batch [40/288] time 0.082 (0.079) data 0.000 (0.006) loss 1.8533 (1.5380) teacher_loss 1.2664 (0.9631) loss_zs_kd 0.0740 (0.0738) loss_oracle 0.3284 (0.3218) acc 68.7500 (74.3750) lr 8.7467e-04 eta 0:08:18
epoch [29/50] batch [60/288] time 0.076 (0.078) data 0.000 (0.004) loss 1.2791 (1.4955) teacher_loss 0.6619 (0.9182) loss_zs_kd 0.0755 (0.0731) loss_oracle 0.3145 (0.3227) acc 84.3750 (75.3125) lr 8.7467e-04 eta 0:08:09
epoch [29/50] batch [80/288] time 0.086 (0.079) data 0.000 (0.003) loss 1.2899 (1.5271) teacher_loss 0.7933 (0.9400) loss_zs_kd 0.0694 (0.0714) loss_oracle 0.3590 (0.3234) acc 84.3750 (74.6875) lr 8.7467e-04 eta 0:08:13
epoch [29/50] batch [100/288] time 0.084 (0.080) data 0.000 (0.002) loss 1.3877 (1.5279) teacher_loss 0.9776 (0.9451) loss_zs_kd 0.0676 (0.0729) loss_oracle 0.2952 (0.3230) acc 84.3750 (74.6875) lr 8.7467e-04 eta 0:08:19
epoch [29/50] batch [120/288] time 0.086 (0.081) data 0.000 (0.002) loss 2.2646 (1.5254) teacher_loss 1.7390 (0.9471) loss_zs_kd 0.0693 (0.0740) loss_oracle 0.3466 (0.3248) acc 56.2500 (74.5052) lr 8.7467e-04 eta 0:08:23
epoch [29/50] batch [140/288] time 0.083 (0.082) data 0.000 (0.002) loss 1.7813 (1.5385) teacher_loss 1.0562 (0.9597) loss_zs_kd 0.0840 (0.0743) loss_oracle 0.3159 (0.3245) acc 75.0000 (74.3527) lr 8.7467e-04 eta 0:08:27
epoch [29/50] batch [160/288] time 0.084 (0.082) data 0.000 (0.002) loss 2.0383 (1.5389) teacher_loss 1.4604 (0.9645) loss_zs_kd 0.0716 (0.0747) loss_oracle 0.3055 (0.3252) acc 59.3750 (74.1602) lr 8.7467e-04 eta 0:08:29
epoch [29/50] batch [180/288] time 0.084 (0.083) data 0.000 (0.001) loss 1.5775 (1.5546) teacher_loss 0.9616 (0.9821) loss_zs_kd 0.0901 (0.0739) loss_oracle 0.3164 (0.3253) acc 81.2500 (73.7153) lr 8.7467e-04 eta 0:08:28
epoch [29/50] batch [200/288] time 0.088 (0.083) data 0.000 (0.001) loss 1.3168 (1.5511) teacher_loss 0.7704 (0.9752) loss_zs_kd 0.0642 (0.0737) loss_oracle 0.3272 (0.3255) acc 75.0000 (74.0000) lr 8.7467e-04 eta 0:08:28
epoch [29/50] batch [220/288] time 0.085 (0.083) data 0.000 (0.001) loss 0.9081 (1.5571) teacher_loss 0.4475 (0.9839) loss_zs_kd 0.0456 (0.0741) loss_oracle 0.3300 (0.3258) acc 87.5000 (73.7784) lr 8.7467e-04 eta 0:08:28
epoch [29/50] batch [240/288] time 0.085 (0.083) data 0.000 (0.001) loss 2.0760 (1.5621) teacher_loss 1.4408 (0.9882) loss_zs_kd 0.0856 (0.0747) loss_oracle 0.3697 (0.3247) acc 56.2500 (73.7500) lr 8.7467e-04 eta 0:08:27
epoch [29/50] batch [260/288] time 0.088 (0.083) data 0.000 (0.001) loss 1.8370 (1.5568) teacher_loss 1.2627 (0.9822) loss_zs_kd 0.0584 (0.0746) loss_oracle 0.3381 (0.3248) acc 71.8750 (73.7500) lr 8.7467e-04 eta 0:08:27
epoch [29/50] batch [280/288] time 0.088 (0.084) data 0.000 (0.001) loss 1.5727 (1.5630) teacher_loss 1.0141 (0.9888) loss_zs_kd 0.0581 (0.0746) loss_oracle 0.3211 (0.3246) acc 78.1250 (73.5826) lr 8.7467e-04 eta 0:08:26
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,394
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,019
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.7%
******* Domain a best val acc:      86.5%, epoch: 27 *******
******* Domain a best val test acc: 83.2%, epoch: 27 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [30/50] batch [20/288] time 0.092 (0.103) data 0.000 (0.015) loss 1.4851 (1.7157) teacher_loss 0.8715 (1.1106) loss_zs_kd 0.1338 (0.0871) loss_oracle 0.3215 (0.3156) acc 75.0000 (70.6250) lr 8.1262e-04 eta 0:10:23
epoch [30/50] batch [40/288] time 0.082 (0.094) data 0.000 (0.008) loss 1.3677 (1.6043) teacher_loss 0.6043 (1.0198) loss_zs_kd 0.1031 (0.0841) loss_oracle 0.3460 (0.3204) acc 81.2500 (72.1875) lr 8.1262e-04 eta 0:09:25
epoch [30/50] batch [60/288] time 0.083 (0.091) data 0.000 (0.005) loss 1.9932 (1.6312) teacher_loss 1.2569 (1.0366) loss_zs_kd 0.0965 (0.0810) loss_oracle 0.3232 (0.3197) acc 68.7500 (71.8750) lr 8.1262e-04 eta 0:09:05
epoch [30/50] batch [80/288] time 0.087 (0.090) data 0.000 (0.004) loss 1.7736 (1.5869) teacher_loss 1.1367 (0.9927) loss_zs_kd 0.0777 (0.0799) loss_oracle 0.3165 (0.3200) acc 68.7500 (73.2422) lr 8.1262e-04 eta 0:08:58
epoch [30/50] batch [100/288] time 0.087 (0.089) data 0.000 (0.003) loss 1.8650 (1.5965) teacher_loss 1.3593 (1.0049) loss_zs_kd 0.1004 (0.0784) loss_oracle 0.3131 (0.3213) acc 68.7500 (73.0000) lr 8.1262e-04 eta 0:08:51
epoch [30/50] batch [120/288] time 0.087 (0.089) data 0.000 (0.003) loss 1.5355 (1.6039) teacher_loss 0.9708 (1.0102) loss_zs_kd 0.0563 (0.0786) loss_oracle 0.3157 (0.3210) acc 75.0000 (72.9948) lr 8.1262e-04 eta 0:08:45
epoch [30/50] batch [140/288] time 0.085 (0.088) data 0.000 (0.002) loss 1.4674 (1.6159) teacher_loss 0.8006 (1.0223) loss_zs_kd 0.1041 (0.0780) loss_oracle 0.3270 (0.3218) acc 81.2500 (72.6562) lr 8.1262e-04 eta 0:08:41
epoch [30/50] batch [160/288] time 0.085 (0.088) data 0.000 (0.002) loss 1.4377 (1.6133) teacher_loss 0.7833 (1.0206) loss_zs_kd 0.0771 (0.0781) loss_oracle 0.3138 (0.3227) acc 78.1250 (72.7734) lr 8.1262e-04 eta 0:08:37
epoch [30/50] batch [180/288] time 0.089 (0.088) data 0.000 (0.002) loss 1.3448 (1.6222) teacher_loss 0.7963 (1.0262) loss_zs_kd 0.0843 (0.0784) loss_oracle 0.3442 (0.3239) acc 84.3750 (72.7604) lr 8.1262e-04 eta 0:08:34
epoch [30/50] batch [200/288] time 0.086 (0.087) data 0.000 (0.002) loss 1.1901 (1.6042) teacher_loss 0.7326 (1.0071) loss_zs_kd 0.0743 (0.0785) loss_oracle 0.3120 (0.3241) acc 71.8750 (73.2969) lr 8.1262e-04 eta 0:08:31
epoch [30/50] batch [220/288] time 0.087 (0.087) data 0.000 (0.002) loss 1.5284 (1.6039) teacher_loss 1.0251 (1.0048) loss_zs_kd 0.0980 (0.0791) loss_oracle 0.3198 (0.3237) acc 71.8750 (73.5085) lr 8.1262e-04 eta 0:08:27
epoch [30/50] batch [240/288] time 0.086 (0.087) data 0.000 (0.002) loss 1.6245 (1.6046) teacher_loss 1.1673 (1.0056) loss_zs_kd 0.0600 (0.0789) loss_oracle 0.2787 (0.3232) acc 75.0000 (73.4375) lr 8.1262e-04 eta 0:08:25
epoch [30/50] batch [260/288] time 0.085 (0.087) data 0.000 (0.001) loss 1.6407 (1.5960) teacher_loss 1.0313 (0.9992) loss_zs_kd 0.0819 (0.0790) loss_oracle 0.3357 (0.3230) acc 75.0000 (73.4615) lr 8.1262e-04 eta 0:08:22
epoch [30/50] batch [280/288] time 0.088 (0.087) data 0.000 (0.001) loss 1.3672 (1.5883) teacher_loss 0.8001 (0.9914) loss_zs_kd 0.0602 (0.0785) loss_oracle 0.3288 (0.3230) acc 75.0000 (73.6272) lr 8.1262e-04 eta 0:08:19
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,032
* accuracy: 83.7%
* error: 16.3%
* macro_f1: 80.3%
******* Domain a best val acc:      86.5%, epoch: 27 *******
******* Domain a best val test acc: 83.2%, epoch: 27 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [31/50] batch [20/288] time 0.083 (0.119) data 0.000 (0.019) loss 2.1347 (1.5973) teacher_loss 1.4453 (1.0334) loss_zs_kd 0.0733 (0.0706) loss_oracle 0.3611 (0.3236) acc 62.5000 (72.9688) lr 7.5131e-04 eta 0:11:25
epoch [31/50] batch [40/288] time 0.085 (0.102) data 0.000 (0.009) loss 1.8071 (1.5906) teacher_loss 1.3124 (1.0232) loss_zs_kd 0.0951 (0.0744) loss_oracle 0.2767 (0.3272) acc 62.5000 (73.0469) lr 7.5131e-04 eta 0:09:41
epoch [31/50] batch [60/288] time 0.084 (0.096) data 0.000 (0.006) loss 1.2831 (1.5980) teacher_loss 0.7028 (1.0277) loss_zs_kd 0.0395 (0.0746) loss_oracle 0.3544 (0.3264) acc 81.2500 (73.0729) lr 7.5131e-04 eta 0:09:05
epoch [31/50] batch [80/288] time 0.085 (0.093) data 0.000 (0.005) loss 1.9099 (1.6023) teacher_loss 1.4135 (1.0203) loss_zs_kd 0.0829 (0.0743) loss_oracle 0.3337 (0.3273) acc 65.6250 (73.0859) lr 7.5131e-04 eta 0:08:46
epoch [31/50] batch [100/288] time 0.085 (0.091) data 0.000 (0.004) loss 1.4757 (1.6010) teacher_loss 0.8320 (1.0140) loss_zs_kd 0.0862 (0.0752) loss_oracle 0.3616 (0.3258) acc 71.8750 (72.7188) lr 7.5131e-04 eta 0:08:37
epoch [31/50] batch [120/288] time 0.086 (0.090) data 0.001 (0.003) loss 1.6824 (1.5879) teacher_loss 1.1473 (1.0022) loss_zs_kd 0.0608 (0.0751) loss_oracle 0.3032 (0.3243) acc 68.7500 (72.8906) lr 7.5131e-04 eta 0:08:29
epoch [31/50] batch [140/288] time 0.085 (0.090) data 0.000 (0.003) loss 1.7370 (1.5857) teacher_loss 1.2347 (1.0026) loss_zs_kd 0.0799 (0.0755) loss_oracle 0.3153 (0.3245) acc 65.6250 (72.7902) lr 7.5131e-04 eta 0:08:23
epoch [31/50] batch [160/288] time 0.086 (0.089) data 0.000 (0.003) loss 1.4573 (1.5837) teacher_loss 1.0315 (1.0025) loss_zs_kd 0.0749 (0.0749) loss_oracle 0.3231 (0.3243) acc 71.8750 (72.9102) lr 7.5131e-04 eta 0:08:18
epoch [31/50] batch [180/288] time 0.089 (0.089) data 0.001 (0.002) loss 1.9933 (1.5916) teacher_loss 1.5183 (1.0118) loss_zs_kd 0.0928 (0.0751) loss_oracle 0.3164 (0.3234) acc 68.7500 (72.7951) lr 7.5131e-04 eta 0:08:15
epoch [31/50] batch [200/288] time 0.085 (0.088) data 0.000 (0.002) loss 1.7990 (1.5880) teacher_loss 1.2779 (1.0106) loss_zs_kd 0.1226 (0.0749) loss_oracle 0.3087 (0.3234) acc 65.6250 (72.7344) lr 7.5131e-04 eta 0:08:11
epoch [31/50] batch [220/288] time 0.084 (0.088) data 0.000 (0.002) loss 1.5545 (1.5810) teacher_loss 0.8742 (1.0051) loss_zs_kd 0.0815 (0.0745) loss_oracle 0.3097 (0.3232) acc 68.7500 (72.9261) lr 7.5131e-04 eta 0:08:08
epoch [31/50] batch [240/288] time 0.090 (0.088) data 0.000 (0.002) loss 1.6321 (1.5861) teacher_loss 0.9149 (1.0073) loss_zs_kd 0.0807 (0.0747) loss_oracle 0.3247 (0.3232) acc 78.1250 (72.8255) lr 7.5131e-04 eta 0:08:05
epoch [31/50] batch [260/288] time 0.085 (0.088) data 0.000 (0.002) loss 1.1983 (1.5828) teacher_loss 0.6729 (1.0035) loss_zs_kd 0.0653 (0.0747) loss_oracle 0.3534 (0.3230) acc 84.3750 (72.9688) lr 7.5131e-04 eta 0:08:02
epoch [31/50] batch [280/288] time 0.085 (0.088) data 0.000 (0.002) loss 1.5209 (1.5783) teacher_loss 0.8693 (1.0005) loss_zs_kd 0.0551 (0.0753) loss_oracle 0.3062 (0.3228) acc 81.2500 (73.0469) lr 7.5131e-04 eta 0:07:59
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,411
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 86.0%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,026
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 79.9%
******* Domain a best val acc:      86.6%, epoch: 31 *******
******* Domain a best val test acc: 83.5%, epoch: 31 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [32/50] batch [20/288] time 0.077 (0.098) data 0.000 (0.014) loss 1.6136 (1.4424) teacher_loss 0.9562 (0.8899) loss_zs_kd 0.0791 (0.0762) loss_oracle 0.3515 (0.3199) acc 81.2500 (77.1875) lr 6.9098e-04 eta 0:08:55
epoch [32/50] batch [40/288] time 0.076 (0.099) data 0.000 (0.007) loss 1.5349 (1.4749) teacher_loss 0.9049 (0.9134) loss_zs_kd 0.0752 (0.0750) loss_oracle 0.3125 (0.3203) acc 78.1250 (76.4844) lr 6.9098e-04 eta 0:08:56
epoch [32/50] batch [60/288] time 0.078 (0.091) data 0.000 (0.005) loss 0.9710 (1.4758) teacher_loss 0.3709 (0.8992) loss_zs_kd 0.0504 (0.0781) loss_oracle 0.3163 (0.3226) acc 87.5000 (76.4062) lr 6.9098e-04 eta 0:08:12
epoch [32/50] batch [80/288] time 0.072 (0.087) data 0.000 (0.004) loss 1.3451 (1.5013) teacher_loss 0.7515 (0.9201) loss_zs_kd 0.0446 (0.0765) loss_oracle 0.3808 (0.3225) acc 75.0000 (76.1328) lr 6.9098e-04 eta 0:07:48
epoch [32/50] batch [100/288] time 0.065 (0.083) data 0.000 (0.003) loss 1.5222 (1.5054) teacher_loss 1.0311 (0.9220) loss_zs_kd 0.0985 (0.0767) loss_oracle 0.3410 (0.3210) acc 65.6250 (75.7188) lr 6.9098e-04 eta 0:07:27
epoch [32/50] batch [120/288] time 0.068 (0.080) data 0.000 (0.003) loss 1.5121 (1.5143) teacher_loss 1.0535 (0.9333) loss_zs_kd 0.0827 (0.0766) loss_oracle 0.2915 (0.3207) acc 68.7500 (75.5469) lr 6.9098e-04 eta 0:07:09
epoch [32/50] batch [140/288] time 0.064 (0.078) data 0.000 (0.002) loss 1.9746 (1.5210) teacher_loss 1.2988 (0.9430) loss_zs_kd 0.0657 (0.0757) loss_oracle 0.3506 (0.3210) acc 56.2500 (75.1786) lr 6.9098e-04 eta 0:06:55
epoch [32/50] batch [160/288] time 0.064 (0.077) data 0.000 (0.002) loss 1.3459 (1.5256) teacher_loss 0.7256 (0.9521) loss_zs_kd 0.0618 (0.0749) loss_oracle 0.3453 (0.3208) acc 78.1250 (74.9414) lr 6.9098e-04 eta 0:06:49
epoch [32/50] batch [180/288] time 0.084 (0.076) data 0.000 (0.002) loss 1.5952 (1.5391) teacher_loss 0.9014 (0.9602) loss_zs_kd 0.0717 (0.0754) loss_oracle 0.3331 (0.3217) acc 78.1250 (74.7569) lr 6.9098e-04 eta 0:06:43
epoch [32/50] batch [200/288] time 0.072 (0.076) data 0.000 (0.002) loss 1.1937 (1.5442) teacher_loss 0.7494 (0.9640) loss_zs_kd 0.0816 (0.0757) loss_oracle 0.3062 (0.3220) acc 78.1250 (74.7188) lr 6.9098e-04 eta 0:06:41
epoch [32/50] batch [220/288] time 0.088 (0.077) data 0.000 (0.001) loss 1.1952 (1.5437) teacher_loss 0.7979 (0.9622) loss_zs_kd 0.0563 (0.0764) loss_oracle 0.3289 (0.3221) acc 75.0000 (74.8011) lr 6.9098e-04 eta 0:06:42
epoch [32/50] batch [240/288] time 0.085 (0.077) data 0.000 (0.001) loss 1.6272 (1.5447) teacher_loss 0.9903 (0.9643) loss_zs_kd 0.0985 (0.0773) loss_oracle 0.2981 (0.3223) acc 65.6250 (74.7396) lr 6.9098e-04 eta 0:06:44
epoch [32/50] batch [260/288] time 0.088 (0.078) data 0.000 (0.001) loss 1.2482 (1.5431) teacher_loss 0.7375 (0.9628) loss_zs_kd 0.0694 (0.0773) loss_oracle 0.3071 (0.3222) acc 75.0000 (74.6755) lr 6.9098e-04 eta 0:06:45
epoch [32/50] batch [280/288] time 0.082 (0.078) data 0.000 (0.001) loss 1.7097 (1.5506) teacher_loss 0.9905 (0.9679) loss_zs_kd 0.0783 (0.0780) loss_oracle 0.3605 (0.3220) acc 75.0000 (74.4866) lr 6.9098e-04 eta 0:06:46
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,401
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.8%
******* Domain a best val acc:      86.6%, epoch: 31 *******
******* Domain a best val test acc: 83.5%, epoch: 31 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [33/50] batch [20/288] time 0.084 (0.103) data 0.000 (0.017) loss 1.5396 (1.6258) teacher_loss 0.8432 (1.0274) loss_zs_kd 0.0764 (0.0788) loss_oracle 0.3240 (0.3362) acc 78.1250 (72.8125) lr 6.3188e-04 eta 0:08:52
epoch [33/50] batch [40/288] time 0.083 (0.093) data 0.000 (0.009) loss 1.7194 (1.6757) teacher_loss 1.1111 (1.0644) loss_zs_kd 0.0391 (0.0805) loss_oracle 0.3041 (0.3280) acc 71.8750 (72.5781) lr 6.3188e-04 eta 0:08:00
epoch [33/50] batch [60/288] time 0.088 (0.090) data 0.001 (0.006) loss 1.6355 (1.6291) teacher_loss 1.0467 (1.0197) loss_zs_kd 0.1081 (0.0800) loss_oracle 0.3227 (0.3249) acc 71.8750 (73.3854) lr 6.3188e-04 eta 0:07:43
epoch [33/50] batch [80/288] time 0.085 (0.089) data 0.000 (0.004) loss 1.2180 (1.6325) teacher_loss 0.6356 (1.0234) loss_zs_kd 0.1017 (0.0806) loss_oracle 0.3364 (0.3248) acc 81.2500 (72.8906) lr 6.3188e-04 eta 0:07:34
epoch [33/50] batch [100/288] time 0.084 (0.091) data 0.000 (0.004) loss 1.5159 (1.6257) teacher_loss 0.9409 (1.0175) loss_zs_kd 0.0873 (0.0820) loss_oracle 0.3411 (0.3250) acc 81.2500 (73.3750) lr 6.3188e-04 eta 0:07:43
epoch [33/50] batch [120/288] time 0.084 (0.090) data 0.000 (0.003) loss 1.4051 (1.6127) teacher_loss 0.8733 (1.0112) loss_zs_kd 0.0706 (0.0814) loss_oracle 0.3248 (0.3242) acc 71.8750 (73.4635) lr 6.3188e-04 eta 0:07:35
epoch [33/50] batch [140/288] time 0.084 (0.089) data 0.000 (0.003) loss 1.8399 (1.5979) teacher_loss 1.1862 (0.9963) loss_zs_kd 0.0764 (0.0803) loss_oracle 0.3439 (0.3245) acc 62.5000 (73.7500) lr 6.3188e-04 eta 0:07:29
epoch [33/50] batch [160/288] time 0.084 (0.089) data 0.000 (0.002) loss 1.8049 (1.6053) teacher_loss 1.1150 (1.0058) loss_zs_kd 0.1078 (0.0797) loss_oracle 0.3382 (0.3249) acc 68.7500 (73.5547) lr 6.3188e-04 eta 0:07:25
epoch [33/50] batch [180/288] time 0.084 (0.088) data 0.000 (0.002) loss 2.0747 (1.6066) teacher_loss 1.4001 (1.0064) loss_zs_kd 0.0946 (0.0797) loss_oracle 0.3829 (0.3247) acc 50.0000 (73.3507) lr 6.3188e-04 eta 0:07:21
epoch [33/50] batch [200/288] time 0.086 (0.088) data 0.000 (0.002) loss 1.2143 (1.6031) teacher_loss 0.6372 (1.0061) loss_zs_kd 0.0631 (0.0788) loss_oracle 0.3103 (0.3239) acc 78.1250 (73.2656) lr 6.3188e-04 eta 0:07:17
epoch [33/50] batch [220/288] time 0.084 (0.088) data 0.000 (0.002) loss 1.6073 (1.6120) teacher_loss 0.9236 (1.0142) loss_zs_kd 0.1032 (0.0787) loss_oracle 0.3522 (0.3241) acc 81.2500 (73.0398) lr 6.3188e-04 eta 0:07:14
epoch [33/50] batch [240/288] time 0.080 (0.087) data 0.000 (0.002) loss 1.6040 (1.6124) teacher_loss 0.9613 (1.0157) loss_zs_kd 0.0921 (0.0783) loss_oracle 0.3018 (0.3240) acc 75.0000 (73.0729) lr 6.3188e-04 eta 0:07:11
epoch [33/50] batch [260/288] time 0.086 (0.087) data 0.000 (0.002) loss 1.6294 (1.6066) teacher_loss 1.1098 (1.0117) loss_zs_kd 0.1107 (0.0783) loss_oracle 0.3151 (0.3241) acc 68.7500 (73.1971) lr 6.3188e-04 eta 0:07:07
epoch [33/50] batch [280/288] time 0.072 (0.086) data 0.000 (0.001) loss 1.6030 (1.6032) teacher_loss 0.9991 (1.0072) loss_zs_kd 0.0985 (0.0785) loss_oracle 0.2874 (0.3242) acc 68.7500 (73.3371) lr 6.3188e-04 eta 0:07:03
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,408
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.7%
******* Domain a best val acc:      86.6%, epoch: 31 *******
******* Domain a best val test acc: 83.5%, epoch: 31 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [34/50] batch [20/288] time 0.084 (0.102) data 0.000 (0.014) loss 1.3995 (1.5423) teacher_loss 0.7623 (0.9775) loss_zs_kd 0.1124 (0.0795) loss_oracle 0.3149 (0.3168) acc 81.2500 (74.2188) lr 5.7422e-04 eta 0:08:15
epoch [34/50] batch [40/288] time 0.084 (0.092) data 0.000 (0.007) loss 1.4442 (1.6192) teacher_loss 0.8650 (1.0367) loss_zs_kd 0.0765 (0.0812) loss_oracle 0.3188 (0.3168) acc 78.1250 (72.8125) lr 5.7422e-04 eta 0:07:28
epoch [34/50] batch [60/288] time 0.082 (0.089) data 0.000 (0.005) loss 1.3652 (1.6309) teacher_loss 0.7241 (1.0434) loss_zs_kd 0.0901 (0.0828) loss_oracle 0.3247 (0.3195) acc 71.8750 (72.3958) lr 5.7422e-04 eta 0:07:12
epoch [34/50] batch [80/288] time 0.085 (0.088) data 0.000 (0.004) loss 1.9200 (1.6359) teacher_loss 1.2553 (1.0434) loss_zs_kd 0.1149 (0.0803) loss_oracle 0.3550 (0.3217) acc 68.7500 (72.1875) lr 5.7422e-04 eta 0:07:05
epoch [34/50] batch [100/288] time 0.085 (0.088) data 0.000 (0.003) loss 2.0512 (1.6102) teacher_loss 1.3683 (1.0292) loss_zs_kd 0.0531 (0.0771) loss_oracle 0.3230 (0.3214) acc 68.7500 (72.7812) lr 5.7422e-04 eta 0:07:00
epoch [34/50] batch [120/288] time 0.084 (0.087) data 0.000 (0.003) loss 1.6009 (1.6092) teacher_loss 1.0900 (1.0265) loss_zs_kd 0.0571 (0.0785) loss_oracle 0.3029 (0.3218) acc 65.6250 (72.5000) lr 5.7422e-04 eta 0:06:57
epoch [34/50] batch [140/288] time 0.082 (0.089) data 0.000 (0.002) loss 1.6563 (1.6189) teacher_loss 1.1261 (1.0342) loss_zs_kd 0.0809 (0.0783) loss_oracle 0.3072 (0.3215) acc 62.5000 (72.2545) lr 5.7422e-04 eta 0:07:01
epoch [34/50] batch [160/288] time 0.081 (0.088) data 0.000 (0.002) loss 1.0125 (1.6046) teacher_loss 0.3779 (1.0193) loss_zs_kd 0.0513 (0.0777) loss_oracle 0.3421 (0.3215) acc 84.3750 (72.5586) lr 5.7422e-04 eta 0:06:55
epoch [34/50] batch [180/288] time 0.085 (0.087) data 0.000 (0.002) loss 1.8689 (1.5933) teacher_loss 1.2770 (1.0112) loss_zs_kd 0.0744 (0.0776) loss_oracle 0.3300 (0.3215) acc 68.7500 (72.8819) lr 5.7422e-04 eta 0:06:51
epoch [34/50] batch [200/288] time 0.083 (0.087) data 0.000 (0.002) loss 1.8884 (1.5878) teacher_loss 1.1968 (1.0056) loss_zs_kd 0.0741 (0.0773) loss_oracle 0.3321 (0.3214) acc 65.6250 (73.1250) lr 5.7422e-04 eta 0:06:48
epoch [34/50] batch [220/288] time 0.087 (0.087) data 0.000 (0.002) loss 1.2986 (1.5900) teacher_loss 0.7245 (1.0040) loss_zs_kd 0.0929 (0.0782) loss_oracle 0.3175 (0.3213) acc 75.0000 (73.1960) lr 5.7422e-04 eta 0:06:46
epoch [34/50] batch [240/288] time 0.081 (0.087) data 0.000 (0.001) loss 1.5020 (1.5933) teacher_loss 1.0496 (1.0056) loss_zs_kd 0.1014 (0.0783) loss_oracle 0.3144 (0.3214) acc 75.0000 (73.2031) lr 5.7422e-04 eta 0:06:42
epoch [34/50] batch [260/288] time 0.085 (0.086) data 0.000 (0.001) loss 1.9489 (1.5841) teacher_loss 1.2161 (0.9984) loss_zs_kd 0.1240 (0.0787) loss_oracle 0.3095 (0.3215) acc 62.5000 (73.3894) lr 5.7422e-04 eta 0:06:40
epoch [34/50] batch [280/288] time 0.072 (0.086) data 0.000 (0.001) loss 1.5331 (1.5804) teacher_loss 1.0043 (0.9951) loss_zs_kd 0.0575 (0.0786) loss_oracle 0.2826 (0.3209) acc 68.7500 (73.4933) lr 5.7422e-04 eta 0:06:36
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,405
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,014
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.5%
******* Domain a best val acc:      86.6%, epoch: 31 *******
******* Domain a best val test acc: 83.5%, epoch: 31 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [35/50] batch [20/288] time 0.081 (0.100) data 0.000 (0.014) loss 1.5846 (1.5958) teacher_loss 1.0161 (1.0094) loss_zs_kd 0.1050 (0.0828) loss_oracle 0.3043 (0.3227) acc 68.7500 (72.8125) lr 5.1825e-04 eta 0:07:41
epoch [35/50] batch [40/288] time 0.081 (0.091) data 0.000 (0.007) loss 1.3407 (1.5485) teacher_loss 0.7801 (0.9696) loss_zs_kd 0.0984 (0.0811) loss_oracle 0.3048 (0.3260) acc 84.3750 (74.4531) lr 5.1825e-04 eta 0:06:55
epoch [35/50] batch [60/288] time 0.083 (0.088) data 0.000 (0.005) loss 1.1822 (1.5358) teacher_loss 0.6934 (0.9566) loss_zs_kd 0.0608 (0.0831) loss_oracle 0.3113 (0.3249) acc 75.0000 (74.2188) lr 5.1825e-04 eta 0:06:39
epoch [35/50] batch [80/288] time 0.080 (0.086) data 0.000 (0.004) loss 1.3696 (1.5261) teacher_loss 0.6138 (0.9384) loss_zs_kd 0.0822 (0.0826) loss_oracle 0.3390 (0.3237) acc 81.2500 (75.0000) lr 5.1825e-04 eta 0:06:31
epoch [35/50] batch [100/288] time 0.084 (0.085) data 0.001 (0.003) loss 1.3441 (1.5305) teacher_loss 0.7775 (0.9467) loss_zs_kd 0.0450 (0.0808) loss_oracle 0.3244 (0.3230) acc 75.0000 (74.3438) lr 5.1825e-04 eta 0:06:25
epoch [35/50] batch [120/288] time 0.086 (0.085) data 0.000 (0.002) loss 1.6427 (1.5432) teacher_loss 1.0317 (0.9581) loss_zs_kd 0.0581 (0.0803) loss_oracle 0.3104 (0.3223) acc 81.2500 (74.3490) lr 5.1825e-04 eta 0:06:21
epoch [35/50] batch [140/288] time 0.081 (0.085) data 0.000 (0.002) loss 1.3397 (1.5455) teacher_loss 0.6905 (0.9604) loss_zs_kd 0.1155 (0.0798) loss_oracle 0.3923 (0.3223) acc 87.5000 (74.3527) lr 5.1825e-04 eta 0:06:18
epoch [35/50] batch [160/288] time 0.108 (0.085) data 0.000 (0.002) loss 1.7151 (1.5344) teacher_loss 0.9502 (0.9490) loss_zs_kd 0.0605 (0.0789) loss_oracle 0.3775 (0.3228) acc 81.2500 (74.6094) lr 5.1825e-04 eta 0:06:16
epoch [35/50] batch [180/288] time 0.085 (0.087) data 0.000 (0.002) loss 1.4652 (1.5431) teacher_loss 0.9827 (0.9566) loss_zs_kd 0.0719 (0.0786) loss_oracle 0.3200 (0.3231) acc 78.1250 (74.2535) lr 5.1825e-04 eta 0:06:23
epoch [35/50] batch [200/288] time 0.088 (0.086) data 0.000 (0.002) loss 1.3373 (1.5411) teacher_loss 0.9557 (0.9575) loss_zs_kd 0.0624 (0.0786) loss_oracle 0.3087 (0.3221) acc 71.8750 (74.1875) lr 5.1825e-04 eta 0:06:21
epoch [35/50] batch [220/288] time 0.085 (0.086) data 0.000 (0.001) loss 1.2806 (1.5378) teacher_loss 0.7771 (0.9535) loss_zs_kd 0.0515 (0.0787) loss_oracle 0.3128 (0.3216) acc 78.1250 (74.2614) lr 5.1825e-04 eta 0:06:18
epoch [35/50] batch [240/288] time 0.083 (0.086) data 0.000 (0.001) loss 1.2492 (1.5440) teacher_loss 0.6063 (0.9615) loss_zs_kd 0.0659 (0.0779) loss_oracle 0.3047 (0.3220) acc 84.3750 (74.0365) lr 5.1825e-04 eta 0:06:16
epoch [35/50] batch [260/288] time 0.084 (0.086) data 0.000 (0.001) loss 1.7377 (1.5477) teacher_loss 1.1843 (0.9642) loss_zs_kd 0.0693 (0.0783) loss_oracle 0.3157 (0.3221) acc 68.7500 (74.0505) lr 5.1825e-04 eta 0:06:14
epoch [35/50] batch [280/288] time 0.085 (0.086) data 0.000 (0.001) loss 1.1875 (1.5541) teacher_loss 0.6723 (0.9694) loss_zs_kd 0.1131 (0.0785) loss_oracle 0.3446 (0.3226) acc 84.3750 (74.0290) lr 5.1825e-04 eta 0:06:12
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,400
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.6%
******* Domain a best val acc:      86.6%, epoch: 31 *******
******* Domain a best val test acc: 83.5%, epoch: 31 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [36/50] batch [20/288] time 0.079 (0.102) data 0.000 (0.016) loss 1.7352 (1.6029) teacher_loss 0.9596 (1.0371) loss_zs_kd 0.0997 (0.0824) loss_oracle 0.3598 (0.3265) acc 68.7500 (72.1875) lr 4.6417e-04 eta 0:07:19
epoch [36/50] batch [40/288] time 0.083 (0.094) data 0.000 (0.008) loss 1.4803 (1.5469) teacher_loss 0.8517 (0.9623) loss_zs_kd 0.0897 (0.0810) loss_oracle 0.3317 (0.3261) acc 75.0000 (74.2969) lr 4.6417e-04 eta 0:06:40
epoch [36/50] batch [60/288] time 0.088 (0.090) data 0.000 (0.006) loss 1.9581 (1.5645) teacher_loss 1.4282 (0.9757) loss_zs_kd 0.0936 (0.0809) loss_oracle 0.3005 (0.3286) acc 59.3750 (74.1146) lr 4.6417e-04 eta 0:06:25
epoch [36/50] batch [80/288] time 0.085 (0.089) data 0.000 (0.004) loss 1.2581 (1.5486) teacher_loss 0.7964 (0.9596) loss_zs_kd 0.0426 (0.0797) loss_oracle 0.3795 (0.3285) acc 78.1250 (74.5703) lr 4.6417e-04 eta 0:06:17
epoch [36/50] batch [100/288] time 0.086 (0.088) data 0.000 (0.003) loss 1.5989 (1.5651) teacher_loss 1.0976 (0.9769) loss_zs_kd 0.1149 (0.0782) loss_oracle 0.3172 (0.3300) acc 75.0000 (74.0938) lr 4.6417e-04 eta 0:06:12
epoch [36/50] batch [120/288] time 0.084 (0.088) data 0.000 (0.003) loss 1.9078 (1.5839) teacher_loss 1.2492 (0.9893) loss_zs_kd 0.1001 (0.0800) loss_oracle 0.3227 (0.3307) acc 62.5000 (73.5938) lr 4.6417e-04 eta 0:06:07
epoch [36/50] batch [140/288] time 0.086 (0.087) data 0.000 (0.002) loss 1.7310 (1.5946) teacher_loss 1.1577 (0.9979) loss_zs_kd 0.0790 (0.0794) loss_oracle 0.3152 (0.3319) acc 68.7500 (73.5714) lr 4.6417e-04 eta 0:06:04
epoch [36/50] batch [160/288] time 0.084 (0.087) data 0.000 (0.002) loss 1.8781 (1.6027) teacher_loss 1.1830 (1.0094) loss_zs_kd 0.0896 (0.0789) loss_oracle 0.3722 (0.3303) acc 68.7500 (73.1250) lr 4.6417e-04 eta 0:06:01
epoch [36/50] batch [180/288] time 0.084 (0.087) data 0.000 (0.002) loss 1.0647 (1.5870) teacher_loss 0.5540 (0.9931) loss_zs_kd 0.0639 (0.0787) loss_oracle 0.2966 (0.3299) acc 87.5000 (73.5938) lr 4.6417e-04 eta 0:05:59
epoch [36/50] batch [200/288] time 0.085 (0.088) data 0.000 (0.002) loss 2.2270 (1.5942) teacher_loss 1.5280 (0.9985) loss_zs_kd 0.0635 (0.0786) loss_oracle 0.3934 (0.3302) acc 59.3750 (73.4844) lr 4.6417e-04 eta 0:06:04
epoch [36/50] batch [220/288] time 0.084 (0.088) data 0.000 (0.002) loss 1.6603 (1.6000) teacher_loss 1.0139 (1.0066) loss_zs_kd 0.0821 (0.0789) loss_oracle 0.3479 (0.3304) acc 75.0000 (73.3949) lr 4.6417e-04 eta 0:06:01
epoch [36/50] batch [240/288] time 0.084 (0.088) data 0.000 (0.002) loss 1.4705 (1.5924) teacher_loss 0.8544 (0.9997) loss_zs_kd 0.0454 (0.0786) loss_oracle 0.3198 (0.3300) acc 81.2500 (73.5026) lr 4.6417e-04 eta 0:05:58
epoch [36/50] batch [260/288] time 0.085 (0.088) data 0.000 (0.001) loss 1.5443 (1.5824) teacher_loss 1.0064 (0.9864) loss_zs_kd 0.0754 (0.0787) loss_oracle 0.3174 (0.3292) acc 71.8750 (73.7380) lr 4.6417e-04 eta 0:05:55
epoch [36/50] batch [280/288] time 0.086 (0.087) data 0.000 (0.001) loss 1.5167 (1.5829) teacher_loss 0.7525 (0.9850) loss_zs_kd 0.1557 (0.0793) loss_oracle 0.3444 (0.3291) acc 87.5000 (73.7388) lr 4.6417e-04 eta 0:05:52
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,015
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.5%
******* Domain a best val acc:      86.6%, epoch: 31 *******
******* Domain a best val test acc: 83.5%, epoch: 31 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [37/50] batch [20/288] time 0.085 (0.095) data 0.000 (0.012) loss 1.5611 (1.5729) teacher_loss 1.0898 (0.9937) loss_zs_kd 0.0684 (0.0801) loss_oracle 0.3302 (0.3181) acc 71.8750 (72.6562) lr 4.1221e-04 eta 0:06:21
epoch [37/50] batch [40/288] time 0.083 (0.088) data 0.000 (0.006) loss 1.3178 (1.5619) teacher_loss 0.6830 (0.9737) loss_zs_kd 0.0678 (0.0780) loss_oracle 0.3349 (0.3201) acc 78.1250 (72.5781) lr 4.1221e-04 eta 0:05:52
epoch [37/50] batch [60/288] time 0.084 (0.086) data 0.000 (0.004) loss 1.9369 (1.5613) teacher_loss 1.2633 (0.9704) loss_zs_kd 0.0557 (0.0767) loss_oracle 0.3474 (0.3222) acc 65.6250 (73.0208) lr 4.1221e-04 eta 0:05:43
epoch [37/50] batch [80/288] time 0.082 (0.085) data 0.000 (0.003) loss 1.4230 (1.5680) teacher_loss 0.9713 (0.9724) loss_zs_kd 0.0613 (0.0799) loss_oracle 0.3402 (0.3228) acc 75.0000 (73.3203) lr 4.1221e-04 eta 0:05:37
epoch [37/50] batch [100/288] time 0.084 (0.085) data 0.000 (0.003) loss 1.8409 (1.5867) teacher_loss 1.2012 (0.9853) loss_zs_kd 0.0705 (0.0826) loss_oracle 0.3133 (0.3246) acc 71.8750 (73.2812) lr 4.1221e-04 eta 0:05:33
epoch [37/50] batch [120/288] time 0.083 (0.085) data 0.000 (0.002) loss 1.3413 (1.5801) teacher_loss 0.7976 (0.9844) loss_zs_kd 0.0594 (0.0816) loss_oracle 0.3455 (0.3240) acc 87.5000 (73.0469) lr 4.1221e-04 eta 0:05:30
epoch [37/50] batch [140/288] time 0.082 (0.084) data 0.000 (0.002) loss 1.7747 (1.5789) teacher_loss 1.3091 (0.9883) loss_zs_kd 0.0760 (0.0809) loss_oracle 0.3232 (0.3242) acc 65.6250 (73.1696) lr 4.1221e-04 eta 0:05:27
epoch [37/50] batch [160/288] time 0.085 (0.084) data 0.000 (0.002) loss 1.6611 (1.5807) teacher_loss 0.9791 (0.9877) loss_zs_kd 0.0695 (0.0806) loss_oracle 0.3308 (0.3246) acc 65.6250 (73.1641) lr 4.1221e-04 eta 0:05:25
epoch [37/50] batch [180/288] time 0.081 (0.084) data 0.000 (0.002) loss 1.7567 (1.5798) teacher_loss 1.1303 (0.9863) loss_zs_kd 0.0633 (0.0805) loss_oracle 0.3241 (0.3244) acc 75.0000 (73.3854) lr 4.1221e-04 eta 0:05:23
epoch [37/50] batch [200/288] time 0.082 (0.084) data 0.000 (0.001) loss 2.2460 (1.5831) teacher_loss 1.5448 (0.9903) loss_zs_kd 0.0842 (0.0805) loss_oracle 0.3247 (0.3248) acc 56.2500 (73.5000) lr 4.1221e-04 eta 0:05:20
epoch [37/50] batch [220/288] time 0.081 (0.084) data 0.000 (0.001) loss 1.7890 (1.5829) teacher_loss 0.9915 (0.9912) loss_zs_kd 0.0850 (0.0806) loss_oracle 0.3519 (0.3245) acc 71.8750 (73.4801) lr 4.1221e-04 eta 0:05:18
epoch [37/50] batch [240/288] time 0.083 (0.085) data 0.000 (0.001) loss 1.6910 (1.5862) teacher_loss 1.2042 (0.9937) loss_zs_kd 0.0704 (0.0806) loss_oracle 0.3414 (0.3247) acc 68.7500 (73.4766) lr 4.1221e-04 eta 0:05:20
epoch [37/50] batch [260/288] time 0.084 (0.084) data 0.000 (0.001) loss 1.7721 (1.5860) teacher_loss 1.1778 (0.9919) loss_zs_kd 0.0757 (0.0803) loss_oracle 0.3097 (0.3243) acc 62.5000 (73.4014) lr 4.1221e-04 eta 0:05:18
epoch [37/50] batch [280/288] time 0.084 (0.084) data 0.000 (0.001) loss 1.7800 (1.5800) teacher_loss 1.2099 (0.9900) loss_zs_kd 0.1008 (0.0801) loss_oracle 0.3205 (0.3238) acc 68.7500 (73.4821) lr 4.1221e-04 eta 0:05:16
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,397
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,014
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.4%
******* Domain a best val acc:      86.6%, epoch: 31 *******
******* Domain a best val test acc: 83.5%, epoch: 31 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [38/50] batch [20/288] time 0.085 (0.105) data 0.000 (0.017) loss 1.2051 (1.6086) teacher_loss 0.6439 (0.9725) loss_zs_kd 0.0588 (0.0802) loss_oracle 0.3175 (0.3245) acc 84.3750 (74.6875) lr 3.6258e-04 eta 0:06:29
epoch [38/50] batch [40/288] time 0.085 (0.093) data 0.000 (0.009) loss 1.3494 (1.6084) teacher_loss 0.8555 (1.0038) loss_zs_kd 0.0596 (0.0782) loss_oracle 0.3062 (0.3279) acc 68.7500 (74.1406) lr 3.6258e-04 eta 0:05:45
epoch [38/50] batch [60/288] time 0.083 (0.090) data 0.000 (0.006) loss 1.4014 (1.5909) teacher_loss 0.7544 (0.9910) loss_zs_kd 0.0963 (0.0770) loss_oracle 0.2934 (0.3246) acc 81.2500 (74.0104) lr 3.6258e-04 eta 0:05:32
epoch [38/50] batch [80/288] time 0.084 (0.089) data 0.000 (0.004) loss 1.3633 (1.6228) teacher_loss 0.8270 (1.0286) loss_zs_kd 0.0606 (0.0763) loss_oracle 0.2980 (0.3229) acc 78.1250 (73.1641) lr 3.6258e-04 eta 0:05:24
epoch [38/50] batch [100/288] time 0.085 (0.088) data 0.000 (0.004) loss 1.4565 (1.6227) teacher_loss 1.0530 (1.0287) loss_zs_kd 0.0748 (0.0773) loss_oracle 0.3027 (0.3228) acc 71.8750 (73.0625) lr 3.6258e-04 eta 0:05:19
epoch [38/50] batch [120/288] time 0.086 (0.087) data 0.000 (0.003) loss 2.0439 (1.6153) teacher_loss 1.3539 (1.0234) loss_zs_kd 0.0974 (0.0763) loss_oracle 0.3427 (0.3232) acc 65.6250 (72.9167) lr 3.6258e-04 eta 0:05:16
epoch [38/50] batch [140/288] time 0.084 (0.087) data 0.000 (0.003) loss 1.8239 (1.6092) teacher_loss 1.2329 (1.0208) loss_zs_kd 0.0659 (0.0762) loss_oracle 0.3820 (0.3234) acc 65.6250 (72.9911) lr 3.6258e-04 eta 0:05:12
epoch [38/50] batch [160/288] time 0.084 (0.087) data 0.000 (0.002) loss 1.9733 (1.6042) teacher_loss 1.3895 (1.0173) loss_zs_kd 0.0567 (0.0766) loss_oracle 0.3435 (0.3243) acc 59.3750 (72.9688) lr 3.6258e-04 eta 0:05:10
epoch [38/50] batch [180/288] time 0.085 (0.086) data 0.000 (0.002) loss 1.2906 (1.6065) teacher_loss 0.7628 (1.0217) loss_zs_kd 0.0637 (0.0768) loss_oracle 0.3330 (0.3243) acc 75.0000 (72.7951) lr 3.6258e-04 eta 0:05:07
epoch [38/50] batch [200/288] time 0.084 (0.086) data 0.000 (0.002) loss 1.7667 (1.5985) teacher_loss 1.0666 (1.0158) loss_zs_kd 0.0865 (0.0766) loss_oracle 0.3543 (0.3235) acc 68.7500 (72.8594) lr 3.6258e-04 eta 0:05:04
epoch [38/50] batch [220/288] time 0.084 (0.086) data 0.000 (0.002) loss 1.6891 (1.5946) teacher_loss 1.1220 (1.0126) loss_zs_kd 0.0683 (0.0768) loss_oracle 0.3156 (0.3233) acc 62.5000 (72.9403) lr 3.6258e-04 eta 0:05:02
epoch [38/50] batch [240/288] time 0.085 (0.086) data 0.000 (0.002) loss 1.3634 (1.5954) teacher_loss 0.9021 (1.0121) loss_zs_kd 0.1014 (0.0771) loss_oracle 0.3097 (0.3229) acc 71.8750 (72.9427) lr 3.6258e-04 eta 0:04:59
epoch [38/50] batch [260/288] time 0.122 (0.086) data 0.000 (0.002) loss 1.6238 (1.5981) teacher_loss 1.0689 (1.0141) loss_zs_kd 0.0902 (0.0776) loss_oracle 0.3170 (0.3231) acc 65.6250 (72.8365) lr 3.6258e-04 eta 0:05:01
epoch [38/50] batch [280/288] time 0.084 (0.086) data 0.000 (0.001) loss 1.0689 (1.5939) teacher_loss 0.5852 (1.0091) loss_zs_kd 0.0704 (0.0782) loss_oracle 0.3510 (0.3236) acc 87.5000 (73.1027) lr 3.6258e-04 eta 0:04:59
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,027
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.1%
******* Domain a best val acc:      86.6%, epoch: 31 *******
******* Domain a best val test acc: 83.5%, epoch: 31 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [39/50] batch [20/288] time 0.085 (0.102) data 0.001 (0.014) loss 1.8679 (1.5381) teacher_loss 1.2242 (0.9519) loss_zs_kd 0.0620 (0.0714) loss_oracle 0.3098 (0.3324) acc 68.7500 (76.0938) lr 3.1545e-04 eta 0:05:51
epoch [39/50] batch [40/288] time 0.078 (0.093) data 0.000 (0.007) loss 2.3457 (1.5665) teacher_loss 1.6518 (0.9857) loss_zs_kd 0.1281 (0.0805) loss_oracle 0.3240 (0.3241) acc 59.3750 (74.3750) lr 3.1545e-04 eta 0:05:19
epoch [39/50] batch [60/288] time 0.085 (0.090) data 0.000 (0.005) loss 2.0540 (1.5616) teacher_loss 1.4881 (0.9714) loss_zs_kd 0.1026 (0.0780) loss_oracle 0.3427 (0.3266) acc 59.3750 (74.4271) lr 3.1545e-04 eta 0:05:06
epoch [39/50] batch [80/288] time 0.086 (0.089) data 0.000 (0.004) loss 1.4016 (1.5456) teacher_loss 0.9027 (0.9550) loss_zs_kd 0.0794 (0.0761) loss_oracle 0.3720 (0.3259) acc 78.1250 (75.1172) lr 3.1545e-04 eta 0:05:00
epoch [39/50] batch [100/288] time 0.084 (0.088) data 0.000 (0.003) loss 1.3119 (1.5441) teacher_loss 0.7826 (0.9550) loss_zs_kd 0.0740 (0.0762) loss_oracle 0.3364 (0.3265) acc 71.8750 (74.7812) lr 3.1545e-04 eta 0:04:55
epoch [39/50] batch [120/288] time 0.083 (0.088) data 0.000 (0.003) loss 1.5815 (1.5573) teacher_loss 0.9033 (0.9645) loss_zs_kd 0.0479 (0.0772) loss_oracle 0.3046 (0.3260) acc 71.8750 (74.2969) lr 3.1545e-04 eta 0:04:52
epoch [39/50] batch [140/288] time 0.086 (0.087) data 0.000 (0.002) loss 1.5373 (1.5571) teacher_loss 0.8730 (0.9603) loss_zs_kd 0.0709 (0.0774) loss_oracle 0.3151 (0.3245) acc 75.0000 (74.1518) lr 3.1545e-04 eta 0:04:48
epoch [39/50] batch [160/288] time 0.083 (0.087) data 0.000 (0.002) loss 1.5170 (1.5554) teacher_loss 0.8660 (0.9596) loss_zs_kd 0.0712 (0.0769) loss_oracle 0.3051 (0.3234) acc 78.1250 (74.1992) lr 3.1545e-04 eta 0:04:46
epoch [39/50] batch [180/288] time 0.086 (0.087) data 0.000 (0.002) loss 1.3518 (1.5588) teacher_loss 0.8399 (0.9669) loss_zs_kd 0.0794 (0.0769) loss_oracle 0.3324 (0.3236) acc 65.6250 (73.9410) lr 3.1545e-04 eta 0:04:43
epoch [39/50] batch [200/288] time 0.084 (0.086) data 0.000 (0.002) loss 1.5364 (1.5699) teacher_loss 1.0548 (0.9796) loss_zs_kd 0.0573 (0.0771) loss_oracle 0.3236 (0.3240) acc 68.7500 (73.5469) lr 3.1545e-04 eta 0:04:41
epoch [39/50] batch [220/288] time 0.084 (0.086) data 0.000 (0.002) loss 1.0943 (1.5666) teacher_loss 0.6018 (0.9790) loss_zs_kd 0.0406 (0.0771) loss_oracle 0.3459 (0.3236) acc 84.3750 (73.6648) lr 3.1545e-04 eta 0:04:39
epoch [39/50] batch [240/288] time 0.083 (0.086) data 0.000 (0.001) loss 1.6559 (1.5725) teacher_loss 0.9207 (0.9850) loss_zs_kd 0.1126 (0.0778) loss_oracle 0.3205 (0.3242) acc 71.8750 (73.4896) lr 3.1545e-04 eta 0:04:37
epoch [39/50] batch [260/288] time 0.083 (0.086) data 0.000 (0.001) loss 1.7796 (1.5713) teacher_loss 1.0872 (0.9837) loss_zs_kd 0.0650 (0.0772) loss_oracle 0.3349 (0.3242) acc 68.7500 (73.6298) lr 3.1545e-04 eta 0:04:35
epoch [39/50] batch [280/288] time 0.083 (0.086) data 0.000 (0.001) loss 1.7815 (1.5763) teacher_loss 1.0887 (0.9854) loss_zs_kd 0.0745 (0.0774) loss_oracle 0.3072 (0.3240) acc 71.8750 (73.6719) lr 3.1545e-04 eta 0:04:32
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,406
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.8%
******* Domain a best val acc:      86.6%, epoch: 31 *******
******* Domain a best val test acc: 83.5%, epoch: 31 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [40/50] batch [20/288] time 0.077 (0.094) data 0.000 (0.013) loss 1.5897 (1.6368) teacher_loss 1.0031 (1.0384) loss_zs_kd 0.0837 (0.0881) loss_oracle 0.3637 (0.3217) acc 62.5000 (69.6875) lr 2.7103e-04 eta 0:04:57
epoch [40/50] batch [40/288] time 0.081 (0.088) data 0.000 (0.007) loss 1.4073 (1.6153) teacher_loss 0.8855 (1.0230) loss_zs_kd 0.0746 (0.0866) loss_oracle 0.3186 (0.3235) acc 78.1250 (71.3281) lr 2.7103e-04 eta 0:04:35
epoch [40/50] batch [60/288] time 0.081 (0.086) data 0.000 (0.004) loss 1.2771 (1.6067) teacher_loss 0.7225 (1.0212) loss_zs_kd 0.0689 (0.0841) loss_oracle 0.3085 (0.3220) acc 78.1250 (72.2396) lr 2.7103e-04 eta 0:04:26
epoch [40/50] batch [80/288] time 0.085 (0.085) data 0.000 (0.003) loss 1.3182 (1.6173) teacher_loss 0.7481 (1.0231) loss_zs_kd 0.0628 (0.0842) loss_oracle 0.2966 (0.3230) acc 81.2500 (72.1094) lr 2.7103e-04 eta 0:04:22
epoch [40/50] batch [100/288] time 0.084 (0.085) data 0.000 (0.003) loss 1.1899 (1.6196) teacher_loss 0.7572 (1.0266) loss_zs_kd 0.1049 (0.0830) loss_oracle 0.3235 (0.3240) acc 81.2500 (71.9688) lr 2.7103e-04 eta 0:04:20
epoch [40/50] batch [120/288] time 0.087 (0.084) data 0.000 (0.002) loss 1.5796 (1.6140) teacher_loss 1.0556 (1.0224) loss_zs_kd 0.0931 (0.0815) loss_oracle 0.3170 (0.3238) acc 71.8750 (71.7969) lr 2.7103e-04 eta 0:04:17
epoch [40/50] batch [140/288] time 0.087 (0.084) data 0.000 (0.002) loss 1.4576 (1.6084) teacher_loss 0.9913 (1.0166) loss_zs_kd 0.0592 (0.0803) loss_oracle 0.3495 (0.3235) acc 78.1250 (72.2768) lr 2.7103e-04 eta 0:04:15
epoch [40/50] batch [160/288] time 0.084 (0.084) data 0.000 (0.002) loss 1.2878 (1.5991) teacher_loss 0.7693 (1.0057) loss_zs_kd 0.0710 (0.0806) loss_oracle 0.3043 (0.3236) acc 75.0000 (72.8125) lr 2.7103e-04 eta 0:04:14
epoch [40/50] batch [180/288] time 0.086 (0.085) data 0.000 (0.002) loss 1.1082 (1.5860) teacher_loss 0.5084 (0.9959) loss_zs_kd 0.0539 (0.0803) loss_oracle 0.3111 (0.3238) acc 90.6250 (73.2639) lr 2.7103e-04 eta 0:04:12
epoch [40/50] batch [200/288] time 0.084 (0.085) data 0.000 (0.001) loss 1.9133 (1.5805) teacher_loss 1.2685 (0.9925) loss_zs_kd 0.1117 (0.0800) loss_oracle 0.3004 (0.3236) acc 62.5000 (73.3750) lr 2.7103e-04 eta 0:04:11
epoch [40/50] batch [220/288] time 0.086 (0.085) data 0.000 (0.001) loss 1.5551 (1.5772) teacher_loss 0.9465 (0.9869) loss_zs_kd 0.0849 (0.0802) loss_oracle 0.3005 (0.3236) acc 78.1250 (73.6080) lr 2.7103e-04 eta 0:04:09
epoch [40/50] batch [240/288] time 0.084 (0.085) data 0.000 (0.001) loss 2.0003 (1.5872) teacher_loss 1.3635 (0.9912) loss_zs_kd 0.0765 (0.0807) loss_oracle 0.3177 (0.3240) acc 62.5000 (73.5547) lr 2.7103e-04 eta 0:04:08
epoch [40/50] batch [260/288] time 0.092 (0.085) data 0.001 (0.001) loss 1.2898 (1.5920) teacher_loss 0.8348 (0.9950) loss_zs_kd 0.0692 (0.0804) loss_oracle 0.3334 (0.3245) acc 78.1250 (73.4375) lr 2.7103e-04 eta 0:04:06
epoch [40/50] batch [280/288] time 0.087 (0.085) data 0.000 (0.001) loss 1.4909 (1.5889) teacher_loss 0.9250 (0.9911) loss_zs_kd 0.0891 (0.0807) loss_oracle 0.3246 (0.3242) acc 78.1250 (73.4821) lr 2.7103e-04 eta 0:04:05
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,404
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.7%
******* Domain a best val acc:      86.6%, epoch: 31 *******
******* Domain a best val test acc: 83.5%, epoch: 31 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [41/50] batch [20/288] time 0.084 (0.108) data 0.000 (0.018) loss 2.0774 (1.6282) teacher_loss 1.4668 (1.0212) loss_zs_kd 0.0889 (0.0796) loss_oracle 0.3149 (0.3233) acc 65.6250 (71.4062) lr 2.2949e-04 eta 0:05:08
epoch [41/50] batch [40/288] time 0.085 (0.097) data 0.000 (0.009) loss 1.5473 (1.6055) teacher_loss 0.8734 (1.0102) loss_zs_kd 0.0599 (0.0791) loss_oracle 0.3099 (0.3248) acc 75.0000 (73.0469) lr 2.2949e-04 eta 0:04:34
epoch [41/50] batch [60/288] time 0.085 (0.092) data 0.000 (0.006) loss 1.5544 (1.5828) teacher_loss 1.0461 (0.9891) loss_zs_kd 0.0562 (0.0773) loss_oracle 0.3125 (0.3251) acc 78.1250 (73.0729) lr 2.2949e-04 eta 0:04:20
epoch [41/50] batch [80/288] time 0.086 (0.091) data 0.000 (0.005) loss 1.4734 (1.5781) teacher_loss 0.9310 (0.9941) loss_zs_kd 0.0608 (0.0765) loss_oracle 0.3129 (0.3250) acc 78.1250 (73.3203) lr 2.2949e-04 eta 0:04:13
epoch [41/50] batch [100/288] time 0.069 (0.089) data 0.000 (0.004) loss 1.6580 (1.5608) teacher_loss 1.1553 (0.9782) loss_zs_kd 0.0778 (0.0772) loss_oracle 0.3169 (0.3237) acc 71.8750 (73.8438) lr 2.2949e-04 eta 0:04:08
epoch [41/50] batch [120/288] time 0.083 (0.088) data 0.000 (0.003) loss 1.5010 (1.5482) teacher_loss 0.9168 (0.9681) loss_zs_kd 0.0826 (0.0779) loss_oracle 0.3249 (0.3238) acc 78.1250 (74.4010) lr 2.2949e-04 eta 0:04:03
epoch [41/50] batch [140/288] time 0.086 (0.088) data 0.001 (0.003) loss 1.8120 (1.5516) teacher_loss 1.2003 (0.9698) loss_zs_kd 0.0975 (0.0773) loss_oracle 0.3122 (0.3231) acc 71.8750 (74.3304) lr 2.2949e-04 eta 0:03:59
epoch [41/50] batch [160/288] time 0.082 (0.087) data 0.000 (0.002) loss 1.2847 (1.5480) teacher_loss 0.6633 (0.9664) loss_zs_kd 0.0698 (0.0772) loss_oracle 0.3717 (0.3240) acc 87.5000 (74.3164) lr 2.2949e-04 eta 0:03:56
epoch [41/50] batch [180/288] time 0.082 (0.086) data 0.000 (0.002) loss 2.0283 (1.5589) teacher_loss 1.4474 (0.9752) loss_zs_kd 0.0580 (0.0772) loss_oracle 0.3196 (0.3240) acc 62.5000 (74.2882) lr 2.2949e-04 eta 0:03:53
epoch [41/50] batch [200/288] time 0.084 (0.086) data 0.000 (0.002) loss 1.6483 (1.5712) teacher_loss 0.9597 (0.9832) loss_zs_kd 0.1158 (0.0770) loss_oracle 0.3203 (0.3243) acc 62.5000 (74.0625) lr 2.2949e-04 eta 0:03:51
epoch [41/50] batch [220/288] time 0.084 (0.086) data 0.000 (0.002) loss 1.4072 (1.5706) teacher_loss 0.7841 (0.9847) loss_zs_kd 0.0803 (0.0768) loss_oracle 0.3176 (0.3240) acc 81.2500 (73.8920) lr 2.2949e-04 eta 0:03:49
epoch [41/50] batch [240/288] time 0.084 (0.086) data 0.000 (0.002) loss 1.6994 (1.5795) teacher_loss 1.0570 (0.9918) loss_zs_kd 0.0824 (0.0769) loss_oracle 0.3479 (0.3241) acc 78.1250 (73.7500) lr 2.2949e-04 eta 0:03:47
epoch [41/50] batch [260/288] time 0.087 (0.086) data 0.000 (0.002) loss 1.5983 (1.5799) teacher_loss 1.0303 (0.9905) loss_zs_kd 0.0637 (0.0772) loss_oracle 0.3148 (0.3242) acc 78.1250 (73.6779) lr 2.2949e-04 eta 0:03:45
epoch [41/50] batch [280/288] time 0.084 (0.086) data 0.000 (0.001) loss 1.6825 (1.5824) teacher_loss 1.0708 (0.9929) loss_zs_kd 0.0626 (0.0774) loss_oracle 0.2940 (0.3241) acc 68.7500 (73.5045) lr 2.2949e-04 eta 0:03:43
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,408
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,026
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 79.9%
******* Domain a best val acc:      86.6%, epoch: 31 *******
******* Domain a best val test acc: 83.5%, epoch: 31 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [42/50] batch [20/288] time 0.083 (0.109) data 0.000 (0.023) loss 1.3990 (1.5896) teacher_loss 0.9769 (1.0199) loss_zs_kd 0.0560 (0.0810) loss_oracle 0.3136 (0.3207) acc 78.1250 (70.7812) lr 1.9098e-04 eta 0:04:39
epoch [42/50] batch [40/288] time 0.064 (0.093) data 0.000 (0.012) loss 1.8549 (1.5878) teacher_loss 1.2532 (1.0212) loss_zs_kd 0.0865 (0.0783) loss_oracle 0.3271 (0.3181) acc 71.8750 (71.9531) lr 1.9098e-04 eta 0:03:56
epoch [42/50] batch [60/288] time 0.068 (0.085) data 0.000 (0.008) loss 1.5122 (1.5611) teacher_loss 0.9500 (0.9873) loss_zs_kd 0.0949 (0.0786) loss_oracle 0.3087 (0.3211) acc 75.0000 (72.8125) lr 1.9098e-04 eta 0:03:36
epoch [42/50] batch [80/288] time 0.065 (0.083) data 0.000 (0.006) loss 1.4036 (1.5351) teacher_loss 0.8690 (0.9609) loss_zs_kd 0.0745 (0.0791) loss_oracle 0.3212 (0.3220) acc 75.0000 (73.4766) lr 1.9098e-04 eta 0:03:28
epoch [42/50] batch [100/288] time 0.078 (0.081) data 0.000 (0.005) loss 1.8744 (1.5433) teacher_loss 1.2095 (0.9654) loss_zs_kd 0.0770 (0.0783) loss_oracle 0.3371 (0.3221) acc 62.5000 (73.5312) lr 1.9098e-04 eta 0:03:22
epoch [42/50] batch [120/288] time 0.079 (0.081) data 0.000 (0.004) loss 1.6084 (1.5569) teacher_loss 0.8725 (0.9799) loss_zs_kd 0.0492 (0.0779) loss_oracle 0.3159 (0.3222) acc 78.1250 (73.3594) lr 1.9098e-04 eta 0:03:19
epoch [42/50] batch [140/288] time 0.083 (0.081) data 0.000 (0.004) loss 1.6465 (1.5679) teacher_loss 1.1343 (0.9863) loss_zs_kd 0.0970 (0.0788) loss_oracle 0.2937 (0.3219) acc 71.8750 (73.1920) lr 1.9098e-04 eta 0:03:19
epoch [42/50] batch [160/288] time 0.084 (0.082) data 0.000 (0.003) loss 1.5580 (1.5735) teacher_loss 0.8849 (0.9896) loss_zs_kd 0.0980 (0.0788) loss_oracle 0.3059 (0.3216) acc 78.1250 (73.1445) lr 1.9098e-04 eta 0:03:18
epoch [42/50] batch [180/288] time 0.083 (0.082) data 0.000 (0.003) loss 1.9190 (1.5783) teacher_loss 1.3438 (0.9948) loss_zs_kd 0.0942 (0.0791) loss_oracle 0.3233 (0.3217) acc 71.8750 (73.0903) lr 1.9098e-04 eta 0:03:16
epoch [42/50] batch [200/288] time 0.083 (0.082) data 0.000 (0.003) loss 1.7226 (1.5831) teacher_loss 1.0709 (0.9974) loss_zs_kd 0.0724 (0.0787) loss_oracle 0.3550 (0.3219) acc 71.8750 (73.1094) lr 1.9098e-04 eta 0:03:15
epoch [42/50] batch [220/288] time 0.082 (0.082) data 0.000 (0.002) loss 1.4559 (1.5777) teacher_loss 0.9347 (0.9932) loss_zs_kd 0.0666 (0.0785) loss_oracle 0.2985 (0.3224) acc 78.1250 (73.1534) lr 1.9098e-04 eta 0:03:14
epoch [42/50] batch [240/288] time 0.082 (0.082) data 0.000 (0.002) loss 1.5202 (1.5780) teacher_loss 1.1210 (0.9938) loss_zs_kd 0.1001 (0.0788) loss_oracle 0.3321 (0.3220) acc 75.0000 (73.2943) lr 1.9098e-04 eta 0:03:12
epoch [42/50] batch [260/288] time 0.080 (0.082) data 0.000 (0.002) loss 2.1290 (1.5817) teacher_loss 1.5281 (0.9983) loss_zs_kd 0.0904 (0.0789) loss_oracle 0.3599 (0.3220) acc 53.1250 (73.1731) lr 1.9098e-04 eta 0:03:11
epoch [42/50] batch [280/288] time 0.084 (0.082) data 0.000 (0.002) loss 1.5460 (1.5797) teacher_loss 0.9964 (0.9977) loss_zs_kd 0.0835 (0.0785) loss_oracle 0.3384 (0.3220) acc 68.7500 (73.3147) lr 1.9098e-04 eta 0:03:10
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,413
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 86.1%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.8%
******* Domain a best val acc:      86.6%, epoch: 42 *******
******* Domain a best val test acc: 83.4%, epoch: 42 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [43/50] batch [20/288] time 0.084 (0.106) data 0.000 (0.017) loss 1.7459 (1.6101) teacher_loss 0.9730 (1.0098) loss_zs_kd 0.1181 (0.0827) loss_oracle 0.3093 (0.3271) acc 87.5000 (74.5312) lr 1.5567e-04 eta 0:04:01
epoch [43/50] batch [40/288] time 0.084 (0.104) data 0.000 (0.009) loss 1.8419 (1.5955) teacher_loss 1.2099 (1.0053) loss_zs_kd 0.0787 (0.0819) loss_oracle 0.3160 (0.3230) acc 71.8750 (73.8281) lr 1.5567e-04 eta 0:03:56
epoch [43/50] batch [60/288] time 0.084 (0.098) data 0.000 (0.006) loss 1.7035 (1.6248) teacher_loss 1.0928 (1.0339) loss_zs_kd 0.0875 (0.0819) loss_oracle 0.3548 (0.3231) acc 75.0000 (73.1771) lr 1.5567e-04 eta 0:03:40
epoch [43/50] batch [80/288] time 0.084 (0.095) data 0.000 (0.004) loss 1.2512 (1.6001) teacher_loss 0.8171 (1.0176) loss_zs_kd 0.0506 (0.0801) loss_oracle 0.3075 (0.3229) acc 75.0000 (73.5547) lr 1.5567e-04 eta 0:03:31
epoch [43/50] batch [100/288] time 0.085 (0.093) data 0.000 (0.004) loss 1.4882 (1.5692) teacher_loss 0.9243 (0.9867) loss_zs_kd 0.0812 (0.0789) loss_oracle 0.3121 (0.3220) acc 68.7500 (74.0000) lr 1.5567e-04 eta 0:03:24
epoch [43/50] batch [120/288] time 0.084 (0.092) data 0.000 (0.003) loss 1.4061 (1.5840) teacher_loss 0.7809 (0.9996) loss_zs_kd 0.0706 (0.0791) loss_oracle 0.3529 (0.3223) acc 75.0000 (73.5417) lr 1.5567e-04 eta 0:03:20
epoch [43/50] batch [140/288] time 0.079 (0.091) data 0.000 (0.003) loss 1.4629 (1.5745) teacher_loss 0.9612 (0.9925) loss_zs_kd 0.0614 (0.0780) loss_oracle 0.3219 (0.3222) acc 68.7500 (73.4375) lr 1.5567e-04 eta 0:03:15
epoch [43/50] batch [160/288] time 0.080 (0.090) data 0.000 (0.002) loss 1.0545 (1.5650) teacher_loss 0.5133 (0.9865) loss_zs_kd 0.0504 (0.0770) loss_oracle 0.3271 (0.3219) acc 87.5000 (73.7695) lr 1.5567e-04 eta 0:03:12
epoch [43/50] batch [180/288] time 0.083 (0.089) data 0.000 (0.002) loss 1.4307 (1.5582) teacher_loss 0.8910 (0.9817) loss_zs_kd 0.0615 (0.0766) loss_oracle 0.2919 (0.3213) acc 81.2500 (73.8542) lr 1.5567e-04 eta 0:03:08
epoch [43/50] batch [200/288] time 0.085 (0.089) data 0.000 (0.002) loss 1.4079 (1.5677) teacher_loss 0.8501 (0.9893) loss_zs_kd 0.0535 (0.0776) loss_oracle 0.3527 (0.3219) acc 75.0000 (73.6250) lr 1.5567e-04 eta 0:03:06
epoch [43/50] batch [220/288] time 0.086 (0.088) data 0.000 (0.002) loss 1.3745 (1.5665) teacher_loss 0.7653 (0.9887) loss_zs_kd 0.0548 (0.0778) loss_oracle 0.3161 (0.3220) acc 75.0000 (73.5653) lr 1.5567e-04 eta 0:03:03
epoch [43/50] batch [240/288] time 0.086 (0.088) data 0.000 (0.002) loss 1.7376 (1.5783) teacher_loss 1.0902 (0.9989) loss_zs_kd 0.0788 (0.0785) loss_oracle 0.3529 (0.3224) acc 78.1250 (73.2552) lr 1.5567e-04 eta 0:03:01
epoch [43/50] batch [260/288] time 0.084 (0.088) data 0.000 (0.002) loss 1.7368 (1.5734) teacher_loss 1.1067 (0.9926) loss_zs_kd 0.1040 (0.0785) loss_oracle 0.3236 (0.3224) acc 68.7500 (73.4856) lr 1.5567e-04 eta 0:02:59
epoch [43/50] batch [280/288] time 0.085 (0.088) data 0.000 (0.001) loss 1.1946 (1.5722) teacher_loss 0.7143 (0.9901) loss_zs_kd 0.0583 (0.0782) loss_oracle 0.3097 (0.3226) acc 78.1250 (73.5379) lr 1.5567e-04 eta 0:02:57
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.7%
******* Domain a best val acc:      86.6%, epoch: 42 *******
******* Domain a best val test acc: 83.4%, epoch: 42 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [44/50] batch [20/288] time 0.082 (0.105) data 0.000 (0.019) loss 1.2209 (1.5415) teacher_loss 0.6545 (0.9507) loss_zs_kd 0.0869 (0.0828) loss_oracle 0.3444 (0.3119) acc 81.2500 (74.6875) lr 1.2369e-04 eta 0:03:29
epoch [44/50] batch [40/288] time 0.087 (0.095) data 0.000 (0.010) loss 1.2842 (1.5834) teacher_loss 0.8252 (0.9993) loss_zs_kd 0.0708 (0.0842) loss_oracle 0.3226 (0.3166) acc 81.2500 (73.2031) lr 1.2369e-04 eta 0:03:07
epoch [44/50] batch [60/288] time 0.141 (0.096) data 0.001 (0.007) loss 1.3827 (1.5597) teacher_loss 0.7987 (0.9836) loss_zs_kd 0.1237 (0.0819) loss_oracle 0.3152 (0.3177) acc 81.2500 (73.8542) lr 1.2369e-04 eta 0:03:07
epoch [44/50] batch [80/288] time 0.087 (0.096) data 0.001 (0.005) loss 1.5941 (1.5611) teacher_loss 1.1144 (0.9829) loss_zs_kd 0.0614 (0.0807) loss_oracle 0.3147 (0.3190) acc 71.8750 (73.8672) lr 1.2369e-04 eta 0:03:05
epoch [44/50] batch [100/288] time 0.086 (0.093) data 0.000 (0.004) loss 1.2091 (1.5357) teacher_loss 0.7013 (0.9611) loss_zs_kd 0.0632 (0.0796) loss_oracle 0.3354 (0.3188) acc 84.3750 (74.5000) lr 1.2369e-04 eta 0:02:58
epoch [44/50] batch [120/288] time 0.084 (0.092) data 0.000 (0.003) loss 1.0836 (1.5334) teacher_loss 0.5586 (0.9550) loss_zs_kd 0.0571 (0.0788) loss_oracle 0.3116 (0.3191) acc 84.3750 (74.3750) lr 1.2369e-04 eta 0:02:53
epoch [44/50] batch [140/288] time 0.084 (0.091) data 0.000 (0.003) loss 1.5040 (1.5418) teacher_loss 0.9337 (0.9653) loss_zs_kd 0.0780 (0.0777) loss_oracle 0.3262 (0.3192) acc 65.6250 (74.1964) lr 1.2369e-04 eta 0:02:49
epoch [44/50] batch [160/288] time 0.084 (0.090) data 0.000 (0.003) loss 1.9756 (1.5451) teacher_loss 1.2116 (0.9681) loss_zs_kd 0.0771 (0.0775) loss_oracle 0.3168 (0.3202) acc 68.7500 (74.0820) lr 1.2369e-04 eta 0:02:46
epoch [44/50] batch [180/288] time 0.084 (0.089) data 0.000 (0.002) loss 1.8680 (1.5508) teacher_loss 1.2225 (0.9709) loss_zs_kd 0.1068 (0.0781) loss_oracle 0.3471 (0.3201) acc 62.5000 (74.0451) lr 1.2369e-04 eta 0:02:43
epoch [44/50] batch [200/288] time 0.086 (0.089) data 0.000 (0.002) loss 1.2799 (1.5566) teacher_loss 0.6117 (0.9728) loss_zs_kd 0.0858 (0.0780) loss_oracle 0.3473 (0.3205) acc 84.3750 (74.0312) lr 1.2369e-04 eta 0:02:41
epoch [44/50] batch [220/288] time 0.086 (0.088) data 0.000 (0.002) loss 1.0471 (1.5461) teacher_loss 0.5709 (0.9649) loss_zs_kd 0.0851 (0.0777) loss_oracle 0.2963 (0.3210) acc 87.5000 (74.1619) lr 1.2369e-04 eta 0:02:38
epoch [44/50] batch [240/288] time 0.080 (0.088) data 0.001 (0.002) loss 1.6728 (1.5578) teacher_loss 1.1987 (0.9758) loss_zs_kd 0.0804 (0.0776) loss_oracle 0.3177 (0.3209) acc 62.5000 (73.7370) lr 1.2369e-04 eta 0:02:36
epoch [44/50] batch [260/288] time 0.085 (0.088) data 0.000 (0.002) loss 1.6019 (1.5576) teacher_loss 0.7882 (0.9741) loss_zs_kd 0.0778 (0.0775) loss_oracle 0.2944 (0.3208) acc 81.2500 (73.8942) lr 1.2369e-04 eta 0:02:34
epoch [44/50] batch [280/288] time 0.085 (0.088) data 0.000 (0.002) loss 1.2705 (1.5594) teacher_loss 0.6977 (0.9742) loss_zs_kd 0.0551 (0.0774) loss_oracle 0.3431 (0.3210) acc 81.2500 (73.7835) lr 1.2369e-04 eta 0:02:31
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,409
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.7%
******* Domain a best val acc:      86.6%, epoch: 42 *******
******* Domain a best val test acc: 83.4%, epoch: 42 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [45/50] batch [20/288] time 0.085 (0.102) data 0.000 (0.015) loss 1.6765 (1.4548) teacher_loss 1.1624 (0.8939) loss_zs_kd 0.0568 (0.0767) loss_oracle 0.3435 (0.3218) acc 68.7500 (75.1562) lr 9.5173e-05 eta 0:02:53
epoch [45/50] batch [40/288] time 0.091 (0.093) data 0.001 (0.008) loss 1.1441 (1.5144) teacher_loss 0.6272 (0.9376) loss_zs_kd 0.0972 (0.0776) loss_oracle 0.3465 (0.3224) acc 87.5000 (73.7500) lr 9.5173e-05 eta 0:02:37
epoch [45/50] batch [60/288] time 0.088 (0.090) data 0.000 (0.005) loss 1.2177 (1.5673) teacher_loss 0.6586 (0.9824) loss_zs_kd 0.0631 (0.0765) loss_oracle 0.3249 (0.3200) acc 81.2500 (72.7604) lr 9.5173e-05 eta 0:02:30
epoch [45/50] batch [80/288] time 0.087 (0.089) data 0.000 (0.004) loss 1.8299 (1.6193) teacher_loss 1.2162 (1.0305) loss_zs_kd 0.0602 (0.0794) loss_oracle 0.3230 (0.3180) acc 68.7500 (71.9531) lr 9.5173e-05 eta 0:02:25
epoch [45/50] batch [100/288] time 0.089 (0.092) data 0.000 (0.003) loss 1.8413 (1.5913) teacher_loss 1.3206 (1.0035) loss_zs_kd 0.0982 (0.0789) loss_oracle 0.3127 (0.3192) acc 65.6250 (72.6562) lr 9.5173e-05 eta 0:02:29
epoch [45/50] batch [120/288] time 0.086 (0.091) data 0.000 (0.003) loss 1.5602 (1.5910) teacher_loss 1.1382 (1.0053) loss_zs_kd 0.0968 (0.0792) loss_oracle 0.2946 (0.3200) acc 68.7500 (72.6562) lr 9.5173e-05 eta 0:02:25
epoch [45/50] batch [140/288] time 0.085 (0.090) data 0.000 (0.002) loss 1.3875 (1.5880) teacher_loss 0.7937 (1.0044) loss_zs_kd 0.0369 (0.0791) loss_oracle 0.2933 (0.3198) acc 81.2500 (72.7902) lr 9.5173e-05 eta 0:02:22
epoch [45/50] batch [160/288] time 0.084 (0.089) data 0.000 (0.002) loss 1.6328 (1.5918) teacher_loss 0.8911 (1.0041) loss_zs_kd 0.0506 (0.0788) loss_oracle 0.3006 (0.3198) acc 65.6250 (72.8711) lr 9.5173e-05 eta 0:02:19
epoch [45/50] batch [180/288] time 0.087 (0.089) data 0.000 (0.002) loss 1.4119 (1.5781) teacher_loss 0.7256 (0.9874) loss_zs_kd 0.0692 (0.0784) loss_oracle 0.3103 (0.3196) acc 84.3750 (73.2639) lr 9.5173e-05 eta 0:02:17
epoch [45/50] batch [200/288] time 0.086 (0.088) data 0.000 (0.002) loss 1.5588 (1.5742) teacher_loss 0.9110 (0.9816) loss_zs_kd 0.0522 (0.0780) loss_oracle 0.3222 (0.3205) acc 81.2500 (73.4219) lr 9.5173e-05 eta 0:02:14
epoch [45/50] batch [220/288] time 0.084 (0.088) data 0.000 (0.002) loss 2.0564 (1.5777) teacher_loss 1.4080 (0.9833) loss_zs_kd 0.0882 (0.0784) loss_oracle 0.3400 (0.3212) acc 65.6250 (73.4517) lr 9.5173e-05 eta 0:02:12
epoch [45/50] batch [240/288] time 0.089 (0.088) data 0.001 (0.002) loss 1.3783 (1.5754) teacher_loss 0.9305 (0.9806) loss_zs_kd 0.0650 (0.0776) loss_oracle 0.3579 (0.3217) acc 75.0000 (73.5938) lr 9.5173e-05 eta 0:02:10
epoch [45/50] batch [260/288] time 0.087 (0.087) data 0.001 (0.001) loss 1.9299 (1.5822) teacher_loss 1.2912 (0.9897) loss_zs_kd 0.0650 (0.0778) loss_oracle 0.3055 (0.3214) acc 65.6250 (73.5577) lr 9.5173e-05 eta 0:02:08
epoch [45/50] batch [280/288] time 0.083 (0.087) data 0.000 (0.001) loss 1.8928 (1.5793) teacher_loss 1.3739 (0.9881) loss_zs_kd 0.0872 (0.0776) loss_oracle 0.3141 (0.3213) acc 59.3750 (73.5938) lr 9.5173e-05 eta 0:02:06
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,408
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.8%
******* Domain a best val acc:      86.6%, epoch: 42 *******
******* Domain a best val test acc: 83.4%, epoch: 42 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [46/50] batch [20/288] time 0.072 (0.093) data 0.000 (0.014) loss 1.9441 (1.5602) teacher_loss 1.2929 (0.9769) loss_zs_kd 0.0773 (0.0785) loss_oracle 0.2949 (0.3168) acc 65.6250 (73.7500) lr 7.0224e-05 eta 0:02:12
epoch [46/50] batch [40/288] time 0.084 (0.086) data 0.000 (0.007) loss 1.6681 (1.5626) teacher_loss 1.1644 (0.9634) loss_zs_kd 0.0423 (0.0782) loss_oracle 0.3454 (0.3223) acc 71.8750 (74.2969) lr 7.0224e-05 eta 0:02:00
epoch [46/50] batch [60/288] time 0.077 (0.085) data 0.000 (0.005) loss 1.9641 (1.5613) teacher_loss 1.2328 (0.9573) loss_zs_kd 0.0829 (0.0781) loss_oracle 0.3545 (0.3227) acc 65.6250 (74.3229) lr 7.0224e-05 eta 0:01:56
epoch [46/50] batch [80/288] time 0.083 (0.084) data 0.000 (0.004) loss 1.2734 (1.5549) teacher_loss 0.7109 (0.9628) loss_zs_kd 0.0601 (0.0772) loss_oracle 0.3592 (0.3208) acc 81.2500 (74.1406) lr 7.0224e-05 eta 0:01:53
epoch [46/50] batch [100/288] time 0.075 (0.083) data 0.000 (0.003) loss 1.3504 (1.5639) teacher_loss 0.7920 (0.9667) loss_zs_kd 0.0656 (0.0759) loss_oracle 0.3525 (0.3217) acc 71.8750 (74.2500) lr 7.0224e-05 eta 0:01:51
epoch [46/50] batch [120/288] time 0.067 (0.083) data 0.000 (0.002) loss 2.0412 (1.5728) teacher_loss 1.4919 (0.9777) loss_zs_kd 0.1009 (0.0764) loss_oracle 0.3333 (0.3217) acc 56.2500 (74.1406) lr 7.0224e-05 eta 0:01:49
epoch [46/50] batch [140/288] time 0.072 (0.082) data 0.000 (0.002) loss 1.4622 (1.5764) teacher_loss 0.9473 (0.9841) loss_zs_kd 0.0594 (0.0769) loss_oracle 0.3223 (0.3202) acc 75.0000 (74.0402) lr 7.0224e-05 eta 0:01:46
epoch [46/50] batch [160/288] time 0.081 (0.082) data 0.000 (0.002) loss 1.6563 (1.5652) teacher_loss 1.1392 (0.9796) loss_zs_kd 0.0791 (0.0763) loss_oracle 0.3316 (0.3206) acc 68.7500 (74.2969) lr 7.0224e-05 eta 0:01:45
epoch [46/50] batch [180/288] time 0.080 (0.082) data 0.000 (0.002) loss 1.5111 (1.5577) teacher_loss 0.8200 (0.9737) loss_zs_kd 0.0913 (0.0768) loss_oracle 0.3181 (0.3207) acc 71.8750 (74.4618) lr 7.0224e-05 eta 0:01:43
epoch [46/50] batch [200/288] time 0.080 (0.082) data 0.000 (0.002) loss 2.2319 (1.5650) teacher_loss 1.6314 (0.9800) loss_zs_kd 0.0748 (0.0772) loss_oracle 0.3061 (0.3210) acc 62.5000 (74.3125) lr 7.0224e-05 eta 0:01:41
epoch [46/50] batch [220/288] time 0.081 (0.082) data 0.000 (0.001) loss 1.6722 (1.5608) teacher_loss 1.1432 (0.9756) loss_zs_kd 0.0783 (0.0779) loss_oracle 0.3259 (0.3205) acc 65.6250 (74.1335) lr 7.0224e-05 eta 0:01:40
epoch [46/50] batch [240/288] time 0.081 (0.082) data 0.000 (0.001) loss 1.7277 (1.5686) teacher_loss 1.2012 (0.9820) loss_zs_kd 0.0846 (0.0779) loss_oracle 0.3039 (0.3203) acc 65.6250 (73.8932) lr 7.0224e-05 eta 0:01:38
epoch [46/50] batch [260/288] time 0.081 (0.082) data 0.000 (0.001) loss 1.5751 (1.5718) teacher_loss 1.0455 (0.9816) loss_zs_kd 0.0581 (0.0778) loss_oracle 0.3135 (0.3206) acc 75.0000 (73.9183) lr 7.0224e-05 eta 0:01:37
epoch [46/50] batch [280/288] time 0.087 (0.082) data 0.000 (0.001) loss 2.0774 (1.5674) teacher_loss 1.4153 (0.9773) loss_zs_kd 0.0628 (0.0777) loss_oracle 0.3473 (0.3205) acc 65.6250 (73.9955) lr 7.0224e-05 eta 0:01:35
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,408
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.7%
******* Domain a best val acc:      86.6%, epoch: 42 *******
******* Domain a best val test acc: 83.4%, epoch: 42 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [47/50] batch [20/288] time 0.085 (0.096) data 0.000 (0.012) loss 1.8022 (1.6165) teacher_loss 1.1971 (1.0480) loss_zs_kd 0.0523 (0.0768) loss_oracle 0.3168 (0.3211) acc 62.5000 (72.1875) lr 4.8943e-05 eta 0:01:48
epoch [47/50] batch [40/288] time 0.089 (0.090) data 0.000 (0.006) loss 1.5686 (1.6219) teacher_loss 0.8852 (1.0231) loss_zs_kd 0.0485 (0.0780) loss_oracle 0.3435 (0.3200) acc 81.2500 (73.4375) lr 4.8943e-05 eta 0:01:40
epoch [47/50] batch [60/288] time 0.084 (0.088) data 0.000 (0.004) loss 1.6094 (1.5690) teacher_loss 1.0050 (0.9824) loss_zs_kd 0.0663 (0.0768) loss_oracle 0.3293 (0.3180) acc 71.8750 (74.0625) lr 4.8943e-05 eta 0:01:36
epoch [47/50] batch [80/288] time 0.085 (0.087) data 0.000 (0.003) loss 1.4438 (1.5770) teacher_loss 0.7620 (0.9896) loss_zs_kd 0.0673 (0.0767) loss_oracle 0.2937 (0.3196) acc 75.0000 (73.7891) lr 4.8943e-05 eta 0:01:33
epoch [47/50] batch [100/288] time 0.089 (0.087) data 0.000 (0.003) loss 1.5151 (1.5635) teacher_loss 1.0311 (0.9827) loss_zs_kd 0.1040 (0.0774) loss_oracle 0.3095 (0.3181) acc 75.0000 (74.0312) lr 4.8943e-05 eta 0:01:31
epoch [47/50] batch [120/288] time 0.084 (0.087) data 0.000 (0.002) loss 1.5643 (1.5747) teacher_loss 0.9010 (0.9900) loss_zs_kd 0.0785 (0.0792) loss_oracle 0.3426 (0.3185) acc 78.1250 (73.9844) lr 4.8943e-05 eta 0:01:29
epoch [47/50] batch [140/288] time 0.099 (0.087) data 0.000 (0.002) loss 1.7415 (1.5741) teacher_loss 1.1432 (0.9908) loss_zs_kd 0.1085 (0.0785) loss_oracle 0.3243 (0.3187) acc 71.8750 (73.9062) lr 4.8943e-05 eta 0:01:27
epoch [47/50] batch [160/288] time 0.088 (0.086) data 0.000 (0.002) loss 1.9103 (1.5772) teacher_loss 1.3688 (0.9929) loss_zs_kd 0.0992 (0.0792) loss_oracle 0.3396 (0.3192) acc 65.6250 (74.0039) lr 4.8943e-05 eta 0:01:25
epoch [47/50] batch [180/288] time 0.083 (0.086) data 0.000 (0.002) loss 1.2092 (1.5678) teacher_loss 0.6201 (0.9859) loss_zs_kd 0.0862 (0.0793) loss_oracle 0.3128 (0.3189) acc 78.1250 (74.0104) lr 4.8943e-05 eta 0:01:23
epoch [47/50] batch [200/288] time 0.084 (0.086) data 0.000 (0.001) loss 1.8022 (1.5705) teacher_loss 1.2379 (0.9915) loss_zs_kd 0.0670 (0.0786) loss_oracle 0.2988 (0.3188) acc 68.7500 (74.0156) lr 4.8943e-05 eta 0:01:21
epoch [47/50] batch [220/288] time 0.084 (0.086) data 0.000 (0.001) loss 1.2317 (1.5782) teacher_loss 0.6312 (0.9961) loss_zs_kd 0.0574 (0.0787) loss_oracle 0.3346 (0.3193) acc 90.6250 (74.0341) lr 4.8943e-05 eta 0:01:19
epoch [47/50] batch [240/288] time 0.088 (0.085) data 0.000 (0.001) loss 1.9197 (1.5755) teacher_loss 1.3631 (0.9919) loss_zs_kd 0.0698 (0.0784) loss_oracle 0.3354 (0.3190) acc 59.3750 (73.9974) lr 4.8943e-05 eta 0:01:17
epoch [47/50] batch [260/288] time 0.085 (0.085) data 0.000 (0.001) loss 1.2326 (1.5645) teacher_loss 0.7155 (0.9825) loss_zs_kd 0.0567 (0.0779) loss_oracle 0.3354 (0.3200) acc 81.2500 (74.1226) lr 4.8943e-05 eta 0:01:16
epoch [47/50] batch [280/288] time 0.089 (0.085) data 0.001 (0.001) loss 1.6912 (1.5611) teacher_loss 0.9895 (0.9814) loss_zs_kd 0.1082 (0.0774) loss_oracle 0.3155 (0.3204) acc 75.0000 (74.0402) lr 4.8943e-05 eta 0:01:14
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,019
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.6%
******* Domain a best val acc:      86.6%, epoch: 42 *******
******* Domain a best val test acc: 83.4%, epoch: 42 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [48/50] batch [20/288] time 0.081 (0.099) data 0.000 (0.014) loss 1.7748 (1.6356) teacher_loss 1.1777 (1.0566) loss_zs_kd 0.0809 (0.0814) loss_oracle 0.3270 (0.3234) acc 68.7500 (71.5625) lr 3.1417e-05 eta 0:01:23
epoch [48/50] batch [40/288] time 0.083 (0.090) data 0.000 (0.007) loss 1.3181 (1.5908) teacher_loss 0.6049 (1.0126) loss_zs_kd 0.0814 (0.0822) loss_oracle 0.3198 (0.3229) acc 81.2500 (72.5000) lr 3.1417e-05 eta 0:01:14
epoch [48/50] batch [60/288] time 0.083 (0.088) data 0.000 (0.005) loss 1.2349 (1.5815) teacher_loss 0.8484 (1.0018) loss_zs_kd 0.0582 (0.0825) loss_oracle 0.3086 (0.3201) acc 81.2500 (72.9167) lr 3.1417e-05 eta 0:01:10
epoch [48/50] batch [80/288] time 0.086 (0.086) data 0.000 (0.004) loss 1.3168 (1.5830) teacher_loss 0.7748 (0.9921) loss_zs_kd 0.0837 (0.0812) loss_oracle 0.2990 (0.3175) acc 75.0000 (72.8516) lr 3.1417e-05 eta 0:01:07
epoch [48/50] batch [100/288] time 0.082 (0.086) data 0.000 (0.003) loss 1.3940 (1.5887) teacher_loss 0.8894 (0.9952) loss_zs_kd 0.0533 (0.0802) loss_oracle 0.2880 (0.3178) acc 75.0000 (73.0625) lr 3.1417e-05 eta 0:01:05
epoch [48/50] batch [120/288] time 0.067 (0.085) data 0.000 (0.002) loss 1.1946 (1.5921) teacher_loss 0.5242 (0.9994) loss_zs_kd 0.0993 (0.0801) loss_oracle 0.3580 (0.3180) acc 87.5000 (73.1771) lr 3.1417e-05 eta 0:01:03
epoch [48/50] batch [140/288] time 0.087 (0.085) data 0.000 (0.002) loss 1.3326 (1.6013) teacher_loss 0.8410 (1.0093) loss_zs_kd 0.0771 (0.0798) loss_oracle 0.3100 (0.3197) acc 81.2500 (73.0134) lr 3.1417e-05 eta 0:01:01
epoch [48/50] batch [160/288] time 0.086 (0.085) data 0.000 (0.002) loss 2.0122 (1.5943) teacher_loss 1.4072 (1.0014) loss_zs_kd 0.0846 (0.0794) loss_oracle 0.3494 (0.3204) acc 65.6250 (73.3984) lr 3.1417e-05 eta 0:00:59
epoch [48/50] batch [180/288] time 0.083 (0.085) data 0.000 (0.002) loss 2.2876 (1.5856) teacher_loss 1.6326 (0.9912) loss_zs_kd 0.1130 (0.0794) loss_oracle 0.3090 (0.3207) acc 53.1250 (73.6806) lr 3.1417e-05 eta 0:00:57
epoch [48/50] batch [200/288] time 0.143 (0.087) data 0.000 (0.002) loss 1.8847 (1.5864) teacher_loss 1.2864 (0.9922) loss_zs_kd 0.0838 (0.0794) loss_oracle 0.3076 (0.3208) acc 65.6250 (73.7188) lr 3.1417e-05 eta 0:00:57
epoch [48/50] batch [220/288] time 0.082 (0.086) data 0.000 (0.001) loss 1.3555 (1.5882) teacher_loss 0.6841 (0.9951) loss_zs_kd 0.0734 (0.0791) loss_oracle 0.3456 (0.3205) acc 81.2500 (73.5795) lr 3.1417e-05 eta 0:00:55
epoch [48/50] batch [240/288] time 0.082 (0.085) data 0.000 (0.001) loss 1.3040 (1.5836) teacher_loss 0.7880 (0.9890) loss_zs_kd 0.0698 (0.0789) loss_oracle 0.3283 (0.3205) acc 75.0000 (73.8932) lr 3.1417e-05 eta 0:00:53
epoch [48/50] batch [260/288] time 0.067 (0.084) data 0.000 (0.001) loss 1.2953 (1.5810) teacher_loss 0.8169 (0.9855) loss_zs_kd 0.0518 (0.0785) loss_oracle 0.3172 (0.3210) acc 68.7500 (73.9784) lr 3.1417e-05 eta 0:00:50
epoch [48/50] batch [280/288] time 0.083 (0.083) data 0.000 (0.001) loss 1.5695 (1.5760) teacher_loss 0.8519 (0.9786) loss_zs_kd 0.1023 (0.0792) loss_oracle 0.3606 (0.3216) acc 75.0000 (74.1964) lr 3.1417e-05 eta 0:00:48
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,019
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.6%
******* Domain a best val acc:      86.6%, epoch: 42 *******
******* Domain a best val test acc: 83.4%, epoch: 42 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [49/50] batch [20/288] time 0.083 (0.097) data 0.000 (0.013) loss 1.2118 (1.5014) teacher_loss 0.6041 (0.8934) loss_zs_kd 0.1017 (0.0748) loss_oracle 0.3197 (0.3243) acc 84.3750 (76.8750) lr 1.7713e-05 eta 0:00:53
epoch [49/50] batch [40/288] time 0.083 (0.090) data 0.000 (0.007) loss 1.3686 (1.5193) teacher_loss 0.7903 (0.9200) loss_zs_kd 0.0554 (0.0760) loss_oracle 0.3243 (0.3214) acc 78.1250 (76.0938) lr 1.7713e-05 eta 0:00:48
epoch [49/50] batch [60/288] time 0.077 (0.088) data 0.000 (0.005) loss 1.5850 (1.5192) teacher_loss 1.0118 (0.9261) loss_zs_kd 0.0797 (0.0793) loss_oracle 0.3065 (0.3213) acc 75.0000 (75.3125) lr 1.7713e-05 eta 0:00:45
epoch [49/50] batch [80/288] time 0.082 (0.086) data 0.000 (0.003) loss 1.4232 (1.5069) teacher_loss 0.9799 (0.9118) loss_zs_kd 0.0806 (0.0818) loss_oracle 0.3079 (0.3220) acc 68.7500 (75.6641) lr 1.7713e-05 eta 0:00:42
epoch [49/50] batch [100/288] time 0.084 (0.086) data 0.000 (0.003) loss 1.4625 (1.5219) teacher_loss 0.9840 (0.9249) loss_zs_kd 0.1133 (0.0823) loss_oracle 0.3319 (0.3221) acc 75.0000 (75.1562) lr 1.7713e-05 eta 0:00:40
epoch [49/50] batch [120/288] time 0.084 (0.086) data 0.000 (0.002) loss 1.4732 (1.5289) teacher_loss 0.9776 (0.9406) loss_zs_kd 0.0530 (0.0818) loss_oracle 0.3302 (0.3221) acc 75.0000 (74.8438) lr 1.7713e-05 eta 0:00:39
epoch [49/50] batch [140/288] time 0.086 (0.085) data 0.000 (0.002) loss 1.6237 (1.5414) teacher_loss 1.0228 (0.9499) loss_zs_kd 0.1151 (0.0815) loss_oracle 0.3129 (0.3224) acc 75.0000 (74.5982) lr 1.7713e-05 eta 0:00:37
epoch [49/50] batch [160/288] time 0.085 (0.085) data 0.000 (0.002) loss 1.6572 (1.5552) teacher_loss 1.1938 (0.9627) loss_zs_kd 0.0814 (0.0812) loss_oracle 0.3109 (0.3225) acc 71.8750 (74.3359) lr 1.7713e-05 eta 0:00:35
epoch [49/50] batch [180/288] time 0.088 (0.085) data 0.000 (0.002) loss 1.5220 (1.5547) teacher_loss 0.8172 (0.9619) loss_zs_kd 0.0874 (0.0801) loss_oracle 0.3154 (0.3216) acc 71.8750 (74.4271) lr 1.7713e-05 eta 0:00:33
epoch [49/50] batch [200/288] time 0.088 (0.085) data 0.000 (0.002) loss 1.5014 (1.5655) teacher_loss 1.1619 (0.9747) loss_zs_kd 0.0446 (0.0794) loss_oracle 0.3162 (0.3220) acc 68.7500 (74.1719) lr 1.7713e-05 eta 0:00:32
epoch [49/50] batch [220/288] time 0.086 (0.085) data 0.000 (0.001) loss 1.3426 (1.5608) teacher_loss 0.6859 (0.9713) loss_zs_kd 0.0875 (0.0791) loss_oracle 0.3371 (0.3218) acc 78.1250 (74.2472) lr 1.7713e-05 eta 0:00:30
epoch [49/50] batch [240/288] time 0.100 (0.086) data 0.001 (0.001) loss 2.0190 (1.5675) teacher_loss 1.1926 (0.9736) loss_zs_kd 0.0999 (0.0794) loss_oracle 0.3620 (0.3220) acc 68.7500 (74.2318) lr 1.7713e-05 eta 0:00:29
epoch [49/50] batch [260/288] time 0.087 (0.086) data 0.000 (0.001) loss 1.8748 (1.5735) teacher_loss 1.3550 (0.9789) loss_zs_kd 0.0780 (0.0798) loss_oracle 0.3273 (0.3217) acc 59.3750 (74.1346) lr 1.7713e-05 eta 0:00:27
epoch [49/50] batch [280/288] time 0.084 (0.086) data 0.000 (0.001) loss 1.6456 (1.5747) teacher_loss 1.0121 (0.9782) loss_zs_kd 0.0804 (0.0801) loss_oracle 0.3246 (0.3214) acc 71.8750 (74.0625) lr 1.7713e-05 eta 0:00:25
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.7%
******* Domain a best val acc:      86.6%, epoch: 42 *******
******* Domain a best val test acc: 83.4%, epoch: 42 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
epoch [50/50] batch [20/288] time 0.088 (0.102) data 0.000 (0.014) loss 1.3292 (1.5284) teacher_loss 0.6802 (0.9317) loss_zs_kd 0.0940 (0.0746) loss_oracle 0.3021 (0.3266) acc 84.3750 (76.2500) lr 7.8853e-06 eta 0:00:27
epoch [50/50] batch [40/288] time 0.083 (0.093) data 0.000 (0.007) loss 1.7337 (1.6059) teacher_loss 1.1329 (1.0064) loss_zs_kd 0.0681 (0.0775) loss_oracle 0.3048 (0.3248) acc 62.5000 (73.5938) lr 7.8853e-06 eta 0:00:23
epoch [50/50] batch [60/288] time 0.084 (0.090) data 0.000 (0.005) loss 1.4356 (1.6052) teacher_loss 0.8971 (1.0097) loss_zs_kd 0.0798 (0.0771) loss_oracle 0.3210 (0.3238) acc 78.1250 (73.0729) lr 7.8853e-06 eta 0:00:20
epoch [50/50] batch [80/288] time 0.084 (0.088) data 0.000 (0.004) loss 1.6562 (1.5944) teacher_loss 1.1114 (1.0057) loss_zs_kd 0.0764 (0.0756) loss_oracle 0.3155 (0.3232) acc 68.7500 (73.3594) lr 7.8853e-06 eta 0:00:18
epoch [50/50] batch [100/288] time 0.085 (0.088) data 0.000 (0.003) loss 1.6749 (1.5823) teacher_loss 0.8486 (0.9954) loss_zs_kd 0.1084 (0.0765) loss_oracle 0.3548 (0.3225) acc 71.8750 (73.9062) lr 7.8853e-06 eta 0:00:16
epoch [50/50] batch [120/288] time 0.085 (0.087) data 0.000 (0.003) loss 1.6933 (1.5899) teacher_loss 1.0455 (0.9968) loss_zs_kd 0.1150 (0.0775) loss_oracle 0.3043 (0.3222) acc 75.0000 (73.7760) lr 7.8853e-06 eta 0:00:14
epoch [50/50] batch [140/288] time 0.084 (0.086) data 0.000 (0.002) loss 1.4003 (1.5864) teacher_loss 0.7094 (0.9960) loss_zs_kd 0.0687 (0.0772) loss_oracle 0.3320 (0.3223) acc 81.2500 (73.8839) lr 7.8853e-06 eta 0:00:12
epoch [50/50] batch [160/288] time 0.085 (0.086) data 0.000 (0.002) loss 2.0791 (1.5925) teacher_loss 1.4618 (1.0009) loss_zs_kd 0.1068 (0.0773) loss_oracle 0.3189 (0.3219) acc 62.5000 (73.7500) lr 7.8853e-06 eta 0:00:11
epoch [50/50] batch [180/288] time 0.085 (0.086) data 0.000 (0.002) loss 1.3980 (1.5858) teacher_loss 0.8606 (0.9951) loss_zs_kd 0.0783 (0.0777) loss_oracle 0.2776 (0.3218) acc 78.1250 (73.8368) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [200/288] time 0.082 (0.086) data 0.000 (0.002) loss 1.5795 (1.5878) teacher_loss 0.9603 (0.9988) loss_zs_kd 0.0774 (0.0779) loss_oracle 0.3483 (0.3214) acc 75.0000 (73.8281) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [220/288] time 0.082 (0.086) data 0.000 (0.001) loss 1.3423 (1.5929) teacher_loss 0.5829 (0.9998) loss_zs_kd 0.0455 (0.0784) loss_oracle 0.3149 (0.3212) acc 84.3750 (73.7926) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [240/288] time 0.085 (0.086) data 0.000 (0.001) loss 1.0147 (1.5910) teacher_loss 0.5450 (1.0003) loss_zs_kd 0.0515 (0.0779) loss_oracle 0.3019 (0.3207) acc 84.3750 (73.8021) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [260/288] time 0.084 (0.086) data 0.000 (0.001) loss 1.4151 (1.5858) teacher_loss 0.9024 (0.9945) loss_zs_kd 0.0707 (0.0783) loss_oracle 0.3315 (0.3207) acc 81.2500 (73.8942) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [280/288] time 0.072 (0.086) data 0.000 (0.001) loss 1.8776 (1.5746) teacher_loss 1.2986 (0.9849) loss_zs_kd 0.0936 (0.0783) loss_oracle 0.3369 (0.3212) acc 62.5000 (74.1629) lr 7.8853e-06 eta 0:00:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,408
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.8%
******* Domain a best val acc:      86.6%, epoch: 42 *******
******* Domain a best val test acc: 83.4%, epoch: 42 *******
******* Domain a best test acc:     83.8%, epoch: 25 *******
Checkpoint saved to icml/multi-dg/tuning/16_seperate_2prompt/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model.pth.tar-50
Finish the whole training
Elapsed: 0:26:43
