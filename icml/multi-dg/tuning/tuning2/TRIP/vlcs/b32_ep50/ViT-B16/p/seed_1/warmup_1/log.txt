Loading trainer: TRIP
Loading dataset: SPG_VLCS
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -----------------------------
Dataset    SPG_VLCS
Source     ['caltech', 'labelme', 'sun']
Target     ['pascal']
# classes  5
# train_x  5,147
# val      2,206
# test     3,376
---------  -----------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,195
[Info] Hyperparameters saved to: icml/multi-dg/tuning/tuning2/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/hyperparameters.json
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/tuning/tuning2/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/160] time 0.079 (0.146) data 0.000 (0.021) loss 1.2721 (1.2343) teacher_loss 0.7983 (0.6787) loss_zs_kd 0.0001 (0.0000) loss_oracle 0.0008 (0.0002) kd_loss 0.4734 (0.5556) acc 71.8750 (75.4688) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3348 (0.3348) gate/usage_min 0.3312 (0.3313) gate/usage_std 0.0015 (0.0015) teacher/entropy 0.6260 (0.5444) teacher/usage_max 0.3962 (0.4744) teacher/usage_min 0.2903 (0.2313) teacher/usage_std 0.0455 (0.1052) nleep/row_max_mean 1525.0288 (1532.5052) nleep/row_max_std 75.8094 (55.8037) nleep/row_min_mean 1521.5259 (1527.7872) lr 1.0000e-05 eta 0:19:23
epoch [1/50] batch [40/160] time 0.095 (0.124) data 0.000 (0.010) loss 1.0076 (1.1485) teacher_loss 0.6720 (0.6659) loss_zs_kd 0.0002 (0.0001) loss_oracle 0.0018 (0.0008) kd_loss 0.3346 (0.4822) acc 75.0000 (75.7031) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3347 (0.3348) gate/usage_min 0.3313 (0.3313) gate/usage_std 0.0015 (0.0015) teacher/entropy 0.7658 (0.6177) teacher/usage_max 0.5492 (0.4721) teacher/usage_min 0.2090 (0.2302) teacher/usage_std 0.1532 (0.1044) nleep/row_max_mean 1518.7971 (1529.5475) nleep/row_max_std 71.4663 (59.0552) nleep/row_min_mean 1516.1426 (1525.6959) lr 1.0000e-05 eta 0:16:25
epoch [1/50] batch [60/160] time 0.155 (0.119) data 0.000 (0.007) loss 0.7814 (1.1285) teacher_loss 0.5321 (0.6924) loss_zs_kd 0.0004 (0.0002) loss_oracle 0.0042 (0.0017) kd_loss 0.2470 (0.4352) acc 81.2500 (75.2083) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3348 (0.3348) gate/usage_min 0.3312 (0.3313) gate/usage_std 0.0015 (0.0015) teacher/entropy 0.8530 (0.6647) teacher/usage_max 0.4732 (0.4697) teacher/usage_min 0.2524 (0.2333) teacher/usage_std 0.0993 (0.1021) nleep/row_max_mean 1534.7770 (1528.6450) nleep/row_max_std 69.7150 (59.1135) nleep/row_min_mean 1532.9285 (1525.2868) lr 1.0000e-05 eta 0:15:46
epoch [1/50] batch [80/160] time 0.139 (0.118) data 0.000 (0.005) loss 1.1110 (1.0902) teacher_loss 0.9108 (0.7018) loss_zs_kd 0.0006 (0.0003) loss_oracle 0.0064 (0.0024) kd_loss 0.1967 (0.3870) acc 75.0000 (74.4531) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3347 (0.3348) gate/usage_min 0.3314 (0.3313) gate/usage_std 0.0014 (0.0015) teacher/entropy 0.9027 (0.7127) teacher/usage_max 0.4196 (0.4583) teacher/usage_min 0.2761 (0.2403) teacher/usage_std 0.0621 (0.0942) nleep/row_max_mean 1525.4740 (1527.4016) nleep/row_max_std 73.4908 (59.7032) nleep/row_min_mean 1523.9672 (1524.4461) lr 1.0000e-05 eta 0:15:37
epoch [1/50] batch [100/160] time 0.118 (0.124) data 0.001 (0.004) loss 0.7742 (1.0497) teacher_loss 0.6263 (0.6979) loss_zs_kd 0.0013 (0.0004) loss_oracle 0.0064 (0.0031) kd_loss 0.1441 (0.3500) acc 81.2500 (74.6562) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3347 (0.3348) gate/usage_min 0.3313 (0.3313) gate/usage_std 0.0015 (0.0015) teacher/entropy 0.9551 (0.7496) teacher/usage_max 0.3998 (0.4486) teacher/usage_min 0.2702 (0.2461) teacher/usage_std 0.0530 (0.0873) nleep/row_max_mean 1514.2443 (1526.0819) nleep/row_max_std 67.8350 (60.1045) nleep/row_min_mean 1513.0591 (1523.4143) lr 1.0000e-05 eta 0:16:19
epoch [1/50] batch [120/160] time 0.099 (0.120) data 0.000 (0.004) loss 0.8342 (1.0059) teacher_loss 0.7047 (0.6872) loss_zs_kd 0.0012 (0.0005) loss_oracle 0.0088 (0.0038) kd_loss 0.1245 (0.3165) acc 75.0000 (74.9740) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3348 (0.3348) gate/usage_min 0.3312 (0.3313) gate/usage_std 0.0015 (0.0015) teacher/entropy 0.9747 (0.7830) teacher/usage_max 0.3983 (0.4386) teacher/usage_min 0.3002 (0.2523) teacher/usage_std 0.0460 (0.0803) nleep/row_max_mean 1492.2166 (1525.8698) nleep/row_max_std 109.2258 (58.6679) nleep/row_min_mean 1491.0973 (1523.4395) lr 1.0000e-05 eta 0:15:48
epoch [1/50] batch [140/160] time 0.105 (0.118) data 0.000 (0.003) loss 0.6288 (0.9751) teacher_loss 0.4993 (0.6835) loss_zs_kd 0.0009 (0.0006) loss_oracle 0.0080 (0.0045) kd_loss 0.1251 (0.2891) acc 87.5000 (75.0893) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3349 (0.3348) gate/usage_min 0.3313 (0.3313) gate/usage_std 0.0015 (0.0015) teacher/entropy 0.9735 (0.8103) teacher/usage_max 0.3894 (0.4300) teacher/usage_min 0.2822 (0.2584) teacher/usage_std 0.0439 (0.0740) nleep/row_max_mean 1525.0400 (1525.3536) nleep/row_max_std 37.2201 (58.4116) nleep/row_min_mean 1523.9004 (1523.1124) lr 1.0000e-05 eta 0:15:30
epoch [1/50] batch [160/160] time 0.096 (0.116) data 0.000 (0.003) loss 1.0558 (0.9558) teacher_loss 0.9573 (0.6865) loss_zs_kd 0.0038 (0.0008) loss_oracle 0.0132 (0.0052) kd_loss 0.0899 (0.2663) acc 65.6250 (75.0000) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3347 (0.3348) gate/usage_min 0.3313 (0.3313) gate/usage_std 0.0015 (0.0015) teacher/entropy 1.0085 (0.8330) teacher/usage_max 0.3476 (0.4245) teacher/usage_min 0.3206 (0.2617) teacher/usage_std 0.0111 (0.0702) nleep/row_max_mean 1524.6951 (1525.0851) nleep/row_max_std 71.2403 (58.0662) nleep/row_min_mean 1523.7267 (1522.9976) lr 2.0000e-03 eta 0:15:10
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,726
* accuracy: 78.2%
* error: 21.8%
* macro_f1: 80.0%
Checkpoint saved to icml/multi-dg/tuning/tuning2/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,923
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 87.2%
Checkpoint saved to icml/multi-dg/tuning/tuning2/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      78.2%, epoch: 1 *******
******* Domain p best val test acc: 86.6%, epoch: 1 *******
******* Domain p best test acc:     86.6%, epoch: 1 *******
epoch [2/50] batch [20/160] time 0.099 (0.117) data 0.000 (0.016) loss 0.8981 (0.8155) teacher_loss 0.6581 (0.6056) loss_zs_kd 0.0092 (0.0100) loss_oracle 0.1270 (0.1276) kd_loss 0.1719 (0.1412) acc 71.8750 (78.2812) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3351 (0.3350) gate/usage_min 0.3316 (0.3313) gate/usage_std 0.0015 (0.0015) teacher/entropy 0.9274 (0.9580) teacher/usage_max 0.4813 (0.4353) teacher/usage_min 0.1843 (0.2294) teacher/usage_std 0.1212 (0.0856) nleep/row_max_mean 1525.3077 (1524.0073) nleep/row_max_std 54.0345 (60.8646) nleep/row_min_mean 1523.9065 (1522.7810) lr 2.0000e-03 eta 0:15:12
epoch [2/50] batch [40/160] time 0.099 (0.110) data 0.000 (0.008) loss 1.3454 (0.9306) teacher_loss 0.6200 (0.5692) loss_zs_kd 0.0119 (0.0100) loss_oracle 0.3972 (0.2088) kd_loss 0.5209 (0.2521) acc 78.1250 (79.5312) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3343 (0.3349) gate/usage_min 0.3327 (0.3317) gate/usage_std 0.0007 (0.0013) teacher/entropy 0.5778 (0.8473) teacher/usage_max 0.6460 (0.5197) teacher/usage_min 0.0900 (0.1886) teacher/usage_std 0.2322 (0.1415) nleep/row_max_mean 1536.0632 (1526.2116) nleep/row_max_std 43.4927 (60.0171) nleep/row_min_mean 1532.3662 (1524.4131) lr 2.0000e-03 eta 0:14:15
epoch [2/50] batch [60/160] time 0.070 (0.114) data 0.000 (0.005) loss 1.1254 (1.0347) teacher_loss 0.3879 (0.5449) loss_zs_kd 0.0180 (0.0121) loss_oracle 0.3779 (0.2832) kd_loss 0.5396 (0.3422) acc 87.5000 (81.0938) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3349 (0.3347) gate/usage_min 0.3318 (0.3319) gate/usage_std 0.0013 (0.0012) teacher/entropy 0.5568 (0.7565) teacher/usage_max 0.5471 (0.5271) teacher/usage_min 0.0627 (0.1508) teacher/usage_std 0.2018 (0.1601) nleep/row_max_mean 1547.9912 (1526.8951) nleep/row_max_std 26.2305 (62.8506) nleep/row_min_mean 1543.3323 (1524.4664) lr 2.0000e-03 eta 0:14:49
epoch [2/50] batch [80/160] time 0.164 (0.121) data 0.000 (0.004) loss 1.2424 (1.1071) teacher_loss 0.2709 (0.5167) loss_zs_kd 0.0194 (0.0149) loss_oracle 0.5285 (0.3403) kd_loss 0.6975 (0.4129) acc 90.6250 (82.3828) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3358 (0.3349) gate/usage_min 0.3305 (0.3316) gate/usage_std 0.0022 (0.0014) teacher/entropy 0.3975 (0.6850) teacher/usage_max 0.5001 (0.5226) teacher/usage_min 0.0678 (0.1340) teacher/usage_std 0.1898 (0.1663) nleep/row_max_mean 1542.1104 (1527.7829) nleep/row_max_std 69.9455 (63.2587) nleep/row_min_mean 1536.8024 (1524.7993) lr 2.0000e-03 eta 0:15:42
epoch [2/50] batch [100/160] time 0.104 (0.119) data 0.000 (0.003) loss 1.2640 (1.1535) teacher_loss 0.2006 (0.4884) loss_zs_kd 0.0329 (0.0177) loss_oracle 0.4629 (0.3660) kd_loss 0.8154 (0.4732) acc 96.8750 (83.5000) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3370 (0.3352) gate/usage_min 0.3298 (0.3313) gate/usage_std 0.0029 (0.0016) teacher/entropy 0.2780 (0.6240) teacher/usage_max 0.5669 (0.5260) teacher/usage_min 0.0951 (0.1302) teacher/usage_std 0.1926 (0.1690) nleep/row_max_mean 1547.2192 (1527.9723) nleep/row_max_std 38.3259 (65.0960) nleep/row_min_mean 1540.4587 (1524.4837) lr 2.0000e-03 eta 0:15:18
epoch [2/50] batch [120/160] time 0.107 (0.116) data 0.000 (0.003) loss 1.4818 (1.1956) teacher_loss 0.5357 (0.4731) loss_zs_kd 0.0181 (0.0198) loss_oracle 0.4857 (0.3916) kd_loss 0.6942 (0.5168) acc 84.3750 (84.0365) gate/entropy 1.0985 (1.0986) gate/usage_max 0.3387 (0.3356) gate/usage_min 0.3293 (0.3310) gate/usage_std 0.0040 (0.0019) teacher/entropy 0.4010 (0.5799) teacher/usage_max 0.4431 (0.5302) teacher/usage_min 0.1753 (0.1346) teacher/usage_std 0.1145 (0.1689) nleep/row_max_mean 1529.5791 (1528.4512) nleep/row_max_std 77.9126 (65.7283) nleep/row_min_mean 1523.6174 (1524.4058) lr 2.0000e-03 eta 0:14:56
epoch [2/50] batch [140/160] time 0.092 (0.114) data 0.000 (0.002) loss 1.1772 (1.2269) teacher_loss 0.2147 (0.4546) loss_zs_kd 0.0166 (0.0206) loss_oracle 0.6212 (0.4205) kd_loss 0.6436 (0.5517) acc 96.8750 (84.6429) gate/entropy 1.0985 (1.0986) gate/usage_max 0.3403 (0.3362) gate/usage_min 0.3296 (0.3308) gate/usage_std 0.0049 (0.0023) teacher/entropy 0.4515 (0.5444) teacher/usage_max 0.4579 (0.5328) teacher/usage_min 0.1249 (0.1396) teacher/usage_std 0.1483 (0.1682) nleep/row_max_mean 1524.6929 (1528.9521) nleep/row_max_std 81.4254 (65.5749) nleep/row_min_mean 1517.8534 (1524.3973) lr 2.0000e-03 eta 0:14:39
epoch [2/50] batch [160/160] time 0.091 (0.111) data 0.000 (0.002) loss 1.5493 (1.2573) teacher_loss 0.5329 (0.4437) loss_zs_kd 0.0177 (0.0211) loss_oracle 0.5065 (0.4418) kd_loss 0.7543 (0.5822) acc 78.1250 (84.8242) gate/entropy 1.0984 (1.0986) gate/usage_max 0.3418 (0.3368) gate/usage_min 0.3270 (0.3305) gate/usage_std 0.0062 (0.0027) teacher/entropy 0.3317 (0.5132) teacher/usage_max 0.6293 (0.5377) teacher/usage_min 0.0707 (0.1335) teacher/usage_std 0.2293 (0.1725) nleep/row_max_mean 1529.3341 (1529.3159) nleep/row_max_std 102.9837 (65.7206) nleep/row_min_mean 1520.1553 (1524.2048) lr 1.9980e-03 eta 0:14:15
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,809
* accuracy: 82.0%
* error: 18.0%
* macro_f1: 84.3%
Checkpoint saved to icml/multi-dg/tuning/tuning2/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,945
* accuracy: 87.2%
* error: 12.8%
* macro_f1: 88.0%
Checkpoint saved to icml/multi-dg/tuning/tuning2/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      82.0%, epoch: 2 *******
******* Domain p best val test acc: 87.2%, epoch: 2 *******
******* Domain p best test acc:     87.2%, epoch: 2 *******
epoch [3/50] batch [20/160] time 0.102 (0.119) data 0.000 (0.015) loss 1.5249 (1.5066) teacher_loss 0.3378 (0.3144) loss_zs_kd 0.0479 (0.0304) loss_oracle 0.6082 (0.6154) kd_loss 0.8591 (0.8693) acc 81.2500 (88.2812) gate/entropy 1.0983 (1.0984) gate/usage_max 0.3434 (0.3426) gate/usage_min 0.3231 (0.3250) gate/usage_std 0.0083 (0.0072) teacher/entropy 0.2231 (0.2167) teacher/usage_max 0.5984 (0.5543) teacher/usage_min 0.0442 (0.0483) teacher/usage_std 0.2269 (0.2157) nleep/row_max_mean 1548.8611 (1531.8701) nleep/row_max_std 46.8497 (61.3655) nleep/row_min_mean 1537.3416 (1520.9028) lr 1.9980e-03 eta 0:15:13
epoch [3/50] batch [40/160] time 0.166 (0.125) data 0.001 (0.008) loss 1.5218 (1.5698) teacher_loss 0.2671 (0.3516) loss_zs_kd 0.0246 (0.0307) loss_oracle 0.6731 (0.6400) kd_loss 0.9058 (0.8828) acc 87.5000 (86.4844) gate/entropy 1.0981 (1.0983) gate/usage_max 0.3443 (0.3432) gate/usage_min 0.3193 (0.3231) gate/usage_std 0.0104 (0.0083) teacher/entropy 0.1750 (0.2015) teacher/usage_max 0.4792 (0.5390) teacher/usage_min 0.0471 (0.0433) teacher/usage_std 0.2024 (0.2147) nleep/row_max_mean 1536.5774 (1529.8653) nleep/row_max_std 72.4071 (64.9946) nleep/row_min_mean 1522.9633 (1518.0604) lr 1.9980e-03 eta 0:15:52
epoch [3/50] batch [60/160] time 0.182 (0.133) data 0.000 (0.005) loss 1.7173 (1.6083) teacher_loss 0.4442 (0.3745) loss_zs_kd 0.0164 (0.0291) loss_oracle 0.7001 (0.6518) kd_loss 0.9149 (0.8934) acc 84.3750 (86.0417) gate/entropy 1.0978 (1.0982) gate/usage_max 0.3450 (0.3437) gate/usage_min 0.3151 (0.3211) gate/usage_std 0.0131 (0.0094) teacher/entropy 0.1578 (0.1882) teacher/usage_max 0.5309 (0.5488) teacher/usage_min 0.0064 (0.0342) teacher/usage_std 0.2329 (0.2225) nleep/row_max_mean 1528.6635 (1530.3572) nleep/row_max_std 83.7736 (64.6672) nleep/row_min_mean 1513.6877 (1517.3290) lr 1.9980e-03 eta 0:16:57
epoch [3/50] batch [80/160] time 0.126 (0.129) data 0.000 (0.004) loss 1.6388 (1.6173) teacher_loss 0.3993 (0.3745) loss_zs_kd 0.0300 (0.0288) loss_oracle 0.6787 (0.6537) kd_loss 0.8851 (0.9015) acc 81.2500 (86.3672) gate/entropy 1.0974 (1.0981) gate/usage_max 0.3458 (0.3441) gate/usage_min 0.3107 (0.3191) gate/usage_std 0.0160 (0.0107) teacher/entropy 0.1806 (0.1769) teacher/usage_max 0.5145 (0.5514) teacher/usage_min 0.0060 (0.0271) teacher/usage_std 0.2319 (0.2268) nleep/row_max_mean 1549.1892 (1531.6148) nleep/row_max_std 31.9112 (64.2111) nleep/row_min_mean 1530.1846 (1517.4284) lr 1.9980e-03 eta 0:16:17
epoch [3/50] batch [100/160] time 0.102 (0.125) data 0.000 (0.003) loss 1.4376 (1.6106) teacher_loss 0.1982 (0.3604) loss_zs_kd 0.0190 (0.0276) loss_oracle 0.6917 (0.6608) kd_loss 0.8840 (0.9061) acc 90.6250 (86.8750) gate/entropy 1.0970 (1.0979) gate/usage_max 0.3477 (0.3445) gate/usage_min 0.3068 (0.3170) gate/usage_std 0.0188 (0.0120) teacher/entropy 0.1761 (0.1693) teacher/usage_max 0.6851 (0.5640) teacher/usage_min 0.0144 (0.0232) teacher/usage_std 0.2748 (0.2324) nleep/row_max_mean 1503.2285 (1532.0892) nleep/row_max_std 118.2608 (63.4114) nleep/row_min_mean 1484.8069 (1516.9557) lr 1.9980e-03 eta 0:15:48
epoch [3/50] batch [120/160] time 0.101 (0.121) data 0.000 (0.003) loss 1.4819 (1.6152) teacher_loss 0.2266 (0.3629) loss_zs_kd 0.0358 (0.0286) loss_oracle 0.5507 (0.6570) kd_loss 0.9621 (0.9094) acc 90.6250 (86.6146) gate/entropy 1.0964 (1.0977) gate/usage_max 0.3524 (0.3454) gate/usage_min 0.3029 (0.3150) gate/usage_std 0.0218 (0.0134) teacher/entropy 0.0900 (0.1626) teacher/usage_max 0.5933 (0.5738) teacher/usage_min 0.0025 (0.0202) teacher/usage_std 0.2463 (0.2367) nleep/row_max_mean 1504.5339 (1532.6749) nleep/row_max_std 87.3996 (62.9475) nleep/row_min_mean 1484.1746 (1516.6767) lr 1.9980e-03 eta 0:15:16
epoch [3/50] batch [140/160] time 0.105 (0.119) data 0.000 (0.002) loss 1.4811 (1.6170) teacher_loss 0.1750 (0.3626) loss_zs_kd 0.0320 (0.0290) loss_oracle 0.6997 (0.6529) kd_loss 0.9402 (0.9135) acc 96.8750 (86.7634) gate/entropy 1.0958 (1.0975) gate/usage_max 0.3571 (0.3468) gate/usage_min 0.2990 (0.3129) gate/usage_std 0.0249 (0.0149) teacher/entropy 0.1059 (0.1551) teacher/usage_max 0.5630 (0.5838) teacher/usage_min 0.0002 (0.0181) teacher/usage_std 0.2412 (0.2410) nleep/row_max_mean 1530.9431 (1532.0562) nleep/row_max_std 46.5029 (64.0312) nleep/row_min_mean 1509.1854 (1515.3074) lr 1.9980e-03 eta 0:14:55
epoch [3/50] batch [160/160] time 0.091 (0.116) data 0.000 (0.002) loss 2.1749 (1.6260) teacher_loss 0.8165 (0.3650) loss_zs_kd 0.0307 (0.0296) loss_oracle 0.7635 (0.6591) kd_loss 0.9614 (0.9167) acc 75.0000 (87.0312) gate/entropy 1.0949 (1.0972) gate/usage_max 0.3623 (0.3484) gate/usage_min 0.2949 (0.3109) gate/usage_std 0.0283 (0.0163) teacher/entropy 0.0660 (0.1480) teacher/usage_max 0.7854 (0.5967) teacher/usage_min 0.0011 (0.0159) teacher/usage_std 0.3312 (0.2467) nleep/row_max_mean 1520.8628 (1532.0213) nleep/row_max_std 95.4319 (64.4050) nleep/row_min_mean 1497.9812 (1514.5744) lr 1.9921e-03 eta 0:14:29
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,816
* accuracy: 82.3%
* error: 17.7%
* macro_f1: 84.0%
Checkpoint saved to icml/multi-dg/tuning/tuning2/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,963
* accuracy: 87.8%
* error: 12.2%
* macro_f1: 88.4%
Checkpoint saved to icml/multi-dg/tuning/tuning2/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      82.3%, epoch: 3 *******
******* Domain p best val test acc: 87.8%, epoch: 3 *******
******* Domain p best test acc:     87.8%, epoch: 3 *******
epoch [4/50] batch [20/160] time 0.098 (0.144) data 0.000 (0.015) loss 1.6314 (1.6676) teacher_loss 0.2346 (0.3259) loss_zs_kd 0.0558 (0.0471) loss_oracle 0.7957 (0.7286) kd_loss 0.9710 (0.9538) acc 90.6250 (89.6875) gate/entropy 1.0940 (1.0945) gate/usage_max 0.3680 (0.3653) gate/usage_min 0.2911 (0.2929) gate/usage_std 0.0319 (0.0302) teacher/entropy 0.0470 (0.0715) teacher/usage_max 0.7619 (0.7267) teacher/usage_min 0.0000 (0.0006) teacher/usage_std 0.3182 (0.3018) nleep/row_max_mean 1545.4597 (1535.4593) nleep/row_max_std 50.0074 (63.4463) nleep/row_min_mean 1516.4857 (1510.3600) lr 1.9921e-03 eta 0:18:00
epoch [4/50] batch [40/160] time 0.090 (0.144) data 0.000 (0.008) loss 1.4663 (1.6210) teacher_loss 0.1508 (0.2861) loss_zs_kd 0.0490 (0.0474) loss_oracle 0.7267 (0.7306) kd_loss 0.9277 (0.9459) acc 100.0000 (91.0938) gate/entropy 1.0928 (1.0939) gate/usage_max 0.3742 (0.3682) gate/usage_min 0.2871 (0.2909) gate/usage_std 0.0358 (0.0320) teacher/entropy 0.0879 (0.0741) teacher/usage_max 0.6706 (0.7283) teacher/usage_min 0.0000 (0.0005) teacher/usage_std 0.2738 (0.3029) nleep/row_max_mean 1544.2891 (1536.2960) nleep/row_max_std 51.0159 (65.0940) nleep/row_min_mean 1518.6921 (1510.8202) lr 1.9921e-03 eta 0:18:00
epoch [4/50] batch [60/160] time 0.110 (0.131) data 0.001 (0.005) loss 1.6055 (1.6234) teacher_loss 0.3090 (0.2928) loss_zs_kd 0.0468 (0.0485) loss_oracle 0.7097 (0.7312) kd_loss 0.9182 (0.9407) acc 93.7500 (91.0938) gate/entropy 1.0915 (1.0933) gate/usage_max 0.3804 (0.3713) gate/usage_min 0.2834 (0.2890) gate/usage_std 0.0396 (0.0339) teacher/entropy 0.0716 (0.0728) teacher/usage_max 0.8113 (0.7421) teacher/usage_min 0.0003 (0.0010) teacher/usage_std 0.3466 (0.3099) nleep/row_max_mean 1522.7074 (1533.1769) nleep/row_max_std 80.8846 (69.0301) nleep/row_min_mean 1499.1212 (1507.7364) lr 1.9921e-03 eta 0:16:14
epoch [4/50] batch [80/160] time 0.108 (0.124) data 0.001 (0.004) loss 1.5808 (1.6270) teacher_loss 0.2558 (0.3021) loss_zs_kd 0.0321 (0.0486) loss_oracle 0.7967 (0.7309) kd_loss 0.9105 (0.9351) acc 90.6250 (90.5078) gate/entropy 1.0899 (1.0927) gate/usage_max 0.3870 (0.3744) gate/usage_min 0.2798 (0.2871) gate/usage_std 0.0438 (0.0359) teacher/entropy 0.0633 (0.0725) teacher/usage_max 0.8350 (0.7475) teacher/usage_min 0.0000 (0.0009) teacher/usage_std 0.3610 (0.3129) nleep/row_max_mean 1534.4050 (1533.6109) nleep/row_max_std 62.9641 (66.7640) nleep/row_min_mean 1507.1987 (1507.9355) lr 1.9921e-03 eta 0:15:23
epoch [4/50] batch [100/160] time 0.108 (0.121) data 0.000 (0.003) loss 1.5688 (1.6400) teacher_loss 0.2761 (0.3190) loss_zs_kd 0.0428 (0.0490) loss_oracle 0.7190 (0.7307) kd_loss 0.9118 (0.9311) acc 96.8750 (90.0938) gate/entropy 1.0883 (1.0919) gate/usage_max 0.3934 (0.3776) gate/usage_min 0.2762 (0.2853) gate/usage_std 0.0479 (0.0379) teacher/entropy 0.0653 (0.0705) teacher/usage_max 0.7474 (0.7539) teacher/usage_min 0.0006 (0.0016) teacher/usage_std 0.3102 (0.3164) nleep/row_max_mean 1535.1244 (1534.1259) nleep/row_max_std 62.1748 (65.7640) nleep/row_min_mean 1507.8010 (1508.1822) lr 1.9921e-03 eta 0:14:58
epoch [4/50] batch [120/160] time 0.140 (0.119) data 0.000 (0.003) loss 1.7930 (1.6419) teacher_loss 0.5014 (0.3249) loss_zs_kd 0.0395 (0.0482) loss_oracle 0.7201 (0.7332) kd_loss 0.9119 (0.9262) acc 87.5000 (89.7917) gate/entropy 1.0863 (1.0912) gate/usage_max 0.4004 (0.3809) gate/usage_min 0.2728 (0.2834) gate/usage_std 0.0523 (0.0400) teacher/entropy 0.0522 (0.0688) teacher/usage_max 0.7603 (0.7609) teacher/usage_min 0.0015 (0.0014) teacher/usage_std 0.3170 (0.3202) nleep/row_max_mean 1523.4775 (1533.1996) nleep/row_max_std 98.8256 (66.7134) nleep/row_min_mean 1494.8342 (1507.0851) lr 1.9921e-03 eta 0:14:42
epoch [4/50] batch [140/160] time 0.112 (0.120) data 0.000 (0.002) loss 1.4753 (1.6387) teacher_loss 0.2301 (0.3303) loss_zs_kd 0.0379 (0.0473) loss_oracle 0.6941 (0.7308) kd_loss 0.8792 (0.9193) acc 93.7500 (89.5536) gate/entropy 1.0842 (1.0903) gate/usage_max 0.4071 (0.3842) gate/usage_min 0.2692 (0.2816) gate/usage_std 0.0567 (0.0421) teacher/entropy 0.0800 (0.0699) teacher/usage_max 0.7365 (0.7638) teacher/usage_min 0.0003 (0.0015) teacher/usage_std 0.3046 (0.3216) nleep/row_max_mean 1527.6921 (1532.9949) nleep/row_max_std 73.6246 (66.1981) nleep/row_min_mean 1502.6924 (1506.7575) lr 1.9921e-03 eta 0:14:47
epoch [4/50] batch [160/160] time 0.099 (0.117) data 0.000 (0.002) loss 1.4710 (1.6321) teacher_loss 0.2461 (0.3314) loss_zs_kd 0.0345 (0.0464) loss_oracle 0.7063 (0.7299) kd_loss 0.8545 (0.9126) acc 90.6250 (89.5312) gate/entropy 1.0819 (1.0894) gate/usage_max 0.4140 (0.3875) gate/usage_min 0.2657 (0.2798) gate/usage_std 0.0612 (0.0442) teacher/entropy 0.0537 (0.0695) teacher/usage_max 0.8977 (0.7718) teacher/usage_min 0.0001 (0.0014) teacher/usage_std 0.4012 (0.3261) nleep/row_max_mean 1528.8391 (1533.0241) nleep/row_max_std 90.4346 (66.2683) nleep/row_min_mean 1500.9014 (1506.6374) lr 1.9823e-03 eta 0:14:23
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,815
* accuracy: 82.3%
* error: 17.7%
* macro_f1: 84.0%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,955
* accuracy: 87.5%
* error: 12.5%
* macro_f1: 88.1%
******* Domain p best val acc:      82.3%, epoch: 3 *******
******* Domain p best val test acc: 87.8%, epoch: 3 *******
******* Domain p best test acc:     87.8%, epoch: 3 *******
epoch [5/50] batch [20/160] time 0.165 (0.154) data 0.000 (0.016) loss 1.3375 (1.5775) teacher_loss 0.1387 (0.3445) loss_zs_kd 0.0209 (0.0397) loss_oracle 0.6812 (0.6923) kd_loss 0.8477 (0.8670) acc 96.8750 (90.4688) gate/entropy 1.0796 (1.0806) gate/usage_max 0.4207 (0.4177) gate/usage_min 0.2627 (0.2641) gate/usage_std 0.0656 (0.0636) teacher/entropy 0.1124 (0.0560) teacher/usage_max 0.6676 (0.8178) teacher/usage_min 0.0000 (0.0019) teacher/usage_std 0.2726 (0.3527) nleep/row_max_mean 1506.4255 (1533.1267) nleep/row_max_std 90.6666 (66.4954) nleep/row_min_mean 1481.1962 (1504.3250) lr 1.9823e-03 eta 0:18:49
epoch [5/50] batch [40/160] time 0.120 (0.139) data 0.001 (0.008) loss 1.4125 (1.5631) teacher_loss 0.2267 (0.3486) loss_zs_kd 0.0303 (0.0383) loss_oracle 0.6702 (0.6865) kd_loss 0.8355 (0.8521) acc 96.8750 (89.2188) gate/entropy 1.0769 (1.0794) gate/usage_max 0.4276 (0.4211) gate/usage_min 0.2593 (0.2625) gate/usage_std 0.0702 (0.0659) teacher/entropy 0.0857 (0.0628) teacher/usage_max 0.7703 (0.8258) teacher/usage_min 0.0000 (0.0013) teacher/usage_std 0.3229 (0.3571) nleep/row_max_mean 1543.9360 (1532.1232) nleep/row_max_std 47.4159 (65.4757) nleep/row_min_mean 1514.8127 (1503.6611) lr 1.9823e-03 eta 0:16:54
epoch [5/50] batch [60/160] time 0.136 (0.133) data 0.002 (0.006) loss 1.7341 (1.5429) teacher_loss 0.6257 (0.3441) loss_zs_kd 0.0496 (0.0403) loss_oracle 0.5844 (0.6696) kd_loss 0.7914 (0.8439) acc 81.2500 (89.2708) gate/entropy 1.0741 (1.0780) gate/usage_max 0.4342 (0.4244) gate/usage_min 0.2563 (0.2609) gate/usage_std 0.0746 (0.0681) teacher/entropy 0.1280 (0.0676) teacher/usage_max 0.7541 (0.8202) teacher/usage_min 0.0090 (0.0021) teacher/usage_std 0.3117 (0.3533) nleep/row_max_mean 1510.9548 (1531.9896) nleep/row_max_std 102.8478 (66.9317) nleep/row_min_mean 1487.5498 (1503.9532) lr 1.9823e-03 eta 0:16:12
epoch [5/50] batch [80/160] time 0.102 (0.127) data 0.000 (0.004) loss 1.6965 (1.5433) teacher_loss 0.5333 (0.3498) loss_zs_kd 0.0530 (0.0409) loss_oracle 0.6830 (0.6651) kd_loss 0.7952 (0.8405) acc 87.5000 (89.3359) gate/entropy 1.0715 (1.0767) gate/usage_max 0.4403 (0.4277) gate/usage_min 0.2534 (0.2594) gate/usage_std 0.0787 (0.0702) teacher/entropy 0.0545 (0.0673) teacher/usage_max 0.9194 (0.8153) teacher/usage_min 0.0002 (0.0018) teacher/usage_std 0.4157 (0.3504) nleep/row_max_mean 1538.0945 (1531.2056) nleep/row_max_std 85.2210 (69.1644) nleep/row_min_mean 1511.6226 (1503.6497) lr 1.9823e-03 eta 0:15:21
epoch [5/50] batch [100/160] time 0.104 (0.122) data 0.000 (0.003) loss 1.4908 (1.5231) teacher_loss 0.3403 (0.3378) loss_zs_kd 0.0253 (0.0396) loss_oracle 0.6934 (0.6613) kd_loss 0.7912 (0.8348) acc 84.3750 (89.3125) gate/entropy 1.0687 (1.0754) gate/usage_max 0.4463 (0.4308) gate/usage_min 0.2508 (0.2579) gate/usage_std 0.0826 (0.0723) teacher/entropy 0.0684 (0.0683) teacher/usage_max 0.8642 (0.8150) teacher/usage_min 0.0000 (0.0020) teacher/usage_std 0.3795 (0.3502) nleep/row_max_mean 1531.1501 (1531.7200) nleep/row_max_std 73.0866 (67.4951) nleep/row_min_mean 1505.3950 (1504.4361) lr 1.9823e-03 eta 0:14:49
epoch [5/50] batch [120/160] time 0.120 (0.120) data 0.000 (0.003) loss 1.5599 (1.5177) teacher_loss 0.4439 (0.3396) loss_zs_kd 0.0425 (0.0394) loss_oracle 0.6425 (0.6588) kd_loss 0.7735 (0.8290) acc 84.3750 (89.2448) gate/entropy 1.0657 (1.0740) gate/usage_max 0.4524 (0.4339) gate/usage_min 0.2481 (0.2565) gate/usage_std 0.0868 (0.0744) teacher/entropy 0.0666 (0.0684) teacher/usage_max 0.8864 (0.8171) teacher/usage_min 0.0000 (0.0024) teacher/usage_std 0.3938 (0.3513) nleep/row_max_mean 1528.2158 (1531.7151) nleep/row_max_std 57.4396 (66.8543) nleep/row_min_mean 1499.6959 (1504.5483) lr 1.9823e-03 eta 0:14:26
epoch [5/50] batch [140/160] time 0.120 (0.118) data 0.000 (0.003) loss 1.7558 (1.5109) teacher_loss 0.6537 (0.3382) loss_zs_kd 0.0416 (0.0400) loss_oracle 0.6722 (0.6533) kd_loss 0.7452 (0.8261) acc 78.1250 (89.1964) gate/entropy 1.0629 (1.0726) gate/usage_max 0.4581 (0.4370) gate/usage_min 0.2457 (0.2551) gate/usage_std 0.0906 (0.0765) teacher/entropy 0.0732 (0.0676) teacher/usage_max 0.9130 (0.8152) teacher/usage_min 0.0001 (0.0029) teacher/usage_std 0.4114 (0.3501) nleep/row_max_mean 1523.0720 (1531.6583) nleep/row_max_std 65.0560 (66.2597) nleep/row_min_mean 1495.8763 (1504.5738) lr 1.9823e-03 eta 0:14:12
epoch [5/50] batch [160/160] time 0.093 (0.115) data 0.000 (0.002) loss 1.6232 (1.4992) teacher_loss 0.5204 (0.3327) loss_zs_kd 0.0303 (0.0402) loss_oracle 0.6291 (0.6474) kd_loss 0.7730 (0.8227) acc 84.3750 (89.4141) gate/entropy 1.0601 (1.0712) gate/usage_max 0.4635 (0.4400) gate/usage_min 0.2434 (0.2538) gate/usage_std 0.0942 (0.0785) teacher/entropy 0.0764 (0.0685) teacher/usage_max 0.8254 (0.8109) teacher/usage_min 0.0000 (0.0032) teacher/usage_std 0.3551 (0.3476) nleep/row_max_mean 1532.8673 (1531.3628) nleep/row_max_std 43.1458 (65.8392) nleep/row_min_mean 1506.8975 (1504.5062) lr 1.9686e-03 eta 0:13:48
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,808
* accuracy: 82.0%
* error: 18.0%
* macro_f1: 83.9%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,972
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 88.6%
Checkpoint saved to icml/multi-dg/tuning/tuning2/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      82.3%, epoch: 3 *******
******* Domain p best val test acc: 87.8%, epoch: 3 *******
******* Domain p best test acc:     88.0%, epoch: 5 *******
epoch [6/50] batch [20/160] time 0.105 (0.143) data 0.000 (0.015) loss 1.3243 (1.4184) teacher_loss 0.1963 (0.3088) loss_zs_kd 0.0414 (0.0429) loss_oracle 0.6148 (0.5955) kd_loss 0.7999 (0.7903) acc 93.7500 (90.1562) gate/entropy 1.0571 (1.0586) gate/usage_max 0.4689 (0.4662) gate/usage_min 0.2411 (0.2423) gate/usage_std 0.0979 (0.0961) teacher/entropy 0.0668 (0.0675) teacher/usage_max 0.7748 (0.8003) teacher/usage_min 0.0030 (0.0044) teacher/usage_std 0.3247 (0.3413) nleep/row_max_mean 1537.1016 (1531.0213) nleep/row_max_std 51.9546 (60.3708) nleep/row_min_mean 1509.8499 (1505.2398) lr 1.9686e-03 eta 0:17:08
epoch [6/50] batch [40/160] time 0.106 (0.124) data 0.000 (0.008) loss 1.3190 (1.4090) teacher_loss 0.1701 (0.3057) loss_zs_kd 0.0449 (0.0423) loss_oracle 0.5966 (0.5960) kd_loss 0.8281 (0.7841) acc 96.8750 (89.6094) gate/entropy 1.0543 (1.0572) gate/usage_max 0.4738 (0.4687) gate/usage_min 0.2393 (0.2413) gate/usage_std 0.1012 (0.0978) teacher/entropy 0.0197 (0.0651) teacher/usage_max 0.8093 (0.8113) teacher/usage_min 0.0308 (0.0051) teacher/usage_std 0.3407 (0.3474) nleep/row_max_mean 1544.5223 (1527.9191) nleep/row_max_std 44.9408 (63.3301) nleep/row_min_mean 1515.5659 (1501.8378) lr 1.9686e-03 eta 0:14:49
epoch [6/50] batch [60/160] time 0.101 (0.118) data 0.000 (0.005) loss 1.3341 (1.4087) teacher_loss 0.2056 (0.3093) loss_zs_kd 0.0552 (0.0422) loss_oracle 0.5558 (0.5881) kd_loss 0.8230 (0.7843) acc 90.6250 (89.7396) gate/entropy 1.0516 (1.0558) gate/usage_max 0.4785 (0.4712) gate/usage_min 0.2374 (0.2403) gate/usage_std 0.1044 (0.0995) teacher/entropy 0.0592 (0.0664) teacher/usage_max 0.7237 (0.8018) teacher/usage_min 0.0051 (0.0046) teacher/usage_std 0.2966 (0.3422) nleep/row_max_mean 1521.1263 (1527.3320) nleep/row_max_std 72.0592 (64.2573) nleep/row_min_mean 1496.7805 (1501.5169) lr 1.9686e-03 eta 0:14:02
epoch [6/50] batch [80/160] time 0.100 (0.114) data 0.000 (0.004) loss 1.3392 (1.4120) teacher_loss 0.2061 (0.3114) loss_zs_kd 0.0772 (0.0429) loss_oracle 0.4624 (0.5825) kd_loss 0.8633 (0.7878) acc 93.7500 (89.6094) gate/entropy 1.0488 (1.0544) gate/usage_max 0.4832 (0.4736) gate/usage_min 0.2355 (0.2394) gate/usage_std 0.1076 (0.1011) teacher/entropy 0.0632 (0.0641) teacher/usage_max 0.6314 (0.7935) teacher/usage_min 0.0008 (0.0038) teacher/usage_std 0.2586 (0.3378) nleep/row_max_mean 1545.7769 (1527.8070) nleep/row_max_std 56.1566 (62.4677) nleep/row_min_mean 1522.6024 (1502.0444) lr 1.9686e-03 eta 0:13:32
epoch [6/50] batch [100/160] time 0.120 (0.112) data 0.000 (0.003) loss 1.3421 (1.4018) teacher_loss 0.1283 (0.3027) loss_zs_kd 0.0356 (0.0428) loss_oracle 0.5112 (0.5766) kd_loss 0.9403 (0.7893) acc 100.0000 (90.0938) gate/entropy 1.0468 (1.0531) gate/usage_max 0.4864 (0.4758) gate/usage_min 0.2341 (0.2384) gate/usage_std 0.1098 (0.1026) teacher/entropy 0.0576 (0.0674) teacher/usage_max 0.5016 (0.7803) teacher/usage_min 0.0011 (0.0062) teacher/usage_std 0.2349 (0.3304) nleep/row_max_mean 1541.8488 (1528.4034) nleep/row_max_std 25.8803 (61.9654) nleep/row_min_mean 1519.6108 (1502.9510) lr 1.9686e-03 eta 0:13:12
epoch [6/50] batch [120/160] time 0.105 (0.111) data 0.000 (0.003) loss 1.3588 (1.3947) teacher_loss 0.1662 (0.2920) loss_zs_kd 0.0883 (0.0454) loss_oracle 0.5537 (0.5750) kd_loss 0.8716 (0.7924) acc 96.8750 (90.6510) gate/entropy 1.0443 (1.0518) gate/usage_max 0.4902 (0.4779) gate/usage_min 0.2323 (0.2376) gate/usage_std 0.1125 (0.1040) teacher/entropy 0.0597 (0.0682) teacher/usage_max 0.6165 (0.7691) teacher/usage_min 0.0001 (0.0064) teacher/usage_std 0.2541 (0.3243) nleep/row_max_mean 1540.0370 (1527.4469) nleep/row_max_std 44.0120 (61.8278) nleep/row_min_mean 1515.0474 (1502.2399) lr 1.9686e-03 eta 0:13:08
epoch [6/50] batch [140/160] time 0.111 (0.111) data 0.000 (0.002) loss 1.3921 (1.3867) teacher_loss 0.2947 (0.2826) loss_zs_kd 0.0355 (0.0457) loss_oracle 0.5331 (0.5742) kd_loss 0.8132 (0.7942) acc 90.6250 (90.9152) gate/entropy 1.0428 (1.0507) gate/usage_max 0.4925 (0.4798) gate/usage_min 0.2313 (0.2367) gate/usage_std 0.1140 (0.1053) teacher/entropy 0.0478 (0.0676) teacher/usage_max 0.7371 (0.7631) teacher/usage_min 0.0013 (0.0072) teacher/usage_std 0.3047 (0.3208) nleep/row_max_mean 1522.2026 (1527.0159) nleep/row_max_std 56.4317 (62.1102) nleep/row_min_mean 1498.4966 (1501.9558) lr 1.9686e-03 eta 0:13:02
epoch [6/50] batch [160/160] time 0.090 (0.110) data 0.000 (0.002) loss 1.5018 (1.3920) teacher_loss 0.3446 (0.2861) loss_zs_kd 0.0622 (0.0465) loss_oracle 0.5924 (0.5742) kd_loss 0.8299 (0.7955) acc 87.5000 (90.8398) gate/entropy 1.0404 (1.0495) gate/usage_max 0.4961 (0.4816) gate/usage_min 0.2297 (0.2360) gate/usage_std 0.1165 (0.1066) teacher/entropy 0.0500 (0.0689) teacher/usage_max 0.6988 (0.7553) teacher/usage_min 0.0002 (0.0087) teacher/usage_std 0.2861 (0.3163) nleep/row_max_mean 1535.8943 (1526.7959) nleep/row_max_std 27.8795 (61.2784) nleep/row_min_mean 1511.2838 (1501.8669) lr 1.9511e-03 eta 0:12:50
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,807
* accuracy: 81.9%
* error: 18.1%
* macro_f1: 83.9%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,947
* accuracy: 87.3%
* error: 12.7%
* macro_f1: 88.1%
******* Domain p best val acc:      82.3%, epoch: 3 *******
******* Domain p best val test acc: 87.8%, epoch: 3 *******
******* Domain p best test acc:     88.0%, epoch: 5 *******
epoch [7/50] batch [20/160] time 0.104 (0.115) data 0.000 (0.014) loss 1.4275 (1.3300) teacher_loss 0.2760 (0.2098) loss_zs_kd 0.0586 (0.0520) loss_oracle 0.5802 (0.5867) kd_loss 0.8321 (0.8008) acc 90.6250 (94.5312) gate/entropy 1.0384 (1.0395) gate/usage_max 0.4991 (0.4975) gate/usage_min 0.2286 (0.2293) gate/usage_std 0.1186 (0.1175) teacher/entropy 0.0777 (0.0690) teacher/usage_max 0.6515 (0.7215) teacher/usage_min 0.0236 (0.0272) teacher/usage_std 0.2564 (0.2923) nleep/row_max_mean 1529.0266 (1526.4174) nleep/row_max_std 47.1503 (55.2852) nleep/row_min_mean 1505.8477 (1502.1309) lr 1.9511e-03 eta 0:13:27
epoch [7/50] batch [40/160] time 0.106 (0.110) data 0.000 (0.007) loss 1.5049 (1.3570) teacher_loss 0.4123 (0.2400) loss_zs_kd 0.0448 (0.0510) loss_oracle 0.6159 (0.5783) kd_loss 0.7622 (0.8023) acc 87.5000 (92.6562) gate/entropy 1.0366 (1.0385) gate/usage_max 0.5018 (0.4990) gate/usage_min 0.2276 (0.2287) gate/usage_std 0.1204 (0.1185) teacher/entropy 0.0571 (0.0753) teacher/usage_max 0.7976 (0.7066) teacher/usage_min 0.0292 (0.0270) teacher/usage_std 0.3335 (0.2856) nleep/row_max_mean 1521.4260 (1524.8813) nleep/row_max_std 84.1746 (58.2647) nleep/row_min_mean 1492.3353 (1500.6258) lr 1.9511e-03 eta 0:12:49
epoch [7/50] batch [60/160] time 0.112 (0.108) data 0.001 (0.005) loss 1.3032 (1.3543) teacher_loss 0.2298 (0.2384) loss_zs_kd 0.0694 (0.0519) loss_oracle 0.5280 (0.5749) kd_loss 0.7747 (0.8026) acc 90.6250 (92.3438) gate/entropy 1.0350 (1.0376) gate/usage_max 0.5042 (0.5004) gate/usage_min 0.2266 (0.2281) gate/usage_std 0.1221 (0.1194) teacher/entropy 0.1008 (0.0753) teacher/usage_max 0.7092 (0.7046) teacher/usage_min 0.0462 (0.0283) teacher/usage_std 0.2778 (0.2848) nleep/row_max_mean 1527.5336 (1524.1682) nleep/row_max_std 49.6042 (60.5008) nleep/row_min_mean 1502.8757 (1499.4656) lr 1.9511e-03 eta 0:12:34
epoch [7/50] batch [80/160] time 0.097 (0.107) data 0.000 (0.004) loss 1.4132 (1.3499) teacher_loss 0.3026 (0.2330) loss_zs_kd 0.0574 (0.0547) loss_oracle 0.5643 (0.5716) kd_loss 0.7998 (0.8038) acc 93.7500 (92.9297) gate/entropy 1.0332 (1.0366) gate/usage_max 0.5066 (0.5017) gate/usage_min 0.2257 (0.2276) gate/usage_std 0.1237 (0.1204) teacher/entropy 0.0905 (0.0778) teacher/usage_max 0.6808 (0.6977) teacher/usage_min 0.0427 (0.0305) teacher/usage_std 0.2636 (0.2800) nleep/row_max_mean 1523.9680 (1523.6386) nleep/row_max_std 64.4876 (61.3587) nleep/row_min_mean 1498.0089 (1498.9126) lr 1.9511e-03 eta 0:12:22
epoch [7/50] batch [100/160] time 0.107 (0.106) data 0.000 (0.003) loss 1.1814 (1.3483) teacher_loss 0.1021 (0.2334) loss_zs_kd 0.0624 (0.0557) loss_oracle 0.5058 (0.5683) kd_loss 0.7953 (0.8030) acc 96.8750 (92.8750) gate/entropy 1.0312 (1.0357) gate/usage_max 0.5094 (0.5030) gate/usage_min 0.2246 (0.2271) gate/usage_std 0.1257 (0.1212) teacher/entropy 0.1160 (0.0795) teacher/usage_max 0.6458 (0.6951) teacher/usage_min 0.0421 (0.0313) teacher/usage_std 0.2469 (0.2785) nleep/row_max_mean 1526.0684 (1522.6676) nleep/row_max_std 72.3104 (62.9095) nleep/row_min_mean 1503.0692 (1497.8559) lr 1.9511e-03 eta 0:12:12
epoch [7/50] batch [120/160] time 0.096 (0.105) data 0.000 (0.003) loss 1.2155 (1.3502) teacher_loss 0.1064 (0.2315) loss_zs_kd 0.0498 (0.0568) loss_oracle 0.5782 (0.5649) kd_loss 0.7952 (0.8078) acc 96.8750 (92.9948) gate/entropy 1.0305 (1.0349) gate/usage_max 0.5105 (0.5042) gate/usage_min 0.2241 (0.2266) gate/usage_std 0.1264 (0.1221) teacher/entropy 0.0641 (0.0786) teacher/usage_max 0.7150 (0.6885) teacher/usage_min 0.0017 (0.0340) teacher/usage_std 0.2933 (0.2743) nleep/row_max_mean 1503.9240 (1522.3605) nleep/row_max_std 91.2261 (63.5385) nleep/row_min_mean 1481.7778 (1497.4350) lr 1.9511e-03 eta 0:12:04
epoch [7/50] batch [140/160] time 0.112 (0.105) data 0.000 (0.002) loss 1.3606 (1.3405) teacher_loss 0.2022 (0.2198) loss_zs_kd 0.0561 (0.0580) loss_oracle 0.5045 (0.5637) kd_loss 0.8780 (0.8098) acc 96.8750 (93.4598) gate/entropy 1.0291 (1.0341) gate/usage_max 0.5124 (0.5053) gate/usage_min 0.2233 (0.2262) gate/usage_std 0.1277 (0.1228) teacher/entropy 0.0606 (0.0783) teacher/usage_max 0.6016 (0.6851) teacher/usage_min 0.0452 (0.0351) teacher/usage_std 0.2276 (0.2725) nleep/row_max_mean 1496.3165 (1522.4437) nleep/row_max_std 79.0576 (63.7215) nleep/row_min_mean 1473.3584 (1497.4469) lr 1.9511e-03 eta 0:12:01
epoch [7/50] batch [160/160] time 0.094 (0.104) data 0.000 (0.002) loss 1.4672 (1.3477) teacher_loss 0.2267 (0.2155) loss_zs_kd 0.0831 (0.0586) loss_oracle 0.5720 (0.5617) kd_loss 0.9129 (0.8221) acc 93.7500 (93.6328) gate/entropy 1.0277 (1.0334) gate/usage_max 0.5142 (0.5063) gate/usage_min 0.2220 (0.2257) gate/usage_std 0.1290 (0.1235) teacher/entropy 0.0401 (0.0752) teacher/usage_max 0.5851 (0.6716) teacher/usage_min 0.0665 (0.0367) teacher/usage_std 0.2120 (0.2664) nleep/row_max_mean 1523.7263 (1522.4827) nleep/row_max_std 57.2884 (63.6046) nleep/row_min_mean 1499.6306 (1497.5230) lr 1.9298e-03 eta 0:11:52
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,841
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 84.9%
Checkpoint saved to icml/multi-dg/tuning/tuning2/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,947
* accuracy: 87.3%
* error: 12.7%
* macro_f1: 88.4%
******* Domain p best val acc:      83.5%, epoch: 7 *******
******* Domain p best val test acc: 87.3%, epoch: 7 *******
******* Domain p best test acc:     88.0%, epoch: 5 *******
epoch [8/50] batch [20/160] time 0.111 (0.125) data 0.000 (0.017) loss 1.5247 (1.3950) teacher_loss 0.1764 (0.1713) loss_zs_kd 0.0819 (0.0710) loss_oracle 0.5575 (0.5721) kd_loss 1.0286 (0.9021) acc 93.7500 (95.9375) gate/entropy 1.0275 (1.0276) gate/usage_max 0.5144 (0.5143) gate/usage_min 0.2215 (0.2218) gate/usage_std 0.1292 (0.1291) teacher/entropy 0.0605 (0.0567) teacher/usage_max 0.5727 (0.5830) teacher/usage_min 0.0520 (0.0461) teacher/usage_std 0.2146 (0.2248) nleep/row_max_mean 1518.3787 (1523.2939) nleep/row_max_std 70.0013 (55.7578) nleep/row_min_mean 1496.4833 (1498.5070) lr 1.9298e-03 eta 0:14:15
epoch [8/50] batch [40/160] time 0.096 (0.112) data 0.000 (0.009) loss 1.4027 (1.3866) teacher_loss 0.1424 (0.1512) loss_zs_kd 0.0828 (0.0723) loss_oracle 0.6133 (0.5792) kd_loss 0.9123 (0.9096) acc 96.8750 (96.1719) gate/entropy 1.0268 (1.0273) gate/usage_max 0.5151 (0.5145) gate/usage_min 0.2205 (0.2214) gate/usage_std 0.1298 (0.1293) teacher/entropy 0.0644 (0.0483) teacher/usage_max 0.5313 (0.5862) teacher/usage_min 0.0069 (0.0424) teacher/usage_std 0.2326 (0.2288) nleep/row_max_mean 1529.0941 (1522.8301) nleep/row_max_std 44.6946 (59.7971) nleep/row_min_mean 1501.8843 (1497.7103) lr 1.9298e-03 eta 0:12:49
epoch [8/50] batch [60/160] time 0.109 (0.109) data 0.000 (0.006) loss 1.4082 (1.3819) teacher_loss 0.0579 (0.1371) loss_zs_kd 0.0785 (0.0718) loss_oracle 0.5873 (0.5741) kd_loss 1.0174 (0.9218) acc 100.0000 (96.4583) gate/entropy 1.0268 (1.0272) gate/usage_max 0.5149 (0.5147) gate/usage_min 0.2198 (0.2210) gate/usage_std 0.1298 (0.1295) teacher/entropy 0.0038 (0.0430) teacher/usage_max 0.4995 (0.5746) teacher/usage_min 0.0316 (0.0411) teacher/usage_std 0.2137 (0.2257) nleep/row_max_mean 1527.9590 (1523.3383) nleep/row_max_std 48.4538 (61.1997) nleep/row_min_mean 1500.7966 (1497.8571) lr 1.9298e-03 eta 0:12:22
epoch [8/50] batch [80/160] time 0.105 (0.111) data 0.000 (0.005) loss 1.3675 (1.3834) teacher_loss 0.0192 (0.1303) loss_zs_kd 0.0659 (0.0720) loss_oracle 0.5209 (0.5803) kd_loss 1.0549 (0.9269) acc 100.0000 (96.5234) gate/entropy 1.0267 (1.0271) gate/usage_max 0.5147 (0.5147) gate/usage_min 0.2189 (0.2206) gate/usage_std 0.1297 (0.1295) teacher/entropy 0.0471 (0.0420) teacher/usage_max 0.4896 (0.5713) teacher/usage_min 0.1356 (0.0425) teacher/usage_std 0.1475 (0.2238) nleep/row_max_mean 1534.0791 (1522.8786) nleep/row_max_std 47.6959 (62.6461) nleep/row_min_mean 1508.6663 (1497.0768) lr 1.9298e-03 eta 0:12:35
epoch [8/50] batch [100/160] time 0.102 (0.110) data 0.000 (0.004) loss 1.4005 (1.3837) teacher_loss 0.0319 (0.1276) loss_zs_kd 0.0670 (0.0717) loss_oracle 0.5734 (0.5775) kd_loss 1.0483 (0.9315) acc 100.0000 (96.5312) gate/entropy 1.0270 (1.0270) gate/usage_max 0.5141 (0.5147) gate/usage_min 0.2182 (0.2201) gate/usage_std 0.1294 (0.1296) teacher/entropy 0.0290 (0.0398) teacher/usage_max 0.4761 (0.5711) teacher/usage_min 0.1176 (0.0429) teacher/usage_std 0.1552 (0.2232) nleep/row_max_mean 1524.5931 (1522.2264) nleep/row_max_std 50.0113 (63.3808) nleep/row_min_mean 1497.3696 (1496.0782) lr 1.9298e-03 eta 0:12:23
epoch [8/50] batch [120/160] time 0.095 (0.108) data 0.000 (0.003) loss 1.4125 (1.3789) teacher_loss 0.0704 (0.1230) loss_zs_kd 0.1090 (0.0731) loss_oracle 0.5884 (0.5790) kd_loss 0.9933 (0.9299) acc 96.8750 (96.6927) gate/entropy 1.0269 (1.0270) gate/usage_max 0.5140 (0.5146) gate/usage_min 0.2173 (0.2197) gate/usage_std 0.1295 (0.1296) teacher/entropy 0.0278 (0.0400) teacher/usage_max 0.4757 (0.5702) teacher/usage_min 0.0556 (0.0438) teacher/usage_std 0.1964 (0.2227) nleep/row_max_mean 1534.0337 (1521.7553) nleep/row_max_std 32.1738 (62.8002) nleep/row_min_mean 1507.3234 (1495.4039) lr 1.9298e-03 eta 0:12:10
epoch [8/50] batch [140/160] time 0.105 (0.107) data 0.000 (0.003) loss 1.3102 (1.3780) teacher_loss 0.0889 (0.1200) loss_zs_kd 0.0740 (0.0740) loss_oracle 0.6234 (0.5823) kd_loss 0.8726 (0.9298) acc 96.8750 (96.8304) gate/entropy 1.0265 (1.0269) gate/usage_max 0.5142 (0.5145) gate/usage_min 0.2163 (0.2193) gate/usage_std 0.1297 (0.1296) teacher/entropy 0.0151 (0.0402) teacher/usage_max 0.6874 (0.5688) teacher/usage_min 0.0979 (0.0443) teacher/usage_std 0.2549 (0.2224) nleep/row_max_mean 1529.5763 (1521.4710) nleep/row_max_std 69.1564 (63.1328) nleep/row_min_mean 1502.9885 (1495.0313) lr 1.9298e-03 eta 0:12:03
epoch [8/50] batch [160/160] time 0.088 (0.105) data 0.000 (0.002) loss 1.4105 (1.3770) teacher_loss 0.0286 (0.1176) loss_zs_kd 0.0676 (0.0747) loss_oracle 0.6053 (0.5833) kd_loss 1.0454 (0.9304) acc 100.0000 (96.8750) gate/entropy 1.0266 (1.0269) gate/usage_max 0.5139 (0.5144) gate/usage_min 0.2156 (0.2189) gate/usage_std 0.1296 (0.1295) teacher/entropy 0.0309 (0.0397) teacher/usage_max 0.5851 (0.5692) teacher/usage_min 0.0403 (0.0448) teacher/usage_std 0.2243 (0.2220) nleep/row_max_mean 1521.9854 (1521.0873) nleep/row_max_std 78.8414 (63.1966) nleep/row_min_mean 1493.9766 (1494.4870) lr 1.9048e-03 eta 0:11:48
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,800
* accuracy: 81.6%
* error: 18.4%
* macro_f1: 84.0%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,879
* accuracy: 85.3%
* error: 14.7%
* macro_f1: 87.2%
******* Domain p best val acc:      83.5%, epoch: 7 *******
******* Domain p best val test acc: 87.3%, epoch: 7 *******
******* Domain p best test acc:     88.0%, epoch: 5 *******
epoch [9/50] batch [20/160] time 0.115 (0.123) data 0.000 (0.015) loss 1.2899 (1.3749) teacher_loss 0.0797 (0.0987) loss_zs_kd 0.0775 (0.0772) loss_oracle 0.6113 (0.5765) kd_loss 0.8658 (0.9493) acc 96.8750 (97.8125) gate/entropy 1.0269 (1.0269) gate/usage_max 0.5131 (0.5132) gate/usage_min 0.2149 (0.2153) gate/usage_std 0.1292 (0.1293) teacher/entropy 0.0246 (0.0310) teacher/usage_max 0.6587 (0.5521) teacher/usage_min 0.0290 (0.0436) teacher/usage_std 0.2575 (0.2174) nleep/row_max_mean 1512.1868 (1516.9604) nleep/row_max_std 76.6835 (64.1064) nleep/row_min_mean 1487.2917 (1489.6744) lr 1.9048e-03 eta 0:13:42
epoch [9/50] batch [40/160] time 0.105 (0.116) data 0.000 (0.008) loss 1.3695 (1.3913) teacher_loss 0.1239 (0.1183) loss_zs_kd 0.0892 (0.0745) loss_oracle 0.5680 (0.5737) kd_loss 0.9170 (0.9489) acc 93.7500 (97.0312) gate/entropy 1.0272 (1.0270) gate/usage_max 0.5124 (0.5129) gate/usage_min 0.2143 (0.2150) gate/usage_std 0.1289 (0.1291) teacher/entropy 0.0917 (0.0377) teacher/usage_max 0.4943 (0.5544) teacher/usage_min 0.0966 (0.0584) teacher/usage_std 0.1710 (0.2105) nleep/row_max_mean 1526.7240 (1517.4360) nleep/row_max_std 49.8973 (61.4304) nleep/row_min_mean 1499.9545 (1490.1571) lr 1.9048e-03 eta 0:12:53
epoch [9/50] batch [60/160] time 0.109 (0.112) data 0.001 (0.005) loss 1.3308 (1.3887) teacher_loss 0.0694 (0.1260) loss_zs_kd 0.0800 (0.0753) loss_oracle 0.6046 (0.5684) kd_loss 0.9191 (0.9409) acc 96.8750 (96.6667) gate/entropy 1.0272 (1.0271) gate/usage_max 0.5121 (0.5127) gate/usage_min 0.2135 (0.2146) gate/usage_std 0.1288 (0.1290) teacher/entropy 0.0437 (0.0454) teacher/usage_max 0.5570 (0.5502) teacher/usage_min 0.0729 (0.0622) teacher/usage_std 0.1993 (0.2074) nleep/row_max_mean 1518.9851 (1516.1786) nleep/row_max_std 71.3873 (62.1616) nleep/row_min_mean 1490.1257 (1488.9618) lr 1.9048e-03 eta 0:12:23
epoch [9/50] batch [80/160] time 0.105 (0.110) data 0.001 (0.004) loss 1.2458 (1.3825) teacher_loss 0.0767 (0.1231) loss_zs_kd 0.0732 (0.0753) loss_oracle 0.5698 (0.5635) kd_loss 0.8476 (0.9400) acc 96.8750 (96.7969) gate/entropy 1.0275 (1.0272) gate/usage_max 0.5114 (0.5124) gate/usage_min 0.2129 (0.2143) gate/usage_std 0.1285 (0.1289) teacher/entropy 0.0773 (0.0496) teacher/usage_max 0.6105 (0.5433) teacher/usage_min 0.0543 (0.0660) teacher/usage_std 0.2271 (0.2038) nleep/row_max_mean 1512.5759 (1515.8361) nleep/row_max_std 58.0028 (62.0740) nleep/row_min_mean 1485.4990 (1488.9044) lr 1.9048e-03 eta 0:12:13
epoch [9/50] batch [100/160] time 0.109 (0.108) data 0.000 (0.003) loss 1.3038 (1.3778) teacher_loss 0.0627 (0.1207) loss_zs_kd 0.0593 (0.0735) loss_oracle 0.5231 (0.5589) kd_loss 0.9499 (0.9408) acc 96.8750 (96.8750) gate/entropy 1.0277 (1.0273) gate/usage_max 0.5109 (0.5122) gate/usage_min 0.2123 (0.2139) gate/usage_std 0.1283 (0.1288) teacher/entropy 0.0901 (0.0519) teacher/usage_max 0.4663 (0.5407) teacher/usage_min 0.0955 (0.0726) teacher/usage_std 0.1685 (0.1999) nleep/row_max_mean 1521.4807 (1515.3781) nleep/row_max_std 43.7443 (62.2087) nleep/row_min_mean 1496.1799 (1488.7563) lr 1.9048e-03 eta 0:11:57
epoch [9/50] batch [120/160] time 0.100 (0.108) data 0.000 (0.003) loss 1.3096 (1.3826) teacher_loss 0.0618 (0.1205) loss_zs_kd 0.0991 (0.0750) loss_oracle 0.5495 (0.5601) kd_loss 0.9235 (0.9445) acc 100.0000 (96.7969) gate/entropy 1.0277 (1.0274) gate/usage_max 0.5105 (0.5119) gate/usage_min 0.2116 (0.2136) gate/usage_std 0.1282 (0.1287) teacher/entropy 0.0605 (0.0527) teacher/usage_max 0.5276 (0.5379) teacher/usage_min 0.0914 (0.0740) teacher/usage_std 0.1812 (0.1982) nleep/row_max_mean 1507.8060 (1515.1507) nleep/row_max_std 76.3576 (62.3550) nleep/row_min_mean 1481.9471 (1488.5506) lr 1.9048e-03 eta 0:11:51
epoch [9/50] batch [140/160] time 0.114 (0.107) data 0.001 (0.002) loss 1.4247 (1.3802) teacher_loss 0.0613 (0.1200) loss_zs_kd 0.0817 (0.0757) loss_oracle 0.5331 (0.5537) kd_loss 1.0560 (0.9455) acc 96.8750 (96.7411) gate/entropy 1.0285 (1.0275) gate/usage_max 0.5091 (0.5116) gate/usage_min 0.2113 (0.2133) gate/usage_std 0.1274 (0.1286) teacher/entropy 0.0517 (0.0565) teacher/usage_max 0.4387 (0.5334) teacher/usage_min 0.1928 (0.0804) teacher/usage_std 0.1034 (0.1934) nleep/row_max_mean 1509.2709 (1514.8498) nleep/row_max_std 60.9771 (62.1804) nleep/row_min_mean 1480.6440 (1488.3103) lr 1.9048e-03 eta 0:11:45
epoch [9/50] batch [160/160] time 0.100 (0.107) data 0.000 (0.002) loss 1.3463 (1.3793) teacher_loss 0.1321 (0.1179) loss_zs_kd 0.0777 (0.0758) loss_oracle 0.4793 (0.5456) kd_loss 0.9357 (0.9506) acc 93.7500 (96.8164) gate/entropy 1.0287 (1.0276) gate/usage_max 0.5084 (0.5113) gate/usage_min 0.2106 (0.2130) gate/usage_std 0.1271 (0.1284) teacher/entropy 0.0926 (0.0590) teacher/usage_max 0.4858 (0.5269) teacher/usage_min 0.1625 (0.0881) teacher/usage_std 0.1326 (0.1877) nleep/row_max_mean 1500.6887 (1514.4053) nleep/row_max_std 71.3383 (62.6376) nleep/row_min_mean 1476.6805 (1487.9147) lr 1.8763e-03 eta 0:11:41
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,818
* accuracy: 82.4%
* error: 17.6%
* macro_f1: 84.3%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,831
* accuracy: 83.9%
* error: 16.1%
* macro_f1: 86.1%
******* Domain p best val acc:      83.5%, epoch: 7 *******
******* Domain p best val test acc: 87.3%, epoch: 7 *******
******* Domain p best test acc:     88.0%, epoch: 5 *******
epoch [10/50] batch [20/160] time 0.109 (0.116) data 0.000 (0.013) loss 1.4158 (1.4061) teacher_loss 0.0755 (0.1157) loss_zs_kd 0.0680 (0.0823) loss_oracle 0.5299 (0.4871) kd_loss 1.0413 (1.0057) acc 96.8750 (96.5625) gate/entropy 1.0287 (1.0288) gate/usage_max 0.5081 (0.5081) gate/usage_min 0.2101 (0.2104) gate/usage_std 0.1270 (0.1270) teacher/entropy 0.1081 (0.0889) teacher/usage_max 0.4276 (0.4575) teacher/usage_min 0.2497 (0.1850) teacher/usage_std 0.0730 (0.1172) nleep/row_max_mean 1539.0690 (1514.5182) nleep/row_max_std 29.8140 (58.0716) nleep/row_min_mean 1510.6365 (1488.1566) lr 1.8763e-03 eta 0:12:41
epoch [10/50] batch [40/160] time 0.104 (0.111) data 0.000 (0.006) loss 1.4631 (1.4129) teacher_loss 0.1937 (0.1122) loss_zs_kd 0.0774 (0.0787) loss_oracle 0.5231 (0.4922) kd_loss 0.9691 (1.0153) acc 90.6250 (96.6406) gate/entropy 1.0293 (1.0290) gate/usage_max 0.5069 (0.5077) gate/usage_min 0.2098 (0.2103) gate/usage_std 0.1264 (0.1267) teacher/entropy 0.0973 (0.0931) teacher/usage_max 0.4635 (0.4559) teacher/usage_min 0.2506 (0.1948) teacher/usage_std 0.0932 (0.1120) nleep/row_max_mean 1506.0405 (1513.3007) nleep/row_max_std 84.6963 (61.6263) nleep/row_min_mean 1483.4238 (1486.7350) lr 1.8763e-03 eta 0:12:03
epoch [10/50] batch [60/160] time 0.106 (0.109) data 0.001 (0.004) loss 1.3446 (1.4206) teacher_loss 0.0684 (0.1112) loss_zs_kd 0.0771 (0.0794) loss_oracle 0.4628 (0.5002) kd_loss 1.0061 (1.0196) acc 100.0000 (96.8750) gate/entropy 1.0299 (1.0292) gate/usage_max 0.5057 (0.5072) gate/usage_min 0.2094 (0.2100) gate/usage_std 0.1257 (0.1265) teacher/entropy 0.1276 (0.0889) teacher/usage_max 0.3917 (0.4573) teacher/usage_min 0.2579 (0.1966) teacher/usage_std 0.0559 (0.1112) nleep/row_max_mean 1508.6688 (1513.4312) nleep/row_max_std 65.9621 (61.7073) nleep/row_min_mean 1483.1123 (1486.6943) lr 1.8763e-03 eta 0:11:45
epoch [10/50] batch [80/160] time 0.097 (0.107) data 0.000 (0.003) loss 1.6843 (1.4295) teacher_loss 0.2208 (0.1110) loss_zs_kd 0.1014 (0.0810) loss_oracle 0.4865 (0.5025) kd_loss 1.1695 (1.0267) acc 96.8750 (96.9531) gate/entropy 1.0303 (1.0295) gate/usage_max 0.5048 (0.5066) gate/usage_min 0.2090 (0.2099) gate/usage_std 0.1252 (0.1262) teacher/entropy 0.0585 (0.0839) teacher/usage_max 0.3759 (0.4598) teacher/usage_min 0.2490 (0.1971) teacher/usage_std 0.0596 (0.1119) nleep/row_max_mean 1523.6594 (1513.3221) nleep/row_max_std 50.9370 (61.0358) nleep/row_min_mean 1497.1802 (1486.6750) lr 1.8763e-03 eta 0:11:33
epoch [10/50] batch [100/160] time 0.098 (0.106) data 0.000 (0.003) loss 1.2857 (1.4274) teacher_loss 0.0382 (0.1052) loss_zs_kd 0.1070 (0.0827) loss_oracle 0.4788 (0.5004) kd_loss 0.9546 (1.0306) acc 100.0000 (97.2500) gate/entropy 1.0309 (1.0297) gate/usage_max 0.5036 (0.5061) gate/usage_min 0.2087 (0.2097) gate/usage_std 0.1246 (0.1259) teacher/entropy 0.1517 (0.0821) teacher/usage_max 0.4101 (0.4638) teacher/usage_min 0.2774 (0.1979) teacher/usage_std 0.0561 (0.1134) nleep/row_max_mean 1515.1560 (1513.7934) nleep/row_max_std 62.1122 (60.3891) nleep/row_min_mean 1490.0479 (1487.1841) lr 1.8763e-03 eta 0:11:25
epoch [10/50] batch [120/160] time 0.107 (0.105) data 0.000 (0.002) loss 1.4992 (1.4348) teacher_loss 0.1323 (0.1092) loss_zs_kd 0.1093 (0.0830) loss_oracle 0.4620 (0.4987) kd_loss 1.0813 (1.0347) acc 96.8750 (97.0833) gate/entropy 1.0311 (1.0300) gate/usage_max 0.5028 (0.5056) gate/usage_min 0.2082 (0.2095) gate/usage_std 0.1243 (0.1257) teacher/entropy 0.0643 (0.0816) teacher/usage_max 0.4078 (0.4625) teacher/usage_min 0.2633 (0.2015) teacher/usage_std 0.0591 (0.1111) nleep/row_max_mean 1526.0724 (1513.7438) nleep/row_max_std 63.1345 (60.9365) nleep/row_min_mean 1497.9414 (1487.1464) lr 1.8763e-03 eta 0:11:18
epoch [10/50] batch [140/160] time 0.104 (0.105) data 0.001 (0.002) loss 1.3730 (1.4421) teacher_loss 0.0436 (0.1038) loss_zs_kd 0.0933 (0.0838) loss_oracle 0.5028 (0.5073) kd_loss 1.0313 (1.0428) acc 100.0000 (97.2768) gate/entropy 1.0323 (1.0302) gate/usage_max 0.5007 (0.5051) gate/usage_min 0.2082 (0.2093) gate/usage_std 0.1231 (0.1254) teacher/entropy 0.0882 (0.0790) teacher/usage_max 0.4255 (0.4646) teacher/usage_min 0.2250 (0.2028) teacher/usage_std 0.0827 (0.1115) nleep/row_max_mean 1496.4636 (1514.0674) nleep/row_max_std 82.6238 (61.0112) nleep/row_min_mean 1471.9535 (1487.2724) lr 1.8763e-03 eta 0:11:13
epoch [10/50] batch [160/160] time 0.078 (0.104) data 0.000 (0.002) loss 1.5962 (1.4546) teacher_loss 0.1656 (0.1086) loss_zs_kd 0.0725 (0.0854) loss_oracle 0.6488 (0.5074) kd_loss 1.0699 (1.0497) acc 93.7500 (97.1484) gate/entropy 1.0328 (1.0305) gate/usage_max 0.4995 (0.5045) gate/usage_min 0.2076 (0.2091) gate/usage_std 0.1226 (0.1251) teacher/entropy 0.1043 (0.0782) teacher/usage_max 0.3445 (0.4657) teacher/usage_min 0.3224 (0.2042) teacher/usage_std 0.0090 (0.1115) nleep/row_max_mean 1512.0444 (1513.4849) nleep/row_max_std 78.6297 (61.8443) nleep/row_min_mean 1484.8450 (1486.6288) lr 1.8443e-03 eta 0:11:05
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,839
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 84.4%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,856
* accuracy: 84.6%
* error: 15.4%
* macro_f1: 86.6%
******* Domain p best val acc:      83.5%, epoch: 7 *******
******* Domain p best val test acc: 87.3%, epoch: 7 *******
******* Domain p best test acc:     88.0%, epoch: 5 *******
epoch [11/50] batch [20/160] time 0.098 (0.136) data 0.000 (0.014) loss 1.5060 (1.5720) teacher_loss 0.0485 (0.1146) loss_zs_kd 0.0775 (0.0758) loss_oracle 0.4696 (0.5415) kd_loss 1.1839 (1.1488) acc 100.0000 (97.0312) gate/entropy 1.0334 (1.0331) gate/usage_max 0.4982 (0.4988) gate/usage_min 0.2076 (0.2076) gate/usage_std 0.1218 (0.1221) teacher/entropy 0.0233 (0.0697) teacher/usage_max 0.4366 (0.4556) teacher/usage_min 0.2442 (0.2225) teacher/usage_std 0.0792 (0.1003) nleep/row_max_mean 1512.8152 (1516.0096) nleep/row_max_std 70.2982 (58.0086) nleep/row_min_mean 1484.4019 (1488.4300) lr 1.8443e-03 eta 0:14:30
epoch [11/50] batch [40/160] time 0.109 (0.123) data 0.000 (0.007) loss 1.4829 (1.5725) teacher_loss 0.0331 (0.1184) loss_zs_kd 0.0821 (0.0770) loss_oracle 0.4440 (0.5327) kd_loss 1.1868 (1.1493) acc 100.0000 (97.3438) gate/entropy 1.0342 (1.0335) gate/usage_max 0.4970 (0.4981) gate/usage_min 0.2079 (0.2077) gate/usage_std 0.1211 (0.1218) teacher/entropy 0.1056 (0.0736) teacher/usage_max 0.5302 (0.4554) teacher/usage_min 0.1062 (0.2181) teacher/usage_std 0.1744 (0.1018) nleep/row_max_mean 1515.9524 (1515.3720) nleep/row_max_std 61.2800 (59.3964) nleep/row_min_mean 1488.1160 (1487.6832) lr 1.8443e-03 eta 0:13:00
epoch [11/50] batch [60/160] time 0.107 (0.117) data 0.001 (0.005) loss 1.4482 (1.5830) teacher_loss 0.0439 (0.1192) loss_zs_kd 0.0774 (0.0761) loss_oracle 0.5711 (0.5574) kd_loss 1.0800 (1.1470) acc 100.0000 (97.3958) gate/entropy 1.0349 (1.0338) gate/usage_max 0.4959 (0.4976) gate/usage_min 0.2082 (0.2078) gate/usage_std 0.1204 (0.1214) teacher/entropy 0.0787 (0.0767) teacher/usage_max 0.5342 (0.4507) teacher/usage_min 0.2095 (0.2216) teacher/usage_std 0.1433 (0.0984) nleep/row_max_mean 1494.3777 (1513.4951) nleep/row_max_std 81.7578 (62.1804) nleep/row_min_mean 1468.0361 (1485.8463) lr 1.8443e-03 eta 0:12:22
epoch [11/50] batch [80/160] time 0.098 (0.113) data 0.000 (0.004) loss 1.6686 (1.5906) teacher_loss 0.2284 (0.1160) loss_zs_kd 0.0671 (0.0775) loss_oracle 0.6473 (0.5802) kd_loss 1.0830 (1.1458) acc 93.7500 (97.3438) gate/entropy 1.0355 (1.0342) gate/usage_max 0.4946 (0.4970) gate/usage_min 0.2082 (0.2079) gate/usage_std 0.1197 (0.1211) teacher/entropy 0.1760 (0.0771) teacher/usage_max 0.4722 (0.4526) teacher/usage_min 0.2410 (0.2186) teacher/usage_std 0.1000 (0.1000) nleep/row_max_mean 1500.0083 (1512.2245) nleep/row_max_std 79.6177 (63.6428) nleep/row_min_mean 1473.6741 (1484.3879) lr 1.8443e-03 eta 0:11:53
epoch [11/50] batch [100/160] time 0.095 (0.110) data 0.000 (0.003) loss 1.5751 (1.5894) teacher_loss 0.0731 (0.1175) loss_zs_kd 0.0779 (0.0760) loss_oracle 0.5064 (0.5761) kd_loss 1.2099 (1.1458) acc 100.0000 (97.3750) gate/entropy 1.0362 (1.0345) gate/usage_max 0.4934 (0.4964) gate/usage_min 0.2084 (0.2080) gate/usage_std 0.1190 (0.1207) teacher/entropy 0.1007 (0.0838) teacher/usage_max 0.5200 (0.4553) teacher/usage_min 0.1707 (0.2111) teacher/usage_std 0.1437 (0.1041) nleep/row_max_mean 1505.3356 (1511.3590) nleep/row_max_std 74.4389 (64.0989) nleep/row_min_mean 1480.5107 (1483.2987) lr 1.8443e-03 eta 0:11:32
epoch [11/50] batch [120/160] time 0.098 (0.108) data 0.000 (0.003) loss 1.6103 (1.5905) teacher_loss 0.0799 (0.1157) loss_zs_kd 0.0720 (0.0752) loss_oracle 0.6517 (0.5746) kd_loss 1.1686 (1.1499) acc 100.0000 (97.3698) gate/entropy 1.0371 (1.0349) gate/usage_max 0.4919 (0.4957) gate/usage_min 0.2088 (0.2081) gate/usage_std 0.1180 (0.1203) teacher/entropy 0.0855 (0.0854) teacher/usage_max 0.4259 (0.4589) teacher/usage_min 0.2120 (0.2050) teacher/usage_std 0.0897 (0.1083) nleep/row_max_mean 1506.8453 (1511.3949) nleep/row_max_std 69.3021 (63.9476) nleep/row_min_mean 1477.3237 (1483.0530) lr 1.8443e-03 eta 0:11:20
epoch [11/50] batch [140/160] time 0.109 (0.107) data 0.000 (0.002) loss 1.6711 (1.5979) teacher_loss 0.1548 (0.1147) loss_zs_kd 0.1077 (0.0747) loss_oracle 0.7126 (0.5840) kd_loss 1.1062 (1.1538) acc 96.8750 (97.4554) gate/entropy 1.0379 (1.0353) gate/usage_max 0.4909 (0.4951) gate/usage_min 0.2095 (0.2083) gate/usage_std 0.1173 (0.1200) teacher/entropy 0.1429 (0.0869) teacher/usage_max 0.4916 (0.4633) teacher/usage_min 0.1627 (0.2002) teacher/usage_std 0.1346 (0.1123) nleep/row_max_mean 1507.6508 (1510.9739) nleep/row_max_std 58.9597 (63.7913) nleep/row_min_mean 1480.2617 (1482.7665) lr 1.8443e-03 eta 0:11:09
epoch [11/50] batch [160/160] time 0.082 (0.105) data 0.000 (0.002) loss 1.4814 (1.6021) teacher_loss 0.0536 (0.1180) loss_zs_kd 0.0672 (0.0742) loss_oracle 0.5976 (0.5832) kd_loss 1.0954 (1.1554) acc 100.0000 (97.3828) gate/entropy 1.0385 (1.0356) gate/usage_max 0.4900 (0.4945) gate/usage_min 0.2101 (0.2085) gate/usage_std 0.1167 (0.1196) teacher/entropy 0.0747 (0.0879) teacher/usage_max 0.4243 (0.4640) teacher/usage_min 0.2826 (0.1971) teacher/usage_std 0.0644 (0.1141) nleep/row_max_mean 1509.8364 (1510.9441) nleep/row_max_std 60.6236 (63.1216) nleep/row_min_mean 1483.5651 (1482.7140) lr 1.8090e-03 eta 0:10:56
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,832
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 84.3%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,904
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 87.5%
******* Domain p best val acc:      83.5%, epoch: 7 *******
******* Domain p best val test acc: 87.3%, epoch: 7 *******
******* Domain p best test acc:     88.0%, epoch: 5 *******
epoch [12/50] batch [20/160] time 0.117 (0.119) data 0.000 (0.015) loss 1.5520 (1.6105) teacher_loss 0.0992 (0.1357) loss_zs_kd 0.0794 (0.0699) loss_oracle 0.5531 (0.5354) kd_loss 1.1366 (1.1722) acc 93.7500 (96.2500) gate/entropy 1.0390 (1.0388) gate/usage_max 0.4894 (0.4895) gate/usage_min 0.2106 (0.2104) gate/usage_std 0.1162 (0.1164) teacher/entropy 0.0723 (0.0969) teacher/usage_max 0.4393 (0.4882) teacher/usage_min 0.2531 (0.1662) teacher/usage_std 0.0782 (0.1362) nleep/row_max_mean 1517.0111 (1505.4066) nleep/row_max_std 46.7708 (63.9264) nleep/row_min_mean 1488.7358 (1478.0136) lr 1.8090e-03 eta 0:12:19
epoch [12/50] batch [40/160] time 0.096 (0.109) data 0.000 (0.008) loss 1.6623 (1.5903) teacher_loss 0.0732 (0.1074) loss_zs_kd 0.0598 (0.0694) loss_oracle 0.6484 (0.5435) kd_loss 1.2349 (1.1763) acc 93.7500 (97.1094) gate/entropy 1.0396 (1.0391) gate/usage_max 0.4886 (0.4892) gate/usage_min 0.2113 (0.2107) gate/usage_std 0.1156 (0.1161) teacher/entropy 0.0631 (0.0898) teacher/usage_max 0.5149 (0.4700) teacher/usage_min 0.1769 (0.1732) teacher/usage_std 0.1391 (0.1266) nleep/row_max_mean 1520.6313 (1507.7380) nleep/row_max_std 43.9368 (61.5111) nleep/row_min_mean 1491.9814 (1480.4924) lr 1.8090e-03 eta 0:11:13
epoch [12/50] batch [60/160] time 0.100 (0.106) data 0.000 (0.005) loss 1.5517 (1.5950) teacher_loss 0.0471 (0.1105) loss_zs_kd 0.0764 (0.0739) loss_oracle 0.5802 (0.5481) kd_loss 1.1763 (1.1735) acc 100.0000 (97.2396) gate/entropy 1.0403 (1.0394) gate/usage_max 0.4877 (0.4888) gate/usage_min 0.2119 (0.2110) gate/usage_std 0.1150 (0.1158) teacher/entropy 0.0865 (0.0847) teacher/usage_max 0.4162 (0.4706) teacher/usage_min 0.1725 (0.1807) teacher/usage_std 0.1137 (0.1243) nleep/row_max_mean 1515.8539 (1507.8029) nleep/row_max_std 44.6481 (61.3863) nleep/row_min_mean 1489.5955 (1480.7887) lr 1.8090e-03 eta 0:10:57
epoch [12/50] batch [80/160] time 0.093 (0.105) data 0.000 (0.004) loss 1.4984 (1.6055) teacher_loss 0.1055 (0.1099) loss_zs_kd 0.0760 (0.0749) loss_oracle 0.5574 (0.5717) kd_loss 1.0761 (1.1723) acc 100.0000 (97.3438) gate/entropy 1.0408 (1.0397) gate/usage_max 0.4869 (0.4885) gate/usage_min 0.2125 (0.2113) gate/usage_std 0.1144 (0.1155) teacher/entropy 0.1193 (0.0810) teacher/usage_max 0.4793 (0.4694) teacher/usage_min 0.2261 (0.1820) teacher/usage_std 0.1070 (0.1230) nleep/row_max_mean 1511.0775 (1508.1657) nleep/row_max_std 58.0796 (60.3830) nleep/row_min_mean 1487.0835 (1481.2827) lr 1.8090e-03 eta 0:10:45
epoch [12/50] batch [100/160] time 0.130 (0.106) data 0.001 (0.003) loss 1.6831 (1.6055) teacher_loss 0.0799 (0.1050) loss_zs_kd 0.1013 (0.0781) loss_oracle 0.6782 (0.5832) kd_loss 1.2135 (1.1698) acc 96.8750 (97.3750) gate/entropy 1.0416 (1.0400) gate/usage_max 0.4856 (0.4879) gate/usage_min 0.2128 (0.2116) gate/usage_std 0.1136 (0.1152) teacher/entropy 0.0741 (0.0803) teacher/usage_max 0.4853 (0.4703) teacher/usage_min 0.1682 (0.1833) teacher/usage_std 0.1298 (0.1230) nleep/row_max_mean 1522.4508 (1507.8217) nleep/row_max_std 48.6854 (60.8437) nleep/row_min_mean 1497.0488 (1481.0720) lr 1.8090e-03 eta 0:10:51
epoch [12/50] batch [120/160] time 0.112 (0.108) data 0.000 (0.003) loss 1.6749 (1.6043) teacher_loss 0.1725 (0.1045) loss_zs_kd 0.0570 (0.0781) loss_oracle 0.6339 (0.5918) kd_loss 1.1570 (1.1648) acc 96.8750 (97.3698) gate/entropy 1.0426 (1.0404) gate/usage_max 0.4840 (0.4874) gate/usage_min 0.2135 (0.2119) gate/usage_std 0.1126 (0.1148) teacher/entropy 0.1067 (0.0799) teacher/usage_max 0.4222 (0.4694) teacher/usage_min 0.1697 (0.1881) teacher/usage_std 0.1158 (0.1206) nleep/row_max_mean 1515.2205 (1507.0975) nleep/row_max_std 54.1019 (62.5400) nleep/row_min_mean 1488.9497 (1480.4867) lr 1.8090e-03 eta 0:11:00
epoch [12/50] batch [140/160] time 0.094 (0.107) data 0.000 (0.002) loss 1.7551 (1.6047) teacher_loss 0.0362 (0.1039) loss_zs_kd 0.0700 (0.0774) loss_oracle 0.8330 (0.6006) kd_loss 1.2674 (1.1618) acc 100.0000 (97.3438) gate/entropy 1.0434 (1.0408) gate/usage_max 0.4825 (0.4868) gate/usage_min 0.2139 (0.2122) gate/usage_std 0.1116 (0.1144) teacher/entropy 0.0269 (0.0776) teacher/usage_max 0.4664 (0.4706) teacher/usage_min 0.1326 (0.1885) teacher/usage_std 0.1444 (0.1207) nleep/row_max_mean 1503.9116 (1506.9594) nleep/row_max_std 76.1983 (62.8929) nleep/row_min_mean 1476.2612 (1480.3359) lr 1.8090e-03 eta 0:10:52
epoch [12/50] batch [160/160] time 0.160 (0.106) data 0.000 (0.002) loss 1.6220 (1.6116) teacher_loss 0.1117 (0.0993) loss_zs_kd 0.0789 (0.0796) loss_oracle 0.6005 (0.6163) kd_loss 1.1705 (1.1644) acc 96.8750 (97.4805) gate/entropy 1.0443 (1.0412) gate/usage_max 0.4809 (0.4861) gate/usage_min 0.2144 (0.2124) gate/usage_std 0.1107 (0.1140) teacher/entropy 0.0942 (0.0744) teacher/usage_max 0.4216 (0.4711) teacher/usage_min 0.1581 (0.1878) teacher/usage_std 0.1239 (0.1213) nleep/row_max_mean 1481.6741 (1507.0287) nleep/row_max_std 102.0571 (62.4358) nleep/row_min_mean 1456.0774 (1480.2628) lr 1.7705e-03 eta 0:10:45
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,843
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 84.8%
Checkpoint saved to icml/multi-dg/tuning/tuning2/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,905
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 87.6%
******* Domain p best val acc:      83.5%, epoch: 12 *******
******* Domain p best val test acc: 86.0%, epoch: 12 *******
******* Domain p best test acc:     88.0%, epoch: 5 *******
epoch [13/50] batch [20/160] time 0.087 (0.112) data 0.000 (0.015) loss 1.6885 (1.6331) teacher_loss 0.0159 (0.0764) loss_zs_kd 0.0791 (0.0939) loss_oracle 0.7325 (0.6370) kd_loss 1.2668 (1.1912) acc 100.0000 (98.1250) gate/entropy 1.0453 (1.0449) gate/usage_max 0.4790 (0.4797) gate/usage_min 0.2147 (0.2146) gate/usage_std 0.1096 (0.1100) teacher/entropy 0.0156 (0.0480) teacher/usage_max 0.4710 (0.5012) teacher/usage_min 0.1543 (0.1618) teacher/usage_std 0.1326 (0.1439) nleep/row_max_mean 1507.5359 (1511.0481) nleep/row_max_std 87.5286 (61.0662) nleep/row_min_mean 1480.1619 (1482.8081) lr 1.7705e-03 eta 0:11:18
epoch [13/50] batch [40/160] time 0.109 (0.105) data 0.000 (0.008) loss 1.6468 (1.6409) teacher_loss 0.0294 (0.0794) loss_zs_kd 0.0763 (0.0929) loss_oracle 0.7666 (0.6635) kd_loss 1.1959 (1.1833) acc 100.0000 (98.3594) gate/entropy 1.0465 (1.0455) gate/usage_max 0.4767 (0.4787) gate/usage_min 0.2154 (0.2149) gate/usage_std 0.1082 (0.1094) teacher/entropy 0.0217 (0.0470) teacher/usage_max 0.4046 (0.4984) teacher/usage_min 0.2182 (0.1673) teacher/usage_std 0.0822 (0.1398) nleep/row_max_mean 1502.0017 (1509.0842) nleep/row_max_std 69.3013 (62.0213) nleep/row_min_mean 1471.4856 (1480.4102) lr 1.7705e-03 eta 0:10:36
epoch [13/50] batch [60/160] time 0.100 (0.103) data 0.001 (0.005) loss 1.9106 (1.6505) teacher_loss 0.2417 (0.0934) loss_zs_kd 0.0709 (0.0856) loss_oracle 0.7789 (0.6644) kd_loss 1.2440 (1.1821) acc 90.6250 (97.9688) gate/entropy 1.0475 (1.0460) gate/usage_max 0.4750 (0.4778) gate/usage_min 0.2159 (0.2151) gate/usage_std 0.1071 (0.1088) teacher/entropy 0.0871 (0.0514) teacher/usage_max 0.5225 (0.5032) teacher/usage_min 0.0693 (0.1641) teacher/usage_std 0.1924 (0.1431) nleep/row_max_mean 1519.5005 (1507.1858) nleep/row_max_std 50.7491 (64.3742) nleep/row_min_mean 1487.5161 (1478.3295) lr 1.7705e-03 eta 0:10:21
epoch [13/50] batch [80/160] time 0.102 (0.102) data 0.000 (0.004) loss 1.6087 (1.6413) teacher_loss 0.0401 (0.0900) loss_zs_kd 0.0831 (0.0861) loss_oracle 0.6583 (0.6558) kd_loss 1.1979 (1.1804) acc 100.0000 (97.9297) gate/entropy 1.0485 (1.0465) gate/usage_max 0.4729 (0.4767) gate/usage_min 0.2163 (0.2154) gate/usage_std 0.1059 (0.1082) teacher/entropy 0.0725 (0.0551) teacher/usage_max 0.5076 (0.5034) teacher/usage_min 0.0986 (0.1591) teacher/usage_std 0.1723 (0.1462) nleep/row_max_mean 1527.7261 (1507.6422) nleep/row_max_std 47.5835 (62.2402) nleep/row_min_mean 1495.8494 (1478.6339) lr 1.7705e-03 eta 0:10:14
epoch [13/50] batch [100/160] time 0.096 (0.102) data 0.000 (0.003) loss 1.5829 (1.6420) teacher_loss 0.0597 (0.0946) loss_zs_kd 0.0505 (0.0853) loss_oracle 0.5804 (0.6437) kd_loss 1.2077 (1.1829) acc 96.8750 (97.7500) gate/entropy 1.0498 (1.0471) gate/usage_max 0.4705 (0.4757) gate/usage_min 0.2171 (0.2157) gate/usage_std 0.1045 (0.1076) teacher/entropy 0.0452 (0.0555) teacher/usage_max 0.4339 (0.5015) teacher/usage_min 0.1504 (0.1557) teacher/usage_std 0.1295 (0.1469) nleep/row_max_mean 1516.1492 (1507.7618) nleep/row_max_std 47.7500 (62.2865) nleep/row_min_mean 1487.4169 (1478.5946) lr 1.7705e-03 eta 0:10:11
epoch [13/50] batch [120/160] time 0.097 (0.102) data 0.000 (0.003) loss 1.6349 (1.6441) teacher_loss 0.0555 (0.0958) loss_zs_kd 0.0883 (0.0844) loss_oracle 0.6415 (0.6422) kd_loss 1.2145 (1.1851) acc 96.8750 (97.6042) gate/entropy 1.0509 (1.0476) gate/usage_max 0.4686 (0.4747) gate/usage_min 0.2181 (0.2160) gate/usage_std 0.1032 (0.1070) teacher/entropy 0.0385 (0.0575) teacher/usage_max 0.5568 (0.5004) teacher/usage_min 0.0896 (0.1501) teacher/usage_std 0.1913 (0.1489) nleep/row_max_mean 1508.2653 (1507.6891) nleep/row_max_std 60.9082 (62.1694) nleep/row_min_mean 1478.0890 (1478.5496) lr 1.7705e-03 eta 0:10:08
epoch [13/50] batch [140/160] time 0.110 (0.103) data 0.000 (0.002) loss 1.5437 (1.6408) teacher_loss 0.0308 (0.0962) loss_zs_kd 0.0650 (0.0832) loss_oracle 0.6103 (0.6347) kd_loss 1.1752 (1.1857) acc 100.0000 (97.7679) gate/entropy 1.0520 (1.0482) gate/usage_max 0.4667 (0.4737) gate/usage_min 0.2188 (0.2163) gate/usage_std 0.1021 (0.1064) teacher/entropy 0.0443 (0.0598) teacher/usage_max 0.6150 (0.5020) teacher/usage_min 0.1025 (0.1447) teacher/usage_std 0.2123 (0.1523) nleep/row_max_mean 1518.8777 (1507.6728) nleep/row_max_std 49.4608 (62.6906) nleep/row_min_mean 1488.6238 (1478.5576) lr 1.7705e-03 eta 0:10:09
epoch [13/50] batch [160/160] time 0.160 (0.105) data 0.000 (0.002) loss 1.5889 (1.6410) teacher_loss 0.0747 (0.0948) loss_zs_kd 0.0876 (0.0814) loss_oracle 0.5285 (0.6352) kd_loss 1.2061 (1.1879) acc 100.0000 (97.7539) gate/entropy 1.0531 (1.0487) gate/usage_max 0.4648 (0.4727) gate/usage_min 0.2198 (0.2167) gate/usage_std 0.1008 (0.1058) teacher/entropy 0.0532 (0.0610) teacher/usage_max 0.4359 (0.5016) teacher/usage_min 0.1331 (0.1404) teacher/usage_std 0.1416 (0.1541) nleep/row_max_mean 1496.2007 (1507.3086) nleep/row_max_std 72.5995 (63.3873) nleep/row_min_mean 1471.4155 (1478.2805) lr 1.7290e-03 eta 0:10:22
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,842
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 84.4%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,924
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 87.9%
******* Domain p best val acc:      83.5%, epoch: 12 *******
******* Domain p best val test acc: 86.0%, epoch: 12 *******
******* Domain p best test acc:     88.0%, epoch: 5 *******
epoch [14/50] batch [20/160] time 0.103 (0.115) data 0.000 (0.015) loss 1.5838 (1.6869) teacher_loss 0.0245 (0.0831) loss_zs_kd 0.0928 (0.0922) loss_oracle 0.6215 (0.6896) kd_loss 1.2021 (1.2129) acc 100.0000 (97.8125) gate/entropy 1.0541 (1.0536) gate/usage_max 0.4628 (0.4638) gate/usage_min 0.2207 (0.2203) gate/usage_std 0.0996 (0.1002) teacher/entropy 0.0361 (0.0504) teacher/usage_max 0.6217 (0.5160) teacher/usage_min 0.0666 (0.1040) teacher/usage_std 0.2272 (0.1739) nleep/row_max_mean 1509.4783 (1511.0141) nleep/row_max_std 69.2133 (59.7886) nleep/row_min_mean 1482.0740 (1481.6242) lr 1.7290e-03 eta 0:11:15
epoch [14/50] batch [40/160] time 0.096 (0.105) data 0.000 (0.008) loss 1.7292 (1.6836) teacher_loss 0.1632 (0.0986) loss_zs_kd 0.0831 (0.0832) loss_oracle 0.6395 (0.6670) kd_loss 1.2047 (1.2099) acc 96.8750 (97.1094) gate/entropy 1.0555 (1.0542) gate/usage_max 0.4604 (0.4627) gate/usage_min 0.2220 (0.2208) gate/usage_std 0.0979 (0.0995) teacher/entropy 0.0512 (0.0561) teacher/usage_max 0.4997 (0.5231) teacher/usage_min 0.0969 (0.1015) teacher/usage_std 0.1717 (0.1786) nleep/row_max_mean 1504.6096 (1509.6583) nleep/row_max_std 79.9996 (60.9280) nleep/row_min_mean 1474.7554 (1480.5940) lr 1.7290e-03 eta 0:10:14
epoch [14/50] batch [60/160] time 0.097 (0.103) data 0.002 (0.005) loss 1.6373 (1.6737) teacher_loss 0.0169 (0.0962) loss_zs_kd 0.0843 (0.0822) loss_oracle 0.7028 (0.6602) kd_loss 1.2268 (1.2063) acc 100.0000 (97.1875) gate/entropy 1.0569 (1.0548) gate/usage_max 0.4579 (0.4616) gate/usage_min 0.2232 (0.2214) gate/usage_std 0.0963 (0.0987) teacher/entropy 0.0239 (0.0580) teacher/usage_max 0.6154 (0.5268) teacher/usage_min 0.0410 (0.0984) teacher/usage_std 0.2346 (0.1819) nleep/row_max_mean 1518.2103 (1508.6022) nleep/row_max_std 46.6148 (63.5041) nleep/row_min_mean 1485.8119 (1479.8261) lr 1.7290e-03 eta 0:10:01
epoch [14/50] batch [80/160] time 0.101 (0.102) data 0.000 (0.004) loss 1.8765 (1.6876) teacher_loss 0.1331 (0.1043) loss_zs_kd 0.0871 (0.0826) loss_oracle 0.7819 (0.6583) kd_loss 1.3090 (1.2129) acc 96.8750 (96.9922) gate/entropy 1.0581 (1.0555) gate/usage_max 0.4556 (0.4604) gate/usage_min 0.2245 (0.2219) gate/usage_std 0.0948 (0.0980) teacher/entropy 0.0238 (0.0547) teacher/usage_max 0.5444 (0.5266) teacher/usage_min 0.0004 (0.0900) teacher/usage_std 0.2382 (0.1864) nleep/row_max_mean 1517.4624 (1509.8565) nleep/row_max_std 34.6515 (61.4900) nleep/row_min_mean 1485.7954 (1481.0209) lr 1.7290e-03 eta 0:09:57
epoch [14/50] batch [100/160] time 0.091 (0.102) data 0.000 (0.003) loss 1.7333 (1.6768) teacher_loss 0.0837 (0.0968) loss_zs_kd 0.0872 (0.0821) loss_oracle 0.7607 (0.6532) kd_loss 1.2257 (1.2124) acc 96.8750 (97.2500) gate/entropy 1.0593 (1.0561) gate/usage_max 0.4532 (0.4592) gate/usage_min 0.2255 (0.2225) gate/usage_std 0.0933 (0.0972) teacher/entropy 0.0272 (0.0542) teacher/usage_max 0.4700 (0.5256) teacher/usage_min 0.1428 (0.0879) teacher/usage_std 0.1389 (0.1871) nleep/row_max_mean 1520.0887 (1510.1039) nleep/row_max_std 46.2053 (61.3625) nleep/row_min_mean 1487.5725 (1481.2517) lr 1.7290e-03 eta 0:09:52
epoch [14/50] batch [120/160] time 0.093 (0.101) data 0.000 (0.003) loss 1.6007 (1.6814) teacher_loss 0.0356 (0.0988) loss_zs_kd 0.0792 (0.0821) loss_oracle 0.6107 (0.6589) kd_loss 1.2202 (1.2121) acc 100.0000 (97.1875) gate/entropy 1.0607 (1.0568) gate/usage_max 0.4505 (0.4579) gate/usage_min 0.2268 (0.2232) gate/usage_std 0.0916 (0.0964) teacher/entropy 0.0318 (0.0536) teacher/usage_max 0.4999 (0.5273) teacher/usage_min 0.0807 (0.0871) teacher/usage_std 0.1816 (0.1884) nleep/row_max_mean 1529.1582 (1510.1963) nleep/row_max_std 32.4499 (61.8194) nleep/row_min_mean 1499.2094 (1481.3136) lr 1.7290e-03 eta 0:09:46
epoch [14/50] batch [140/160] time 0.102 (0.101) data 0.000 (0.002) loss 1.6884 (1.6833) teacher_loss 0.1268 (0.1006) loss_zs_kd 0.0627 (0.0817) loss_oracle 0.6614 (0.6596) kd_loss 1.1995 (1.2121) acc 96.8750 (97.1652) gate/entropy 1.0621 (1.0575) gate/usage_max 0.4478 (0.4567) gate/usage_min 0.2284 (0.2238) gate/usage_std 0.0898 (0.0956) teacher/entropy 0.0639 (0.0538) teacher/usage_max 0.4682 (0.5292) teacher/usage_min 0.0869 (0.0836) teacher/usage_std 0.1745 (0.1906) nleep/row_max_mean 1497.0237 (1510.2998) nleep/row_max_std 85.5483 (61.9824) nleep/row_min_mean 1468.6860 (1481.3196) lr 1.7290e-03 eta 0:09:46
epoch [14/50] batch [160/160] time 0.082 (0.104) data 0.000 (0.002) loss 1.6449 (1.6855) teacher_loss 0.0388 (0.1020) loss_zs_kd 0.0663 (0.0807) loss_oracle 0.7203 (0.6611) kd_loss 1.2128 (1.2127) acc 100.0000 (97.1875) gate/entropy 1.0637 (1.0581) gate/usage_max 0.4450 (0.4554) gate/usage_min 0.2303 (0.2245) gate/usage_std 0.0879 (0.0947) teacher/entropy 0.0285 (0.0528) teacher/usage_max 0.5200 (0.5276) teacher/usage_min 0.0737 (0.0812) teacher/usage_std 0.1893 (0.1914) nleep/row_max_mean 1515.1587 (1510.5177) nleep/row_max_std 74.2559 (61.8462) nleep/row_min_mean 1486.2670 (1481.6052) lr 1.6845e-03 eta 0:10:00
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,828
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 84.6%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,957
* accuracy: 87.6%
* error: 12.4%
* macro_f1: 88.7%
******* Domain p best val acc:      83.5%, epoch: 12 *******
******* Domain p best val test acc: 86.0%, epoch: 12 *******
******* Domain p best test acc:     88.0%, epoch: 5 *******
epoch [15/50] batch [20/160] time 0.095 (0.123) data 0.000 (0.018) loss 1.6498 (1.7076) teacher_loss 0.0337 (0.1129) loss_zs_kd 0.0824 (0.0702) loss_oracle 0.7103 (0.6602) kd_loss 1.2197 (1.2295) acc 100.0000 (96.7188) gate/entropy 1.0650 (1.0643) gate/usage_max 0.4424 (0.4438) gate/usage_min 0.2317 (0.2309) gate/usage_std 0.0862 (0.0871) teacher/entropy 0.0284 (0.0362) teacher/usage_max 0.5135 (0.5312) teacher/usage_min 0.0611 (0.0467) teacher/usage_std 0.1958 (0.2088) nleep/row_max_mean 1529.5422 (1514.3158) nleep/row_max_std 27.8363 (55.0533) nleep/row_min_mean 1498.1960 (1485.3655) lr 1.6845e-03 eta 0:11:44
epoch [15/50] batch [40/160] time 0.103 (0.113) data 0.000 (0.009) loss 1.6980 (1.7085) teacher_loss 0.1481 (0.1154) loss_zs_kd 0.0688 (0.0736) loss_oracle 0.5581 (0.6611) kd_loss 1.2365 (1.2257) acc 96.8750 (96.8750) gate/entropy 1.0664 (1.0650) gate/usage_max 0.4399 (0.4424) gate/usage_min 0.2336 (0.2318) gate/usage_std 0.0843 (0.0861) teacher/entropy 0.0455 (0.0428) teacher/usage_max 0.5115 (0.5418) teacher/usage_min 0.0293 (0.0472) teacher/usage_std 0.2160 (0.2115) nleep/row_max_mean 1506.7732 (1512.4281) nleep/row_max_std 56.6087 (58.0884) nleep/row_min_mean 1479.8425 (1483.5976) lr 1.6845e-03 eta 0:10:45
epoch [15/50] batch [60/160] time 0.127 (0.113) data 0.000 (0.006) loss 1.7201 (1.6927) teacher_loss 0.1806 (0.1068) loss_zs_kd 0.1000 (0.0748) loss_oracle 0.5832 (0.6439) kd_loss 1.1979 (1.2265) acc 93.7500 (97.3438) gate/entropy 1.0678 (1.0657) gate/usage_max 0.4371 (0.4410) gate/usage_min 0.2354 (0.2327) gate/usage_std 0.0825 (0.0852) teacher/entropy 0.0384 (0.0422) teacher/usage_max 0.5434 (0.5433) teacher/usage_min 0.0500 (0.0449) teacher/usage_std 0.2080 (0.2141) nleep/row_max_mean 1503.1180 (1512.2045) nleep/row_max_std 55.0048 (58.0681) nleep/row_min_mean 1473.3861 (1483.3451) lr 1.6845e-03 eta 0:10:45
epoch [15/50] batch [80/160] time 0.105 (0.113) data 0.000 (0.005) loss 1.5094 (1.6853) teacher_loss 0.0659 (0.1094) loss_zs_kd 0.0774 (0.0747) loss_oracle 0.5333 (0.6274) kd_loss 1.1382 (1.2249) acc 100.0000 (97.2266) gate/entropy 1.0691 (1.0664) gate/usage_max 0.4344 (0.4397) gate/usage_min 0.2371 (0.2336) gate/usage_std 0.0806 (0.0843) teacher/entropy 0.0684 (0.0401) teacher/usage_max 0.6375 (0.5413) teacher/usage_min 0.0419 (0.0446) teacher/usage_std 0.2433 (0.2136) nleep/row_max_mean 1490.7983 (1511.0838) nleep/row_max_std 82.6574 (59.0288) nleep/row_min_mean 1462.9927 (1482.5469) lr 1.6845e-03 eta 0:10:39
epoch [15/50] batch [100/160] time 0.093 (0.110) data 0.000 (0.004) loss 1.6107 (1.6802) teacher_loss 0.0634 (0.1054) loss_zs_kd 0.0741 (0.0747) loss_oracle 0.6053 (0.6300) kd_loss 1.2076 (1.2224) acc 96.8750 (97.2500) gate/entropy 1.0704 (1.0671) gate/usage_max 0.4317 (0.4384) gate/usage_min 0.2388 (0.2345) gate/usage_std 0.0788 (0.0834) teacher/entropy 0.0333 (0.0392) teacher/usage_max 0.4785 (0.5432) teacher/usage_min 0.0862 (0.0436) teacher/usage_std 0.1756 (0.2150) nleep/row_max_mean 1508.7146 (1510.7890) nleep/row_max_std 69.5634 (60.0052) nleep/row_min_mean 1480.2544 (1482.3344) lr 1.6845e-03 eta 0:10:25
epoch [15/50] batch [120/160] time 0.107 (0.109) data 0.000 (0.003) loss 1.6368 (1.6773) teacher_loss 0.1029 (0.1067) loss_zs_kd 0.0709 (0.0754) loss_oracle 0.6201 (0.6305) kd_loss 1.1885 (1.2176) acc 93.7500 (97.2396) gate/entropy 1.0719 (1.0678) gate/usage_max 0.4285 (0.4370) gate/usage_min 0.2407 (0.2353) gate/usage_std 0.0767 (0.0825) teacher/entropy 0.0205 (0.0393) teacher/usage_max 0.5579 (0.5434) teacher/usage_min 0.0663 (0.0449) teacher/usage_std 0.2029 (0.2142) nleep/row_max_mean 1500.8278 (1510.0740) nleep/row_max_std 78.1139 (61.0185) nleep/row_min_mean 1471.9951 (1481.6382) lr 1.6845e-03 eta 0:10:13
epoch [15/50] batch [140/160] time 0.141 (0.109) data 0.000 (0.003) loss 1.5714 (1.6775) teacher_loss 0.0556 (0.1044) loss_zs_kd 0.0911 (0.0752) loss_oracle 0.5475 (0.6343) kd_loss 1.1965 (1.2183) acc 100.0000 (97.2991) gate/entropy 1.0730 (1.0684) gate/usage_max 0.4260 (0.4356) gate/usage_min 0.2422 (0.2362) gate/usage_std 0.0750 (0.0815) teacher/entropy 0.0410 (0.0375) teacher/usage_max 0.5255 (0.5447) teacher/usage_min 0.0272 (0.0420) teacher/usage_std 0.2188 (0.2160) nleep/row_max_mean 1503.3676 (1510.4385) nleep/row_max_std 73.3589 (61.1468) nleep/row_min_mean 1476.8799 (1481.8380) lr 1.6845e-03 eta 0:10:15
epoch [15/50] batch [160/160] time 0.155 (0.110) data 0.000 (0.002) loss 1.7206 (1.6826) teacher_loss 0.1295 (0.1064) loss_zs_kd 0.0761 (0.0762) loss_oracle 0.7222 (0.6396) kd_loss 1.1920 (1.2183) acc 96.8750 (97.2266) gate/entropy 1.0745 (1.0691) gate/usage_max 0.4229 (0.4342) gate/usage_min 0.2446 (0.2371) gate/usage_std 0.0728 (0.0806) teacher/entropy 0.0290 (0.0362) teacher/usage_max 0.5052 (0.5466) teacher/usage_min 0.0585 (0.0410) teacher/usage_std 0.1964 (0.2170) nleep/row_max_mean 1504.5090 (1509.9144) nleep/row_max_std 85.9206 (62.4157) nleep/row_min_mean 1476.1327 (1481.2640) lr 1.6374e-03 eta 0:10:15
Evaluate on the *val* set
=> result
* total: 2,206
* correct: 1,829
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 84.5%
Evaluate on the *test* set
=> result
* total: 3,376
* correct: 2,995
* accuracy: 88.7%
* error: 11.3%
* macro_f1: 89.6%
Checkpoint saved to icml/multi-dg/tuning/tuning2/TRIP/vlcs/b32_ep50/ViT-B16/p/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain p best val acc:      83.5%, epoch: 12 *******
******* Domain p best val test acc: 86.0%, epoch: 12 *******
******* Domain p best test acc:     88.7%, epoch: 15 *******
epoch [16/50] batch [20/160] time 0.095 (0.111) data 0.000 (0.013) loss 1.5414 (1.7081) teacher_loss 0.1100 (0.0922) loss_zs_kd 0.0628 (0.0805) loss_oracle 0.5455 (0.7105) kd_loss 1.1273 (1.2204) acc 96.8750 (97.3438) gate/entropy 1.0760 (1.0752) gate/usage_max 0.4196 (0.4213) gate/usage_min 0.2469 (0.2457) gate/usage_std 0.0705 (0.0717) teacher/entropy 0.0659 (0.0202) teacher/usage_max 0.5720 (0.5490) teacher/usage_min 0.0627 (0.0217) teacher/usage_std 0.2092 (0.2302) nleep/row_max_mean 1489.7307 (1513.0228) nleep/row_max_std 96.1396 (61.1968) nleep/row_min_mean 1461.0156 (1482.5528) lr 1.6374e-03 eta 0:10:16
epoch [16/50] batch [40/160] time 0.105 (0.100) data 0.000 (0.006) loss 1.6063 (1.6797) teacher_loss 0.0917 (0.0893) loss_zs_kd 0.0453 (0.0777) loss_oracle 0.6379 (0.6842) kd_loss 1.1729 (1.2094) acc 96.8750 (97.1875) gate/entropy 1.0773 (1.0759) gate/usage_max 0.4164 (0.4197) gate/usage_min 0.2490 (0.2468) gate/usage_std 0.0684 (0.0706) teacher/entropy 0.0361 (0.0264) teacher/usage_max 0.6117 (0.5585) teacher/usage_min 0.0004 (0.0243) teacher/usage_std 0.2525 (0.2304) nleep/row_max_mean 1517.6116 (1510.9002) nleep/row_max_std 54.6151 (63.8764) nleep/row_min_mean 1486.4143 (1480.3388) lr 1.6374e-03 eta 0:09:13
epoch [16/50] batch [60/160] time 0.101 (0.101) data 0.001 (0.004) loss 1.6185 (1.6856) teacher_loss 0.0337 (0.0952) loss_zs_kd 0.0625 (0.0777) loss_oracle 0.6667 (0.6868) kd_loss 1.2202 (1.2081) acc 100.0000 (97.2917) gate/entropy 1.0787 (1.0766) gate/usage_max 0.4135 (0.4181) gate/usage_min 0.2514 (0.2479) gate/usage_std 0.0662 (0.0695) teacher/entropy 0.0068 (0.0283) teacher/usage_max 0.5298 (0.5621) teacher/usage_min 0.0014 (0.0240) teacher/usage_std 0.2360 (0.2307) nleep/row_max_mean 1517.5383 (1510.2017) nleep/row_max_std 53.6182 (63.6559) nleep/row_min_mean 1485.0334 (1479.3438) lr 1.6374e-03 eta 0:09:19
epoch [16/50] batch [80/160] time 0.106 (0.101) data 0.001 (0.003) loss 1.8072 (1.6774) teacher_loss 0.1736 (0.0995) loss_zs_kd 0.0558 (0.0761) loss_oracle 0.7367 (0.6715) kd_loss 1.2374 (1.2041) acc 93.7500 (97.2266) gate/entropy 1.0798 (1.0773) gate/usage_max 0.4108 (0.4166) gate/usage_min 0.2536 (0.2491) gate/usage_std 0.0642 (0.0684) teacher/entropy 0.0328 (0.0289) teacher/usage_max 0.6720 (0.5662) teacher/usage_min 0.0468 (0.0240) teacher/usage_std 0.2579 (0.2319) nleep/row_max_mean 1520.3640 (1510.9570) nleep/row_max_std 77.9398 (61.9988) nleep/row_min_mean 1484.4709 (1480.0128) lr 1.6374e-03 eta 0:09:19
epoch [16/50] batch [100/160] time 0.098 (0.102) data 0.000 (0.003) loss 1.6281 (1.6767) teacher_loss 0.0444 (0.1033) loss_zs_kd 0.0769 (0.0777) loss_oracle 0.6021 (0.6636) kd_loss 1.2443 (1.2027) acc 100.0000 (97.0938) gate/entropy 1.0809 (1.0779) gate/usage_max 0.4080 (0.4151) gate/usage_min 0.2556 (0.2502) gate/usage_std 0.0622 (0.0673) teacher/entropy 0.0075 (0.0275) teacher/usage_max 0.5921 (0.5648) teacher/usage_min 0.0001 (0.0231) teacher/usage_std 0.2473 (0.2318) nleep/row_max_mean 1516.3728 (1510.7208) nleep/row_max_std 43.7494 (61.6157) nleep/row_min_mean 1485.4170 (1479.8949) lr 1.6374e-03 eta 0:09:20
