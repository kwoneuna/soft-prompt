Loading trainer: TRIP
Loading dataset: SPG_TerraIncognita
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------------------------------------------
Dataset    SPG_TerraIncognita
Source     ['location_100', 'location_43', 'location_46']
Target     ['location_38']
# classes  10
# train_x  10,216
# val      4,378
# test     9,736
---------  ----------------------------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
alpha_logit                                        1
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,196
[Info] Hyperparameters saved to: icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/hyperparameters.json
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/319] time 0.082 (0.134) data 0.000 (0.031) loss 2.2249 (2.9654) teacher_loss 1.2525 (2.0157) loss_zs_kd 0.0003 (0.0001) loss_oracle 0.0132 (0.0052) acc 59.3750 (37.0312) lr 1.0000e-05 eta 0:35:39
epoch [1/50] batch [40/319] time 0.071 (0.106) data 0.000 (0.016) loss 2.4883 (2.8466) teacher_loss 1.5742 (1.9122) loss_zs_kd 0.0016 (0.0003) loss_oracle 0.0116 (0.0110) acc 56.2500 (39.6875) lr 1.0000e-05 eta 0:28:09
epoch [1/50] batch [60/319] time 0.082 (0.095) data 0.000 (0.010) loss 3.1319 (2.8822) teacher_loss 2.1706 (1.9405) loss_zs_kd 0.0027 (0.0008) loss_oracle 0.0230 (0.0151) acc 31.2500 (39.1667) lr 1.0000e-05 eta 0:25:05
epoch [1/50] batch [80/319] time 0.084 (0.090) data 0.000 (0.008) loss 2.9412 (2.8666) teacher_loss 1.9689 (1.9216) loss_zs_kd 0.0041 (0.0015) loss_oracle 0.0998 (0.0243) acc 43.7500 (38.8672) lr 1.0000e-05 eta 0:23:55
epoch [1/50] batch [100/319] time 0.086 (0.088) data 0.000 (0.006) loss 3.2551 (2.8354) teacher_loss 2.1719 (1.8828) loss_zs_kd 0.0079 (0.0025) loss_oracle 0.1561 (0.0440) acc 25.0000 (39.5000) lr 1.0000e-05 eta 0:23:18
epoch [1/50] batch [120/319] time 0.099 (0.087) data 0.000 (0.005) loss 2.7716 (2.8375) teacher_loss 1.8670 (1.8808) loss_zs_kd 0.0065 (0.0035) loss_oracle 0.0683 (0.0556) acc 40.6250 (39.6354) lr 1.0000e-05 eta 0:23:03
epoch [1/50] batch [140/319] time 0.067 (0.087) data 0.000 (0.005) loss 2.8817 (2.8350) teacher_loss 2.0516 (1.8807) loss_zs_kd 0.0103 (0.0045) loss_oracle 0.0213 (0.0554) acc 37.5000 (39.5536) lr 1.0000e-05 eta 0:22:53
epoch [1/50] batch [160/319] time 0.078 (0.086) data 0.000 (0.004) loss 2.7508 (2.8275) teacher_loss 1.8230 (1.8767) loss_zs_kd 0.0140 (0.0058) loss_oracle 0.0385 (0.0522) acc 34.3750 (39.6875) lr 1.0000e-05 eta 0:22:35
epoch [1/50] batch [180/319] time 0.083 (0.086) data 0.000 (0.004) loss 3.0572 (2.8226) teacher_loss 2.1707 (1.8733) loss_zs_kd 0.0239 (0.0073) loss_oracle 0.0677 (0.0523) acc 28.1250 (39.6354) lr 1.0000e-05 eta 0:22:29
epoch [1/50] batch [200/319] time 0.088 (0.085) data 0.000 (0.003) loss 3.0457 (2.8204) teacher_loss 1.9916 (1.8686) loss_zs_kd 0.0253 (0.0089) loss_oracle 0.1543 (0.0573) acc 31.2500 (39.6250) lr 1.0000e-05 eta 0:22:22
epoch [1/50] batch [220/319] time 0.084 (0.085) data 0.000 (0.003) loss 2.7187 (2.8305) teacher_loss 1.7086 (1.8743) loss_zs_kd 0.0324 (0.0108) loss_oracle 0.1291 (0.0653) acc 43.7500 (39.2188) lr 1.0000e-05 eta 0:22:12
epoch [1/50] batch [240/319] time 0.075 (0.086) data 0.000 (0.003) loss 2.9037 (2.8428) teacher_loss 1.9302 (1.8808) loss_zs_kd 0.0450 (0.0128) loss_oracle 0.1509 (0.0729) acc 28.1250 (38.9323) lr 1.0000e-05 eta 0:22:23
epoch [1/50] batch [260/319] time 0.076 (0.085) data 0.000 (0.003) loss 2.9862 (2.8433) teacher_loss 1.9825 (1.8779) loss_zs_kd 0.0413 (0.0148) loss_oracle 0.2138 (0.0797) acc 40.6250 (38.8942) lr 1.0000e-05 eta 0:22:17
epoch [1/50] batch [280/319] time 0.077 (0.085) data 0.000 (0.002) loss 3.1946 (2.8452) teacher_loss 2.0931 (1.8740) loss_zs_kd 0.0603 (0.0172) loss_oracle 0.2860 (0.0910) acc 31.2500 (38.9621) lr 1.0000e-05 eta 0:22:07
epoch [1/50] batch [300/319] time 0.090 (0.084) data 0.000 (0.002) loss 3.2723 (2.8451) teacher_loss 2.2809 (1.8698) loss_zs_kd 0.0591 (0.0197) loss_oracle 0.1741 (0.0996) acc 31.2500 (38.9583) lr 1.0000e-05 eta 0:22:00
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 1,777
* accuracy: 40.6%
* error: 59.4%
* macro_f1: 26.2%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 3,228
* accuracy: 33.2%
* error: 66.8%
* macro_f1: 14.2%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      40.6%, epoch: 1 *******
******* Domain 2 best val test acc: 33.2%, epoch: 1 *******
******* Domain 2 best test acc:     33.2%, epoch: 1 *******
epoch [2/50] batch [20/319] time 0.076 (0.127) data 0.000 (0.032) loss 3.1882 (3.0480) teacher_loss 1.6593 (1.6238) loss_zs_kd 0.4238 (0.3261) loss_oracle 1.0564 (0.8872) acc 37.5000 (39.5312) lr 2.0000e-03 eta 0:32:55
epoch [2/50] batch [40/319] time 0.079 (0.103) data 0.000 (0.016) loss 3.5733 (3.0611) teacher_loss 2.0280 (1.5656) loss_zs_kd 0.7690 (0.3992) loss_oracle 1.0666 (0.9794) acc 25.0000 (42.8906) lr 2.0000e-03 eta 0:26:51
epoch [2/50] batch [60/319] time 0.079 (0.094) data 0.000 (0.011) loss 2.8875 (3.0419) teacher_loss 1.3197 (1.5244) loss_zs_kd 0.4551 (0.4767) loss_oracle 1.0489 (1.0050) acc 56.2500 (43.6458) lr 2.0000e-03 eta 0:24:31
epoch [2/50] batch [80/319] time 0.078 (0.092) data 0.000 (0.008) loss 3.2235 (3.0355) teacher_loss 1.6061 (1.5059) loss_zs_kd 0.5142 (0.4744) loss_oracle 1.0335 (1.0125) acc 40.6250 (44.1406) lr 2.0000e-03 eta 0:23:49
epoch [2/50] batch [100/319] time 0.080 (0.089) data 0.000 (0.007) loss 3.0291 (3.0293) teacher_loss 1.4793 (1.4899) loss_zs_kd 0.9274 (0.5194) loss_oracle 0.9991 (1.0111) acc 50.0000 (44.4375) lr 2.0000e-03 eta 0:23:06
epoch [2/50] batch [120/319] time 0.087 (0.088) data 0.000 (0.005) loss 2.5148 (3.0014) teacher_loss 0.9728 (1.4638) loss_zs_kd 0.6428 (0.6101) loss_oracle 0.9748 (1.0104) acc 65.6250 (45.2865) lr 2.0000e-03 eta 0:22:38
epoch [2/50] batch [140/319] time 0.087 (0.087) data 0.000 (0.005) loss 2.8989 (2.9780) teacher_loss 1.3822 (1.4430) loss_zs_kd 0.5274 (0.5886) loss_oracle 1.0125 (1.0095) acc 46.8750 (46.3170) lr 2.0000e-03 eta 0:22:26
epoch [2/50] batch [160/319] time 0.086 (0.087) data 0.000 (0.004) loss 2.8211 (2.9445) teacher_loss 1.2763 (1.4129) loss_zs_kd 0.6229 (0.5914) loss_oracle 1.0140 (1.0067) acc 62.5000 (47.5000) lr 2.0000e-03 eta 0:22:21
epoch [2/50] batch [180/319] time 0.086 (0.086) data 0.000 (0.004) loss 3.1430 (2.9314) teacher_loss 1.5882 (1.4075) loss_zs_kd 0.3751 (0.5731) loss_oracle 1.0084 (1.0028) acc 43.7500 (47.8819) lr 2.0000e-03 eta 0:22:14
epoch [2/50] batch [200/319] time 0.069 (0.085) data 0.000 (0.003) loss 3.1197 (2.9439) teacher_loss 1.5075 (1.4152) loss_zs_kd 0.5396 (0.5672) loss_oracle 0.9470 (1.0014) acc 43.7500 (47.9531) lr 2.0000e-03 eta 0:21:58
epoch [2/50] batch [220/319] time 0.061 (0.084) data 0.000 (0.003) loss 3.3777 (2.9495) teacher_loss 1.7609 (1.4158) loss_zs_kd 0.4477 (0.5716) loss_oracle 0.9607 (1.0006) acc 37.5000 (47.8551) lr 2.0000e-03 eta 0:21:33
epoch [2/50] batch [240/319] time 0.083 (0.084) data 0.000 (0.003) loss 2.7505 (2.9527) teacher_loss 1.1398 (1.4119) loss_zs_kd 0.6003 (0.5720) loss_oracle 1.0041 (1.0009) acc 50.0000 (48.1380) lr 2.0000e-03 eta 0:21:26
epoch [2/50] batch [260/319] time 0.077 (0.084) data 0.000 (0.003) loss 2.8328 (2.9425) teacher_loss 1.2558 (1.3977) loss_zs_kd 0.6146 (0.5829) loss_oracle 0.9389 (0.9998) acc 46.8750 (48.5577) lr 2.0000e-03 eta 0:21:24
epoch [2/50] batch [280/319] time 0.069 (0.083) data 0.000 (0.003) loss 2.8988 (2.9446) teacher_loss 1.2935 (1.3969) loss_zs_kd 0.5454 (0.5850) loss_oracle 1.0334 (0.9991) acc 37.5000 (48.3705) lr 2.0000e-03 eta 0:21:18
epoch [2/50] batch [300/319] time 0.075 (0.083) data 0.000 (0.002) loss 2.9425 (2.9482) teacher_loss 1.3620 (1.3961) loss_zs_kd 0.8443 (0.6048) loss_oracle 1.0273 (1.0004) acc 59.3750 (48.3958) lr 2.0000e-03 eta 0:21:14
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,037
* accuracy: 46.5%
* error: 53.5%
* macro_f1: 37.5%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 3,588
* accuracy: 36.9%
* error: 63.1%
* macro_f1: 22.0%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      46.5%, epoch: 2 *******
******* Domain 2 best val test acc: 36.9%, epoch: 2 *******
******* Domain 2 best test acc:     36.9%, epoch: 2 *******
epoch [3/50] batch [20/319] time 0.064 (0.100) data 0.000 (0.026) loss 2.6947 (2.8208) teacher_loss 1.1051 (1.2231) loss_zs_kd 0.4081 (0.6637) loss_oracle 1.0420 (1.0313) acc 50.0000 (52.5000) lr 1.9980e-03 eta 0:25:24
epoch [3/50] batch [40/319] time 0.082 (0.087) data 0.000 (0.013) loss 2.8693 (2.8389) teacher_loss 1.2910 (1.2606) loss_zs_kd 0.6623 (0.6139) loss_oracle 1.0252 (1.0330) acc 46.8750 (50.8594) lr 1.9980e-03 eta 0:22:06
epoch [3/50] batch [60/319] time 0.066 (0.083) data 0.001 (0.009) loss 2.7119 (2.8490) teacher_loss 1.0989 (1.2732) loss_zs_kd 0.8470 (0.6467) loss_oracle 1.0749 (1.0278) acc 62.5000 (50.3646) lr 1.9980e-03 eta 0:21:04
epoch [3/50] batch [80/319] time 0.067 (0.080) data 0.000 (0.007) loss 2.6631 (2.8564) teacher_loss 1.0924 (1.2780) loss_zs_kd 0.8066 (0.7338) loss_oracle 1.0517 (1.0293) acc 56.2500 (49.7266) lr 1.9980e-03 eta 0:20:13
epoch [3/50] batch [100/319] time 0.065 (0.077) data 0.000 (0.005) loss 2.9095 (2.8518) teacher_loss 1.3330 (1.2743) loss_zs_kd 1.2859 (0.8122) loss_oracle 1.0074 (1.0312) acc 56.2500 (50.4062) lr 1.9980e-03 eta 0:19:34
epoch [3/50] batch [120/319] time 0.079 (0.077) data 0.000 (0.005) loss 2.9396 (2.8448) teacher_loss 1.4491 (1.2742) loss_zs_kd 0.7095 (0.8025) loss_oracle 1.0434 (1.0293) acc 43.7500 (50.6510) lr 1.9980e-03 eta 0:19:28
epoch [3/50] batch [140/319] time 0.083 (0.077) data 0.000 (0.004) loss 2.7336 (2.8447) teacher_loss 1.1495 (1.2799) loss_zs_kd 0.7620 (0.8022) loss_oracle 1.0648 (1.0300) acc 62.5000 (50.7143) lr 1.9980e-03 eta 0:19:34
epoch [3/50] batch [160/319] time 0.078 (0.078) data 0.000 (0.004) loss 2.6781 (2.8486) teacher_loss 1.1692 (1.2853) loss_zs_kd 0.6139 (0.7931) loss_oracle 1.0448 (1.0332) acc 65.6250 (50.8594) lr 1.9980e-03 eta 0:19:34
epoch [3/50] batch [180/319] time 0.074 (0.078) data 0.000 (0.003) loss 2.7240 (2.8420) teacher_loss 1.2164 (1.2816) loss_zs_kd 0.6053 (0.7866) loss_oracle 0.9475 (1.0320) acc 56.2500 (51.0069) lr 1.9980e-03 eta 0:19:36
epoch [3/50] batch [200/319] time 0.139 (0.079) data 0.001 (0.003) loss 3.0232 (2.8383) teacher_loss 1.4144 (1.2802) loss_zs_kd 0.8613 (0.7780) loss_oracle 1.0491 (1.0294) acc 53.1250 (51.0312) lr 1.9980e-03 eta 0:20:00
epoch [3/50] batch [220/319] time 0.082 (0.080) data 0.000 (0.003) loss 2.7241 (2.8338) teacher_loss 1.2183 (1.2799) loss_zs_kd 0.6267 (0.7722) loss_oracle 0.9895 (1.0248) acc 53.1250 (50.9943) lr 1.9980e-03 eta 0:20:08
epoch [3/50] batch [240/319] time 0.085 (0.080) data 0.000 (0.002) loss 3.1594 (2.8291) teacher_loss 1.6257 (1.2765) loss_zs_kd 0.8939 (0.7710) loss_oracle 0.9860 (1.0228) acc 46.8750 (51.2500) lr 1.9980e-03 eta 0:20:08
epoch [3/50] batch [260/319] time 0.077 (0.080) data 0.000 (0.002) loss 2.6553 (2.8265) teacher_loss 1.2006 (1.2763) loss_zs_kd 0.7366 (0.7657) loss_oracle 0.9189 (1.0197) acc 43.7500 (51.1418) lr 1.9980e-03 eta 0:20:04
epoch [3/50] batch [280/319] time 0.079 (0.080) data 0.000 (0.002) loss 2.6661 (2.8150) teacher_loss 1.0927 (1.2687) loss_zs_kd 1.0167 (0.7765) loss_oracle 1.0821 (1.0180) acc 62.5000 (51.3393) lr 1.9980e-03 eta 0:19:59
epoch [3/50] batch [300/319] time 0.074 (0.080) data 0.000 (0.002) loss 2.5241 (2.8008) teacher_loss 1.0402 (1.2572) loss_zs_kd 0.7503 (0.7979) loss_oracle 1.0007 (1.0185) acc 71.8750 (52.0938) lr 1.9980e-03 eta 0:19:59
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,173
* accuracy: 49.6%
* error: 50.4%
* macro_f1: 34.1%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 2,097
* accuracy: 21.5%
* error: 78.5%
* macro_f1: 16.6%
******* Domain 2 best val acc:      49.6%, epoch: 3 *******
******* Domain 2 best val test acc: 21.5%, epoch: 3 *******
******* Domain 2 best test acc:     36.9%, epoch: 2 *******
epoch [4/50] batch [20/319] time 0.081 (0.119) data 0.000 (0.037) loss 3.0597 (2.8642) teacher_loss 1.5075 (1.3062) loss_zs_kd 0.8258 (0.6875) loss_oracle 1.0800 (1.0439) acc 31.2500 (48.7500) lr 1.9921e-03 eta 0:29:37
epoch [4/50] batch [40/319] time 0.085 (0.100) data 0.000 (0.018) loss 2.7690 (2.8744) teacher_loss 1.2762 (1.3194) loss_zs_kd 0.5565 (0.6819) loss_oracle 1.0693 (1.0462) acc 53.1250 (48.9062) lr 1.9921e-03 eta 0:24:51
epoch [4/50] batch [60/319] time 0.084 (0.095) data 0.000 (0.012) loss 2.9013 (2.8218) teacher_loss 1.3834 (1.2754) loss_zs_kd 1.0936 (0.7397) loss_oracle 1.0140 (1.0381) acc 62.5000 (51.5625) lr 1.9921e-03 eta 0:23:37
epoch [4/50] batch [80/319] time 0.089 (0.092) data 0.000 (0.009) loss 2.8824 (2.8282) teacher_loss 1.3643 (1.2863) loss_zs_kd 0.9156 (0.7510) loss_oracle 0.9479 (1.0273) acc 46.8750 (51.1719) lr 1.9921e-03 eta 0:22:57
epoch [4/50] batch [100/319] time 0.067 (0.090) data 0.000 (0.008) loss 2.9547 (2.8279) teacher_loss 1.3949 (1.2809) loss_zs_kd 1.0274 (0.8130) loss_oracle 1.0162 (1.0291) acc 50.0000 (51.3125) lr 1.9921e-03 eta 0:22:18
epoch [4/50] batch [120/319] time 0.086 (0.089) data 0.001 (0.006) loss 2.8341 (2.8226) teacher_loss 1.2498 (1.2752) loss_zs_kd 1.2291 (0.8699) loss_oracle 1.0378 (1.0314) acc 56.2500 (51.4323) lr 1.9921e-03 eta 0:21:56
epoch [4/50] batch [140/319] time 0.085 (0.087) data 0.000 (0.005) loss 2.8225 (2.8185) teacher_loss 1.2257 (1.2678) loss_zs_kd 1.3961 (0.9208) loss_oracle 1.1017 (1.0336) acc 59.3750 (52.0982) lr 1.9921e-03 eta 0:21:27
epoch [4/50] batch [160/319] time 0.074 (0.086) data 0.000 (0.005) loss 2.9836 (2.8070) teacher_loss 1.4883 (1.2555) loss_zs_kd 1.3152 (0.9659) loss_oracle 1.0069 (1.0379) acc 34.3750 (52.3828) lr 1.9921e-03 eta 0:21:15
epoch [4/50] batch [180/319] time 0.086 (0.085) data 0.000 (0.004) loss 2.7324 (2.8001) teacher_loss 1.2068 (1.2476) loss_zs_kd 1.1768 (1.0096) loss_oracle 1.0596 (1.0406) acc 59.3750 (52.6389) lr 1.9921e-03 eta 0:21:04
epoch [4/50] batch [200/319] time 0.079 (0.085) data 0.000 (0.004) loss 2.5429 (2.7873) teacher_loss 0.9774 (1.2310) loss_zs_kd 1.4219 (1.0516) loss_oracle 1.0508 (1.0447) acc 59.3750 (53.3125) lr 1.9921e-03 eta 0:21:02
epoch [4/50] batch [220/319] time 0.084 (0.085) data 0.000 (0.004) loss 2.6253 (2.7745) teacher_loss 1.0760 (1.2167) loss_zs_kd 1.3437 (1.0746) loss_oracle 1.1157 (1.0499) acc 65.6250 (53.9062) lr 1.9921e-03 eta 0:20:53
epoch [4/50] batch [240/319] time 0.081 (0.085) data 0.000 (0.003) loss 2.7307 (2.7690) teacher_loss 1.2135 (1.2093) loss_zs_kd 1.3518 (1.0983) loss_oracle 1.1451 (1.0557) acc 59.3750 (54.2708) lr 1.9921e-03 eta 0:20:50
epoch [4/50] batch [260/319] time 0.097 (0.084) data 0.001 (0.003) loss 2.6966 (2.7672) teacher_loss 1.1227 (1.2065) loss_zs_kd 1.3079 (1.1112) loss_oracle 1.0653 (1.0579) acc 50.0000 (54.2788) lr 1.9921e-03 eta 0:20:44
epoch [4/50] batch [280/319] time 0.076 (0.084) data 0.000 (0.003) loss 2.5721 (2.7584) teacher_loss 1.0000 (1.1960) loss_zs_kd 1.3011 (1.1208) loss_oracle 1.0878 (1.0607) acc 53.1250 (54.8661) lr 1.9921e-03 eta 0:20:34
epoch [4/50] batch [300/319] time 0.076 (0.084) data 0.000 (0.003) loss 2.4750 (2.7482) teacher_loss 0.8681 (1.1844) loss_zs_kd 1.3312 (1.1365) loss_oracle 1.1042 (1.0624) acc 75.0000 (55.3125) lr 1.9921e-03 eta 0:20:27
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,214
* accuracy: 50.6%
* error: 49.4%
* macro_f1: 41.5%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,709
* accuracy: 58.6%
* error: 41.4%
* macro_f1: 23.9%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      50.6%, epoch: 4 *******
******* Domain 2 best val test acc: 58.6%, epoch: 4 *******
******* Domain 2 best test acc:     58.6%, epoch: 4 *******
epoch [5/50] batch [20/319] time 0.072 (0.105) data 0.000 (0.033) loss 2.8377 (2.6428) teacher_loss 1.2093 (1.0541) loss_zs_kd 1.0828 (1.2822) loss_oracle 1.0735 (1.0747) acc 53.1250 (57.0312) lr 1.9823e-03 eta 0:25:36
epoch [5/50] batch [40/319] time 0.084 (0.094) data 0.000 (0.017) loss 2.4372 (2.6252) teacher_loss 0.8856 (1.0328) loss_zs_kd 1.2263 (1.3185) loss_oracle 1.0569 (1.0750) acc 68.7500 (61.0938) lr 1.9823e-03 eta 0:22:53
epoch [5/50] batch [60/319] time 0.071 (0.088) data 0.000 (0.011) loss 2.4513 (2.6408) teacher_loss 0.8590 (1.0560) loss_zs_kd 1.0580 (1.2465) loss_oracle 1.0790 (1.0703) acc 59.3750 (60.9375) lr 1.9823e-03 eta 0:21:24
epoch [5/50] batch [80/319] time 0.060 (0.082) data 0.000 (0.008) loss 2.3018 (2.6316) teacher_loss 0.8137 (1.0514) loss_zs_kd 0.8894 (1.1760) loss_oracle 1.0116 (1.0692) acc 71.8750 (61.7578) lr 1.9823e-03 eta 0:20:02
epoch [5/50] batch [100/319] time 0.082 (0.082) data 0.000 (0.007) loss 2.6969 (2.6263) teacher_loss 1.1604 (1.0518) loss_zs_kd 1.1366 (1.1579) loss_oracle 0.9947 (1.0660) acc 65.6250 (61.8125) lr 1.9823e-03 eta 0:19:50
epoch [5/50] batch [120/319] time 0.079 (0.082) data 0.000 (0.006) loss 2.6637 (2.6498) teacher_loss 1.1140 (1.0735) loss_zs_kd 1.0749 (1.1524) loss_oracle 1.0156 (1.0655) acc 56.2500 (60.3646) lr 1.9823e-03 eta 0:19:54
epoch [5/50] batch [140/319] time 0.084 (0.082) data 0.000 (0.005) loss 2.7685 (2.6540) teacher_loss 1.2170 (1.0761) loss_zs_kd 1.3058 (1.1733) loss_oracle 1.0628 (1.0651) acc 53.1250 (59.8884) lr 1.9823e-03 eta 0:19:51
epoch [5/50] batch [160/319] time 0.085 (0.082) data 0.000 (0.004) loss 2.6481 (2.6452) teacher_loss 1.0957 (1.0698) loss_zs_kd 1.2590 (1.2033) loss_oracle 1.0261 (1.0639) acc 65.6250 (60.3320) lr 1.9823e-03 eta 0:19:55
epoch [5/50] batch [180/319] time 0.075 (0.082) data 0.000 (0.004) loss 2.2420 (2.6289) teacher_loss 0.6545 (1.0557) loss_zs_kd 1.2102 (1.1954) loss_oracle 1.0573 (1.0632) acc 71.8750 (61.1979) lr 1.9823e-03 eta 0:19:54
epoch [5/50] batch [200/319] time 0.074 (0.085) data 0.000 (0.004) loss 2.4244 (2.6179) teacher_loss 0.8634 (1.0466) loss_zs_kd 1.4084 (1.1942) loss_oracle 1.0378 (1.0621) acc 65.6250 (61.7969) lr 1.9823e-03 eta 0:20:24
epoch [5/50] batch [220/319] time 0.075 (0.084) data 0.000 (0.003) loss 2.2757 (2.6103) teacher_loss 0.7089 (1.0406) loss_zs_kd 1.1233 (1.2013) loss_oracle 1.0849 (1.0612) acc 71.8750 (62.0739) lr 1.9823e-03 eta 0:20:19
epoch [5/50] batch [240/319] time 0.080 (0.083) data 0.000 (0.003) loss 2.2921 (2.6025) teacher_loss 0.7919 (1.0338) loss_zs_kd 1.0673 (1.1954) loss_oracle 1.0576 (1.0608) acc 75.0000 (62.4870) lr 1.9823e-03 eta 0:20:05
epoch [5/50] batch [260/319] time 0.080 (0.084) data 0.000 (0.003) loss 2.8408 (2.6017) teacher_loss 1.2566 (1.0344) loss_zs_kd 1.1791 (1.1908) loss_oracle 1.0814 (1.0612) acc 50.0000 (62.5721) lr 1.9823e-03 eta 0:20:05
epoch [5/50] batch [280/319] time 0.069 (0.082) data 0.000 (0.003) loss 2.3288 (2.5993) teacher_loss 0.7580 (1.0325) loss_zs_kd 0.8878 (1.1803) loss_oracle 1.0564 (1.0609) acc 71.8750 (62.5558) lr 1.9823e-03 eta 0:19:44
epoch [5/50] batch [300/319] time 0.083 (0.082) data 0.000 (0.002) loss 2.6837 (2.5937) teacher_loss 1.1697 (1.0280) loss_zs_kd 1.3723 (1.1846) loss_oracle 1.0200 (1.0603) acc 59.3750 (62.7604) lr 1.9823e-03 eta 0:19:43
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,447
* accuracy: 55.9%
* error: 44.1%
* macro_f1: 49.1%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,499
* accuracy: 56.5%
* error: 43.5%
* macro_f1: 25.2%
******* Domain 2 best val acc:      55.9%, epoch: 5 *******
******* Domain 2 best val test acc: 56.5%, epoch: 5 *******
******* Domain 2 best test acc:     58.6%, epoch: 4 *******
epoch [6/50] batch [20/319] time 0.089 (0.118) data 0.000 (0.028) loss 2.7120 (2.4702) teacher_loss 1.1165 (0.9072) loss_zs_kd 1.3838 (1.2664) loss_oracle 1.0697 (1.0577) acc 59.3750 (71.4062) lr 1.9686e-03 eta 0:28:16
epoch [6/50] batch [40/319] time 0.086 (0.098) data 0.000 (0.014) loss 2.5788 (2.4358) teacher_loss 1.0442 (0.8798) loss_zs_kd 1.3395 (1.2875) loss_oracle 1.0036 (1.0481) acc 68.7500 (70.6250) lr 1.9686e-03 eta 0:23:27
epoch [6/50] batch [60/319] time 0.074 (0.091) data 0.000 (0.009) loss 2.5849 (2.4526) teacher_loss 1.0497 (0.9018) loss_zs_kd 1.3237 (1.3187) loss_oracle 1.0084 (1.0436) acc 65.6250 (69.1146) lr 1.9686e-03 eta 0:21:44
epoch [6/50] batch [80/319] time 0.090 (0.089) data 0.000 (0.007) loss 2.6602 (2.4602) teacher_loss 1.0901 (0.9096) loss_zs_kd 1.2411 (1.3047) loss_oracle 1.0793 (1.0437) acc 65.6250 (68.6719) lr 1.9686e-03 eta 0:21:03
epoch [6/50] batch [100/319] time 0.078 (0.087) data 0.000 (0.006) loss 2.3043 (2.4734) teacher_loss 0.7822 (0.9208) loss_zs_kd 1.6204 (1.3127) loss_oracle 0.9809 (1.0415) acc 75.0000 (68.1562) lr 1.9686e-03 eta 0:20:47
epoch [6/50] batch [120/319] time 0.088 (0.087) data 0.000 (0.005) loss 2.4942 (2.4785) teacher_loss 0.8928 (0.9249) loss_zs_kd 2.0383 (1.3679) loss_oracle 1.0708 (1.0432) acc 71.8750 (67.9688) lr 1.9686e-03 eta 0:20:41
epoch [6/50] batch [140/319] time 0.086 (0.087) data 0.000 (0.004) loss 2.5717 (2.4841) teacher_loss 1.1069 (0.9324) loss_zs_kd 1.1871 (1.3748) loss_oracle 1.0659 (1.0434) acc 68.7500 (67.6562) lr 1.9686e-03 eta 0:20:32
epoch [6/50] batch [160/319] time 0.088 (0.086) data 0.000 (0.004) loss 2.6556 (2.4868) teacher_loss 1.0757 (0.9357) loss_zs_kd 1.2472 (1.3753) loss_oracle 1.0885 (1.0458) acc 68.7500 (67.4219) lr 1.9686e-03 eta 0:20:24
epoch [6/50] batch [180/319] time 0.086 (0.086) data 0.000 (0.003) loss 2.6331 (2.4934) teacher_loss 1.1356 (0.9440) loss_zs_kd 1.5016 (1.3863) loss_oracle 1.0663 (1.0477) acc 56.2500 (67.2917) lr 1.9686e-03 eta 0:20:18
epoch [6/50] batch [200/319] time 0.082 (0.086) data 0.000 (0.003) loss 2.7343 (2.5011) teacher_loss 1.1505 (0.9521) loss_zs_kd 1.1609 (1.3800) loss_oracle 1.0911 (1.0492) acc 62.5000 (66.9688) lr 1.9686e-03 eta 0:20:15
epoch [6/50] batch [220/319] time 0.085 (0.085) data 0.000 (0.003) loss 2.5676 (2.4970) teacher_loss 1.0484 (0.9477) loss_zs_kd 1.5031 (1.4035) loss_oracle 0.9877 (1.0492) acc 65.6250 (66.9886) lr 1.9686e-03 eta 0:20:07
epoch [6/50] batch [240/319] time 0.087 (0.085) data 0.000 (0.003) loss 2.7611 (2.4949) teacher_loss 1.2106 (0.9462) loss_zs_kd 1.7401 (1.4381) loss_oracle 1.0176 (1.0476) acc 59.3750 (67.0312) lr 1.9686e-03 eta 0:20:04
epoch [6/50] batch [260/319] time 0.076 (0.085) data 0.000 (0.002) loss 2.4169 (2.4905) teacher_loss 0.8199 (0.9414) loss_zs_kd 1.9383 (1.4744) loss_oracle 1.0713 (1.0477) acc 65.6250 (67.1274) lr 1.9686e-03 eta 0:20:01
epoch [6/50] batch [280/319] time 0.073 (0.085) data 0.000 (0.002) loss 2.4511 (2.4885) teacher_loss 0.9085 (0.9382) loss_zs_kd 1.4763 (1.4899) loss_oracle 1.0573 (1.0483) acc 56.2500 (67.1094) lr 1.9686e-03 eta 0:19:53
epoch [6/50] batch [300/319] time 0.079 (0.084) data 0.000 (0.002) loss 2.2745 (2.4859) teacher_loss 0.6797 (0.9342) loss_zs_kd 1.6101 (1.4840) loss_oracle 1.0855 (1.0496) acc 78.1250 (67.1979) lr 1.9686e-03 eta 0:19:47
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,596
* accuracy: 59.3%
* error: 40.7%
* macro_f1: 50.7%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,183
* accuracy: 53.2%
* error: 46.8%
* macro_f1: 25.6%
******* Domain 2 best val acc:      59.3%, epoch: 6 *******
******* Domain 2 best val test acc: 53.2%, epoch: 6 *******
******* Domain 2 best test acc:     58.6%, epoch: 4 *******
epoch [7/50] batch [20/319] time 0.075 (0.107) data 0.000 (0.025) loss 2.1360 (2.4394) teacher_loss 0.6320 (0.9018) loss_zs_kd 1.5234 (1.5141) loss_oracle 1.0962 (1.0491) acc 75.0000 (68.5938) lr 1.9511e-03 eta 0:24:54
epoch [7/50] batch [40/319] time 0.074 (0.092) data 0.000 (0.013) loss 2.7195 (2.4511) teacher_loss 1.1365 (0.9120) loss_zs_kd 1.6870 (1.5114) loss_oracle 1.0904 (1.0563) acc 59.3750 (67.2656) lr 1.9511e-03 eta 0:21:25
epoch [7/50] batch [60/319] time 0.066 (0.087) data 0.000 (0.009) loss 2.2712 (2.4285) teacher_loss 0.7822 (0.8862) loss_zs_kd 1.5121 (1.5207) loss_oracle 1.0265 (1.0549) acc 75.0000 (68.2812) lr 1.9511e-03 eta 0:20:09
epoch [7/50] batch [80/319] time 0.065 (0.082) data 0.000 (0.007) loss 2.4911 (2.4256) teacher_loss 0.9676 (0.8817) loss_zs_kd 1.5287 (1.5329) loss_oracle 1.0749 (1.0579) acc 71.8750 (68.7891) lr 1.9511e-03 eta 0:19:04
epoch [7/50] batch [100/319] time 0.074 (0.080) data 0.000 (0.005) loss 2.4889 (2.4428) teacher_loss 0.9767 (0.8956) loss_zs_kd 1.3298 (1.5097) loss_oracle 0.9934 (1.0603) acc 62.5000 (68.2188) lr 1.9511e-03 eta 0:18:39
epoch [7/50] batch [120/319] time 0.077 (0.080) data 0.000 (0.004) loss 2.9320 (2.4391) teacher_loss 1.3506 (0.8913) loss_zs_kd 1.6862 (1.5128) loss_oracle 1.0740 (1.0614) acc 53.1250 (68.3333) lr 1.9511e-03 eta 0:18:33
epoch [7/50] batch [140/319] time 0.082 (0.080) data 0.000 (0.004) loss 2.4751 (2.4473) teacher_loss 0.9083 (0.9013) loss_zs_kd 1.6100 (1.5101) loss_oracle 1.0535 (1.0600) acc 68.7500 (68.0804) lr 1.9511e-03 eta 0:18:37
epoch [7/50] batch [160/319] time 0.075 (0.081) data 0.000 (0.003) loss 2.3140 (2.4584) teacher_loss 0.7109 (0.9128) loss_zs_kd 1.4977 (1.5110) loss_oracle 1.0919 (1.0604) acc 71.8750 (67.3828) lr 1.9511e-03 eta 0:18:38
epoch [7/50] batch [180/319] time 0.142 (0.081) data 0.000 (0.003) loss 2.4898 (2.4582) teacher_loss 0.9312 (0.9133) loss_zs_kd 1.6191 (1.5151) loss_oracle 1.0641 (1.0619) acc 71.8750 (67.4132) lr 1.9511e-03 eta 0:18:40
epoch [7/50] batch [200/319] time 0.083 (0.082) data 0.000 (0.003) loss 2.8705 (2.4581) teacher_loss 1.2726 (0.9121) loss_zs_kd 1.5160 (1.5042) loss_oracle 1.0756 (1.0644) acc 53.1250 (67.3125) lr 1.9511e-03 eta 0:19:01
epoch [7/50] batch [220/319] time 0.057 (0.082) data 0.000 (0.003) loss 2.5270 (2.4548) teacher_loss 0.9988 (0.9087) loss_zs_kd 1.3584 (1.4858) loss_oracle 1.0700 (1.0651) acc 68.7500 (67.4716) lr 1.9511e-03 eta 0:18:56
epoch [7/50] batch [240/319] time 0.088 (0.082) data 0.000 (0.002) loss 2.2865 (2.4545) teacher_loss 0.7819 (0.9082) loss_zs_kd 1.4496 (1.4742) loss_oracle 1.0213 (1.0661) acc 71.8750 (67.4349) lr 1.9511e-03 eta 0:18:49
epoch [7/50] batch [260/319] time 0.078 (0.082) data 0.000 (0.002) loss 2.3505 (2.4561) teacher_loss 0.7361 (0.9093) loss_zs_kd 1.3763 (1.4722) loss_oracle 1.0680 (1.0655) acc 75.0000 (67.3918) lr 1.9511e-03 eta 0:18:46
epoch [7/50] batch [280/319] time 0.078 (0.082) data 0.000 (0.002) loss 2.2608 (2.4572) teacher_loss 0.7589 (0.9113) loss_zs_kd 1.5135 (1.4677) loss_oracle 1.0797 (1.0646) acc 78.1250 (67.4888) lr 1.9511e-03 eta 0:18:43
epoch [7/50] batch [300/319] time 0.055 (0.081) data 0.000 (0.002) loss 2.5378 (2.4552) teacher_loss 1.0033 (0.9094) loss_zs_kd 1.4789 (1.4618) loss_oracle 1.0681 (1.0644) acc 50.0000 (67.5312) lr 1.9511e-03 eta 0:18:31
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,522
* accuracy: 57.6%
* error: 42.4%
* macro_f1: 50.9%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,375
* accuracy: 55.2%
* error: 44.8%
* macro_f1: 23.3%
******* Domain 2 best val acc:      59.3%, epoch: 6 *******
******* Domain 2 best val test acc: 53.2%, epoch: 6 *******
******* Domain 2 best test acc:     58.6%, epoch: 4 *******
epoch [8/50] batch [20/319] time 0.077 (0.117) data 0.000 (0.032) loss 2.9033 (2.5606) teacher_loss 1.3250 (1.0376) loss_zs_kd 1.4031 (1.4013) loss_oracle 1.0715 (1.0531) acc 50.0000 (61.4062) lr 1.9298e-03 eta 0:26:40
epoch [8/50] batch [40/319] time 0.070 (0.096) data 0.000 (0.016) loss 2.2153 (2.4819) teacher_loss 0.7136 (0.9469) loss_zs_kd 1.4489 (1.4047) loss_oracle 1.0714 (1.0550) acc 75.0000 (64.7656) lr 1.9298e-03 eta 0:21:58
epoch [8/50] batch [60/319] time 0.073 (0.091) data 0.000 (0.011) loss 2.4412 (2.4608) teacher_loss 0.9028 (0.9213) loss_zs_kd 1.4355 (1.4076) loss_oracle 1.0348 (1.0602) acc 71.8750 (66.6667) lr 1.9298e-03 eta 0:20:44
epoch [8/50] batch [80/319] time 0.080 (0.088) data 0.000 (0.008) loss 2.4009 (2.4431) teacher_loss 0.8423 (0.8996) loss_zs_kd 1.1299 (1.3957) loss_oracle 1.0912 (1.0622) acc 68.7500 (67.6172) lr 1.9298e-03 eta 0:19:57
epoch [8/50] batch [100/319] time 0.075 (0.086) data 0.000 (0.007) loss 2.3850 (2.4600) teacher_loss 0.8020 (0.9138) loss_zs_kd 1.4185 (1.3829) loss_oracle 1.1284 (1.0684) acc 68.7500 (67.5312) lr 1.9298e-03 eta 0:19:25
epoch [8/50] batch [120/319] time 0.100 (0.085) data 0.000 (0.006) loss 2.5319 (2.4469) teacher_loss 0.9729 (0.8964) loss_zs_kd 1.6419 (1.3854) loss_oracle 1.0822 (1.0727) acc 68.7500 (68.2812) lr 1.9298e-03 eta 0:19:16
epoch [8/50] batch [140/319] time 0.086 (0.085) data 0.000 (0.005) loss 2.4047 (2.4486) teacher_loss 0.8136 (0.8980) loss_zs_kd 1.6924 (1.3845) loss_oracle 1.1102 (1.0739) acc 68.7500 (67.8348) lr 1.9298e-03 eta 0:19:11
epoch [8/50] batch [160/319] time 0.084 (0.085) data 0.000 (0.004) loss 2.6924 (2.4485) teacher_loss 1.1314 (0.8989) loss_zs_kd 1.2873 (1.3985) loss_oracle 1.0818 (1.0741) acc 59.3750 (68.0273) lr 1.9298e-03 eta 0:19:09
epoch [8/50] batch [180/319] time 0.083 (0.084) data 0.000 (0.004) loss 2.5870 (2.4532) teacher_loss 1.0678 (0.9032) loss_zs_kd 1.3842 (1.4080) loss_oracle 1.1254 (1.0767) acc 65.6250 (68.1771) lr 1.9298e-03 eta 0:19:02
epoch [8/50] batch [200/319] time 0.077 (0.084) data 0.000 (0.003) loss 2.4211 (2.4478) teacher_loss 0.8909 (0.8984) loss_zs_kd 1.5490 (1.4031) loss_oracle 1.0522 (1.0762) acc 62.5000 (68.0781) lr 1.9298e-03 eta 0:18:56
epoch [8/50] batch [220/319] time 0.080 (0.084) data 0.000 (0.003) loss 2.0240 (2.4467) teacher_loss 0.4682 (0.8961) loss_zs_kd 1.5065 (1.4030) loss_oracle 1.0541 (1.0767) acc 87.5000 (68.2955) lr 1.9298e-03 eta 0:18:52
epoch [8/50] batch [240/319] time 0.088 (0.084) data 0.000 (0.003) loss 2.5011 (2.4469) teacher_loss 0.9499 (0.8952) loss_zs_kd 1.3418 (1.4186) loss_oracle 1.0789 (1.0776) acc 68.7500 (68.3073) lr 1.9298e-03 eta 0:18:52
epoch [8/50] batch [260/319] time 0.083 (0.084) data 0.000 (0.003) loss 2.7507 (2.4503) teacher_loss 1.1826 (0.8977) loss_zs_kd 1.5888 (1.4379) loss_oracle 1.0300 (1.0777) acc 59.3750 (68.1490) lr 1.9298e-03 eta 0:18:51
epoch [8/50] batch [280/319] time 0.097 (0.084) data 0.000 (0.003) loss 2.5579 (2.4454) teacher_loss 1.0129 (0.8924) loss_zs_kd 1.4804 (1.4434) loss_oracle 1.0682 (1.0780) acc 65.6250 (68.1920) lr 1.9298e-03 eta 0:18:50
epoch [8/50] batch [300/319] time 0.078 (0.084) data 0.000 (0.002) loss 2.6669 (2.4481) teacher_loss 1.0947 (0.8958) loss_zs_kd 1.1795 (1.4437) loss_oracle 1.1305 (1.0781) acc 53.1250 (68.0104) lr 1.9298e-03 eta 0:18:47
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,444
* accuracy: 55.8%
* error: 44.2%
* macro_f1: 47.6%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,687
* accuracy: 58.4%
* error: 41.6%
* macro_f1: 25.5%
******* Domain 2 best val acc:      59.3%, epoch: 6 *******
******* Domain 2 best val test acc: 53.2%, epoch: 6 *******
******* Domain 2 best test acc:     58.6%, epoch: 4 *******
epoch [9/50] batch [20/319] time 0.085 (0.123) data 0.000 (0.034) loss 2.1936 (2.3626) teacher_loss 0.5923 (0.8008) loss_zs_kd 1.5811 (1.3950) loss_oracle 1.1292 (1.0907) acc 78.1250 (70.6250) lr 1.9048e-03 eta 0:27:19
epoch [9/50] batch [40/319] time 0.080 (0.102) data 0.000 (0.017) loss 2.2680 (2.3737) teacher_loss 0.7493 (0.8166) loss_zs_kd 1.4363 (1.4019) loss_oracle 1.0946 (1.0819) acc 75.0000 (71.6406) lr 1.9048e-03 eta 0:22:41
epoch [9/50] batch [60/319] time 0.073 (0.093) data 0.000 (0.011) loss 2.6420 (2.4124) teacher_loss 1.0993 (0.8550) loss_zs_kd 1.3818 (1.3737) loss_oracle 1.0781 (1.0811) acc 46.8750 (70.4167) lr 1.9048e-03 eta 0:20:42
epoch [9/50] batch [80/319] time 0.076 (0.090) data 0.000 (0.009) loss 2.2940 (2.4229) teacher_loss 0.7350 (0.8674) loss_zs_kd 1.3173 (1.3956) loss_oracle 1.0763 (1.0811) acc 65.6250 (70.1953) lr 1.9048e-03 eta 0:19:55
epoch [9/50] batch [100/319] time 0.085 (0.087) data 0.000 (0.007) loss 2.8102 (2.4313) teacher_loss 1.3016 (0.8773) loss_zs_kd 1.2378 (1.3606) loss_oracle 1.0523 (1.0804) acc 62.5000 (69.4375) lr 1.9048e-03 eta 0:19:17
epoch [9/50] batch [120/319] time 0.090 (0.087) data 0.000 (0.006) loss 2.6629 (2.4546) teacher_loss 1.0625 (0.9009) loss_zs_kd 1.1622 (1.3376) loss_oracle 1.0852 (1.0804) acc 65.6250 (68.3333) lr 1.9048e-03 eta 0:19:09
epoch [9/50] batch [140/319] time 0.083 (0.086) data 0.000 (0.005) loss 2.8974 (2.4651) teacher_loss 1.3666 (0.9137) loss_zs_kd 1.4237 (1.3385) loss_oracle 1.0827 (1.0792) acc 43.7500 (67.8125) lr 1.9048e-03 eta 0:19:01
epoch [9/50] batch [160/319] time 0.092 (0.085) data 0.000 (0.004) loss 2.3209 (2.4584) teacher_loss 0.7503 (0.9085) loss_zs_kd 1.7326 (1.3702) loss_oracle 1.1058 (1.0773) acc 75.0000 (68.1836) lr 1.9048e-03 eta 0:18:51
epoch [9/50] batch [180/319] time 0.114 (0.086) data 0.001 (0.004) loss 2.2909 (2.4500) teacher_loss 0.7578 (0.9000) loss_zs_kd 1.7585 (1.3983) loss_oracle 1.0787 (1.0773) acc 65.6250 (68.4375) lr 1.9048e-03 eta 0:18:51
epoch [9/50] batch [200/319] time 0.085 (0.085) data 0.001 (0.004) loss 2.6638 (2.4488) teacher_loss 1.0447 (0.8996) loss_zs_kd 1.4827 (1.4108) loss_oracle 1.0851 (1.0766) acc 56.2500 (68.5938) lr 1.9048e-03 eta 0:18:47
epoch [9/50] batch [220/319] time 0.083 (0.085) data 0.000 (0.003) loss 2.3159 (2.4486) teacher_loss 0.7813 (0.8987) loss_zs_kd 1.3482 (1.4189) loss_oracle 1.0486 (1.0769) acc 65.6250 (68.2955) lr 1.9048e-03 eta 0:18:42
epoch [9/50] batch [240/319] time 0.102 (0.085) data 0.000 (0.003) loss 2.3690 (2.4495) teacher_loss 0.8382 (0.8994) loss_zs_kd 1.6292 (1.4178) loss_oracle 1.0755 (1.0773) acc 68.7500 (68.2682) lr 1.9048e-03 eta 0:18:44
epoch [9/50] batch [260/319] time 0.082 (0.085) data 0.000 (0.003) loss 2.5780 (2.4448) teacher_loss 1.1162 (0.8957) loss_zs_kd 1.2937 (1.4160) loss_oracle 1.0448 (1.0774) acc 53.1250 (68.2572) lr 1.9048e-03 eta 0:18:37
epoch [9/50] batch [280/319] time 0.075 (0.085) data 0.000 (0.003) loss 2.1575 (2.4499) teacher_loss 0.6708 (0.9009) loss_zs_kd 1.4652 (1.4141) loss_oracle 1.0554 (1.0781) acc 87.5000 (67.9911) lr 1.9048e-03 eta 0:18:34
epoch [9/50] batch [300/319] time 0.086 (0.085) data 0.000 (0.003) loss 2.8227 (2.4546) teacher_loss 1.2549 (0.9059) loss_zs_kd 1.4561 (1.4170) loss_oracle 1.0676 (1.0779) acc 56.2500 (67.6979) lr 1.9048e-03 eta 0:18:29
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,628
* accuracy: 60.0%
* error: 40.0%
* macro_f1: 50.1%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,294
* accuracy: 54.4%
* error: 45.6%
* macro_f1: 23.9%
******* Domain 2 best val acc:      60.0%, epoch: 9 *******
******* Domain 2 best val test acc: 54.4%, epoch: 9 *******
******* Domain 2 best test acc:     58.6%, epoch: 4 *******
epoch [10/50] batch [20/319] time 0.079 (0.114) data 0.000 (0.028) loss 2.6137 (2.4720) teacher_loss 1.0847 (0.9222) loss_zs_kd 1.6236 (1.3148) loss_oracle 1.0863 (1.0705) acc 53.1250 (66.2500) lr 1.8763e-03 eta 0:24:43
epoch [10/50] batch [40/319] time 0.087 (0.099) data 0.000 (0.014) loss 2.4792 (2.4473) teacher_loss 0.8796 (0.8934) loss_zs_kd 1.3152 (1.4333) loss_oracle 1.0698 (1.0687) acc 62.5000 (67.1875) lr 1.8763e-03 eta 0:21:35
epoch [10/50] batch [60/319] time 0.095 (0.091) data 0.000 (0.010) loss 2.5826 (2.4392) teacher_loss 0.9770 (0.8895) loss_zs_kd 1.6280 (1.4650) loss_oracle 1.0956 (1.0654) acc 65.6250 (67.5521) lr 1.8763e-03 eta 0:19:49
epoch [10/50] batch [80/319] time 0.077 (0.090) data 0.000 (0.007) loss 2.0128 (2.4399) teacher_loss 0.5850 (0.8931) loss_zs_kd 1.4459 (1.4827) loss_oracle 1.0141 (1.0668) acc 87.5000 (67.6562) lr 1.8763e-03 eta 0:19:23
epoch [10/50] batch [100/319] time 0.076 (0.088) data 0.000 (0.006) loss 2.2021 (2.4356) teacher_loss 0.6307 (0.8870) loss_zs_kd 1.6630 (1.5034) loss_oracle 1.1004 (1.0698) acc 71.8750 (67.5625) lr 1.8763e-03 eta 0:19:03
epoch [10/50] batch [120/319] time 0.084 (0.087) data 0.000 (0.005) loss 2.4533 (2.4371) teacher_loss 0.8850 (0.8878) loss_zs_kd 1.4492 (1.5183) loss_oracle 1.0578 (1.0706) acc 62.5000 (67.3438) lr 1.8763e-03 eta 0:18:41
epoch [10/50] batch [140/319] time 0.074 (0.085) data 0.000 (0.004) loss 2.2572 (2.4322) teacher_loss 0.7294 (0.8825) loss_zs_kd 1.4240 (1.5087) loss_oracle 1.1095 (1.0710) acc 75.0000 (67.8125) lr 1.8763e-03 eta 0:18:25
epoch [10/50] batch [160/319] time 0.080 (0.084) data 0.000 (0.004) loss 2.4681 (2.4291) teacher_loss 0.9147 (0.8792) loss_zs_kd 1.6655 (1.5083) loss_oracle 1.0826 (1.0717) acc 65.6250 (68.1836) lr 1.8763e-03 eta 0:18:10
epoch [10/50] batch [180/319] time 0.076 (0.084) data 0.000 (0.003) loss 2.3930 (2.4364) teacher_loss 0.8952 (0.8889) loss_zs_kd 1.5590 (1.5154) loss_oracle 1.0541 (1.0696) acc 62.5000 (68.2465) lr 1.8763e-03 eta 0:18:01
epoch [10/50] batch [200/319] time 0.092 (0.084) data 0.000 (0.003) loss 2.1562 (2.4287) teacher_loss 0.5826 (0.8798) loss_zs_kd 1.6733 (1.5196) loss_oracle 1.0966 (1.0712) acc 84.3750 (68.6094) lr 1.8763e-03 eta 0:17:57
epoch [10/50] batch [220/319] time 0.082 (0.083) data 0.000 (0.003) loss 2.1535 (2.4218) teacher_loss 0.5754 (0.8728) loss_zs_kd 1.7245 (1.5386) loss_oracle 1.1155 (1.0714) acc 71.8750 (69.0767) lr 1.8763e-03 eta 0:17:45
epoch [10/50] batch [240/319] time 0.072 (0.083) data 0.000 (0.003) loss 2.5681 (2.4262) teacher_loss 0.9840 (0.8771) loss_zs_kd 1.6683 (1.5436) loss_oracle 1.1022 (1.0723) acc 65.6250 (68.8542) lr 1.8763e-03 eta 0:17:40
epoch [10/50] batch [260/319] time 0.077 (0.082) data 0.000 (0.002) loss 2.4519 (2.4264) teacher_loss 0.8842 (0.8772) loss_zs_kd 1.7609 (1.5449) loss_oracle 1.0575 (1.0718) acc 68.7500 (68.8582) lr 1.8763e-03 eta 0:17:31
epoch [10/50] batch [280/319] time 0.083 (0.082) data 0.000 (0.002) loss 2.3020 (2.4197) teacher_loss 0.6914 (0.8701) loss_zs_kd 1.7186 (1.5498) loss_oracle 1.1180 (1.0726) acc 71.8750 (69.0960) lr 1.8763e-03 eta 0:17:29
epoch [10/50] batch [300/319] time 0.077 (0.082) data 0.000 (0.002) loss 2.4945 (2.4196) teacher_loss 0.9886 (0.8701) loss_zs_kd 1.4553 (1.5493) loss_oracle 1.0846 (1.0730) acc 59.3750 (69.2396) lr 1.8763e-03 eta 0:17:26
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,506
* accuracy: 57.2%
* error: 42.8%
* macro_f1: 49.9%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,687
* accuracy: 58.4%
* error: 41.6%
* macro_f1: 26.0%
******* Domain 2 best val acc:      60.0%, epoch: 9 *******
******* Domain 2 best val test acc: 54.4%, epoch: 9 *******
******* Domain 2 best test acc:     58.6%, epoch: 4 *******
epoch [11/50] batch [20/319] time 0.078 (0.108) data 0.000 (0.026) loss 2.3706 (2.3505) teacher_loss 0.8605 (0.8081) loss_zs_kd 1.5918 (1.3970) loss_oracle 1.0420 (1.0690) acc 68.7500 (70.3125) lr 1.8443e-03 eta 0:23:00
epoch [11/50] batch [40/319] time 0.079 (0.095) data 0.000 (0.013) loss 2.0787 (2.3513) teacher_loss 0.5700 (0.8071) loss_zs_kd 1.4246 (1.5049) loss_oracle 1.0650 (1.0642) acc 81.2500 (72.2656) lr 1.8443e-03 eta 0:20:12
epoch [11/50] batch [60/319] time 0.087 (0.091) data 0.001 (0.009) loss 2.4321 (2.3550) teacher_loss 0.9107 (0.8140) loss_zs_kd 1.6891 (1.5455) loss_oracle 1.0690 (1.0595) acc 84.3750 (71.7708) lr 1.8443e-03 eta 0:19:17
epoch [11/50] batch [80/319] time 0.089 (0.090) data 0.000 (0.007) loss 2.6340 (2.3646) teacher_loss 1.1066 (0.8206) loss_zs_kd 1.6029 (1.5565) loss_oracle 1.1072 (1.0655) acc 56.2500 (71.6797) lr 1.8443e-03 eta 0:19:01
epoch [11/50] batch [100/319] time 0.084 (0.089) data 0.000 (0.005) loss 2.4067 (2.3801) teacher_loss 0.8510 (0.8354) loss_zs_kd 1.4261 (1.5498) loss_oracle 1.0720 (1.0680) acc 81.2500 (71.0938) lr 1.8443e-03 eta 0:18:42
epoch [11/50] batch [120/319] time 0.085 (0.088) data 0.000 (0.005) loss 2.4296 (2.3817) teacher_loss 0.9104 (0.8391) loss_zs_kd 1.6942 (1.5531) loss_oracle 1.0859 (1.0693) acc 62.5000 (70.6771) lr 1.8443e-03 eta 0:18:28
epoch [11/50] batch [140/319] time 0.074 (0.086) data 0.000 (0.004) loss 2.9773 (2.3943) teacher_loss 1.4163 (0.8545) loss_zs_kd 1.5904 (1.5574) loss_oracle 1.1250 (1.0680) acc 50.0000 (70.3125) lr 1.8443e-03 eta 0:18:09
epoch [11/50] batch [160/319] time 0.076 (0.085) data 0.000 (0.003) loss 2.0575 (2.4084) teacher_loss 0.5123 (0.8689) loss_zs_kd 1.6864 (1.5517) loss_oracle 1.0728 (1.0695) acc 84.3750 (69.5508) lr 1.8443e-03 eta 0:17:55
epoch [11/50] batch [180/319] time 0.076 (0.087) data 0.000 (0.003) loss 2.6040 (2.4105) teacher_loss 1.0807 (0.8713) loss_zs_kd 1.3500 (1.5490) loss_oracle 1.0788 (1.0702) acc 62.5000 (69.5139) lr 1.8443e-03 eta 0:18:15
epoch [11/50] batch [200/319] time 0.082 (0.087) data 0.000 (0.003) loss 2.4597 (2.4086) teacher_loss 0.9286 (0.8718) loss_zs_kd 1.4483 (1.5473) loss_oracle 1.0797 (1.0687) acc 71.8750 (69.3438) lr 1.8443e-03 eta 0:18:07
epoch [11/50] batch [220/319] time 0.082 (0.086) data 0.000 (0.003) loss 2.5272 (2.4018) teacher_loss 1.0058 (0.8666) loss_zs_kd 1.4934 (1.5252) loss_oracle 1.0329 (1.0690) acc 65.6250 (69.4602) lr 1.8443e-03 eta 0:18:02
epoch [11/50] batch [240/319] time 0.084 (0.086) data 0.000 (0.002) loss 2.3280 (2.3988) teacher_loss 0.7478 (0.8638) loss_zs_kd 1.4611 (1.5159) loss_oracle 1.1041 (1.0692) acc 81.2500 (69.7135) lr 1.8443e-03 eta 0:17:57
epoch [11/50] batch [260/319] time 0.073 (0.085) data 0.000 (0.002) loss 2.1924 (2.3970) teacher_loss 0.7115 (0.8623) loss_zs_kd 1.8356 (1.5160) loss_oracle 0.9875 (1.0699) acc 68.7500 (69.6875) lr 1.8443e-03 eta 0:17:47
epoch [11/50] batch [280/319] time 0.083 (0.085) data 0.000 (0.002) loss 2.7120 (2.3939) teacher_loss 1.1455 (0.8596) loss_zs_kd 1.2496 (1.5173) loss_oracle 1.1024 (1.0706) acc 56.2500 (69.6987) lr 1.8443e-03 eta 0:17:44
epoch [11/50] batch [300/319] time 0.080 (0.085) data 0.000 (0.002) loss 2.9655 (2.3985) teacher_loss 1.4376 (0.8644) loss_zs_kd 1.3600 (1.5046) loss_oracle 1.0515 (1.0706) acc 43.7500 (69.5417) lr 1.8443e-03 eta 0:17:36
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,490
* accuracy: 56.9%
* error: 43.1%
* macro_f1: 48.9%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,539
* accuracy: 56.9%
* error: 43.1%
* macro_f1: 25.1%
******* Domain 2 best val acc:      60.0%, epoch: 9 *******
******* Domain 2 best val test acc: 54.4%, epoch: 9 *******
******* Domain 2 best test acc:     58.6%, epoch: 4 *******
epoch [12/50] batch [20/319] time 0.075 (0.115) data 0.000 (0.030) loss 2.4435 (2.3378) teacher_loss 0.8858 (0.8106) loss_zs_kd 1.5500 (1.6696) loss_oracle 1.1117 (1.0798) acc 71.8750 (72.3438) lr 1.8090e-03 eta 0:23:53
epoch [12/50] batch [40/319] time 0.083 (0.099) data 0.000 (0.015) loss 2.1205 (2.3708) teacher_loss 0.5925 (0.8503) loss_zs_kd 1.6936 (1.6568) loss_oracle 1.0607 (1.0732) acc 78.1250 (69.9219) lr 1.8090e-03 eta 0:20:23
epoch [12/50] batch [60/319] time 0.078 (0.091) data 0.000 (0.010) loss 2.2759 (2.3831) teacher_loss 0.8819 (0.8625) loss_zs_kd 1.6174 (1.6744) loss_oracle 0.9484 (1.0752) acc 71.8750 (69.7396) lr 1.8090e-03 eta 0:18:46
epoch [12/50] batch [80/319] time 0.075 (0.088) data 0.000 (0.008) loss 2.3858 (2.3760) teacher_loss 0.8905 (0.8529) loss_zs_kd 1.3947 (1.6738) loss_oracle 1.0690 (1.0771) acc 68.7500 (70.0000) lr 1.8090e-03 eta 0:18:05
epoch [12/50] batch [100/319] time 0.077 (0.086) data 0.000 (0.006) loss 2.3673 (2.3876) teacher_loss 0.8270 (0.8632) loss_zs_kd 1.8553 (1.6425) loss_oracle 1.1020 (1.0796) acc 71.8750 (69.5938) lr 1.8090e-03 eta 0:17:39
epoch [12/50] batch [120/319] time 0.075 (0.085) data 0.000 (0.005) loss 2.3504 (2.3959) teacher_loss 0.7827 (0.8712) loss_zs_kd 1.6706 (1.6403) loss_oracle 1.0349 (1.0804) acc 68.7500 (69.2708) lr 1.8090e-03 eta 0:17:27
epoch [12/50] batch [140/319] time 0.080 (0.084) data 0.000 (0.004) loss 2.3573 (2.3874) teacher_loss 0.8459 (0.8587) loss_zs_kd 1.5635 (1.6407) loss_oracle 1.1027 (1.0860) acc 78.1250 (69.8214) lr 1.8090e-03 eta 0:17:16
epoch [12/50] batch [160/319] time 0.068 (0.083) data 0.000 (0.004) loss 2.4364 (2.3800) teacher_loss 0.8970 (0.8509) loss_zs_kd 1.8948 (1.6374) loss_oracle 1.0840 (1.0869) acc 65.6250 (70.3320) lr 1.8090e-03 eta 0:17:01
epoch [12/50] batch [180/319] time 0.077 (0.083) data 0.000 (0.004) loss 2.3485 (2.3741) teacher_loss 0.8128 (0.8422) loss_zs_kd 1.4108 (1.6425) loss_oracle 1.0689 (1.0917) acc 75.0000 (70.8160) lr 1.8090e-03 eta 0:16:53
epoch [12/50] batch [200/319] time 0.076 (0.083) data 0.000 (0.003) loss 2.3618 (2.3714) teacher_loss 0.8539 (0.8394) loss_zs_kd 1.4108 (1.6270) loss_oracle 1.1228 (1.0907) acc 65.6250 (70.7656) lr 1.8090e-03 eta 0:16:52
epoch [12/50] batch [220/319] time 0.084 (0.082) data 0.000 (0.003) loss 2.3881 (2.3653) teacher_loss 0.8560 (0.8321) loss_zs_kd 1.3845 (1.6139) loss_oracle 1.0867 (1.0926) acc 75.0000 (71.1222) lr 1.8090e-03 eta 0:16:45
epoch [12/50] batch [240/319] time 0.081 (0.082) data 0.000 (0.003) loss 2.4246 (2.3680) teacher_loss 0.8327 (0.8319) loss_zs_kd 1.6820 (1.6112) loss_oracle 1.0950 (1.0949) acc 71.8750 (71.1198) lr 1.8090e-03 eta 0:16:43
epoch [12/50] batch [260/319] time 0.069 (0.081) data 0.000 (0.003) loss 2.1984 (2.3647) teacher_loss 0.7032 (0.8289) loss_zs_kd 1.4560 (1.6040) loss_oracle 1.0882 (1.0948) acc 71.8750 (71.2139) lr 1.8090e-03 eta 0:16:30
epoch [12/50] batch [280/319] time 0.081 (0.081) data 0.000 (0.002) loss 2.0794 (2.3630) teacher_loss 0.5545 (0.8290) loss_zs_kd 1.4603 (1.6025) loss_oracle 1.0804 (1.0932) acc 81.2500 (71.1049) lr 1.8090e-03 eta 0:16:25
epoch [12/50] batch [300/319] time 0.079 (0.081) data 0.000 (0.002) loss 2.4412 (2.3591) teacher_loss 0.8962 (0.8265) loss_zs_kd 1.4282 (1.6054) loss_oracle 1.0443 (1.0921) acc 65.6250 (71.1875) lr 1.8090e-03 eta 0:16:25
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,626
* accuracy: 60.0%
* error: 40.0%
* macro_f1: 50.9%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,299
* accuracy: 54.4%
* error: 45.6%
* macro_f1: 24.0%
******* Domain 2 best val acc:      60.0%, epoch: 9 *******
******* Domain 2 best val test acc: 54.4%, epoch: 9 *******
******* Domain 2 best test acc:     58.6%, epoch: 4 *******
epoch [13/50] batch [20/319] time 0.078 (0.116) data 0.000 (0.030) loss 2.2501 (2.3937) teacher_loss 0.7984 (0.8851) loss_zs_kd 1.5316 (1.4787) loss_oracle 1.0181 (1.0710) acc 62.5000 (70.7812) lr 1.7705e-03 eta 0:23:23
epoch [13/50] batch [40/319] time 0.086 (0.097) data 0.000 (0.015) loss 2.7860 (2.3479) teacher_loss 1.2505 (0.8317) loss_zs_kd 1.5580 (1.4916) loss_oracle 1.1317 (1.0752) acc 59.3750 (71.2500) lr 1.7705e-03 eta 0:19:32
epoch [13/50] batch [60/319] time 0.084 (0.092) data 0.000 (0.010) loss 2.6410 (2.3628) teacher_loss 1.1293 (0.8442) loss_zs_kd 1.4579 (1.4647) loss_oracle 1.1093 (1.0777) acc 62.5000 (70.8854) lr 1.7705e-03 eta 0:18:30
epoch [13/50] batch [80/319] time 0.073 (0.089) data 0.000 (0.008) loss 2.1525 (2.3612) teacher_loss 0.6758 (0.8416) loss_zs_kd 1.1989 (1.4583) loss_oracle 1.0998 (1.0789) acc 81.2500 (70.7812) lr 1.7705e-03 eta 0:17:48
epoch [13/50] batch [100/319] time 0.082 (0.088) data 0.000 (0.006) loss 2.1369 (2.3538) teacher_loss 0.6132 (0.8359) loss_zs_kd 1.5063 (1.4517) loss_oracle 1.0842 (1.0793) acc 84.3750 (71.1562) lr 1.7705e-03 eta 0:17:33
epoch [13/50] batch [120/319] time 0.087 (0.088) data 0.000 (0.005) loss 2.4264 (2.3543) teacher_loss 0.9257 (0.8385) loss_zs_kd 1.4515 (1.4495) loss_oracle 1.0806 (1.0795) acc 65.6250 (71.4062) lr 1.7705e-03 eta 0:17:30
epoch [13/50] batch [140/319] time 0.080 (0.087) data 0.000 (0.005) loss 2.7047 (2.3604) teacher_loss 1.2365 (0.8448) loss_zs_kd 1.4786 (1.4505) loss_oracle 1.0826 (1.0790) acc 62.5000 (70.9375) lr 1.7705e-03 eta 0:17:18
epoch [13/50] batch [160/319] time 0.082 (0.085) data 0.000 (0.004) loss 2.3779 (2.3607) teacher_loss 0.8434 (0.8460) loss_zs_kd 1.6163 (1.4567) loss_oracle 1.0522 (1.0775) acc 71.8750 (70.8594) lr 1.7705e-03 eta 0:17:02
epoch [13/50] batch [180/319] time 0.085 (0.085) data 0.000 (0.004) loss 2.2552 (2.3659) teacher_loss 0.7414 (0.8508) loss_zs_kd 1.4046 (1.4696) loss_oracle 1.1176 (1.0792) acc 81.2500 (70.8160) lr 1.7705e-03 eta 0:16:57
epoch [13/50] batch [200/319] time 0.084 (0.087) data 0.000 (0.003) loss 2.4774 (2.3635) teacher_loss 0.8825 (0.8495) loss_zs_kd 1.6137 (1.4916) loss_oracle 1.1793 (1.0825) acc 62.5000 (70.8594) lr 1.7705e-03 eta 0:17:16
epoch [13/50] batch [220/319] time 0.094 (0.087) data 0.000 (0.003) loss 2.3163 (2.3587) teacher_loss 0.8018 (0.8426) loss_zs_kd 1.6612 (1.4945) loss_oracle 1.1150 (1.0870) acc 75.0000 (71.1790) lr 1.7705e-03 eta 0:17:11
epoch [13/50] batch [240/319] time 0.086 (0.086) data 0.000 (0.003) loss 2.2514 (2.3610) teacher_loss 0.7642 (0.8425) loss_zs_kd 1.5009 (1.4931) loss_oracle 1.1223 (1.0906) acc 71.8750 (71.2240) lr 1.7705e-03 eta 0:17:07
epoch [13/50] batch [260/319] time 0.080 (0.086) data 0.000 (0.003) loss 2.2288 (2.3614) teacher_loss 0.7371 (0.8424) loss_zs_kd 1.4838 (1.4941) loss_oracle 1.0795 (1.0923) acc 71.8750 (70.9736) lr 1.7705e-03 eta 0:17:02
epoch [13/50] batch [280/319] time 0.086 (0.086) data 0.000 (0.002) loss 2.2870 (2.3618) teacher_loss 0.8529 (0.8438) loss_zs_kd 1.5746 (1.4907) loss_oracle 1.0262 (1.0933) acc 75.0000 (70.9040) lr 1.7705e-03 eta 0:16:54
epoch [13/50] batch [300/319] time 0.081 (0.085) data 0.000 (0.002) loss 2.4382 (2.3582) teacher_loss 0.8658 (0.8404) loss_zs_kd 1.6281 (1.4920) loss_oracle 1.1986 (1.0942) acc 65.6250 (70.9792) lr 1.7705e-03 eta 0:16:48
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,500
* accuracy: 57.1%
* error: 42.9%
* macro_f1: 47.5%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,515
* accuracy: 56.6%
* error: 43.4%
* macro_f1: 22.2%
******* Domain 2 best val acc:      60.0%, epoch: 9 *******
******* Domain 2 best val test acc: 54.4%, epoch: 9 *******
******* Domain 2 best test acc:     58.6%, epoch: 4 *******
epoch [14/50] batch [20/319] time 0.075 (0.113) data 0.001 (0.030) loss 2.2706 (2.3534) teacher_loss 0.7389 (0.8219) loss_zs_kd 1.8042 (1.6484) loss_oracle 1.0719 (1.1036) acc 78.1250 (71.8750) lr 1.7290e-03 eta 0:22:06
epoch [14/50] batch [40/319] time 0.080 (0.095) data 0.000 (0.015) loss 2.3717 (2.3507) teacher_loss 0.8111 (0.8163) loss_zs_kd 1.3487 (1.6206) loss_oracle 1.1349 (1.1129) acc 75.0000 (72.1875) lr 1.7290e-03 eta 0:18:41
epoch [14/50] batch [60/319] time 0.093 (0.091) data 0.001 (0.010) loss 2.4936 (2.3827) teacher_loss 0.8741 (0.8408) loss_zs_kd 1.9716 (1.5881) loss_oracle 1.1496 (1.1240) acc 68.7500 (70.5208) lr 1.7290e-03 eta 0:17:51
epoch [14/50] batch [80/319] time 0.075 (0.089) data 0.000 (0.008) loss 2.6375 (2.4063) teacher_loss 1.0307 (0.8519) loss_zs_kd 1.4997 (1.5476) loss_oracle 1.1887 (1.1375) acc 59.3750 (69.8828) lr 1.7290e-03 eta 0:17:20
epoch [14/50] batch [100/319] time 0.074 (0.087) data 0.000 (0.006) loss 2.2129 (2.4146) teacher_loss 0.6222 (0.8524) loss_zs_kd 1.4545 (1.5131) loss_oracle 1.1432 (1.1390) acc 81.2500 (69.6875) lr 1.7290e-03 eta 0:16:54
epoch [14/50] batch [120/319] time 0.075 (0.084) data 0.000 (0.005) loss 2.1690 (2.4169) teacher_loss 0.6315 (0.8524) loss_zs_kd 1.6005 (1.5040) loss_oracle 1.1814 (1.1429) acc 78.1250 (70.0000) lr 1.7290e-03 eta 0:16:25
epoch [14/50] batch [140/319] time 0.093 (0.084) data 0.000 (0.005) loss 2.5116 (2.4147) teacher_loss 0.9533 (0.8461) loss_zs_kd 1.6622 (1.5110) loss_oracle 1.1435 (1.1469) acc 71.8750 (70.4911) lr 1.7290e-03 eta 0:16:16
epoch [14/50] batch [160/319] time 0.085 (0.084) data 0.000 (0.004) loss 2.6677 (2.4253) teacher_loss 1.1438 (0.8558) loss_zs_kd 1.3837 (1.5165) loss_oracle 1.1378 (1.1487) acc 62.5000 (69.8828) lr 1.7290e-03 eta 0:16:15
epoch [14/50] batch [180/319] time 0.083 (0.084) data 0.000 (0.004) loss 2.3885 (2.4221) teacher_loss 0.7943 (0.8538) loss_zs_kd 1.4982 (1.5204) loss_oracle 1.1368 (1.1487) acc 71.8750 (70.0521) lr 1.7290e-03 eta 0:16:15
epoch [14/50] batch [200/319] time 0.069 (0.084) data 0.000 (0.003) loss 2.4309 (2.4227) teacher_loss 0.7768 (0.8521) loss_zs_kd 1.6042 (1.5242) loss_oracle 1.2438 (1.1502) acc 62.5000 (69.9844) lr 1.7290e-03 eta 0:16:13
epoch [14/50] batch [220/319] time 0.069 (0.084) data 0.000 (0.003) loss 2.4012 (2.4247) teacher_loss 0.8347 (0.8536) loss_zs_kd 1.4709 (1.5213) loss_oracle 1.1814 (1.1522) acc 75.0000 (70.0000) lr 1.7290e-03 eta 0:16:10
epoch [14/50] batch [240/319] time 0.083 (0.084) data 0.000 (0.003) loss 2.4861 (2.4211) teacher_loss 0.9669 (0.8503) loss_zs_kd 1.6340 (1.5179) loss_oracle 1.2085 (1.1530) acc 71.8750 (70.2474) lr 1.7290e-03 eta 0:16:07
epoch [14/50] batch [260/319] time 0.083 (0.083) data 0.000 (0.003) loss 2.3633 (2.4217) teacher_loss 0.8349 (0.8498) loss_zs_kd 1.3057 (1.5113) loss_oracle 1.1885 (1.1541) acc 78.1250 (70.3726) lr 1.7290e-03 eta 0:16:00
epoch [14/50] batch [280/319] time 0.081 (0.083) data 0.000 (0.002) loss 2.5729 (2.4208) teacher_loss 0.9546 (0.8484) loss_zs_kd 1.4942 (1.5112) loss_oracle 1.1573 (1.1535) acc 56.2500 (70.3460) lr 1.7290e-03 eta 0:15:56
epoch [14/50] batch [300/319] time 0.089 (0.083) data 0.000 (0.002) loss 2.5908 (2.4229) teacher_loss 1.0388 (0.8497) loss_zs_kd 1.5137 (1.5086) loss_oracle 1.1023 (1.1534) acc 56.2500 (70.3438) lr 1.7290e-03 eta 0:15:54
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,476
* accuracy: 56.6%
* error: 43.4%
* macro_f1: 48.5%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,560
* accuracy: 57.1%
* error: 42.9%
* macro_f1: 23.8%
******* Domain 2 best val acc:      60.0%, epoch: 9 *******
******* Domain 2 best val test acc: 54.4%, epoch: 9 *******
******* Domain 2 best test acc:     58.6%, epoch: 4 *******
epoch [15/50] batch [20/319] time 0.084 (0.115) data 0.000 (0.033) loss 2.7710 (2.4748) teacher_loss 1.2600 (0.9148) loss_zs_kd 1.4905 (1.4977) loss_oracle 1.1350 (1.1415) acc 46.8750 (67.9688) lr 1.6845e-03 eta 0:21:54
epoch [15/50] batch [40/319] time 0.073 (0.097) data 0.000 (0.017) loss 2.0698 (2.4265) teacher_loss 0.5632 (0.8599) loss_zs_kd 1.4220 (1.5046) loss_oracle 1.1038 (1.1364) acc 87.5000 (70.1562) lr 1.6845e-03 eta 0:18:29
epoch [15/50] batch [60/319] time 0.083 (0.091) data 0.001 (0.011) loss 2.5300 (2.4415) teacher_loss 0.9322 (0.8749) loss_zs_kd 1.6074 (1.4981) loss_oracle 1.1566 (1.1394) acc 65.6250 (69.5312) lr 1.6845e-03 eta 0:17:21
epoch [15/50] batch [80/319] time 0.089 (0.090) data 0.001 (0.008) loss 2.2559 (2.4364) teacher_loss 0.6613 (0.8624) loss_zs_kd 1.3664 (1.4955) loss_oracle 1.0962 (1.1418) acc 75.0000 (70.0391) lr 1.6845e-03 eta 0:17:00
epoch [15/50] batch [100/319] time 0.080 (0.088) data 0.000 (0.007) loss 2.3718 (2.4231) teacher_loss 0.7852 (0.8518) loss_zs_kd 1.2370 (1.4704) loss_oracle 1.1728 (1.1427) acc 75.0000 (70.6250) lr 1.6845e-03 eta 0:16:43
epoch [15/50] batch [120/319] time 0.087 (0.087) data 0.000 (0.006) loss 2.3186 (2.4278) teacher_loss 0.7693 (0.8579) loss_zs_kd 1.3993 (1.4539) loss_oracle 1.1497 (1.1425) acc 75.0000 (70.4688) lr 1.6845e-03 eta 0:16:29
epoch [15/50] batch [140/319] time 0.077 (0.086) data 0.000 (0.005) loss 2.4763 (2.4263) teacher_loss 0.8507 (0.8579) loss_zs_kd 1.6489 (1.4440) loss_oracle 1.2176 (1.1442) acc 65.6250 (70.3795) lr 1.6845e-03 eta 0:16:14
epoch [15/50] batch [160/319] time 0.080 (0.088) data 0.000 (0.004) loss 2.2204 (2.4166) teacher_loss 0.6400 (0.8516) loss_zs_kd 1.3507 (1.4394) loss_oracle 1.1556 (1.1427) acc 84.3750 (70.4883) lr 1.6845e-03 eta 0:16:34
epoch [15/50] batch [180/319] time 0.078 (0.087) data 0.000 (0.004) loss 2.4341 (2.4086) teacher_loss 0.8276 (0.8425) loss_zs_kd 1.3075 (1.4271) loss_oracle 1.1740 (1.1431) acc 71.8750 (70.9201) lr 1.6845e-03 eta 0:16:18
epoch [15/50] batch [200/319] time 0.091 (0.086) data 0.000 (0.004) loss 2.7563 (2.4279) teacher_loss 1.1389 (0.8593) loss_zs_kd 1.4298 (1.4218) loss_oracle 1.1182 (1.1432) acc 53.1250 (70.3594) lr 1.6845e-03 eta 0:16:10
epoch [15/50] batch [220/319] time 0.080 (0.086) data 0.000 (0.003) loss 2.8641 (2.4312) teacher_loss 1.2407 (0.8628) loss_zs_kd 1.4633 (1.4239) loss_oracle 1.1672 (1.1415) acc 56.2500 (70.1705) lr 1.6845e-03 eta 0:16:06
epoch [15/50] batch [240/319] time 0.086 (0.086) data 0.000 (0.003) loss 2.3629 (2.4284) teacher_loss 0.7566 (0.8582) loss_zs_kd 1.6548 (1.4287) loss_oracle 1.1880 (1.1411) acc 71.8750 (70.3906) lr 1.6845e-03 eta 0:16:02
epoch [15/50] batch [260/319] time 0.080 (0.086) data 0.000 (0.003) loss 2.5584 (2.4377) teacher_loss 0.9428 (0.8665) loss_zs_kd 1.4187 (1.4377) loss_oracle 1.1107 (1.1402) acc 71.8750 (70.1202) lr 1.6845e-03 eta 0:16:00
epoch [15/50] batch [280/319] time 0.081 (0.085) data 0.000 (0.003) loss 2.3259 (2.4426) teacher_loss 0.8115 (0.8723) loss_zs_kd 1.3033 (1.4327) loss_oracle 1.1501 (1.1395) acc 71.8750 (70.0112) lr 1.6845e-03 eta 0:15:57
epoch [15/50] batch [300/319] time 0.077 (0.085) data 0.000 (0.002) loss 2.5499 (2.4456) teacher_loss 1.0543 (0.8767) loss_zs_kd 1.1646 (1.4183) loss_oracle 1.1498 (1.1380) acc 59.3750 (69.8021) lr 1.6845e-03 eta 0:15:53
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,566
* accuracy: 58.6%
* error: 41.4%
* macro_f1: 51.7%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,720
* accuracy: 58.8%
* error: 41.2%
* macro_f1: 25.0%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      60.0%, epoch: 9 *******
******* Domain 2 best val test acc: 54.4%, epoch: 9 *******
******* Domain 2 best test acc:     58.8%, epoch: 15 *******
epoch [16/50] batch [20/319] time 0.090 (0.124) data 0.000 (0.030) loss 2.2552 (2.4686) teacher_loss 0.6957 (0.9236) loss_zs_kd 1.3618 (1.3258) loss_oracle 1.1346 (1.1203) acc 68.7500 (67.9688) lr 1.6374e-03 eta 0:23:02
epoch [16/50] batch [40/319] time 0.076 (0.102) data 0.000 (0.015) loss 2.6614 (2.4634) teacher_loss 1.0623 (0.9206) loss_zs_kd 1.3157 (1.3370) loss_oracle 1.1383 (1.1174) acc 62.5000 (67.7344) lr 1.6374e-03 eta 0:18:57
epoch [16/50] batch [60/319] time 0.074 (0.094) data 0.001 (0.010) loss 2.2146 (2.4535) teacher_loss 0.6403 (0.9029) loss_zs_kd 1.3816 (1.3544) loss_oracle 1.1060 (1.1194) acc 78.1250 (68.4896) lr 1.6374e-03 eta 0:17:25
epoch [16/50] batch [80/319] time 0.077 (0.090) data 0.001 (0.008) loss 2.2743 (2.4471) teacher_loss 0.7575 (0.8922) loss_zs_kd 1.3265 (1.3492) loss_oracle 1.1094 (1.1195) acc 65.6250 (68.7500) lr 1.6374e-03 eta 0:16:38
epoch [16/50] batch [100/319] time 0.078 (0.088) data 0.000 (0.006) loss 2.6270 (2.4470) teacher_loss 1.0730 (0.8889) loss_zs_kd 1.2305 (1.3697) loss_oracle 1.1327 (1.1185) acc 62.5000 (68.7812) lr 1.6374e-03 eta 0:16:17
epoch [16/50] batch [120/319] time 0.086 (0.087) data 0.000 (0.005) loss 2.3985 (2.4439) teacher_loss 0.7764 (0.8806) loss_zs_kd 1.6606 (1.3914) loss_oracle 1.0949 (1.1176) acc 71.8750 (69.2448) lr 1.6374e-03 eta 0:16:04
epoch [16/50] batch [140/319] time 0.073 (0.087) data 0.000 (0.005) loss 2.2989 (2.4623) teacher_loss 0.7194 (0.8950) loss_zs_kd 1.7334 (1.4031) loss_oracle 1.1088 (1.1185) acc 84.3750 (68.6384) lr 1.6374e-03 eta 0:15:54
epoch [16/50] batch [160/319] time 0.071 (0.086) data 0.000 (0.004) loss 2.2494 (2.4661) teacher_loss 0.6967 (0.8995) loss_zs_kd 1.3581 (1.4124) loss_oracle 1.1067 (1.1175) acc 81.2500 (68.4961) lr 1.6374e-03 eta 0:15:45
epoch [16/50] batch [180/319] time 0.075 (0.085) data 0.000 (0.004) loss 2.2141 (2.4577) teacher_loss 0.7247 (0.8922) loss_zs_kd 1.3338 (1.4100) loss_oracle 1.0520 (1.1160) acc 75.0000 (68.8194) lr 1.6374e-03 eta 0:15:32
epoch [16/50] batch [200/319] time 0.079 (0.084) data 0.000 (0.003) loss 2.1898 (2.4478) teacher_loss 0.6305 (0.8831) loss_zs_kd 1.2987 (1.4158) loss_oracle 1.1410 (1.1148) acc 78.1250 (69.0469) lr 1.6374e-03 eta 0:15:24
epoch [16/50] batch [220/319] time 0.085 (0.084) data 0.000 (0.003) loss 2.4059 (2.4444) teacher_loss 0.8737 (0.8807) loss_zs_kd 1.3710 (1.4283) loss_oracle 1.0835 (1.1137) acc 65.6250 (69.1761) lr 1.6374e-03 eta 0:15:22
epoch [16/50] batch [240/319] time 0.080 (0.084) data 0.000 (0.003) loss 2.3217 (2.4414) teacher_loss 0.7884 (0.8796) loss_zs_kd 1.9613 (1.4368) loss_oracle 1.0845 (1.1111) acc 78.1250 (69.3229) lr 1.6374e-03 eta 0:15:19
epoch [16/50] batch [260/319] time 0.087 (0.084) data 0.000 (0.003) loss 2.2274 (2.4363) teacher_loss 0.6751 (0.8761) loss_zs_kd 1.3827 (1.4313) loss_oracle 1.0936 (1.1098) acc 75.0000 (69.4111) lr 1.6374e-03 eta 0:15:19
epoch [16/50] batch [280/319] time 0.083 (0.084) data 0.000 (0.002) loss 2.5507 (2.4316) teacher_loss 0.9995 (0.8717) loss_zs_kd 1.6183 (1.4402) loss_oracle 1.0769 (1.1087) acc 65.6250 (69.5871) lr 1.6374e-03 eta 0:15:18
epoch [16/50] batch [300/319] time 0.097 (0.085) data 0.000 (0.002) loss 2.3259 (2.4261) teacher_loss 0.8593 (0.8680) loss_zs_kd 1.6472 (1.4576) loss_oracle 0.9988 (1.1070) acc 68.7500 (69.6667) lr 1.6374e-03 eta 0:15:19
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,498
* accuracy: 57.1%
* error: 42.9%
* macro_f1: 49.0%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,752
* accuracy: 59.1%
* error: 40.9%
* macro_f1: 25.3%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      60.0%, epoch: 9 *******
******* Domain 2 best val test acc: 54.4%, epoch: 9 *******
******* Domain 2 best test acc:     59.1%, epoch: 16 *******
epoch [17/50] batch [20/319] time 0.079 (0.126) data 0.000 (0.039) loss 2.3156 (2.3672) teacher_loss 0.7932 (0.8204) loss_zs_kd 1.9174 (1.6193) loss_oracle 1.0013 (1.0747) acc 75.0000 (70.9375) lr 1.5878e-03 eta 0:22:46
epoch [17/50] batch [40/319] time 0.076 (0.104) data 0.000 (0.020) loss 2.4309 (2.3998) teacher_loss 0.9064 (0.8602) loss_zs_kd 1.6953 (1.5857) loss_oracle 1.0814 (1.0765) acc 62.5000 (69.6875) lr 1.5878e-03 eta 0:18:40
epoch [17/50] batch [60/319] time 0.080 (0.095) data 0.000 (0.013) loss 2.5251 (2.4032) teacher_loss 0.9491 (0.8594) loss_zs_kd 1.4439 (1.5679) loss_oracle 1.1172 (1.0791) acc 59.3750 (69.2708) lr 1.5878e-03 eta 0:17:06
epoch [17/50] batch [80/319] time 0.085 (0.092) data 0.000 (0.010) loss 2.1712 (2.3814) teacher_loss 0.6259 (0.8381) loss_zs_kd 1.9608 (1.5798) loss_oracle 1.0909 (1.0791) acc 68.7500 (69.8047) lr 1.5878e-03 eta 0:16:30
epoch [17/50] batch [100/319] time 0.077 (0.089) data 0.000 (0.008) loss 2.1898 (2.3829) teacher_loss 0.6425 (0.8418) loss_zs_kd 1.7555 (1.5959) loss_oracle 1.0936 (1.0786) acc 84.3750 (69.7812) lr 1.5878e-03 eta 0:15:59
epoch [17/50] batch [120/319] time 0.078 (0.090) data 0.000 (0.007) loss 2.4930 (2.4126) teacher_loss 0.9471 (0.8719) loss_zs_kd 1.4956 (1.5818) loss_oracle 1.0807 (1.0790) acc 56.2500 (68.6198) lr 1.5878e-03 eta 0:16:03
epoch [17/50] batch [140/319] time 0.083 (0.089) data 0.000 (0.006) loss 2.5031 (2.4254) teacher_loss 0.9417 (0.8847) loss_zs_kd 1.6171 (1.5627) loss_oracle 1.0877 (1.0761) acc 59.3750 (68.2143) lr 1.5878e-03 eta 0:15:47
epoch [17/50] batch [160/319] time 0.072 (0.088) data 0.000 (0.005) loss 2.2119 (2.4274) teacher_loss 0.6934 (0.8862) loss_zs_kd 1.3939 (1.5645) loss_oracle 1.0652 (1.0757) acc 75.0000 (68.3203) lr 1.5878e-03 eta 0:15:36
epoch [17/50] batch [180/319] time 0.088 (0.087) data 0.000 (0.005) loss 2.2628 (2.4245) teacher_loss 0.7733 (0.8838) loss_zs_kd 1.6942 (1.5686) loss_oracle 1.0290 (1.0749) acc 71.8750 (68.5938) lr 1.5878e-03 eta 0:15:30
epoch [17/50] batch [200/319] time 0.081 (0.087) data 0.000 (0.004) loss 2.1323 (2.4265) teacher_loss 0.5789 (0.8863) loss_zs_kd 1.7606 (1.5687) loss_oracle 1.0667 (1.0742) acc 71.8750 (68.6094) lr 1.5878e-03 eta 0:15:25
epoch [17/50] batch [220/319] time 0.088 (0.086) data 0.000 (0.004) loss 2.4306 (2.4318) teacher_loss 0.8682 (0.8926) loss_zs_kd 1.6846 (1.5750) loss_oracle 1.1029 (1.0726) acc 62.5000 (68.5795) lr 1.5878e-03 eta 0:15:17
epoch [17/50] batch [240/319] time 0.077 (0.086) data 0.000 (0.003) loss 2.4817 (2.4263) teacher_loss 0.9091 (0.8872) loss_zs_kd 1.6488 (1.5686) loss_oracle 1.1035 (1.0729) acc 62.5000 (68.8932) lr 1.5878e-03 eta 0:15:11
epoch [17/50] batch [260/319] time 0.081 (0.085) data 0.000 (0.003) loss 2.4061 (2.4226) teacher_loss 0.8784 (0.8826) loss_zs_kd 1.5270 (1.5620) loss_oracle 1.1105 (1.0741) acc 71.8750 (69.0385) lr 1.5878e-03 eta 0:15:01
epoch [17/50] batch [280/319] time 0.076 (0.085) data 0.000 (0.003) loss 2.0241 (2.4163) teacher_loss 0.5066 (0.8743) loss_zs_kd 1.5296 (1.5556) loss_oracle 1.0816 (1.0756) acc 81.2500 (69.3862) lr 1.5878e-03 eta 0:14:58
epoch [17/50] batch [300/319] time 0.084 (0.085) data 0.000 (0.003) loss 2.4016 (2.4168) teacher_loss 0.8702 (0.8733) loss_zs_kd 1.5263 (1.5501) loss_oracle 1.0658 (1.0763) acc 68.7500 (69.3438) lr 1.5878e-03 eta 0:14:52
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,572
* accuracy: 58.7%
* error: 41.3%
* macro_f1: 49.7%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,040
* accuracy: 51.8%
* error: 48.2%
* macro_f1: 24.7%
******* Domain 2 best val acc:      60.0%, epoch: 9 *******
******* Domain 2 best val test acc: 54.4%, epoch: 9 *******
******* Domain 2 best test acc:     59.1%, epoch: 16 *******
epoch [18/50] batch [20/319] time 0.083 (0.122) data 0.000 (0.039) loss 2.3032 (2.3439) teacher_loss 0.7296 (0.8012) loss_zs_kd 1.3014 (1.3675) loss_oracle 1.0628 (1.0867) acc 78.1250 (72.3438) lr 1.5358e-03 eta 0:21:23
epoch [18/50] batch [40/319] time 0.082 (0.101) data 0.000 (0.020) loss 2.1716 (2.3337) teacher_loss 0.6401 (0.7826) loss_zs_kd 1.5332 (1.4539) loss_oracle 1.0954 (1.0900) acc 75.0000 (72.9688) lr 1.5358e-03 eta 0:17:36
epoch [18/50] batch [60/319] time 0.070 (0.094) data 0.000 (0.013) loss 2.3966 (2.3400) teacher_loss 0.8896 (0.7905) loss_zs_kd 1.4399 (1.4565) loss_oracle 1.0506 (1.0874) acc 75.0000 (72.6042) lr 1.5358e-03 eta 0:16:24
epoch [18/50] batch [80/319] time 0.080 (0.090) data 0.000 (0.010) loss 2.2129 (2.3513) teacher_loss 0.6632 (0.8039) loss_zs_kd 1.1841 (1.4394) loss_oracle 1.1159 (1.0872) acc 68.7500 (71.8359) lr 1.5358e-03 eta 0:15:44
epoch [18/50] batch [100/319] time 0.076 (0.088) data 0.000 (0.008) loss 2.0811 (2.3492) teacher_loss 0.5670 (0.8015) loss_zs_kd 1.6797 (1.4308) loss_oracle 1.0781 (1.0889) acc 81.2500 (71.8750) lr 1.5358e-03 eta 0:15:16
epoch [18/50] batch [120/319] time 0.063 (0.086) data 0.000 (0.007) loss 2.3332 (2.3600) teacher_loss 0.7811 (0.8134) loss_zs_kd 1.8728 (1.4518) loss_oracle 1.1255 (1.0888) acc 71.8750 (71.4062) lr 1.5358e-03 eta 0:14:57
epoch [18/50] batch [140/319] time 0.072 (0.085) data 0.000 (0.006) loss 2.2288 (2.3569) teacher_loss 0.6212 (0.8107) loss_zs_kd 1.6354 (1.4479) loss_oracle 1.1046 (1.0878) acc 75.0000 (71.4955) lr 1.5358e-03 eta 0:14:40
epoch [18/50] batch [160/319] time 0.089 (0.084) data 0.000 (0.005) loss 2.3872 (2.3661) teacher_loss 0.8361 (0.8207) loss_zs_kd 1.3694 (1.4296) loss_oracle 1.1090 (1.0879) acc 71.8750 (71.0938) lr 1.5358e-03 eta 0:14:29
epoch [18/50] batch [180/319] time 0.083 (0.084) data 0.000 (0.005) loss 2.5131 (2.3611) teacher_loss 0.9485 (0.8154) loss_zs_kd 1.5146 (1.4285) loss_oracle 1.0957 (1.0875) acc 62.5000 (71.2847) lr 1.5358e-03 eta 0:14:25
epoch [18/50] batch [200/319] time 0.078 (0.083) data 0.000 (0.004) loss 2.3760 (2.3610) teacher_loss 0.8216 (0.8174) loss_zs_kd 1.5107 (1.4318) loss_oracle 1.0635 (1.0851) acc 71.8750 (71.2031) lr 1.5358e-03 eta 0:14:22
epoch [18/50] batch [220/319] time 0.072 (0.083) data 0.000 (0.004) loss 2.6213 (2.3624) teacher_loss 1.0850 (0.8192) loss_zs_kd 1.4140 (1.4392) loss_oracle 1.0857 (1.0845) acc 59.3750 (71.0938) lr 1.5358e-03 eta 0:14:18
epoch [18/50] batch [240/319] time 0.083 (0.083) data 0.000 (0.003) loss 2.3194 (2.3604) teacher_loss 0.7045 (0.8175) loss_zs_kd 1.3481 (1.4306) loss_oracle 1.0960 (1.0844) acc 71.8750 (71.1458) lr 1.5358e-03 eta 0:14:17
epoch [18/50] batch [260/319] time 0.087 (0.083) data 0.000 (0.003) loss 2.5267 (2.3601) teacher_loss 0.9436 (0.8185) loss_zs_kd 1.6651 (1.4322) loss_oracle 1.0343 (1.0840) acc 68.7500 (71.1418) lr 1.5358e-03 eta 0:14:15
epoch [18/50] batch [280/319] time 0.077 (0.083) data 0.000 (0.003) loss 2.5695 (2.3654) teacher_loss 1.0013 (0.8237) loss_zs_kd 1.3156 (1.4287) loss_oracle 1.1312 (1.0844) acc 65.6250 (71.0826) lr 1.5358e-03 eta 0:14:14
epoch [18/50] batch [300/319] time 0.120 (0.084) data 0.000 (0.003) loss 2.0385 (2.3664) teacher_loss 0.5242 (0.8245) loss_zs_kd 1.7055 (1.4339) loss_oracle 1.0599 (1.0845) acc 84.3750 (71.0417) lr 1.5358e-03 eta 0:14:24
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,659
* accuracy: 60.7%
* error: 39.3%
* macro_f1: 51.6%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,484
* accuracy: 56.3%
* error: 43.7%
* macro_f1: 24.2%
******* Domain 2 best val acc:      60.7%, epoch: 18 *******
******* Domain 2 best val test acc: 56.3%, epoch: 18 *******
******* Domain 2 best test acc:     59.1%, epoch: 16 *******
epoch [19/50] batch [20/319] time 0.073 (0.120) data 0.000 (0.035) loss 2.0739 (2.3272) teacher_loss 0.5213 (0.7824) loss_zs_kd 1.2665 (1.2778) loss_oracle 1.0718 (1.0848) acc 87.5000 (73.4375) lr 1.4818e-03 eta 0:20:24
epoch [19/50] batch [40/319] time 0.086 (0.101) data 0.000 (0.017) loss 2.3276 (2.3578) teacher_loss 0.7607 (0.8044) loss_zs_kd 1.3851 (1.3411) loss_oracle 1.1105 (1.0868) acc 68.7500 (71.7969) lr 1.4818e-03 eta 0:17:03
epoch [19/50] batch [60/319] time 0.085 (0.095) data 0.000 (0.012) loss 2.3942 (2.3578) teacher_loss 0.8769 (0.8076) loss_zs_kd 1.1853 (1.3742) loss_oracle 1.0709 (1.0867) acc 68.7500 (71.0938) lr 1.4818e-03 eta 0:16:05
epoch [19/50] batch [80/319] time 0.102 (0.093) data 0.000 (0.009) loss 2.4710 (2.3540) teacher_loss 0.9289 (0.8112) loss_zs_kd 1.5440 (1.3681) loss_oracle 1.0315 (1.0787) acc 65.6250 (70.9375) lr 1.4818e-03 eta 0:15:41
epoch [19/50] batch [100/319] time 0.073 (0.093) data 0.000 (0.007) loss 2.2865 (2.3631) teacher_loss 0.7569 (0.8217) loss_zs_kd 1.5395 (1.3919) loss_oracle 1.0802 (1.0806) acc 78.1250 (70.8750) lr 1.4818e-03 eta 0:15:41
epoch [19/50] batch [120/319] time 0.093 (0.091) data 0.000 (0.006) loss 2.6527 (2.3657) teacher_loss 1.1154 (0.8231) loss_zs_kd 1.2205 (1.4058) loss_oracle 1.0675 (1.0826) acc 56.2500 (70.9635) lr 1.4818e-03 eta 0:15:15
epoch [19/50] batch [140/319] time 0.095 (0.090) data 0.000 (0.005) loss 2.1447 (2.3699) teacher_loss 0.6717 (0.8273) loss_zs_kd 1.3241 (1.4041) loss_oracle 1.0633 (1.0836) acc 71.8750 (71.0045) lr 1.4818e-03 eta 0:15:01
epoch [19/50] batch [160/319] time 0.079 (0.088) data 0.000 (0.005) loss 2.5422 (2.3727) teacher_loss 0.9720 (0.8308) loss_zs_kd 1.4268 (1.4069) loss_oracle 1.1360 (1.0839) acc 59.3750 (70.9570) lr 1.4818e-03 eta 0:14:48
epoch [19/50] batch [180/319] time 0.078 (0.087) data 0.000 (0.004) loss 2.2760 (2.3748) teacher_loss 0.7124 (0.8322) loss_zs_kd 1.5673 (1.4132) loss_oracle 1.0571 (1.0831) acc 78.1250 (70.9549) lr 1.4818e-03 eta 0:14:34
epoch [19/50] batch [200/319] time 0.077 (0.087) data 0.000 (0.004) loss 2.1879 (2.3760) teacher_loss 0.7178 (0.8345) loss_zs_kd 1.4479 (1.4337) loss_oracle 1.0213 (1.0819) acc 75.0000 (71.0000) lr 1.4818e-03 eta 0:14:28
epoch [19/50] batch [220/319] time 0.077 (0.086) data 0.000 (0.003) loss 2.6421 (2.3729) teacher_loss 1.0965 (0.8304) loss_zs_kd 1.4714 (1.4491) loss_oracle 1.0779 (1.0807) acc 65.6250 (71.1222) lr 1.4818e-03 eta 0:14:20
epoch [19/50] batch [240/319] time 0.078 (0.085) data 0.000 (0.003) loss 2.5038 (2.3733) teacher_loss 0.9247 (0.8305) loss_zs_kd 1.3097 (1.4608) loss_oracle 1.0893 (1.0809) acc 68.7500 (71.2891) lr 1.4818e-03 eta 0:14:11
epoch [19/50] batch [260/319] time 0.076 (0.085) data 0.000 (0.003) loss 2.6481 (2.3809) teacher_loss 1.1979 (0.8371) loss_zs_kd 1.1393 (1.4581) loss_oracle 1.0501 (1.0824) acc 62.5000 (71.1298) lr 1.4818e-03 eta 0:14:05
epoch [19/50] batch [280/319] time 0.073 (0.085) data 0.000 (0.003) loss 2.1318 (2.3776) teacher_loss 0.5658 (0.8326) loss_zs_kd 1.3114 (1.4510) loss_oracle 1.0406 (1.0836) acc 84.3750 (71.3504) lr 1.4818e-03 eta 0:13:59
epoch [19/50] batch [300/319] time 0.077 (0.084) data 0.000 (0.003) loss 2.4485 (2.3777) teacher_loss 0.9228 (0.8315) loss_zs_kd 1.2322 (1.4441) loss_oracle 1.0678 (1.0853) acc 71.8750 (71.3021) lr 1.4818e-03 eta 0:13:54
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,701
* accuracy: 61.7%
* error: 38.3%
* macro_f1: 55.2%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,302
* accuracy: 54.5%
* error: 45.5%
* macro_f1: 26.1%
******* Domain 2 best val acc:      61.7%, epoch: 19 *******
******* Domain 2 best val test acc: 54.5%, epoch: 19 *******
******* Domain 2 best test acc:     59.1%, epoch: 16 *******
epoch [20/50] batch [20/319] time 0.085 (0.114) data 0.000 (0.026) loss 2.2736 (2.4032) teacher_loss 0.7392 (0.8368) loss_zs_kd 1.3832 (1.4605) loss_oracle 1.0759 (1.0999) acc 78.1250 (71.2500) lr 1.4258e-03 eta 0:18:41
epoch [20/50] batch [40/319] time 0.084 (0.099) data 0.001 (0.013) loss 2.3012 (2.3793) teacher_loss 0.7545 (0.8238) loss_zs_kd 1.2980 (1.4570) loss_oracle 1.1101 (1.0992) acc 78.1250 (71.8750) lr 1.4258e-03 eta 0:16:12
epoch [20/50] batch [60/319] time 0.077 (0.094) data 0.000 (0.009) loss 2.1055 (2.4006) teacher_loss 0.5622 (0.8502) loss_zs_kd 1.1728 (1.4551) loss_oracle 1.1131 (1.1025) acc 75.0000 (70.8333) lr 1.4258e-03 eta 0:15:23
epoch [20/50] batch [80/319] time 0.083 (0.090) data 0.000 (0.007) loss 2.3669 (2.3872) teacher_loss 0.8009 (0.8360) loss_zs_kd 1.7398 (1.4665) loss_oracle 1.1264 (1.1037) acc 71.8750 (71.1719) lr 1.4258e-03 eta 0:14:46
epoch [20/50] batch [100/319] time 0.077 (0.088) data 0.000 (0.005) loss 2.2631 (2.3770) teacher_loss 0.6843 (0.8227) loss_zs_kd 1.5215 (1.4548) loss_oracle 1.1234 (1.1054) acc 75.0000 (71.4688) lr 1.4258e-03 eta 0:14:25
epoch [20/50] batch [120/319] time 0.081 (0.087) data 0.000 (0.005) loss 2.1691 (2.3615) teacher_loss 0.5883 (0.8104) loss_zs_kd 1.5726 (1.4560) loss_oracle 1.1651 (1.1017) acc 78.1250 (71.7448) lr 1.4258e-03 eta 0:14:14
epoch [20/50] batch [140/319] time 0.071 (0.086) data 0.000 (0.004) loss 2.2704 (2.3612) teacher_loss 0.7158 (0.8133) loss_zs_kd 1.4524 (1.4642) loss_oracle 1.0966 (1.1006) acc 78.1250 (71.7857) lr 1.4258e-03 eta 0:13:58
epoch [20/50] batch [160/319] time 0.078 (0.085) data 0.000 (0.003) loss 2.2573 (2.3607) teacher_loss 0.6574 (0.8107) loss_zs_kd 1.6318 (1.4698) loss_oracle 1.1447 (1.1022) acc 71.8750 (71.6016) lr 1.4258e-03 eta 0:13:45
epoch [20/50] batch [180/319] time 0.090 (0.084) data 0.000 (0.003) loss 2.5444 (2.3740) teacher_loss 0.9790 (0.8214) loss_zs_kd 1.5949 (1.4702) loss_oracle 1.1268 (1.1029) acc 59.3750 (71.1111) lr 1.4258e-03 eta 0:13:37
epoch [20/50] batch [200/319] time 0.080 (0.084) data 0.000 (0.003) loss 2.3062 (2.3748) teacher_loss 0.7825 (0.8217) loss_zs_kd 1.7537 (1.4720) loss_oracle 1.1153 (1.1034) acc 84.3750 (71.2656) lr 1.4258e-03 eta 0:13:30
epoch [20/50] batch [220/319] time 0.075 (0.083) data 0.000 (0.003) loss 2.3977 (2.3726) teacher_loss 0.8193 (0.8208) loss_zs_kd 1.5031 (1.4827) loss_oracle 1.1184 (1.1023) acc 71.8750 (71.1506) lr 1.4258e-03 eta 0:13:23
epoch [20/50] batch [240/319] time 0.080 (0.083) data 0.000 (0.002) loss 2.4256 (2.3744) teacher_loss 0.8824 (0.8226) loss_zs_kd 1.4332 (1.4887) loss_oracle 1.0657 (1.1012) acc 62.5000 (71.1589) lr 1.4258e-03 eta 0:13:18
epoch [20/50] batch [260/319] time 0.079 (0.082) data 0.000 (0.002) loss 2.5489 (2.3738) teacher_loss 0.9877 (0.8221) loss_zs_kd 1.2570 (1.4904) loss_oracle 1.0819 (1.1004) acc 68.7500 (71.2981) lr 1.4258e-03 eta 0:13:13
epoch [20/50] batch [280/319] time 0.074 (0.084) data 0.000 (0.002) loss 2.2550 (2.3740) teacher_loss 0.6903 (0.8227) loss_zs_kd 1.3808 (1.4850) loss_oracle 1.1265 (1.1005) acc 81.2500 (71.3393) lr 1.4258e-03 eta 0:13:24
epoch [20/50] batch [300/319] time 0.079 (0.083) data 0.000 (0.002) loss 2.5843 (2.3752) teacher_loss 1.0663 (0.8239) loss_zs_kd 1.6701 (1.4816) loss_oracle 1.0984 (1.1004) acc 65.6250 (71.3438) lr 1.4258e-03 eta 0:13:18
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,648
* accuracy: 60.5%
* error: 39.5%
* macro_f1: 53.0%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,679
* accuracy: 58.3%
* error: 41.7%
* macro_f1: 25.1%
******* Domain 2 best val acc:      61.7%, epoch: 19 *******
******* Domain 2 best val test acc: 54.5%, epoch: 19 *******
******* Domain 2 best test acc:     59.1%, epoch: 16 *******
epoch [21/50] batch [20/319] time 0.072 (0.113) data 0.000 (0.026) loss 2.1835 (2.3487) teacher_loss 0.5897 (0.7912) loss_zs_kd 1.6577 (1.4925) loss_oracle 1.1174 (1.0975) acc 75.0000 (72.8125) lr 1.3681e-03 eta 0:18:03
epoch [21/50] batch [40/319] time 0.084 (0.098) data 0.000 (0.013) loss 2.5607 (2.3825) teacher_loss 1.0074 (0.8283) loss_zs_kd 1.2357 (1.4807) loss_oracle 1.0956 (1.1053) acc 71.8750 (70.3125) lr 1.3681e-03 eta 0:15:37
epoch [21/50] batch [60/319] time 0.080 (0.094) data 0.000 (0.009) loss 2.3773 (2.3837) teacher_loss 0.7842 (0.8271) loss_zs_kd 1.5354 (1.4692) loss_oracle 1.1338 (1.1073) acc 68.7500 (70.7812) lr 1.3681e-03 eta 0:14:50
epoch [21/50] batch [80/319] time 0.101 (0.095) data 0.002 (0.007) loss 2.1562 (2.3927) teacher_loss 0.5685 (0.8369) loss_zs_kd 1.2727 (1.4286) loss_oracle 1.1215 (1.1068) acc 78.1250 (70.1562) lr 1.3681e-03 eta 0:15:04
epoch [21/50] batch [100/319] time 0.074 (0.093) data 0.000 (0.005) loss 2.1547 (2.3868) teacher_loss 0.6020 (0.8314) loss_zs_kd 1.5084 (1.4073) loss_oracle 1.0965 (1.1051) acc 75.0000 (70.5625) lr 1.3681e-03 eta 0:14:37
epoch [21/50] batch [120/319] time 0.081 (0.091) data 0.000 (0.005) loss 2.1974 (2.3796) teacher_loss 0.6322 (0.8257) loss_zs_kd 1.4331 (1.4206) loss_oracle 1.1207 (1.1034) acc 81.2500 (70.9115) lr 1.3681e-03 eta 0:14:16
epoch [21/50] batch [140/319] time 0.071 (0.089) data 0.000 (0.004) loss 2.3495 (2.3725) teacher_loss 0.8045 (0.8181) loss_zs_kd 1.7021 (1.4352) loss_oracle 1.0928 (1.1042) acc 75.0000 (71.4955) lr 1.3681e-03 eta 0:14:01
epoch [21/50] batch [160/319] time 0.075 (0.088) data 0.000 (0.003) loss 2.1814 (2.3757) teacher_loss 0.7171 (0.8225) loss_zs_kd 1.5852 (1.4464) loss_oracle 1.0109 (1.1015) acc 78.1250 (71.3281) lr 1.3681e-03 eta 0:13:48
epoch [21/50] batch [180/319] time 0.081 (0.087) data 0.000 (0.003) loss 2.4945 (2.3705) teacher_loss 0.9599 (0.8183) loss_zs_kd 1.5578 (1.4619) loss_oracle 1.0703 (1.0996) acc 68.7500 (71.5278) lr 1.3681e-03 eta 0:13:41
epoch [21/50] batch [200/319] time 0.081 (0.087) data 0.000 (0.003) loss 2.6941 (2.3726) teacher_loss 1.0961 (0.8215) loss_zs_kd 1.6080 (1.4596) loss_oracle 1.1222 (1.0983) acc 59.3750 (71.3125) lr 1.3681e-03 eta 0:13:35
epoch [21/50] batch [220/319] time 0.084 (0.087) data 0.000 (0.003) loss 2.2649 (2.3735) teacher_loss 0.7919 (0.8232) loss_zs_kd 1.2187 (1.4547) loss_oracle 1.0180 (1.0961) acc 75.0000 (71.1932) lr 1.3681e-03 eta 0:13:30
epoch [21/50] batch [240/319] time 0.086 (0.086) data 0.000 (0.002) loss 2.2728 (2.3752) teacher_loss 0.6991 (0.8251) loss_zs_kd 1.5855 (1.4517) loss_oracle 1.0811 (1.0964) acc 75.0000 (71.1328) lr 1.3681e-03 eta 0:13:26
epoch [21/50] batch [260/319] time 0.081 (0.086) data 0.001 (0.002) loss 2.4189 (2.3792) teacher_loss 0.9434 (0.8309) loss_zs_kd 1.7417 (1.4581) loss_oracle 1.0558 (1.0946) acc 65.6250 (70.8053) lr 1.3681e-03 eta 0:13:19
epoch [21/50] batch [280/319] time 0.085 (0.085) data 0.000 (0.002) loss 2.3606 (2.3806) teacher_loss 0.8234 (0.8330) loss_zs_kd 1.4213 (1.4605) loss_oracle 1.0608 (1.0933) acc 65.6250 (70.6696) lr 1.3681e-03 eta 0:13:12
epoch [21/50] batch [300/319] time 0.079 (0.085) data 0.000 (0.002) loss 2.2966 (2.3766) teacher_loss 0.7353 (0.8295) loss_zs_kd 1.2274 (1.4544) loss_oracle 1.1221 (1.0928) acc 71.8750 (70.9479) lr 1.3681e-03 eta 0:13:06
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,675
* accuracy: 61.1%
* error: 38.9%
* macro_f1: 54.6%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,548
* accuracy: 57.0%
* error: 43.0%
* macro_f1: 25.5%
******* Domain 2 best val acc:      61.7%, epoch: 19 *******
******* Domain 2 best val test acc: 54.5%, epoch: 19 *******
******* Domain 2 best test acc:     59.1%, epoch: 16 *******
epoch [22/50] batch [20/319] time 0.077 (0.120) data 0.000 (0.035) loss 2.1940 (2.3222) teacher_loss 0.6990 (0.7967) loss_zs_kd 1.6529 (1.3984) loss_oracle 1.1170 (1.0707) acc 75.0000 (74.2188) lr 1.3090e-03 eta 0:18:32
epoch [22/50] batch [40/319] time 0.077 (0.099) data 0.000 (0.018) loss 2.3361 (2.3062) teacher_loss 0.7951 (0.7714) loss_zs_kd 1.6465 (1.4080) loss_oracle 1.1168 (1.0878) acc 75.0000 (74.2969) lr 1.3090e-03 eta 0:15:10
epoch [22/50] batch [60/319] time 0.081 (0.092) data 0.000 (0.012) loss 2.2057 (2.3172) teacher_loss 0.6884 (0.7857) loss_zs_kd 1.4029 (1.4046) loss_oracle 1.0856 (1.0851) acc 81.2500 (74.4271) lr 1.3090e-03 eta 0:14:03
epoch [22/50] batch [80/319] time 0.084 (0.090) data 0.000 (0.009) loss 2.8706 (2.3351) teacher_loss 1.3925 (0.8033) loss_zs_kd 1.4126 (1.3964) loss_oracle 1.1226 (1.0856) acc 50.0000 (73.1641) lr 1.3090e-03 eta 0:13:42
epoch [22/50] batch [100/319] time 0.086 (0.089) data 0.000 (0.007) loss 2.2866 (2.3431) teacher_loss 0.7259 (0.8084) loss_zs_kd 1.2580 (1.3869) loss_oracle 1.1429 (1.0886) acc 71.8750 (72.0938) lr 1.3090e-03 eta 0:13:31
epoch [22/50] batch [120/319] time 0.083 (0.087) data 0.000 (0.006) loss 2.7923 (2.3559) teacher_loss 1.2563 (0.8201) loss_zs_kd 1.3912 (1.3904) loss_oracle 1.1131 (1.0910) acc 65.6250 (72.0052) lr 1.3090e-03 eta 0:13:16
epoch [22/50] batch [140/319] time 0.082 (0.087) data 0.000 (0.005) loss 2.4560 (2.3674) teacher_loss 0.9523 (0.8308) loss_zs_kd 1.2076 (1.4023) loss_oracle 0.9842 (1.0912) acc 65.6250 (71.4509) lr 1.3090e-03 eta 0:13:11
epoch [22/50] batch [160/319] time 0.086 (0.086) data 0.000 (0.005) loss 2.4638 (2.3717) teacher_loss 0.9369 (0.8357) loss_zs_kd 1.1574 (1.3919) loss_oracle 1.1596 (1.0922) acc 81.2500 (71.4062) lr 1.3090e-03 eta 0:13:02
epoch [22/50] batch [180/319] time 0.084 (0.086) data 0.001 (0.004) loss 2.8773 (2.3665) teacher_loss 1.3453 (0.8299) loss_zs_kd 1.4336 (1.3873) loss_oracle 1.0795 (1.0927) acc 53.1250 (71.4757) lr 1.3090e-03 eta 0:12:58
epoch [22/50] batch [200/319] time 0.081 (0.085) data 0.000 (0.004) loss 2.3603 (2.3677) teacher_loss 0.8470 (0.8301) loss_zs_kd 1.2552 (1.3760) loss_oracle 1.0465 (1.0943) acc 78.1250 (71.5156) lr 1.3090e-03 eta 0:12:53
epoch [22/50] batch [220/319] time 0.082 (0.085) data 0.000 (0.003) loss 2.4204 (2.3695) teacher_loss 0.8956 (0.8319) loss_zs_kd 1.1561 (1.3707) loss_oracle 1.0744 (1.0931) acc 65.6250 (71.4205) lr 1.3090e-03 eta 0:12:48
epoch [22/50] batch [240/319] time 0.084 (0.085) data 0.000 (0.003) loss 2.5721 (2.3657) teacher_loss 1.0463 (0.8277) loss_zs_kd 1.5719 (1.3688) loss_oracle 1.0509 (1.0915) acc 62.5000 (71.4323) lr 1.3090e-03 eta 0:12:45
epoch [22/50] batch [260/319] time 0.090 (0.085) data 0.000 (0.003) loss 2.3475 (2.3704) teacher_loss 0.7907 (0.8316) loss_zs_kd 1.2664 (1.3769) loss_oracle 1.0971 (1.0916) acc 81.2500 (71.2981) lr 1.3090e-03 eta 0:12:43
epoch [22/50] batch [280/319] time 0.077 (0.086) data 0.000 (0.003) loss 2.1011 (2.3677) teacher_loss 0.5714 (0.8299) loss_zs_kd 1.3040 (1.3757) loss_oracle 1.1096 (1.0912) acc 78.1250 (71.3504) lr 1.3090e-03 eta 0:12:52
epoch [22/50] batch [300/319] time 0.083 (0.086) data 0.000 (0.003) loss 2.4725 (2.3710) teacher_loss 0.9644 (0.8342) loss_zs_kd 1.5800 (1.3800) loss_oracle 1.0532 (1.0891) acc 65.6250 (71.2812) lr 1.3090e-03 eta 0:12:46
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,652
* accuracy: 60.6%
* error: 39.4%
* macro_f1: 54.1%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,514
* accuracy: 56.6%
* error: 43.4%
* macro_f1: 24.7%
******* Domain 2 best val acc:      61.7%, epoch: 19 *******
******* Domain 2 best val test acc: 54.5%, epoch: 19 *******
******* Domain 2 best test acc:     59.1%, epoch: 16 *******
epoch [23/50] batch [20/319] time 0.079 (0.112) data 0.000 (0.027) loss 2.5436 (2.3960) teacher_loss 1.0307 (0.8628) loss_zs_kd 1.3424 (1.4537) loss_oracle 1.1267 (1.0851) acc 68.7500 (70.9375) lr 1.2487e-03 eta 0:16:39
epoch [23/50] batch [40/319] time 0.087 (0.097) data 0.000 (0.014) loss 2.2704 (2.3982) teacher_loss 0.7782 (0.8644) loss_zs_kd 1.5583 (1.4122) loss_oracle 1.0732 (1.0838) acc 78.1250 (69.6875) lr 1.2487e-03 eta 0:14:26
epoch [23/50] batch [60/319] time 0.116 (0.098) data 0.000 (0.009) loss 2.5150 (2.3748) teacher_loss 1.0682 (0.8382) loss_zs_kd 1.4537 (1.4545) loss_oracle 1.0431 (1.0836) acc 56.2500 (70.5729) lr 1.2487e-03 eta 0:14:28
epoch [23/50] batch [80/319] time 0.080 (0.093) data 0.000 (0.007) loss 2.0858 (2.3632) teacher_loss 0.6568 (0.8275) loss_zs_kd 1.6170 (1.4802) loss_oracle 1.1110 (1.0848) acc 78.1250 (70.8984) lr 1.2487e-03 eta 0:13:43
epoch [23/50] batch [100/319] time 0.083 (0.091) data 0.000 (0.006) loss 2.2071 (2.3703) teacher_loss 0.6658 (0.8335) loss_zs_kd 1.6604 (1.4931) loss_oracle 1.0863 (1.0883) acc 87.5000 (71.0312) lr 1.2487e-03 eta 0:13:26
epoch [23/50] batch [120/319] time 0.073 (0.089) data 0.000 (0.005) loss 2.1787 (2.3671) teacher_loss 0.6491 (0.8275) loss_zs_kd 1.4322 (1.4974) loss_oracle 1.1492 (1.0928) acc 75.0000 (70.9896) lr 1.2487e-03 eta 0:13:04
epoch [23/50] batch [140/319] time 0.077 (0.088) data 0.000 (0.004) loss 2.2761 (2.3665) teacher_loss 0.7718 (0.8242) loss_zs_kd 1.4813 (1.5093) loss_oracle 1.1219 (1.0965) acc 65.6250 (71.0268) lr 1.2487e-03 eta 0:12:50
epoch [23/50] batch [160/319] time 0.084 (0.087) data 0.000 (0.004) loss 2.2183 (2.3656) teacher_loss 0.6605 (0.8209) loss_zs_kd 1.4928 (1.5154) loss_oracle 1.1324 (1.0982) acc 78.1250 (71.0938) lr 1.2487e-03 eta 0:12:39
epoch [23/50] batch [180/319] time 0.073 (0.086) data 0.000 (0.003) loss 2.6454 (2.3603) teacher_loss 1.0650 (0.8130) loss_zs_kd 1.5254 (1.5151) loss_oracle 1.1472 (1.1007) acc 62.5000 (71.4583) lr 1.2487e-03 eta 0:12:29
epoch [23/50] batch [200/319] time 0.085 (0.085) data 0.000 (0.003) loss 2.0013 (2.3587) teacher_loss 0.4430 (0.8108) loss_zs_kd 1.3981 (1.5140) loss_oracle 1.1115 (1.1020) acc 84.3750 (71.7031) lr 1.2487e-03 eta 0:12:21
epoch [23/50] batch [220/319] time 0.090 (0.085) data 0.000 (0.003) loss 2.4897 (2.3560) teacher_loss 0.9185 (0.8051) loss_zs_kd 1.5697 (1.5217) loss_oracle 1.1442 (1.1038) acc 75.0000 (71.9460) lr 1.2487e-03 eta 0:12:17
epoch [23/50] batch [240/319] time 0.084 (0.084) data 0.000 (0.002) loss 2.1150 (2.3594) teacher_loss 0.5551 (0.8067) loss_zs_kd 1.4965 (1.5217) loss_oracle 1.0882 (1.1049) acc 84.3750 (71.9271) lr 1.2487e-03 eta 0:12:12
epoch [23/50] batch [260/319] time 0.078 (0.085) data 0.000 (0.002) loss 2.3291 (2.3595) teacher_loss 0.8000 (0.8071) loss_zs_kd 1.5917 (1.5247) loss_oracle 1.1021 (1.1043) acc 71.8750 (71.9351) lr 1.2487e-03 eta 0:12:13
epoch [23/50] batch [280/319] time 0.080 (0.084) data 0.000 (0.002) loss 2.1494 (2.3612) teacher_loss 0.6535 (0.8085) loss_zs_kd 1.5532 (1.5264) loss_oracle 0.9854 (1.1041) acc 78.1250 (71.9085) lr 1.2487e-03 eta 0:12:10
epoch [23/50] batch [300/319] time 0.083 (0.084) data 0.000 (0.002) loss 2.5585 (2.3610) teacher_loss 0.9877 (0.8087) loss_zs_kd 1.4828 (1.5143) loss_oracle 1.0935 (1.1038) acc 62.5000 (71.9479) lr 1.2487e-03 eta 0:12:08
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,595
* accuracy: 59.3%
* error: 40.7%
* macro_f1: 54.4%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,163
* accuracy: 53.0%
* error: 47.0%
* macro_f1: 25.8%
******* Domain 2 best val acc:      61.7%, epoch: 19 *******
******* Domain 2 best val test acc: 54.5%, epoch: 19 *******
******* Domain 2 best test acc:     59.1%, epoch: 16 *******
epoch [24/50] batch [20/319] time 0.087 (0.118) data 0.000 (0.031) loss 2.2114 (2.3341) teacher_loss 0.6631 (0.7933) loss_zs_kd 1.6305 (1.4557) loss_oracle 1.0342 (1.0634) acc 81.2500 (73.5938) lr 1.1874e-03 eta 0:16:50
epoch [24/50] batch [40/319] time 0.082 (0.100) data 0.000 (0.016) loss 2.5984 (2.3187) teacher_loss 1.0968 (0.7852) loss_zs_kd 1.3473 (1.5007) loss_oracle 1.0655 (1.0712) acc 62.5000 (73.5156) lr 1.1874e-03 eta 0:14:15
epoch [24/50] batch [60/319] time 0.074 (0.093) data 0.001 (0.011) loss 2.3160 (2.3519) teacher_loss 0.7542 (0.8196) loss_zs_kd 1.5877 (1.4692) loss_oracle 1.0948 (1.0752) acc 71.8750 (72.6562) lr 1.1874e-03 eta 0:13:15
epoch [24/50] batch [80/319] time 0.084 (0.089) data 0.000 (0.008) loss 2.3090 (2.3393) teacher_loss 0.7702 (0.8052) loss_zs_kd 1.7644 (1.4593) loss_oracle 1.0908 (1.0807) acc 75.0000 (73.2031) lr 1.1874e-03 eta 0:12:41
epoch [24/50] batch [100/319] time 0.075 (0.087) data 0.000 (0.006) loss 2.8396 (2.3455) teacher_loss 1.3361 (0.8092) loss_zs_kd 1.7988 (1.4763) loss_oracle 1.1032 (1.0836) acc 50.0000 (72.9375) lr 1.1874e-03 eta 0:12:19
epoch [24/50] batch [120/319] time 0.074 (0.086) data 0.000 (0.005) loss 2.4144 (2.3565) teacher_loss 0.8643 (0.8220) loss_zs_kd 1.6233 (1.4840) loss_oracle 1.0859 (1.0820) acc 62.5000 (71.9271) lr 1.1874e-03 eta 0:12:06
epoch [24/50] batch [140/319] time 0.090 (0.085) data 0.001 (0.005) loss 2.3124 (2.3549) teacher_loss 0.7849 (0.8206) loss_zs_kd 1.5319 (1.4842) loss_oracle 1.1059 (1.0813) acc 68.7500 (71.8750) lr 1.1874e-03 eta 0:11:57
epoch [24/50] batch [160/319] time 0.081 (0.084) data 0.000 (0.004) loss 2.3991 (2.3509) teacher_loss 0.8795 (0.8177) loss_zs_kd 1.3610 (1.4796) loss_oracle 1.0389 (1.0807) acc 62.5000 (71.7383) lr 1.1874e-03 eta 0:11:52
epoch [24/50] batch [180/319] time 0.083 (0.084) data 0.000 (0.004) loss 2.6053 (2.3598) teacher_loss 1.0808 (0.8280) loss_zs_kd 1.4876 (1.4849) loss_oracle 1.0743 (1.0799) acc 62.5000 (71.3542) lr 1.1874e-03 eta 0:11:51
epoch [24/50] batch [200/319] time 0.083 (0.085) data 0.000 (0.003) loss 2.3198 (2.3549) teacher_loss 0.8345 (0.8246) loss_zs_kd 1.5607 (1.4909) loss_oracle 1.0785 (1.0784) acc 71.8750 (71.4531) lr 1.1874e-03 eta 0:11:50
epoch [24/50] batch [220/319] time 0.076 (0.084) data 0.000 (0.003) loss 2.4185 (2.3533) teacher_loss 0.8767 (0.8231) loss_zs_kd 1.6863 (1.5007) loss_oracle 1.1131 (1.0775) acc 59.3750 (71.4205) lr 1.1874e-03 eta 0:11:45
epoch [24/50] batch [240/319] time 0.082 (0.084) data 0.000 (0.003) loss 2.4615 (2.3507) teacher_loss 0.9505 (0.8190) loss_zs_kd 1.8424 (1.5119) loss_oracle 1.1105 (1.0787) acc 59.3750 (71.5365) lr 1.1874e-03 eta 0:11:41
epoch [24/50] batch [260/319] time 0.131 (0.084) data 0.000 (0.003) loss 2.6066 (2.3536) teacher_loss 1.0366 (0.8215) loss_zs_kd 1.3872 (1.5174) loss_oracle 1.1102 (1.0793) acc 65.6250 (71.4543) lr 1.1874e-03 eta 0:11:43
epoch [24/50] batch [280/319] time 0.082 (0.085) data 0.000 (0.002) loss 2.5832 (2.3586) teacher_loss 1.0141 (0.8263) loss_zs_kd 1.4175 (1.5219) loss_oracle 1.1142 (1.0806) acc 56.2500 (71.2946) lr 1.1874e-03 eta 0:11:48
epoch [24/50] batch [300/319] time 0.080 (0.085) data 0.000 (0.002) loss 2.3834 (2.3588) teacher_loss 0.8501 (0.8264) loss_zs_kd 1.6827 (1.5205) loss_oracle 1.0335 (1.0808) acc 75.0000 (71.2188) lr 1.1874e-03 eta 0:11:44
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,713
* accuracy: 62.0%
* error: 38.0%
* macro_f1: 54.6%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,608
* accuracy: 57.6%
* error: 42.4%
* macro_f1: 25.2%
******* Domain 2 best val acc:      62.0%, epoch: 24 *******
******* Domain 2 best val test acc: 57.6%, epoch: 24 *******
******* Domain 2 best test acc:     59.1%, epoch: 16 *******
epoch [25/50] batch [20/319] time 0.090 (0.114) data 0.000 (0.030) loss 2.4717 (2.3613) teacher_loss 0.9653 (0.8345) loss_zs_kd 1.4003 (1.4809) loss_oracle 1.0234 (1.0612) acc 59.3750 (70.4688) lr 1.1253e-03 eta 0:15:45
epoch [25/50] batch [40/319] time 0.130 (0.103) data 0.000 (0.015) loss 2.5869 (2.3906) teacher_loss 0.9789 (0.8578) loss_zs_kd 1.5505 (1.4828) loss_oracle 1.1052 (1.0681) acc 62.5000 (69.6094) lr 1.1253e-03 eta 0:14:10
epoch [25/50] batch [60/319] time 0.084 (0.099) data 0.000 (0.010) loss 2.4853 (2.3660) teacher_loss 0.9680 (0.8324) loss_zs_kd 1.5219 (1.4700) loss_oracle 1.0527 (1.0797) acc 75.0000 (70.8333) lr 1.1253e-03 eta 0:13:37
epoch [25/50] batch [80/319] time 0.081 (0.094) data 0.000 (0.008) loss 2.1647 (2.3574) teacher_loss 0.7193 (0.8281) loss_zs_kd 1.3854 (1.4712) loss_oracle 1.0405 (1.0772) acc 75.0000 (70.7031) lr 1.1253e-03 eta 0:12:53
epoch [25/50] batch [100/319] time 0.081 (0.092) data 0.000 (0.006) loss 2.5385 (2.3488) teacher_loss 0.9932 (0.8180) loss_zs_kd 1.4351 (1.4577) loss_oracle 1.1100 (1.0787) acc 71.8750 (71.5000) lr 1.1253e-03 eta 0:12:33
epoch [25/50] batch [120/319] time 0.086 (0.090) data 0.000 (0.005) loss 2.2561 (2.3553) teacher_loss 0.7404 (0.8223) loss_zs_kd 1.7038 (1.4497) loss_oracle 1.0818 (1.0798) acc 71.8750 (71.4323) lr 1.1253e-03 eta 0:12:18
epoch [25/50] batch [140/319] time 0.073 (0.089) data 0.000 (0.005) loss 2.2717 (2.3544) teacher_loss 0.7229 (0.8223) loss_zs_kd 1.6821 (1.4531) loss_oracle 1.0763 (1.0803) acc 78.1250 (71.4509) lr 1.1253e-03 eta 0:12:04
epoch [25/50] batch [160/319] time 0.077 (0.088) data 0.000 (0.004) loss 2.4375 (2.3632) teacher_loss 0.9994 (0.8318) loss_zs_kd 1.5118 (1.4518) loss_oracle 1.0835 (1.0805) acc 56.2500 (71.0742) lr 1.1253e-03 eta 0:11:51
epoch [25/50] batch [180/319] time 0.080 (0.087) data 0.000 (0.004) loss 2.3255 (2.3642) teacher_loss 0.7495 (0.8334) loss_zs_kd 1.2304 (1.4422) loss_oracle 1.1531 (1.0822) acc 68.7500 (70.8854) lr 1.1253e-03 eta 0:11:43
epoch [25/50] batch [200/319] time 0.084 (0.086) data 0.000 (0.003) loss 2.2815 (2.3600) teacher_loss 0.7791 (0.8281) loss_zs_kd 1.5045 (1.4468) loss_oracle 1.0523 (1.0848) acc 65.6250 (71.1562) lr 1.1253e-03 eta 0:11:38
epoch [25/50] batch [220/319] time 0.077 (0.086) data 0.000 (0.003) loss 2.2688 (2.3616) teacher_loss 0.7449 (0.8299) loss_zs_kd 1.4564 (1.4430) loss_oracle 1.1057 (1.0866) acc 65.6250 (71.1648) lr 1.1253e-03 eta 0:11:34
epoch [25/50] batch [240/319] time 0.083 (0.086) data 0.000 (0.003) loss 2.3531 (2.3586) teacher_loss 0.7743 (0.8250) loss_zs_kd 1.4549 (1.4373) loss_oracle 1.1250 (1.0888) acc 59.3750 (71.1849) lr 1.1253e-03 eta 0:11:30
epoch [25/50] batch [260/319] time 0.078 (0.085) data 0.000 (0.003) loss 2.3368 (2.3623) teacher_loss 0.7687 (0.8280) loss_zs_kd 1.4778 (1.4312) loss_oracle 1.1216 (1.0890) acc 62.5000 (70.9976) lr 1.1253e-03 eta 0:11:26
epoch [25/50] batch [280/319] time 0.083 (0.085) data 0.000 (0.002) loss 2.4851 (2.3671) teacher_loss 1.0219 (0.8328) loss_zs_kd 1.3068 (1.4231) loss_oracle 1.0388 (1.0884) acc 56.2500 (70.8147) lr 1.1253e-03 eta 0:11:22
epoch [25/50] batch [300/319] time 0.089 (0.085) data 0.000 (0.002) loss 2.4794 (2.3746) teacher_loss 0.9731 (0.8403) loss_zs_kd 1.2237 (1.4175) loss_oracle 1.1085 (1.0893) acc 68.7500 (70.4479) lr 1.1253e-03 eta 0:11:18
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,508
* accuracy: 57.3%
* error: 42.7%
* macro_f1: 52.8%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,833
* accuracy: 59.9%
* error: 40.1%
* macro_f1: 25.8%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      62.0%, epoch: 24 *******
******* Domain 2 best val test acc: 57.6%, epoch: 24 *******
******* Domain 2 best test acc:     59.9%, epoch: 25 *******
epoch [26/50] batch [20/319] time 0.077 (0.119) data 0.000 (0.036) loss 2.4580 (2.4422) teacher_loss 0.8961 (0.9135) loss_zs_kd 1.2005 (1.5126) loss_oracle 1.0410 (1.0868) acc 65.6250 (68.1250) lr 1.0628e-03 eta 0:15:47
epoch [26/50] batch [40/319] time 0.104 (0.101) data 0.000 (0.018) loss 2.2144 (2.4135) teacher_loss 0.7220 (0.8858) loss_zs_kd 1.2706 (1.4057) loss_oracle 1.0778 (1.0887) acc 84.3750 (68.9062) lr 1.0628e-03 eta 0:13:18
epoch [26/50] batch [60/319] time 0.071 (0.092) data 0.000 (0.012) loss 2.1924 (2.3776) teacher_loss 0.7576 (0.8531) loss_zs_kd 1.2080 (1.3872) loss_oracle 1.0745 (1.0921) acc 78.1250 (70.2604) lr 1.0628e-03 eta 0:12:10
epoch [26/50] batch [80/319] time 0.083 (0.090) data 0.000 (0.009) loss 2.4327 (2.3863) teacher_loss 0.8534 (0.8571) loss_zs_kd 1.6551 (1.4020) loss_oracle 1.1084 (1.0980) acc 62.5000 (70.2734) lr 1.0628e-03 eta 0:11:46
epoch [26/50] batch [100/319] time 0.080 (0.088) data 0.000 (0.007) loss 2.2916 (2.3933) teacher_loss 0.8655 (0.8655) loss_zs_kd 1.3365 (1.4129) loss_oracle 1.0365 (1.0959) acc 71.8750 (69.6875) lr 1.0628e-03 eta 0:11:32
epoch [26/50] batch [120/319] time 0.074 (0.086) data 0.000 (0.006) loss 2.5390 (2.3878) teacher_loss 1.0405 (0.8569) loss_zs_kd 1.5384 (1.4192) loss_oracle 1.0705 (1.0954) acc 50.0000 (69.5052) lr 1.0628e-03 eta 0:11:16
epoch [26/50] batch [140/319] time 0.078 (0.085) data 0.000 (0.005) loss 2.4841 (2.3877) teacher_loss 0.9066 (0.8580) loss_zs_kd 1.2470 (1.4139) loss_oracle 1.1321 (1.0943) acc 62.5000 (69.3304) lr 1.0628e-03 eta 0:11:03
epoch [26/50] batch [160/319] time 0.078 (0.084) data 0.000 (0.005) loss 2.2274 (2.3810) teacher_loss 0.7618 (0.8530) loss_zs_kd 1.2556 (1.4002) loss_oracle 1.0726 (1.0938) acc 71.8750 (69.3750) lr 1.0628e-03 eta 0:10:58
epoch [26/50] batch [180/319] time 0.082 (0.084) data 0.000 (0.004) loss 2.3981 (2.3815) teacher_loss 0.8511 (0.8515) loss_zs_kd 1.4330 (1.4050) loss_oracle 1.0731 (1.0946) acc 71.8750 (69.4792) lr 1.0628e-03 eta 0:10:55
epoch [26/50] batch [200/319] time 0.086 (0.084) data 0.000 (0.004) loss 2.5153 (2.3789) teacher_loss 0.9614 (0.8486) loss_zs_kd 1.2728 (1.4010) loss_oracle 1.1192 (1.0957) acc 75.0000 (69.6875) lr 1.0628e-03 eta 0:10:54
epoch [26/50] batch [220/319] time 0.090 (0.084) data 0.000 (0.004) loss 2.0855 (2.3779) teacher_loss 0.5582 (0.8495) loss_zs_kd 1.0758 (1.3998) loss_oracle 1.1061 (1.0945) acc 78.1250 (69.5739) lr 1.0628e-03 eta 0:10:52
epoch [26/50] batch [240/319] time 0.093 (0.085) data 0.001 (0.003) loss 2.2504 (2.3803) teacher_loss 0.7167 (0.8511) loss_zs_kd 1.4873 (1.3980) loss_oracle 1.1165 (1.0945) acc 78.1250 (69.5443) lr 1.0628e-03 eta 0:11:00
epoch [26/50] batch [260/319] time 0.086 (0.085) data 0.000 (0.003) loss 2.2229 (2.3747) teacher_loss 0.7120 (0.8454) loss_zs_kd 1.1625 (1.3966) loss_oracle 1.0798 (1.0942) acc 78.1250 (69.6995) lr 1.0628e-03 eta 0:10:57
epoch [26/50] batch [280/319] time 0.079 (0.085) data 0.000 (0.003) loss 2.2920 (2.3763) teacher_loss 0.7644 (0.8464) loss_zs_kd 1.4234 (1.3976) loss_oracle 1.0613 (1.0939) acc 62.5000 (69.7768) lr 1.0628e-03 eta 0:10:55
epoch [26/50] batch [300/319] time 0.082 (0.085) data 0.000 (0.003) loss 2.7323 (2.3803) teacher_loss 1.2771 (0.8499) loss_zs_kd 1.5027 (1.3998) loss_oracle 1.0561 (1.0934) acc 50.0000 (69.5417) lr 1.0628e-03 eta 0:10:53
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,650
* accuracy: 60.5%
* error: 39.5%
* macro_f1: 53.4%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,708
* accuracy: 58.6%
* error: 41.4%
* macro_f1: 25.7%
******* Domain 2 best val acc:      62.0%, epoch: 24 *******
******* Domain 2 best val test acc: 57.6%, epoch: 24 *******
******* Domain 2 best test acc:     59.9%, epoch: 25 *******
epoch [27/50] batch [20/319] time 0.086 (0.114) data 0.000 (0.024) loss 2.0933 (2.3634) teacher_loss 0.5264 (0.8102) loss_zs_kd 1.3496 (1.5224) loss_oracle 1.1117 (1.0993) acc 84.3750 (70.9375) lr 1.0000e-03 eta 0:14:27
epoch [27/50] batch [40/319] time 0.074 (0.090) data 0.000 (0.012) loss 2.1588 (2.3773) teacher_loss 0.5481 (0.8223) loss_zs_kd 1.7592 (1.5501) loss_oracle 1.1485 (1.1057) acc 84.3750 (71.0938) lr 1.0000e-03 eta 0:11:25
epoch [27/50] batch [60/319] time 0.087 (0.086) data 0.001 (0.008) loss 2.3014 (2.3578) teacher_loss 0.8321 (0.8103) loss_zs_kd 1.6623 (1.5357) loss_oracle 0.9867 (1.1026) acc 68.7500 (71.6146) lr 1.0000e-03 eta 0:10:56
epoch [27/50] batch [80/319] time 0.080 (0.086) data 0.000 (0.006) loss 2.4682 (2.3585) teacher_loss 0.9388 (0.8132) loss_zs_kd 1.7730 (1.5363) loss_oracle 1.1285 (1.1022) acc 56.2500 (71.3672) lr 1.0000e-03 eta 0:10:47
epoch [27/50] batch [100/319] time 0.091 (0.085) data 0.000 (0.005) loss 2.6324 (2.3578) teacher_loss 1.0491 (0.8165) loss_zs_kd 1.5123 (1.5320) loss_oracle 1.1652 (1.1017) acc 59.3750 (70.9688) lr 1.0000e-03 eta 0:10:45
epoch [27/50] batch [120/319] time 0.075 (0.085) data 0.000 (0.004) loss 2.4348 (2.3715) teacher_loss 0.9197 (0.8315) loss_zs_kd 1.5055 (1.5148) loss_oracle 1.0852 (1.0993) acc 65.6250 (70.5990) lr 1.0000e-03 eta 0:10:37
epoch [27/50] batch [140/319] time 0.085 (0.084) data 0.000 (0.004) loss 2.2379 (2.3738) teacher_loss 0.7021 (0.8344) loss_zs_kd 1.3313 (1.4954) loss_oracle 1.1207 (1.1002) acc 78.1250 (70.6250) lr 1.0000e-03 eta 0:10:29
epoch [27/50] batch [160/319] time 0.080 (0.083) data 0.000 (0.003) loss 2.1458 (2.3690) teacher_loss 0.5914 (0.8287) loss_zs_kd 1.3493 (1.4862) loss_oracle 1.1201 (1.1025) acc 84.3750 (71.0156) lr 1.0000e-03 eta 0:10:21
epoch [27/50] batch [180/319] time 0.080 (0.083) data 0.000 (0.003) loss 2.1338 (2.3717) teacher_loss 0.5892 (0.8283) loss_zs_kd 1.2059 (1.4761) loss_oracle 1.0839 (1.1055) acc 84.3750 (71.1285) lr 1.0000e-03 eta 0:10:20
epoch [27/50] batch [200/319] time 0.083 (0.083) data 0.000 (0.003) loss 2.1384 (2.3720) teacher_loss 0.5535 (0.8296) loss_zs_kd 1.7292 (1.4703) loss_oracle 1.1490 (1.1055) acc 87.5000 (71.0781) lr 1.0000e-03 eta 0:10:20
epoch [27/50] batch [220/319] time 0.081 (0.083) data 0.000 (0.002) loss 2.3699 (2.3712) teacher_loss 0.8473 (0.8265) loss_zs_kd 1.4050 (1.4721) loss_oracle 1.0950 (1.1074) acc 62.5000 (71.3352) lr 1.0000e-03 eta 0:10:20
epoch [27/50] batch [240/319] time 0.084 (0.084) data 0.000 (0.002) loss 2.5436 (2.3757) teacher_loss 0.9913 (0.8278) loss_zs_kd 1.1609 (1.4580) loss_oracle 1.1047 (1.1093) acc 62.5000 (71.4714) lr 1.0000e-03 eta 0:10:20
epoch [27/50] batch [260/319] time 0.084 (0.084) data 0.000 (0.002) loss 2.3861 (2.3778) teacher_loss 0.8093 (0.8286) loss_zs_kd 1.2846 (1.4410) loss_oracle 1.0937 (1.1093) acc 68.7500 (71.4663) lr 1.0000e-03 eta 0:10:18
epoch [27/50] batch [280/319] time 0.087 (0.084) data 0.000 (0.002) loss 2.5484 (2.3811) teacher_loss 1.0301 (0.8314) loss_zs_kd 1.3706 (1.4231) loss_oracle 1.0602 (1.1100) acc 56.2500 (71.4062) lr 1.0000e-03 eta 0:10:17
epoch [27/50] batch [300/319] time 0.083 (0.084) data 0.000 (0.002) loss 2.7304 (2.3749) teacher_loss 1.0913 (0.8242) loss_zs_kd 1.7394 (1.4117) loss_oracle 1.1150 (1.1107) acc 65.6250 (71.7083) lr 1.0000e-03 eta 0:10:15
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,467
* accuracy: 56.3%
* error: 43.7%
* macro_f1: 49.8%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 4,233
* accuracy: 43.5%
* error: 56.5%
* macro_f1: 22.4%
******* Domain 2 best val acc:      62.0%, epoch: 24 *******
******* Domain 2 best val test acc: 57.6%, epoch: 24 *******
******* Domain 2 best test acc:     59.9%, epoch: 25 *******
epoch [28/50] batch [20/319] time 0.080 (0.116) data 0.000 (0.033) loss 2.4582 (2.3685) teacher_loss 0.9017 (0.7990) loss_zs_kd 1.3905 (1.2856) loss_oracle 1.0991 (1.1086) acc 62.5000 (72.0312) lr 9.3721e-04 eta 0:14:11
epoch [28/50] batch [40/319] time 0.083 (0.100) data 0.000 (0.017) loss 2.5399 (2.3825) teacher_loss 0.9785 (0.8198) loss_zs_kd 1.4958 (1.2621) loss_oracle 1.1128 (1.1052) acc 59.3750 (70.8594) lr 9.3721e-04 eta 0:12:09
epoch [28/50] batch [60/319] time 0.088 (0.094) data 0.000 (0.011) loss 2.2597 (2.4042) teacher_loss 0.6848 (0.8467) loss_zs_kd 1.3529 (1.2856) loss_oracle 1.1482 (1.1064) acc 84.3750 (70.1562) lr 9.3721e-04 eta 0:11:23
epoch [28/50] batch [80/319] time 0.080 (0.091) data 0.000 (0.008) loss 2.2297 (2.3764) teacher_loss 0.6557 (0.8191) loss_zs_kd 1.2455 (1.2796) loss_oracle 1.1051 (1.1042) acc 75.0000 (70.7812) lr 9.3721e-04 eta 0:11:03
epoch [28/50] batch [100/319] time 0.079 (0.089) data 0.000 (0.007) loss 2.3550 (2.3681) teacher_loss 0.7712 (0.8097) loss_zs_kd 1.3925 (1.3040) loss_oracle 1.1403 (1.1048) acc 68.7500 (71.3125) lr 9.3721e-04 eta 0:10:44
epoch [28/50] batch [120/319] time 0.061 (0.087) data 0.000 (0.006) loss 2.3102 (2.3530) teacher_loss 0.7216 (0.7933) loss_zs_kd 1.5966 (1.3076) loss_oracle 1.1614 (1.1062) acc 78.1250 (72.1875) lr 9.3721e-04 eta 0:10:26
epoch [28/50] batch [140/319] time 0.082 (0.085) data 0.000 (0.005) loss 2.1869 (2.3490) teacher_loss 0.6499 (0.7920) loss_zs_kd 1.4364 (1.3172) loss_oracle 1.1216 (1.1043) acc 68.7500 (72.1205) lr 9.3721e-04 eta 0:10:13
epoch [28/50] batch [160/319] time 0.087 (0.085) data 0.000 (0.004) loss 2.0936 (2.3471) teacher_loss 0.5137 (0.7903) loss_zs_kd 1.4996 (1.3240) loss_oracle 1.0933 (1.1024) acc 84.3750 (72.3438) lr 9.3721e-04 eta 0:10:07
epoch [28/50] batch [180/319] time 0.080 (0.085) data 0.000 (0.004) loss 2.1648 (2.3390) teacher_loss 0.6537 (0.7838) loss_zs_kd 1.2301 (1.3210) loss_oracle 1.0885 (1.1023) acc 81.2500 (72.4479) lr 9.3721e-04 eta 0:10:05
epoch [28/50] batch [200/319] time 0.077 (0.084) data 0.000 (0.004) loss 2.3549 (2.3408) teacher_loss 0.8058 (0.7865) loss_zs_kd 1.4581 (1.3245) loss_oracle 1.1391 (1.1032) acc 71.8750 (72.3125) lr 9.3721e-04 eta 0:09:58
epoch [28/50] batch [220/319] time 0.122 (0.085) data 0.001 (0.003) loss 2.8645 (2.3482) teacher_loss 1.2925 (0.7924) loss_zs_kd 1.2070 (1.3342) loss_oracle 1.1509 (1.1038) acc 50.0000 (72.1307) lr 9.3721e-04 eta 0:10:07
epoch [28/50] batch [240/319] time 0.085 (0.085) data 0.000 (0.003) loss 2.3709 (2.3495) teacher_loss 0.8169 (0.7932) loss_zs_kd 1.2070 (1.3327) loss_oracle 1.0682 (1.1039) acc 75.0000 (72.1745) lr 9.3721e-04 eta 0:10:00
epoch [28/50] batch [260/319] time 0.076 (0.084) data 0.000 (0.003) loss 2.2023 (2.3463) teacher_loss 0.6209 (0.7900) loss_zs_kd 1.3677 (1.3330) loss_oracle 1.0657 (1.1024) acc 87.5000 (72.3438) lr 9.3721e-04 eta 0:09:54
epoch [28/50] batch [280/319] time 0.061 (0.083) data 0.000 (0.003) loss 2.2453 (2.3446) teacher_loss 0.7288 (0.7891) loss_zs_kd 1.3749 (1.3393) loss_oracle 1.0948 (1.1023) acc 75.0000 (72.3661) lr 9.3721e-04 eta 0:09:48
epoch [28/50] batch [300/319] time 0.068 (0.082) data 0.000 (0.002) loss 2.6208 (2.3433) teacher_loss 1.0890 (0.7885) loss_zs_kd 1.4146 (1.3462) loss_oracle 1.1251 (1.1005) acc 65.6250 (72.4062) lr 9.3721e-04 eta 0:09:38
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,654
* accuracy: 60.6%
* error: 39.4%
* macro_f1: 53.0%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,658
* accuracy: 58.1%
* error: 41.9%
* macro_f1: 25.8%
******* Domain 2 best val acc:      62.0%, epoch: 24 *******
******* Domain 2 best val test acc: 57.6%, epoch: 24 *******
******* Domain 2 best test acc:     59.9%, epoch: 25 *******
epoch [29/50] batch [20/319] time 0.101 (0.127) data 0.001 (0.026) loss 2.3269 (2.2850) teacher_loss 0.7728 (0.7484) loss_zs_kd 1.3310 (1.3370) loss_oracle 1.1038 (1.0855) acc 78.1250 (74.3750) lr 8.7467e-04 eta 0:14:49
epoch [29/50] batch [40/319] time 0.079 (0.104) data 0.000 (0.013) loss 2.2873 (2.2990) teacher_loss 0.6527 (0.7592) loss_zs_kd 1.4084 (1.3600) loss_oracle 1.1576 (1.0837) acc 84.3750 (73.4375) lr 8.7467e-04 eta 0:12:07
epoch [29/50] batch [60/319] time 0.078 (0.097) data 0.001 (0.009) loss 2.1980 (2.3140) teacher_loss 0.6147 (0.7721) loss_zs_kd 1.3988 (1.3823) loss_oracle 1.1084 (1.0873) acc 81.2500 (72.9167) lr 8.7467e-04 eta 0:11:16
epoch [29/50] batch [80/319] time 0.081 (0.092) data 0.000 (0.007) loss 2.1387 (2.3133) teacher_loss 0.6268 (0.7692) loss_zs_kd 1.5493 (1.4008) loss_oracle 1.0962 (1.0909) acc 81.2500 (73.2422) lr 8.7467e-04 eta 0:10:41
epoch [29/50] batch [100/319] time 0.077 (0.091) data 0.000 (0.005) loss 2.2855 (2.3150) teacher_loss 0.7247 (0.7662) loss_zs_kd 1.6426 (1.4096) loss_oracle 1.1119 (1.0943) acc 71.8750 (73.1562) lr 8.7467e-04 eta 0:10:26
epoch [29/50] batch [120/319] time 0.089 (0.089) data 0.000 (0.005) loss 2.3133 (2.3110) teacher_loss 0.7821 (0.7635) loss_zs_kd 1.3835 (1.4263) loss_oracle 1.1125 (1.0952) acc 78.1250 (73.4375) lr 8.7467e-04 eta 0:10:16
epoch [29/50] batch [140/319] time 0.088 (0.089) data 0.000 (0.004) loss 2.5078 (2.3106) teacher_loss 0.9201 (0.7614) loss_zs_kd 1.8016 (1.4431) loss_oracle 1.1643 (1.0939) acc 65.6250 (73.4598) lr 8.7467e-04 eta 0:10:09
epoch [29/50] batch [160/319] time 0.082 (0.088) data 0.000 (0.004) loss 2.4429 (2.3256) teacher_loss 0.8562 (0.7758) loss_zs_kd 1.3870 (1.4542) loss_oracle 1.1483 (1.0957) acc 78.1250 (72.9492) lr 8.7467e-04 eta 0:10:01
epoch [29/50] batch [180/319] time 0.073 (0.087) data 0.000 (0.003) loss 2.3136 (2.3380) teacher_loss 0.7643 (0.7860) loss_zs_kd 1.2403 (1.4617) loss_oracle 1.0409 (1.0962) acc 71.8750 (72.4479) lr 8.7467e-04 eta 0:09:52
epoch [29/50] batch [200/319] time 0.083 (0.086) data 0.000 (0.003) loss 2.5744 (2.3524) teacher_loss 1.0012 (0.8004) loss_zs_kd 1.5638 (1.4662) loss_oracle 1.0704 (1.0964) acc 62.5000 (72.1562) lr 8.7467e-04 eta 0:09:47
epoch [29/50] batch [220/319] time 0.086 (0.086) data 0.000 (0.003) loss 2.6921 (2.3580) teacher_loss 1.1833 (0.8068) loss_zs_kd 1.2732 (1.4562) loss_oracle 1.1215 (1.0966) acc 50.0000 (71.9176) lr 8.7467e-04 eta 0:09:44
epoch [29/50] batch [240/319] time 0.098 (0.086) data 0.000 (0.002) loss 2.5818 (2.3674) teacher_loss 0.9599 (0.8167) loss_zs_kd 1.5810 (1.4527) loss_oracle 1.1495 (1.0964) acc 71.8750 (71.4844) lr 8.7467e-04 eta 0:09:42
epoch [29/50] batch [260/319] time 0.082 (0.085) data 0.000 (0.002) loss 2.5242 (2.3774) teacher_loss 0.9604 (0.8266) loss_zs_kd 1.4412 (1.4440) loss_oracle 1.0827 (1.0960) acc 71.8750 (71.0817) lr 8.7467e-04 eta 0:09:37
epoch [29/50] batch [280/319] time 0.081 (0.085) data 0.000 (0.002) loss 2.2726 (2.3796) teacher_loss 0.7555 (0.8297) loss_zs_kd 1.3196 (1.4349) loss_oracle 1.0989 (1.0956) acc 78.1250 (70.9263) lr 8.7467e-04 eta 0:09:33
epoch [29/50] batch [300/319] time 0.085 (0.085) data 0.000 (0.002) loss 2.4584 (2.3829) teacher_loss 0.8983 (0.8335) loss_zs_kd 1.4338 (1.4303) loss_oracle 1.1038 (1.0948) acc 75.0000 (70.8125) lr 8.7467e-04 eta 0:09:30
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,694
* accuracy: 61.5%
* error: 38.5%
* macro_f1: 54.1%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,596
* accuracy: 57.5%
* error: 42.5%
* macro_f1: 23.8%
******* Domain 2 best val acc:      62.0%, epoch: 24 *******
******* Domain 2 best val test acc: 57.6%, epoch: 24 *******
******* Domain 2 best test acc:     59.9%, epoch: 25 *******
epoch [30/50] batch [20/319] time 0.077 (0.096) data 0.000 (0.027) loss 2.1415 (2.4217) teacher_loss 0.6080 (0.8728) loss_zs_kd 1.2203 (1.3139) loss_oracle 1.0843 (1.0933) acc 84.3750 (69.5312) lr 8.1262e-04 eta 0:10:42
epoch [30/50] batch [40/319] time 0.084 (0.089) data 0.000 (0.013) loss 2.3208 (2.4478) teacher_loss 0.7137 (0.9002) loss_zs_kd 1.3958 (1.3557) loss_oracle 1.1420 (1.0925) acc 68.7500 (68.7500) lr 8.1262e-04 eta 0:09:55
epoch [30/50] batch [60/319] time 0.078 (0.086) data 0.000 (0.009) loss 2.3370 (2.4568) teacher_loss 0.7801 (0.9074) loss_zs_kd 1.5153 (1.3691) loss_oracle 1.1332 (1.0945) acc 68.7500 (68.1250) lr 8.1262e-04 eta 0:09:32
epoch [30/50] batch [80/319] time 0.075 (0.086) data 0.000 (0.007) loss 2.3332 (2.4654) teacher_loss 0.8207 (0.9133) loss_zs_kd 1.3050 (1.3912) loss_oracle 1.0288 (1.0952) acc 65.6250 (67.8516) lr 8.1262e-04 eta 0:09:26
epoch [30/50] batch [100/319] time 0.084 (0.084) data 0.000 (0.006) loss 2.5220 (2.4669) teacher_loss 0.9615 (0.9141) loss_zs_kd 1.3084 (1.3672) loss_oracle 1.1072 (1.0943) acc 68.7500 (68.0000) lr 8.1262e-04 eta 0:09:16
epoch [30/50] batch [120/319] time 0.088 (0.084) data 0.000 (0.005) loss 2.5254 (2.4737) teacher_loss 0.9290 (0.9249) loss_zs_kd 1.4065 (1.3601) loss_oracle 1.1335 (1.0915) acc 62.5000 (67.5260) lr 8.1262e-04 eta 0:09:12
epoch [30/50] batch [140/319] time 0.083 (0.084) data 0.000 (0.004) loss 2.2736 (2.4619) teacher_loss 0.7332 (0.9150) loss_zs_kd 1.2563 (1.3650) loss_oracle 1.0752 (1.0902) acc 71.8750 (67.8125) lr 8.1262e-04 eta 0:09:10
epoch [30/50] batch [160/319] time 0.073 (0.083) data 0.000 (0.004) loss 2.2184 (2.4576) teacher_loss 0.7059 (0.9121) loss_zs_kd 1.3499 (1.3792) loss_oracle 1.0634 (1.0895) acc 71.8750 (67.6953) lr 8.1262e-04 eta 0:09:05
epoch [30/50] batch [180/319] time 0.069 (0.083) data 0.000 (0.003) loss 2.4212 (2.4574) teacher_loss 0.8485 (0.9125) loss_zs_kd 1.5653 (1.3776) loss_oracle 1.0604 (1.0869) acc 65.6250 (67.5000) lr 8.1262e-04 eta 0:09:00
epoch [30/50] batch [200/319] time 0.077 (0.083) data 0.000 (0.003) loss 2.5008 (2.4526) teacher_loss 0.9497 (0.9087) loss_zs_kd 1.4237 (1.3829) loss_oracle 1.0286 (1.0847) acc 65.6250 (67.6094) lr 8.1262e-04 eta 0:08:56
epoch [30/50] batch [220/319] time 0.076 (0.082) data 0.000 (0.003) loss 2.3321 (2.4512) teacher_loss 0.7769 (0.9095) loss_zs_kd 1.4275 (1.3923) loss_oracle 1.0711 (1.0827) acc 75.0000 (67.5994) lr 8.1262e-04 eta 0:08:53
epoch [30/50] batch [240/319] time 0.069 (0.084) data 0.000 (0.002) loss 2.3725 (2.4561) teacher_loss 0.7619 (0.9151) loss_zs_kd 1.3474 (1.4008) loss_oracle 1.0956 (1.0814) acc 78.1250 (67.2526) lr 8.1262e-04 eta 0:09:00
epoch [30/50] batch [260/319] time 0.078 (0.083) data 0.000 (0.002) loss 2.3535 (2.4535) teacher_loss 0.7673 (0.9130) loss_zs_kd 1.4407 (1.4029) loss_oracle 1.1175 (1.0810) acc 78.1250 (67.3077) lr 8.1262e-04 eta 0:08:56
epoch [30/50] batch [280/319] time 0.087 (0.083) data 0.000 (0.002) loss 2.3083 (2.4450) teacher_loss 0.7472 (0.9060) loss_zs_kd 1.4325 (1.4051) loss_oracle 1.0846 (1.0796) acc 81.2500 (67.6004) lr 8.1262e-04 eta 0:08:55
epoch [30/50] batch [300/319] time 0.071 (0.083) data 0.000 (0.002) loss 2.7946 (2.4445) teacher_loss 1.2285 (0.9047) loss_zs_kd 1.2412 (1.4058) loss_oracle 1.1193 (1.0794) acc 59.3750 (67.7396) lr 8.1262e-04 eta 0:08:52
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,701
* accuracy: 61.7%
* error: 38.3%
* macro_f1: 56.6%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,680
* accuracy: 58.3%
* error: 41.7%
* macro_f1: 24.6%
******* Domain 2 best val acc:      62.0%, epoch: 24 *******
******* Domain 2 best val test acc: 57.6%, epoch: 24 *******
******* Domain 2 best test acc:     59.9%, epoch: 25 *******
epoch [31/50] batch [20/319] time 0.078 (0.103) data 0.001 (0.025) loss 2.2860 (2.3882) teacher_loss 0.7332 (0.8416) loss_zs_kd 1.5166 (1.3732) loss_oracle 1.0469 (1.0773) acc 84.3750 (71.4062) lr 7.5131e-04 eta 0:10:55
epoch [31/50] batch [40/319] time 0.083 (0.099) data 0.000 (0.013) loss 2.5110 (2.4135) teacher_loss 0.9793 (0.8724) loss_zs_kd 1.1941 (1.3831) loss_oracle 1.0880 (1.0798) acc 59.3750 (69.6094) lr 7.5131e-04 eta 0:10:30
epoch [31/50] batch [60/319] time 0.084 (0.094) data 0.000 (0.009) loss 2.5290 (2.4343) teacher_loss 0.9389 (0.8920) loss_zs_kd 1.3184 (1.3683) loss_oracle 1.1112 (1.0814) acc 62.5000 (68.8021) lr 7.5131e-04 eta 0:09:51
epoch [31/50] batch [80/319] time 0.086 (0.090) data 0.000 (0.006) loss 2.5083 (2.4306) teacher_loss 0.9326 (0.8909) loss_zs_kd 1.3898 (1.3535) loss_oracle 1.1257 (1.0792) acc 59.3750 (68.3984) lr 7.5131e-04 eta 0:09:27
epoch [31/50] batch [100/319] time 0.067 (0.087) data 0.000 (0.005) loss 2.6443 (2.4260) teacher_loss 1.0549 (0.8897) loss_zs_kd 1.5396 (1.3518) loss_oracle 1.1370 (1.0779) acc 65.6250 (68.4375) lr 7.5131e-04 eta 0:09:07
epoch [31/50] batch [120/319] time 0.074 (0.085) data 0.000 (0.004) loss 2.2575 (2.4272) teacher_loss 0.7126 (0.8897) loss_zs_kd 1.1582 (1.3541) loss_oracle 1.0598 (1.0772) acc 71.8750 (68.5156) lr 7.5131e-04 eta 0:08:54
epoch [31/50] batch [140/319] time 0.078 (0.085) data 0.000 (0.004) loss 2.4028 (2.4258) teacher_loss 0.8188 (0.8880) loss_zs_kd 1.4345 (1.3583) loss_oracle 1.0959 (1.0786) acc 59.3750 (68.5045) lr 7.5131e-04 eta 0:08:47
epoch [31/50] batch [160/319] time 0.078 (0.084) data 0.000 (0.003) loss 2.4480 (2.4230) teacher_loss 0.9219 (0.8844) loss_zs_kd 1.3657 (1.3717) loss_oracle 1.0907 (1.0792) acc 56.2500 (68.5742) lr 7.5131e-04 eta 0:08:42
epoch [31/50] batch [180/319] time 0.071 (0.084) data 0.000 (0.003) loss 2.4651 (2.4201) teacher_loss 0.9009 (0.8807) loss_zs_kd 1.2945 (1.3684) loss_oracle 1.0426 (1.0780) acc 62.5000 (68.6632) lr 7.5131e-04 eta 0:08:38
epoch [31/50] batch [200/319] time 0.081 (0.083) data 0.000 (0.003) loss 2.3772 (2.4200) teacher_loss 0.8566 (0.8810) loss_zs_kd 1.4348 (1.3691) loss_oracle 1.0739 (1.0768) acc 68.7500 (68.5938) lr 7.5131e-04 eta 0:08:35
epoch [31/50] batch [220/319] time 0.080 (0.083) data 0.000 (0.003) loss 2.0148 (2.4138) teacher_loss 0.4657 (0.8759) loss_zs_kd 1.6994 (1.3730) loss_oracle 1.0429 (1.0772) acc 90.6250 (68.9631) lr 7.5131e-04 eta 0:08:33
epoch [31/50] batch [240/319] time 0.072 (0.083) data 0.000 (0.002) loss 2.5396 (2.4096) teacher_loss 0.9764 (0.8721) loss_zs_kd 1.1965 (1.3761) loss_oracle 1.0873 (1.0762) acc 59.3750 (68.8542) lr 7.5131e-04 eta 0:08:31
epoch [31/50] batch [260/319] time 0.080 (0.083) data 0.000 (0.002) loss 2.3433 (2.4049) teacher_loss 0.8453 (0.8678) loss_zs_kd 1.4675 (1.3780) loss_oracle 1.0392 (1.0760) acc 62.5000 (69.0986) lr 7.5131e-04 eta 0:08:27
epoch [31/50] batch [280/319] time 0.079 (0.083) data 0.000 (0.002) loss 2.3173 (2.4107) teacher_loss 0.8322 (0.8739) loss_zs_kd 1.4582 (1.3772) loss_oracle 1.0990 (1.0762) acc 68.7500 (68.9062) lr 7.5131e-04 eta 0:08:27
epoch [31/50] batch [300/319] time 0.078 (0.083) data 0.000 (0.002) loss 2.1256 (2.4099) teacher_loss 0.6032 (0.8733) loss_zs_kd 1.2726 (1.3761) loss_oracle 1.0156 (1.0757) acc 71.8750 (68.9896) lr 7.5131e-04 eta 0:08:26
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,697
* accuracy: 61.6%
* error: 38.4%
* macro_f1: 54.3%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,613
* accuracy: 57.7%
* error: 42.3%
* macro_f1: 25.5%
******* Domain 2 best val acc:      62.0%, epoch: 24 *******
******* Domain 2 best val test acc: 57.6%, epoch: 24 *******
******* Domain 2 best test acc:     59.9%, epoch: 25 *******
epoch [32/50] batch [20/319] time 0.081 (0.116) data 0.000 (0.027) loss 2.3870 (2.3954) teacher_loss 0.8390 (0.8504) loss_zs_kd 1.7108 (1.4932) loss_oracle 1.0575 (1.0814) acc 65.6250 (68.4375) lr 6.9098e-04 eta 0:11:39
epoch [32/50] batch [40/319] time 0.083 (0.098) data 0.000 (0.014) loss 2.3142 (2.3922) teacher_loss 0.8167 (0.8502) loss_zs_kd 1.4970 (1.4889) loss_oracle 1.0510 (1.0777) acc 71.8750 (69.7656) lr 6.9098e-04 eta 0:09:47
epoch [32/50] batch [60/319] time 0.078 (0.093) data 0.000 (0.009) loss 2.3393 (2.3887) teacher_loss 0.8575 (0.8494) loss_zs_kd 1.4013 (1.4801) loss_oracle 1.0982 (1.0743) acc 65.6250 (70.1562) lr 6.9098e-04 eta 0:09:15
epoch [32/50] batch [80/319] time 0.076 (0.089) data 0.000 (0.007) loss 2.2641 (2.3873) teacher_loss 0.7637 (0.8515) loss_zs_kd 1.4059 (1.4758) loss_oracle 1.0190 (1.0724) acc 68.7500 (69.9219) lr 6.9098e-04 eta 0:08:50
epoch [32/50] batch [100/319] time 0.084 (0.086) data 0.000 (0.006) loss 2.2751 (2.3980) teacher_loss 0.7220 (0.8649) loss_zs_kd 1.2428 (1.4672) loss_oracle 1.0349 (1.0702) acc 78.1250 (69.5312) lr 6.9098e-04 eta 0:08:35
epoch [32/50] batch [120/319] time 0.085 (0.086) data 0.000 (0.005) loss 2.1342 (2.3876) teacher_loss 0.6333 (0.8544) loss_zs_kd 1.6714 (1.4529) loss_oracle 1.0748 (1.0702) acc 78.1250 (70.0260) lr 6.9098e-04 eta 0:08:29
epoch [32/50] batch [140/319] time 0.075 (0.085) data 0.000 (0.004) loss 2.3522 (2.3890) teacher_loss 0.8213 (0.8567) loss_zs_kd 1.7431 (1.4538) loss_oracle 1.0603 (1.0711) acc 68.7500 (69.8438) lr 6.9098e-04 eta 0:08:24
epoch [32/50] batch [160/319] time 0.080 (0.084) data 0.000 (0.004) loss 2.3583 (2.3907) teacher_loss 0.8058 (0.8587) loss_zs_kd 1.3577 (1.4497) loss_oracle 1.0279 (1.0708) acc 68.7500 (69.9023) lr 6.9098e-04 eta 0:08:17
epoch [32/50] batch [180/319] time 0.088 (0.084) data 0.000 (0.003) loss 2.5631 (2.3834) teacher_loss 1.0264 (0.8519) loss_zs_kd 1.2674 (1.4495) loss_oracle 1.0770 (1.0700) acc 56.2500 (70.0347) lr 6.9098e-04 eta 0:08:15
epoch [32/50] batch [200/319] time 0.079 (0.084) data 0.000 (0.003) loss 2.3340 (2.3837) teacher_loss 0.7914 (0.8533) loss_zs_kd 1.2812 (1.4465) loss_oracle 1.0843 (1.0697) acc 65.6250 (69.9844) lr 6.9098e-04 eta 0:08:11
epoch [32/50] batch [220/319] time 0.072 (0.083) data 0.000 (0.003) loss 2.3280 (2.3782) teacher_loss 0.7775 (0.8476) loss_zs_kd 1.0964 (1.4297) loss_oracle 1.0801 (1.0702) acc 75.0000 (70.2415) lr 6.9098e-04 eta 0:08:06
epoch [32/50] batch [240/319] time 0.071 (0.084) data 0.000 (0.003) loss 2.0774 (2.3782) teacher_loss 0.5686 (0.8472) loss_zs_kd 1.1375 (1.4184) loss_oracle 1.0455 (1.0706) acc 81.2500 (70.2734) lr 6.9098e-04 eta 0:08:11
epoch [32/50] batch [260/319] time 0.077 (0.084) data 0.000 (0.002) loss 2.3104 (2.3811) teacher_loss 0.7927 (0.8482) loss_zs_kd 1.4097 (1.4165) loss_oracle 1.0755 (1.0710) acc 84.3750 (70.2284) lr 6.9098e-04 eta 0:08:07
epoch [32/50] batch [280/319] time 0.083 (0.084) data 0.000 (0.002) loss 2.5363 (2.3835) teacher_loss 0.9951 (0.8500) loss_zs_kd 1.2397 (1.4172) loss_oracle 1.0754 (1.0711) acc 71.8750 (70.2232) lr 6.9098e-04 eta 0:08:05
epoch [32/50] batch [300/319] time 0.086 (0.084) data 0.000 (0.002) loss 2.2041 (2.3805) teacher_loss 0.6790 (0.8471) loss_zs_kd 1.2617 (1.4220) loss_oracle 1.0141 (1.0715) acc 78.1250 (70.1979) lr 6.9098e-04 eta 0:08:04
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,760
* accuracy: 63.0%
* error: 37.0%
* macro_f1: 55.1%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,520
* accuracy: 56.7%
* error: 43.3%
* macro_f1: 24.9%
******* Domain 2 best val acc:      63.0%, epoch: 32 *******
******* Domain 2 best val test acc: 56.7%, epoch: 32 *******
******* Domain 2 best test acc:     59.9%, epoch: 25 *******
epoch [33/50] batch [20/319] time 0.123 (0.133) data 0.001 (0.028) loss 2.6126 (2.3529) teacher_loss 1.0117 (0.8020) loss_zs_kd 1.5150 (1.5372) loss_oracle 1.1243 (1.0954) acc 59.3750 (70.0000) lr 6.3188e-04 eta 0:12:42
epoch [33/50] batch [40/319] time 0.081 (0.106) data 0.000 (0.014) loss 2.3422 (2.3682) teacher_loss 0.8069 (0.8272) loss_zs_kd 1.4323 (1.4861) loss_oracle 1.0635 (1.0866) acc 68.7500 (70.6250) lr 6.3188e-04 eta 0:10:02
epoch [33/50] batch [60/319] time 0.081 (0.099) data 0.000 (0.010) loss 1.8999 (2.3558) teacher_loss 0.3694 (0.8129) loss_zs_kd 1.2819 (1.4453) loss_oracle 1.1059 (1.0869) acc 93.7500 (71.6146) lr 6.3188e-04 eta 0:09:20
epoch [33/50] batch [80/319] time 0.080 (0.095) data 0.000 (0.007) loss 2.2155 (2.3501) teacher_loss 0.7351 (0.8074) loss_zs_kd 1.6598 (1.4487) loss_oracle 1.0166 (1.0818) acc 78.1250 (71.8750) lr 6.3188e-04 eta 0:08:55
epoch [33/50] batch [100/319] time 0.086 (0.092) data 0.000 (0.006) loss 2.6835 (2.3528) teacher_loss 1.1236 (0.8092) loss_zs_kd 1.4713 (1.4642) loss_oracle 1.1013 (1.0834) acc 65.6250 (71.7500) lr 6.3188e-04 eta 0:08:37
epoch [33/50] batch [120/319] time 0.097 (0.090) data 0.000 (0.005) loss 2.7658 (2.3611) teacher_loss 1.2902 (0.8207) loss_zs_kd 1.4004 (1.4621) loss_oracle 1.0714 (1.0817) acc 59.3750 (71.0677) lr 6.3188e-04 eta 0:08:23
epoch [33/50] batch [140/319] time 0.079 (0.088) data 0.000 (0.004) loss 2.2851 (2.3718) teacher_loss 0.7466 (0.8312) loss_zs_kd 1.4540 (1.4449) loss_oracle 1.0408 (1.0808) acc 75.0000 (70.8259) lr 6.3188e-04 eta 0:08:14
epoch [33/50] batch [160/319] time 0.077 (0.087) data 0.000 (0.004) loss 2.0067 (2.3670) teacher_loss 0.4979 (0.8272) loss_zs_kd 1.1333 (1.4308) loss_oracle 1.0594 (1.0801) acc 78.1250 (70.8789) lr 6.3188e-04 eta 0:08:06
epoch [33/50] batch [180/319] time 0.066 (0.086) data 0.000 (0.003) loss 2.5594 (2.3763) teacher_loss 1.0132 (0.8376) loss_zs_kd 1.7510 (1.4140) loss_oracle 1.0508 (1.0794) acc 62.5000 (70.7118) lr 6.3188e-04 eta 0:07:56
epoch [33/50] batch [200/319] time 0.060 (0.084) data 0.000 (0.003) loss 2.1526 (2.3738) teacher_loss 0.6202 (0.8351) loss_zs_kd 1.7669 (1.4129) loss_oracle 1.0495 (1.0791) acc 81.2500 (70.9062) lr 6.3188e-04 eta 0:07:42
epoch [33/50] batch [220/319] time 0.064 (0.082) data 0.000 (0.003) loss 1.9908 (2.3693) teacher_loss 0.4401 (0.8311) loss_zs_kd 1.3729 (1.4073) loss_oracle 1.1065 (1.0782) acc 87.5000 (71.0369) lr 6.3188e-04 eta 0:07:31
epoch [33/50] batch [240/319] time 0.064 (0.080) data 0.000 (0.003) loss 2.1946 (2.3657) teacher_loss 0.6675 (0.8265) loss_zs_kd 1.6230 (1.4122) loss_oracle 1.0248 (1.0776) acc 75.0000 (71.1198) lr 6.3188e-04 eta 0:07:22
epoch [33/50] batch [260/319] time 0.065 (0.079) data 0.000 (0.002) loss 2.4029 (2.3727) teacher_loss 0.8975 (0.8333) loss_zs_kd 1.4044 (1.4170) loss_oracle 1.1059 (1.0778) acc 65.6250 (70.9135) lr 6.3188e-04 eta 0:07:14
epoch [33/50] batch [280/319] time 0.062 (0.078) data 0.000 (0.002) loss 2.3755 (2.3690) teacher_loss 0.8313 (0.8299) loss_zs_kd 1.2169 (1.4190) loss_oracle 1.0597 (1.0786) acc 78.1250 (71.0156) lr 6.3188e-04 eta 0:07:06
epoch [33/50] batch [300/319] time 0.061 (0.077) data 0.000 (0.002) loss 2.2300 (2.3698) teacher_loss 0.6361 (0.8309) loss_zs_kd 1.2642 (1.4208) loss_oracle 1.0780 (1.0788) acc 78.1250 (70.9583) lr 6.3188e-04 eta 0:06:59
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,690
* accuracy: 61.4%
* error: 38.6%
* macro_f1: 55.3%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,812
* accuracy: 59.7%
* error: 40.3%
* macro_f1: 26.9%
******* Domain 2 best val acc:      63.0%, epoch: 32 *******
******* Domain 2 best val test acc: 56.7%, epoch: 32 *******
******* Domain 2 best test acc:     59.9%, epoch: 25 *******
epoch [34/50] batch [20/319] time 0.077 (0.115) data 0.000 (0.032) loss 2.4393 (2.3234) teacher_loss 0.8855 (0.7745) loss_zs_kd 1.6113 (1.3431) loss_oracle 1.0841 (1.1008) acc 75.0000 (72.6562) lr 5.7422e-04 eta 0:10:19
epoch [34/50] batch [40/319] time 0.078 (0.096) data 0.000 (0.016) loss 2.2418 (2.3375) teacher_loss 0.7435 (0.7953) loss_zs_kd 1.2021 (1.3890) loss_oracle 1.0645 (1.0873) acc 71.8750 (71.7188) lr 5.7422e-04 eta 0:08:35
epoch [34/50] batch [60/319] time 0.059 (0.090) data 0.001 (0.011) loss 2.5771 (2.3299) teacher_loss 1.0053 (0.7880) loss_zs_kd 1.4216 (1.3884) loss_oracle 1.1059 (1.0846) acc 62.5000 (71.7708) lr 5.7422e-04 eta 0:08:04
epoch [34/50] batch [80/319] time 0.080 (0.086) data 0.000 (0.008) loss 2.3354 (2.3310) teacher_loss 0.7534 (0.7872) loss_zs_kd 1.4845 (1.3789) loss_oracle 1.1309 (1.0853) acc 71.8750 (71.3281) lr 5.7422e-04 eta 0:07:38
epoch [34/50] batch [100/319] time 0.076 (0.084) data 0.000 (0.007) loss 2.1926 (2.3338) teacher_loss 0.5739 (0.7910) loss_zs_kd 1.6811 (1.4103) loss_oracle 1.1563 (1.0862) acc 84.3750 (71.3750) lr 5.7422e-04 eta 0:07:27
epoch [34/50] batch [120/319] time 0.077 (0.083) data 0.000 (0.005) loss 2.2821 (2.3190) teacher_loss 0.7443 (0.7734) loss_zs_kd 1.5260 (1.4275) loss_oracle 1.0847 (1.0867) acc 78.1250 (72.0052) lr 5.7422e-04 eta 0:07:21
epoch [34/50] batch [140/319] time 0.084 (0.083) data 0.000 (0.005) loss 2.4734 (2.3274) teacher_loss 0.8621 (0.7792) loss_zs_kd 1.5635 (1.4388) loss_oracle 1.1565 (1.0869) acc 68.7500 (71.8080) lr 5.7422e-04 eta 0:07:16
epoch [34/50] batch [160/319] time 0.078 (0.083) data 0.000 (0.004) loss 2.2971 (2.3303) teacher_loss 0.7748 (0.7827) loss_zs_kd 1.6039 (1.4498) loss_oracle 1.1034 (1.0869) acc 75.0000 (71.9531) lr 5.7422e-04 eta 0:07:16
epoch [34/50] batch [180/319] time 0.077 (0.082) data 0.000 (0.004) loss 2.4568 (2.3431) teacher_loss 0.9357 (0.7947) loss_zs_kd 1.7875 (1.4566) loss_oracle 1.0662 (1.0866) acc 62.5000 (71.6493) lr 5.7422e-04 eta 0:07:11
epoch [34/50] batch [200/319] time 0.076 (0.082) data 0.000 (0.003) loss 2.4449 (2.3538) teacher_loss 0.9530 (0.8056) loss_zs_kd 1.1802 (1.4448) loss_oracle 1.0509 (1.0845) acc 71.8750 (71.2969) lr 5.7422e-04 eta 0:07:08
epoch [34/50] batch [220/319] time 0.083 (0.082) data 0.000 (0.003) loss 2.1331 (2.3565) teacher_loss 0.5860 (0.8089) loss_zs_kd 1.4630 (1.4414) loss_oracle 1.0984 (1.0843) acc 81.2500 (71.3068) lr 5.7422e-04 eta 0:07:05
epoch [34/50] batch [240/319] time 0.078 (0.081) data 0.000 (0.003) loss 2.4249 (2.3533) teacher_loss 0.9254 (0.8073) loss_zs_kd 1.4988 (1.4405) loss_oracle 1.0644 (1.0827) acc 56.2500 (71.4323) lr 5.7422e-04 eta 0:07:02
epoch [34/50] batch [260/319] time 0.079 (0.083) data 0.001 (0.003) loss 2.0894 (2.3497) teacher_loss 0.6168 (0.8039) loss_zs_kd 1.2505 (1.4350) loss_oracle 0.9870 (1.0816) acc 81.2500 (71.7188) lr 5.7422e-04 eta 0:07:08
epoch [34/50] batch [280/319] time 0.086 (0.083) data 0.000 (0.002) loss 2.2818 (2.3499) teacher_loss 0.7430 (0.8039) loss_zs_kd 1.5044 (1.4283) loss_oracle 1.0941 (1.0819) acc 65.6250 (71.8192) lr 5.7422e-04 eta 0:07:05
epoch [34/50] batch [300/319] time 0.086 (0.083) data 0.000 (0.002) loss 2.2342 (2.3513) teacher_loss 0.7094 (0.8066) loss_zs_kd 1.3630 (1.4236) loss_oracle 1.1354 (1.0809) acc 84.3750 (71.8542) lr 5.7422e-04 eta 0:07:04
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,708
* accuracy: 61.9%
* error: 38.1%
* macro_f1: 56.2%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,540
* accuracy: 56.9%
* error: 43.1%
* macro_f1: 26.4%
******* Domain 2 best val acc:      63.0%, epoch: 32 *******
******* Domain 2 best val test acc: 56.7%, epoch: 32 *******
******* Domain 2 best test acc:     59.9%, epoch: 25 *******
epoch [35/50] batch [20/319] time 0.069 (0.115) data 0.000 (0.029) loss 2.5708 (2.3236) teacher_loss 0.9992 (0.7770) loss_zs_kd 1.3906 (1.3453) loss_oracle 1.1075 (1.0846) acc 65.6250 (73.5938) lr 5.1825e-04 eta 0:09:46
epoch [35/50] batch [40/319] time 0.076 (0.098) data 0.000 (0.015) loss 2.1921 (2.3066) teacher_loss 0.6587 (0.7632) loss_zs_kd 1.5219 (1.3582) loss_oracle 1.0793 (1.0772) acc 81.2500 (74.3750) lr 5.1825e-04 eta 0:08:15
epoch [35/50] batch [60/319] time 0.073 (0.098) data 0.001 (0.010) loss 2.3146 (2.3414) teacher_loss 0.8315 (0.8040) loss_zs_kd 1.3637 (1.3592) loss_oracle 1.0420 (1.0753) acc 75.0000 (72.7083) lr 5.1825e-04 eta 0:08:15
epoch [35/50] batch [80/319] time 0.079 (0.093) data 0.000 (0.007) loss 2.4822 (2.3417) teacher_loss 0.9265 (0.8033) loss_zs_kd 1.4085 (1.3643) loss_oracle 1.0755 (1.0735) acc 78.1250 (73.1641) lr 5.1825e-04 eta 0:07:48
epoch [35/50] batch [100/319] time 0.056 (0.090) data 0.000 (0.006) loss 2.5097 (2.3364) teacher_loss 1.0533 (0.7995) loss_zs_kd 1.4594 (1.3679) loss_oracle 1.0400 (1.0743) acc 65.6250 (73.1250) lr 5.1825e-04 eta 0:07:30
epoch [35/50] batch [120/319] time 0.078 (0.087) data 0.000 (0.005) loss 2.5304 (2.3411) teacher_loss 0.9706 (0.8047) loss_zs_kd 1.3966 (1.3777) loss_oracle 1.1176 (1.0727) acc 68.7500 (72.5000) lr 5.1825e-04 eta 0:07:11
epoch [35/50] batch [140/319] time 0.085 (0.086) data 0.000 (0.004) loss 2.4193 (2.3378) teacher_loss 0.9313 (0.8017) loss_zs_kd 1.4236 (1.3794) loss_oracle 1.0810 (1.0708) acc 75.0000 (72.7455) lr 5.1825e-04 eta 0:07:04
epoch [35/50] batch [160/319] time 0.089 (0.085) data 0.000 (0.004) loss 2.5146 (2.3411) teacher_loss 0.9391 (0.8056) loss_zs_kd 1.3885 (1.3844) loss_oracle 1.1416 (1.0714) acc 68.7500 (72.4609) lr 5.1825e-04 eta 0:07:01
epoch [35/50] batch [180/319] time 0.086 (0.085) data 0.000 (0.003) loss 2.6639 (2.3544) teacher_loss 1.1023 (0.8177) loss_zs_kd 1.0856 (1.3730) loss_oracle 1.0761 (1.0708) acc 65.6250 (71.9444) lr 5.1825e-04 eta 0:06:59
epoch [35/50] batch [200/319] time 0.079 (0.085) data 0.000 (0.003) loss 2.3269 (2.3648) teacher_loss 0.7983 (0.8289) loss_zs_kd 1.1021 (1.3617) loss_oracle 1.0746 (1.0700) acc 75.0000 (71.7344) lr 5.1825e-04 eta 0:06:57
epoch [35/50] batch [220/319] time 0.085 (0.085) data 0.000 (0.003) loss 2.7321 (2.3583) teacher_loss 1.1887 (0.8250) loss_zs_kd 1.1771 (1.3486) loss_oracle 1.0782 (1.0686) acc 68.7500 (71.8608) lr 5.1825e-04 eta 0:06:55
epoch [35/50] batch [240/319] time 0.080 (0.085) data 0.000 (0.003) loss 2.4990 (2.3614) teacher_loss 0.9233 (0.8298) loss_zs_kd 1.2930 (1.3383) loss_oracle 1.1006 (1.0671) acc 59.3750 (71.5885) lr 5.1825e-04 eta 0:06:52
epoch [35/50] batch [260/319] time 0.086 (0.085) data 0.001 (0.002) loss 2.2233 (2.3565) teacher_loss 0.6890 (0.8260) loss_zs_kd 1.3664 (1.3359) loss_oracle 1.0327 (1.0666) acc 75.0000 (71.5264) lr 5.1825e-04 eta 0:06:49
epoch [35/50] batch [280/319] time 0.079 (0.084) data 0.000 (0.002) loss 2.4456 (2.3606) teacher_loss 0.8804 (0.8304) loss_zs_kd 1.4989 (1.3431) loss_oracle 1.0961 (1.0671) acc 68.7500 (71.4955) lr 5.1825e-04 eta 0:06:46
epoch [35/50] batch [300/319] time 0.088 (0.084) data 0.000 (0.002) loss 2.3194 (2.3599) teacher_loss 0.7893 (0.8304) loss_zs_kd 1.4550 (1.3473) loss_oracle 1.0710 (1.0675) acc 75.0000 (71.5625) lr 5.1825e-04 eta 0:06:44
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,760
* accuracy: 63.0%
* error: 37.0%
* macro_f1: 57.6%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,840
* accuracy: 60.0%
* error: 40.0%
* macro_f1: 26.6%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain 2 best val acc:      63.0%, epoch: 32 *******
******* Domain 2 best val test acc: 56.7%, epoch: 32 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [36/50] batch [20/319] time 0.079 (0.103) data 0.000 (0.027) loss 2.3231 (2.3836) teacher_loss 0.8574 (0.8590) loss_zs_kd 1.0140 (1.3525) loss_oracle 0.9956 (1.0724) acc 62.5000 (67.1875) lr 4.6417e-04 eta 0:08:10
epoch [36/50] batch [40/319] time 0.081 (0.092) data 0.000 (0.014) loss 2.4018 (2.4006) teacher_loss 0.8787 (0.8756) loss_zs_kd 1.3682 (1.3724) loss_oracle 1.0659 (1.0648) acc 71.8750 (67.2656) lr 4.6417e-04 eta 0:07:14
epoch [36/50] batch [60/319] time 0.081 (0.087) data 0.000 (0.009) loss 2.4268 (2.3750) teacher_loss 0.8578 (0.8515) loss_zs_kd 1.6075 (1.4014) loss_oracle 1.0536 (1.0628) acc 68.7500 (69.1146) lr 4.6417e-04 eta 0:06:50
epoch [36/50] batch [80/319] time 0.085 (0.086) data 0.000 (0.007) loss 2.2647 (2.3849) teacher_loss 0.7317 (0.8604) loss_zs_kd 1.5918 (1.4026) loss_oracle 1.0761 (1.0640) acc 78.1250 (69.0625) lr 4.6417e-04 eta 0:06:44
epoch [36/50] batch [100/319] time 0.087 (0.086) data 0.000 (0.006) loss 2.2965 (2.3823) teacher_loss 0.6932 (0.8588) loss_zs_kd 1.5688 (1.4059) loss_oracle 1.1030 (1.0621) acc 84.3750 (69.5312) lr 4.6417e-04 eta 0:06:41
epoch [36/50] batch [120/319] time 0.081 (0.085) data 0.000 (0.005) loss 2.2018 (2.3833) teacher_loss 0.6558 (0.8596) loss_zs_kd 1.6127 (1.4058) loss_oracle 1.0784 (1.0639) acc 68.7500 (69.2969) lr 4.6417e-04 eta 0:06:37
epoch [36/50] batch [140/319] time 0.082 (0.085) data 0.000 (0.004) loss 2.4072 (2.3774) teacher_loss 0.9101 (0.8533) loss_zs_kd 1.2887 (1.4121) loss_oracle 1.0272 (1.0628) acc 62.5000 (69.7768) lr 4.6417e-04 eta 0:06:34
epoch [36/50] batch [160/319] time 0.083 (0.085) data 0.000 (0.004) loss 2.3462 (2.3738) teacher_loss 0.7714 (0.8480) loss_zs_kd 1.2521 (1.4114) loss_oracle 1.0773 (1.0637) acc 75.0000 (70.0000) lr 4.6417e-04 eta 0:06:33
epoch [36/50] batch [180/319] time 0.078 (0.085) data 0.000 (0.003) loss 2.3682 (2.3670) teacher_loss 0.8105 (0.8407) loss_zs_kd 1.5976 (1.4042) loss_oracle 1.1179 (1.0645) acc 71.8750 (70.2431) lr 4.6417e-04 eta 0:06:31
epoch [36/50] batch [200/319] time 0.080 (0.085) data 0.000 (0.003) loss 2.2145 (2.3642) teacher_loss 0.6353 (0.8375) loss_zs_kd 1.3655 (1.4030) loss_oracle 1.1187 (1.0650) acc 71.8750 (70.4062) lr 4.6417e-04 eta 0:06:28
epoch [36/50] batch [220/319] time 0.080 (0.084) data 0.000 (0.003) loss 2.2286 (2.3622) teacher_loss 0.7494 (0.8350) loss_zs_kd 1.5063 (1.4072) loss_oracle 1.0193 (1.0651) acc 75.0000 (70.5966) lr 4.6417e-04 eta 0:06:25
epoch [36/50] batch [240/319] time 0.149 (0.085) data 0.000 (0.003) loss 2.2812 (2.3601) teacher_loss 0.7927 (0.8338) loss_zs_kd 1.6257 (1.4133) loss_oracle 0.9881 (1.0645) acc 71.8750 (70.7292) lr 4.6417e-04 eta 0:06:25
epoch [36/50] batch [260/319] time 0.069 (0.085) data 0.000 (0.002) loss 2.6459 (2.3610) teacher_loss 1.0666 (0.8348) loss_zs_kd 1.1670 (1.4137) loss_oracle 1.1211 (1.0656) acc 53.1250 (70.5288) lr 4.6417e-04 eta 0:06:24
epoch [36/50] batch [280/319] time 0.081 (0.084) data 0.000 (0.002) loss 2.5417 (2.3603) teacher_loss 1.0379 (0.8327) loss_zs_kd 1.3669 (1.4160) loss_oracle 1.0951 (1.0665) acc 62.5000 (70.6808) lr 4.6417e-04 eta 0:06:19
epoch [36/50] batch [300/319] time 0.074 (0.084) data 0.000 (0.002) loss 2.4570 (2.3593) teacher_loss 0.9292 (0.8316) loss_zs_kd 1.3962 (1.4164) loss_oracle 1.0274 (1.0656) acc 68.7500 (70.8021) lr 4.6417e-04 eta 0:06:14
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,767
* accuracy: 63.2%
* error: 36.8%
* macro_f1: 57.1%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,708
* accuracy: 58.6%
* error: 41.4%
* macro_f1: 25.2%
******* Domain 2 best val acc:      63.2%, epoch: 36 *******
******* Domain 2 best val test acc: 58.6%, epoch: 36 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [37/50] batch [20/319] time 0.082 (0.112) data 0.000 (0.025) loss 2.3866 (2.3326) teacher_loss 0.8642 (0.7873) loss_zs_kd 1.5694 (1.3819) loss_oracle 1.0328 (1.0700) acc 68.7500 (71.8750) lr 4.1221e-04 eta 0:08:15
epoch [37/50] batch [40/319] time 0.076 (0.104) data 0.000 (0.013) loss 2.5465 (2.3353) teacher_loss 1.0141 (0.7975) loss_zs_kd 1.3284 (1.3729) loss_oracle 1.0338 (1.0659) acc 71.8750 (72.0312) lr 4.1221e-04 eta 0:07:41
epoch [37/50] batch [60/319] time 0.087 (0.096) data 0.000 (0.009) loss 2.4226 (2.3683) teacher_loss 0.9104 (0.8265) loss_zs_kd 1.5282 (1.3650) loss_oracle 1.0923 (1.0714) acc 65.6250 (71.3542) lr 4.1221e-04 eta 0:07:03
epoch [37/50] batch [80/319] time 0.078 (0.093) data 0.000 (0.006) loss 2.5977 (2.3837) teacher_loss 0.9988 (0.8420) loss_zs_kd 1.4506 (1.3646) loss_oracle 1.1303 (1.0727) acc 56.2500 (70.4688) lr 4.1221e-04 eta 0:06:48
epoch [37/50] batch [100/319] time 0.088 (0.091) data 0.000 (0.005) loss 2.2629 (2.3723) teacher_loss 0.7599 (0.8337) loss_zs_kd 1.5965 (1.3741) loss_oracle 1.0895 (1.0748) acc 75.0000 (70.4375) lr 4.1221e-04 eta 0:06:38
epoch [37/50] batch [120/319] time 0.079 (0.090) data 0.000 (0.004) loss 2.5721 (2.3762) teacher_loss 0.9833 (0.8369) loss_zs_kd 1.3869 (1.3723) loss_oracle 1.1424 (1.0752) acc 71.8750 (70.5729) lr 4.1221e-04 eta 0:06:32
epoch [37/50] batch [140/319] time 0.070 (0.089) data 0.000 (0.004) loss 2.1841 (2.3692) teacher_loss 0.6355 (0.8302) loss_zs_kd 1.4351 (1.3710) loss_oracle 1.0842 (1.0743) acc 75.0000 (70.5804) lr 4.1221e-04 eta 0:06:25
epoch [37/50] batch [160/319] time 0.079 (0.088) data 0.000 (0.003) loss 2.5397 (2.3706) teacher_loss 0.9485 (0.8336) loss_zs_kd 1.2432 (1.3677) loss_oracle 1.0724 (1.0732) acc 65.6250 (70.4492) lr 4.1221e-04 eta 0:06:17
epoch [37/50] batch [180/319] time 0.062 (0.087) data 0.000 (0.003) loss 2.3605 (2.3683) teacher_loss 0.8629 (0.8325) loss_zs_kd 1.6997 (1.3621) loss_oracle 1.0916 (1.0730) acc 71.8750 (70.5903) lr 4.1221e-04 eta 0:06:13
epoch [37/50] batch [200/319] time 0.074 (0.086) data 0.000 (0.003) loss 2.4291 (2.3672) teacher_loss 0.8755 (0.8315) loss_zs_kd 1.2082 (1.3624) loss_oracle 0.9834 (1.0717) acc 78.1250 (70.6562) lr 4.1221e-04 eta 0:06:07
epoch [37/50] batch [220/319] time 0.081 (0.085) data 0.000 (0.003) loss 2.6191 (2.3642) teacher_loss 1.0513 (0.8278) loss_zs_kd 1.2957 (1.3577) loss_oracle 1.0534 (1.0710) acc 65.6250 (70.9375) lr 4.1221e-04 eta 0:06:02
epoch [37/50] batch [240/319] time 0.092 (0.085) data 0.000 (0.002) loss 2.4775 (2.3597) teacher_loss 0.9078 (0.8229) loss_zs_kd 1.3794 (1.3546) loss_oracle 1.0458 (1.0708) acc 71.8750 (71.1458) lr 4.1221e-04 eta 0:05:58
epoch [37/50] batch [260/319] time 0.080 (0.084) data 0.000 (0.002) loss 2.4954 (2.3672) teacher_loss 0.9705 (0.8305) loss_zs_kd 1.3113 (1.3541) loss_oracle 1.0980 (1.0715) acc 68.7500 (70.6971) lr 4.1221e-04 eta 0:05:54
epoch [37/50] batch [280/319] time 0.081 (0.084) data 0.000 (0.002) loss 2.3746 (2.3635) teacher_loss 0.8073 (0.8279) loss_zs_kd 1.3044 (1.3546) loss_oracle 1.1117 (1.0709) acc 75.0000 (70.7478) lr 4.1221e-04 eta 0:05:51
epoch [37/50] batch [300/319] time 0.085 (0.084) data 0.000 (0.002) loss 2.3169 (2.3605) teacher_loss 0.7974 (0.8239) loss_zs_kd 1.4142 (1.3589) loss_oracle 1.0335 (1.0711) acc 65.6250 (70.8958) lr 4.1221e-04 eta 0:05:49
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,789
* accuracy: 63.7%
* error: 36.3%
* macro_f1: 58.0%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,726
* accuracy: 58.8%
* error: 41.2%
* macro_f1: 25.7%
******* Domain 2 best val acc:      63.7%, epoch: 37 *******
******* Domain 2 best val test acc: 58.8%, epoch: 37 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [38/50] batch [20/319] time 0.086 (0.105) data 0.000 (0.025) loss 2.3438 (2.4345) teacher_loss 0.7612 (0.9063) loss_zs_kd 1.4438 (1.3055) loss_oracle 1.1087 (1.0724) acc 68.7500 (68.5938) lr 3.6258e-04 eta 0:07:13
epoch [38/50] batch [40/319] time 0.086 (0.094) data 0.000 (0.013) loss 2.6836 (2.3933) teacher_loss 1.0809 (0.8597) loss_zs_kd 1.1945 (1.3203) loss_oracle 1.0904 (1.0631) acc 59.3750 (70.1562) lr 3.6258e-04 eta 0:06:26
epoch [38/50] batch [60/319] time 0.097 (0.089) data 0.001 (0.009) loss 2.5908 (2.3961) teacher_loss 1.0567 (0.8576) loss_zs_kd 1.4754 (1.3454) loss_oracle 1.0673 (1.0703) acc 78.1250 (70.4688) lr 3.6258e-04 eta 0:06:05
epoch [38/50] batch [80/319] time 0.067 (0.086) data 0.000 (0.007) loss 2.5896 (2.3745) teacher_loss 1.0796 (0.8362) loss_zs_kd 1.9086 (1.3735) loss_oracle 1.1080 (1.0739) acc 59.3750 (70.5859) lr 3.6258e-04 eta 0:05:51
epoch [38/50] batch [100/319] time 0.080 (0.083) data 0.000 (0.005) loss 2.3002 (2.3616) teacher_loss 0.7469 (0.8200) loss_zs_kd 1.5905 (1.3952) loss_oracle 1.0896 (1.0747) acc 75.0000 (71.1562) lr 3.6258e-04 eta 0:05:36
epoch [38/50] batch [120/319] time 0.067 (0.081) data 0.000 (0.004) loss 2.2494 (2.3616) teacher_loss 0.6789 (0.8183) loss_zs_kd 1.4694 (1.4007) loss_oracle 1.1042 (1.0771) acc 71.8750 (71.2760) lr 3.6258e-04 eta 0:05:27
epoch [38/50] batch [140/319] time 0.079 (0.080) data 0.000 (0.004) loss 2.5677 (2.3611) teacher_loss 0.9864 (0.8154) loss_zs_kd 1.5257 (1.4104) loss_oracle 1.1455 (1.0782) acc 68.7500 (71.4732) lr 3.6258e-04 eta 0:05:21
epoch [38/50] batch [160/319] time 0.076 (0.080) data 0.000 (0.003) loss 2.2261 (2.3539) teacher_loss 0.7029 (0.8090) loss_zs_kd 1.3159 (1.4106) loss_oracle 1.0470 (1.0775) acc 78.1250 (71.6797) lr 3.6258e-04 eta 0:05:17
epoch [38/50] batch [180/319] time 0.064 (0.080) data 0.000 (0.003) loss 2.3487 (2.3579) teacher_loss 0.8118 (0.8131) loss_zs_kd 1.3084 (1.4064) loss_oracle 1.0851 (1.0782) acc 62.5000 (71.4410) lr 3.6258e-04 eta 0:05:17
epoch [38/50] batch [200/319] time 0.085 (0.080) data 0.000 (0.003) loss 2.4789 (2.3596) teacher_loss 0.9924 (0.8146) loss_zs_kd 1.4322 (1.4101) loss_oracle 1.0532 (1.0784) acc 59.3750 (71.3281) lr 3.6258e-04 eta 0:05:15
epoch [38/50] batch [220/319] time 0.088 (0.080) data 0.000 (0.003) loss 2.4515 (2.3556) teacher_loss 0.9634 (0.8116) loss_zs_kd 1.2437 (1.4052) loss_oracle 1.0102 (1.0771) acc 65.6250 (71.3636) lr 3.6258e-04 eta 0:05:15
epoch [38/50] batch [240/319] time 0.080 (0.082) data 0.000 (0.002) loss 2.2761 (2.3499) teacher_loss 0.8039 (0.8069) loss_zs_kd 1.1516 (1.4082) loss_oracle 1.0643 (1.0758) acc 68.7500 (71.4323) lr 3.6258e-04 eta 0:05:20
epoch [38/50] batch [260/319] time 0.076 (0.082) data 0.000 (0.002) loss 2.6463 (2.3542) teacher_loss 1.1510 (0.8115) loss_zs_kd 1.6820 (1.4109) loss_oracle 1.0622 (1.0757) acc 71.8750 (71.4303) lr 3.6258e-04 eta 0:05:19
epoch [38/50] batch [280/319] time 0.068 (0.082) data 0.000 (0.002) loss 2.1938 (2.3568) teacher_loss 0.6651 (0.8140) loss_zs_kd 1.5873 (1.4161) loss_oracle 1.0510 (1.0752) acc 75.0000 (71.3393) lr 3.6258e-04 eta 0:05:16
epoch [38/50] batch [300/319] time 0.084 (0.082) data 0.000 (0.002) loss 2.6763 (2.3626) teacher_loss 1.1863 (0.8201) loss_zs_kd 1.7366 (1.4229) loss_oracle 1.0367 (1.0744) acc 59.3750 (71.0729) lr 3.6258e-04 eta 0:05:14
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,714
* accuracy: 62.0%
* error: 38.0%
* macro_f1: 57.4%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,783
* accuracy: 59.4%
* error: 40.6%
* macro_f1: 25.3%
******* Domain 2 best val acc:      63.7%, epoch: 37 *******
******* Domain 2 best val test acc: 58.8%, epoch: 37 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [39/50] batch [20/319] time 0.083 (0.113) data 0.000 (0.028) loss 2.4337 (2.3993) teacher_loss 0.9115 (0.8574) loss_zs_kd 1.4927 (1.3959) loss_oracle 1.1039 (1.0768) acc 68.7500 (72.8125) lr 3.1545e-04 eta 0:07:11
epoch [39/50] batch [40/319] time 0.084 (0.108) data 0.000 (0.014) loss 2.6969 (2.3893) teacher_loss 1.1846 (0.8503) loss_zs_kd 1.3611 (1.3964) loss_oracle 1.0078 (1.0759) acc 56.2500 (72.5000) lr 3.1545e-04 eta 0:06:50
epoch [39/50] batch [60/319] time 0.075 (0.100) data 0.000 (0.010) loss 2.0967 (2.3878) teacher_loss 0.5740 (0.8461) loss_zs_kd 1.3092 (1.3947) loss_oracle 1.0892 (1.0767) acc 87.5000 (72.1354) lr 3.1545e-04 eta 0:06:16
epoch [39/50] batch [80/319] time 0.078 (0.095) data 0.000 (0.007) loss 2.6354 (2.3819) teacher_loss 1.1294 (0.8451) loss_zs_kd 1.6554 (1.4043) loss_oracle 1.0264 (1.0748) acc 59.3750 (71.0938) lr 3.1545e-04 eta 0:05:54
epoch [39/50] batch [100/319] time 0.074 (0.092) data 0.000 (0.006) loss 2.6549 (2.3887) teacher_loss 1.0827 (0.8514) loss_zs_kd 1.3958 (1.4018) loss_oracle 1.0678 (1.0748) acc 62.5000 (71.1875) lr 3.1545e-04 eta 0:05:42
epoch [39/50] batch [120/319] time 0.080 (0.090) data 0.000 (0.005) loss 2.4920 (2.3780) teacher_loss 0.9427 (0.8420) loss_zs_kd 1.2009 (1.4027) loss_oracle 1.1006 (1.0726) acc 59.3750 (71.4062) lr 3.1545e-04 eta 0:05:33
epoch [39/50] batch [140/319] time 0.067 (0.088) data 0.000 (0.004) loss 2.2520 (2.3736) teacher_loss 0.6273 (0.8355) loss_zs_kd 1.5729 (1.4039) loss_oracle 1.1259 (1.0730) acc 75.0000 (71.8080) lr 3.1545e-04 eta 0:05:25
epoch [39/50] batch [160/319] time 0.078 (0.087) data 0.000 (0.004) loss 2.1922 (2.3689) teacher_loss 0.7195 (0.8331) loss_zs_kd 1.6243 (1.4081) loss_oracle 0.9846 (1.0705) acc 65.6250 (71.6602) lr 3.1545e-04 eta 0:05:19
epoch [39/50] batch [180/319] time 0.086 (0.087) data 0.000 (0.003) loss 2.1880 (2.3692) teacher_loss 0.7780 (0.8348) loss_zs_kd 1.4068 (1.4060) loss_oracle 1.0079 (1.0686) acc 75.0000 (71.6667) lr 3.1545e-04 eta 0:05:16
epoch [39/50] batch [200/319] time 0.077 (0.086) data 0.000 (0.003) loss 2.1745 (2.3656) teacher_loss 0.6407 (0.8328) loss_zs_kd 1.2037 (1.4020) loss_oracle 1.0829 (1.0683) acc 84.3750 (71.5312) lr 3.1545e-04 eta 0:05:13
epoch [39/50] batch [220/319] time 0.077 (0.085) data 0.000 (0.003) loss 2.2862 (2.3621) teacher_loss 0.7741 (0.8290) loss_zs_kd 1.4542 (1.4057) loss_oracle 1.0785 (1.0691) acc 75.0000 (71.7898) lr 3.1545e-04 eta 0:05:07
epoch [39/50] batch [240/319] time 0.076 (0.084) data 0.000 (0.003) loss 2.5180 (2.3672) teacher_loss 1.0320 (0.8356) loss_zs_kd 1.3824 (1.4035) loss_oracle 1.0580 (1.0685) acc 65.6250 (71.4844) lr 3.1545e-04 eta 0:05:02
epoch [39/50] batch [260/319] time 0.078 (0.084) data 0.000 (0.002) loss 2.2507 (2.3692) teacher_loss 0.7333 (0.8380) loss_zs_kd 1.2914 (1.4004) loss_oracle 1.0547 (1.0674) acc 65.6250 (71.2861) lr 3.1545e-04 eta 0:04:58
epoch [39/50] batch [280/319] time 0.076 (0.083) data 0.000 (0.002) loss 2.7912 (2.3706) teacher_loss 1.2578 (0.8395) loss_zs_kd 1.3927 (1.3968) loss_oracle 1.0697 (1.0672) acc 53.1250 (71.1830) lr 3.1545e-04 eta 0:04:55
epoch [39/50] batch [300/319] time 0.073 (0.083) data 0.000 (0.002) loss 2.3669 (2.3707) teacher_loss 0.7635 (0.8398) loss_zs_kd 1.6529 (1.3940) loss_oracle 1.0881 (1.0668) acc 78.1250 (71.2292) lr 3.1545e-04 eta 0:04:52
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,805
* accuracy: 64.1%
* error: 35.9%
* macro_f1: 57.9%
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,668
* accuracy: 58.2%
* error: 41.8%
* macro_f1: 24.8%
******* Domain 2 best val acc:      64.1%, epoch: 39 *******
******* Domain 2 best val test acc: 58.2%, epoch: 39 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [40/50] batch [20/319] time 0.075 (0.111) data 0.000 (0.026) loss 2.3547 (2.3331) teacher_loss 0.7517 (0.8061) loss_zs_kd 1.2765 (1.3177) loss_oracle 1.0891 (1.0627) acc 71.8750 (71.8750) lr 2.7103e-04 eta 0:06:26
epoch [40/50] batch [40/319] time 0.074 (0.095) data 0.000 (0.013) loss 2.3377 (2.3913) teacher_loss 0.7858 (0.8615) loss_zs_kd 1.2811 (1.3412) loss_oracle 1.0511 (1.0634) acc 75.0000 (70.3906) lr 2.7103e-04 eta 0:05:29
epoch [40/50] batch [60/319] time 0.088 (0.091) data 0.000 (0.009) loss 2.3752 (2.3828) teacher_loss 0.8221 (0.8526) loss_zs_kd 1.4464 (1.3561) loss_oracle 1.0950 (1.0625) acc 65.6250 (70.6771) lr 2.7103e-04 eta 0:05:12
epoch [40/50] batch [80/319] time 0.081 (0.089) data 0.000 (0.007) loss 2.4120 (2.3633) teacher_loss 0.8869 (0.8336) loss_zs_kd 1.2639 (1.3670) loss_oracle 1.0987 (1.0654) acc 65.6250 (70.9375) lr 2.7103e-04 eta 0:05:03
epoch [40/50] batch [100/319] time 0.086 (0.086) data 0.001 (0.005) loss 2.2828 (2.3585) teacher_loss 0.7997 (0.8291) loss_zs_kd 1.1423 (1.3711) loss_oracle 0.9953 (1.0653) acc 71.8750 (71.2812) lr 2.7103e-04 eta 0:04:54
epoch [40/50] batch [120/319] time 0.084 (0.085) data 0.000 (0.004) loss 2.2903 (2.3456) teacher_loss 0.7422 (0.8196) loss_zs_kd 1.5716 (1.3778) loss_oracle 1.0718 (1.0637) acc 75.0000 (71.9271) lr 2.7103e-04 eta 0:04:48
epoch [40/50] batch [140/319] time 0.088 (0.085) data 0.000 (0.004) loss 2.3506 (2.3527) teacher_loss 0.8088 (0.8276) loss_zs_kd 1.5436 (1.3733) loss_oracle 1.0944 (1.0643) acc 71.8750 (71.6295) lr 2.7103e-04 eta 0:04:46
epoch [40/50] batch [160/319] time 0.084 (0.085) data 0.000 (0.003) loss 2.4649 (2.3477) teacher_loss 0.9622 (0.8214) loss_zs_kd 1.3741 (1.3860) loss_oracle 1.0647 (1.0653) acc 62.5000 (71.9922) lr 2.7103e-04 eta 0:04:43
epoch [40/50] batch [180/319] time 0.079 (0.084) data 0.000 (0.003) loss 2.7630 (2.3463) teacher_loss 1.2000 (0.8202) loss_zs_kd 1.4108 (1.3913) loss_oracle 1.0907 (1.0658) acc 65.6250 (72.0833) lr 2.7103e-04 eta 0:04:41
epoch [40/50] batch [200/319] time 0.071 (0.084) data 0.000 (0.003) loss 2.4722 (2.3596) teacher_loss 0.9448 (0.8322) loss_zs_kd 1.4251 (1.3952) loss_oracle 1.0931 (1.0676) acc 71.8750 (71.8125) lr 2.7103e-04 eta 0:04:38
epoch [40/50] batch [220/319] time 0.061 (0.082) data 0.000 (0.003) loss 2.3858 (2.3629) teacher_loss 0.9256 (0.8361) loss_zs_kd 1.4137 (1.3967) loss_oracle 1.0542 (1.0669) acc 56.2500 (71.4915) lr 2.7103e-04 eta 0:04:31
epoch [40/50] batch [240/319] time 0.118 (0.083) data 0.001 (0.002) loss 2.5949 (2.3585) teacher_loss 1.0552 (0.8320) loss_zs_kd 1.4128 (1.3952) loss_oracle 1.0662 (1.0670) acc 62.5000 (71.5495) lr 2.7103e-04 eta 0:04:30
epoch [40/50] batch [260/319] time 0.081 (0.083) data 0.000 (0.002) loss 2.3863 (2.3590) teacher_loss 0.8252 (0.8324) loss_zs_kd 1.6814 (1.4033) loss_oracle 1.0589 (1.0672) acc 68.7500 (71.5264) lr 2.7103e-04 eta 0:04:29
epoch [40/50] batch [280/319] time 0.072 (0.083) data 0.000 (0.002) loss 2.6687 (2.3615) teacher_loss 1.1059 (0.8340) loss_zs_kd 1.3018 (1.4029) loss_oracle 1.0433 (1.0673) acc 59.3750 (71.3504) lr 2.7103e-04 eta 0:04:26
epoch [40/50] batch [300/319] time 0.077 (0.082) data 0.000 (0.002) loss 2.4526 (2.3656) teacher_loss 0.9780 (0.8381) loss_zs_kd 1.2409 (1.4029) loss_oracle 1.0663 (1.0665) acc 68.7500 (71.1667) lr 2.7103e-04 eta 0:04:24
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,739
* accuracy: 62.6%
* error: 37.4%
* macro_f1: 56.9%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,755
* accuracy: 59.1%
* error: 40.9%
* macro_f1: 25.6%
******* Domain 2 best val acc:      64.1%, epoch: 39 *******
******* Domain 2 best val test acc: 58.2%, epoch: 39 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [41/50] batch [20/319] time 0.080 (0.119) data 0.000 (0.035) loss 2.5627 (2.3028) teacher_loss 0.9634 (0.7747) loss_zs_kd 1.5185 (1.4470) loss_oracle 1.0683 (1.0631) acc 59.3750 (72.1875) lr 2.2949e-04 eta 0:06:17
epoch [41/50] batch [40/319] time 0.065 (0.105) data 0.000 (0.018) loss 2.5363 (2.3315) teacher_loss 0.9671 (0.8022) loss_zs_kd 1.6762 (1.4141) loss_oracle 1.0865 (1.0623) acc 68.7500 (71.4844) lr 2.2949e-04 eta 0:05:31
epoch [41/50] batch [60/319] time 0.088 (0.096) data 0.000 (0.012) loss 2.3570 (2.3472) teacher_loss 0.8915 (0.8176) loss_zs_kd 1.4540 (1.4134) loss_oracle 1.0974 (1.0651) acc 71.8750 (71.0417) lr 2.2949e-04 eta 0:04:59
epoch [41/50] batch [80/319] time 0.083 (0.093) data 0.000 (0.009) loss 2.2294 (2.3359) teacher_loss 0.6730 (0.8052) loss_zs_kd 1.5776 (1.4377) loss_oracle 0.9941 (1.0663) acc 71.8750 (71.7188) lr 2.2949e-04 eta 0:04:48
epoch [41/50] batch [100/319] time 0.091 (0.091) data 0.001 (0.007) loss 2.3569 (2.3407) teacher_loss 0.8524 (0.8098) loss_zs_kd 1.4220 (1.4417) loss_oracle 1.0792 (1.0666) acc 81.2500 (71.7188) lr 2.2949e-04 eta 0:04:41
epoch [41/50] batch [120/319] time 0.073 (0.089) data 0.000 (0.006) loss 2.1404 (2.3480) teacher_loss 0.5727 (0.8160) loss_zs_kd 1.4105 (1.4368) loss_oracle 1.0493 (1.0673) acc 78.1250 (71.5625) lr 2.2949e-04 eta 0:04:34
epoch [41/50] batch [140/319] time 0.083 (0.088) data 0.000 (0.005) loss 2.0803 (2.3412) teacher_loss 0.5487 (0.8093) loss_zs_kd 1.4122 (1.4320) loss_oracle 1.0236 (1.0663) acc 84.3750 (71.8973) lr 2.2949e-04 eta 0:04:29
epoch [41/50] batch [160/319] time 0.079 (0.088) data 0.000 (0.005) loss 2.5162 (2.3431) teacher_loss 0.9557 (0.8109) loss_zs_kd 1.3018 (1.4276) loss_oracle 1.0235 (1.0671) acc 68.7500 (71.9727) lr 2.2949e-04 eta 0:04:25
epoch [41/50] batch [180/319] time 0.078 (0.086) data 0.000 (0.004) loss 2.2381 (2.3466) teacher_loss 0.7813 (0.8140) loss_zs_kd 1.3206 (1.4251) loss_oracle 0.9535 (1.0672) acc 75.0000 (71.9097) lr 2.2949e-04 eta 0:04:20
epoch [41/50] batch [200/319] time 0.083 (0.086) data 0.000 (0.004) loss 2.1916 (2.3385) teacher_loss 0.6070 (0.8063) loss_zs_kd 1.4654 (1.4182) loss_oracle 1.0782 (1.0668) acc 78.1250 (72.1719) lr 2.2949e-04 eta 0:04:17
epoch [41/50] batch [220/319] time 0.077 (0.086) data 0.000 (0.003) loss 2.2938 (2.3380) teacher_loss 0.8163 (0.8072) loss_zs_kd 1.4673 (1.4177) loss_oracle 1.0177 (1.0654) acc 75.0000 (72.2727) lr 2.2949e-04 eta 0:04:14
epoch [41/50] batch [240/319] time 0.078 (0.085) data 0.000 (0.003) loss 2.4288 (2.3376) teacher_loss 0.9125 (0.8069) loss_zs_kd 1.4001 (1.4214) loss_oracle 1.0473 (1.0662) acc 59.3750 (72.3828) lr 2.2949e-04 eta 0:04:11
epoch [41/50] batch [260/319] time 0.080 (0.085) data 0.000 (0.003) loss 2.3887 (2.3414) teacher_loss 0.8486 (0.8109) loss_zs_kd 1.3780 (1.4207) loss_oracle 1.0482 (1.0658) acc 71.8750 (72.1394) lr 2.2949e-04 eta 0:04:09
epoch [41/50] batch [280/319] time 0.081 (0.085) data 0.000 (0.003) loss 2.2533 (2.3451) teacher_loss 0.8011 (0.8161) loss_zs_kd 1.1860 (1.4198) loss_oracle 1.1024 (1.0651) acc 84.3750 (71.9754) lr 2.2949e-04 eta 0:04:07
epoch [41/50] batch [300/319] time 0.083 (0.085) data 0.000 (0.003) loss 2.4471 (2.3444) teacher_loss 0.9503 (0.8157) loss_zs_kd 1.2509 (1.4150) loss_oracle 1.0754 (1.0654) acc 68.7500 (71.9479) lr 2.2949e-04 eta 0:04:04
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,752
* accuracy: 62.9%
* error: 37.1%
* macro_f1: 57.9%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,778
* accuracy: 59.3%
* error: 40.7%
* macro_f1: 26.0%
******* Domain 2 best val acc:      64.1%, epoch: 39 *******
******* Domain 2 best val test acc: 58.2%, epoch: 39 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [42/50] batch [20/319] time 0.083 (0.118) data 0.000 (0.029) loss 2.3342 (2.3621) teacher_loss 0.8160 (0.8486) loss_zs_kd 1.6187 (1.3654) loss_oracle 1.0452 (1.0526) acc 68.7500 (71.5625) lr 1.9098e-04 eta 0:05:37
epoch [42/50] batch [40/319] time 0.080 (0.101) data 0.000 (0.015) loss 2.0331 (2.3706) teacher_loss 0.4984 (0.8529) loss_zs_kd 1.3021 (1.3668) loss_oracle 1.0946 (1.0561) acc 81.2500 (69.6875) lr 1.9098e-04 eta 0:04:45
epoch [42/50] batch [60/319] time 0.090 (0.095) data 0.001 (0.010) loss 2.2509 (2.3733) teacher_loss 0.7245 (0.8560) loss_zs_kd 1.2656 (1.3795) loss_oracle 1.0171 (1.0547) acc 81.2500 (70.1562) lr 1.9098e-04 eta 0:04:26
epoch [42/50] batch [80/319] time 0.079 (0.091) data 0.000 (0.008) loss 2.5207 (2.3772) teacher_loss 1.0260 (0.8584) loss_zs_kd 1.4100 (1.3832) loss_oracle 1.0125 (1.0561) acc 53.1250 (70.0000) lr 1.9098e-04 eta 0:04:15
epoch [42/50] batch [100/319] time 0.083 (0.089) data 0.000 (0.006) loss 2.7416 (2.3872) teacher_loss 1.1785 (0.8639) loss_zs_kd 1.5196 (1.3839) loss_oracle 1.0993 (1.0570) acc 65.6250 (70.1562) lr 1.9098e-04 eta 0:04:07
epoch [42/50] batch [120/319] time 0.077 (0.087) data 0.000 (0.005) loss 2.4707 (2.3782) teacher_loss 0.9131 (0.8576) loss_zs_kd 1.5386 (1.3901) loss_oracle 1.0922 (1.0561) acc 68.7500 (70.4427) lr 1.9098e-04 eta 0:04:00
epoch [42/50] batch [140/319] time 0.072 (0.086) data 0.000 (0.004) loss 2.0940 (2.3744) teacher_loss 0.5659 (0.8553) loss_zs_kd 1.3755 (1.3877) loss_oracle 1.0710 (1.0542) acc 84.3750 (70.6027) lr 1.9098e-04 eta 0:03:55
epoch [42/50] batch [160/319] time 0.074 (0.085) data 0.000 (0.004) loss 2.8379 (2.3736) teacher_loss 1.2506 (0.8573) loss_zs_kd 1.4742 (1.3792) loss_oracle 1.1074 (1.0529) acc 56.2500 (70.4883) lr 1.9098e-04 eta 0:03:51
epoch [42/50] batch [180/319] time 0.081 (0.085) data 0.000 (0.004) loss 2.2796 (2.3685) teacher_loss 0.7303 (0.8517) loss_zs_kd 1.4466 (1.3866) loss_oracle 1.0937 (1.0546) acc 65.6250 (70.7639) lr 1.9098e-04 eta 0:03:47
epoch [42/50] batch [200/319] time 0.069 (0.084) data 0.000 (0.003) loss 2.4123 (2.3654) teacher_loss 0.9275 (0.8466) loss_zs_kd 1.1954 (1.3876) loss_oracle 0.9808 (1.0554) acc 62.5000 (70.8906) lr 1.9098e-04 eta 0:03:44
epoch [42/50] batch [220/319] time 0.082 (0.084) data 0.000 (0.003) loss 2.4408 (2.3585) teacher_loss 0.9087 (0.8387) loss_zs_kd 1.2493 (1.3899) loss_oracle 1.0618 (1.0553) acc 65.6250 (71.2074) lr 1.9098e-04 eta 0:03:42
epoch [42/50] batch [240/319] time 0.081 (0.086) data 0.000 (0.003) loss 2.4295 (2.3550) teacher_loss 0.9237 (0.8357) loss_zs_kd 1.4868 (1.3930) loss_oracle 1.0212 (1.0557) acc 71.8750 (71.3151) lr 1.9098e-04 eta 0:03:45
epoch [42/50] batch [260/319] time 0.076 (0.085) data 0.000 (0.003) loss 2.3548 (2.3505) teacher_loss 0.7632 (0.8306) loss_zs_kd 1.5774 (1.3957) loss_oracle 1.1004 (1.0559) acc 71.8750 (71.4423) lr 1.9098e-04 eta 0:03:42
epoch [42/50] batch [280/319] time 0.078 (0.085) data 0.000 (0.002) loss 2.3312 (2.3490) teacher_loss 0.8162 (0.8282) loss_zs_kd 1.1873 (1.3992) loss_oracle 1.0530 (1.0565) acc 71.8750 (71.6183) lr 1.9098e-04 eta 0:03:39
epoch [42/50] batch [300/319] time 0.074 (0.084) data 0.000 (0.002) loss 2.1949 (2.3523) teacher_loss 0.6881 (0.8313) loss_zs_kd 1.4927 (1.4022) loss_oracle 1.0447 (1.0558) acc 81.2500 (71.4375) lr 1.9098e-04 eta 0:03:36
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,771
* accuracy: 63.3%
* error: 36.7%
* macro_f1: 57.5%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,738
* accuracy: 58.9%
* error: 41.1%
* macro_f1: 25.5%
******* Domain 2 best val acc:      64.1%, epoch: 39 *******
******* Domain 2 best val test acc: 58.2%, epoch: 39 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [43/50] batch [20/319] time 0.082 (0.116) data 0.000 (0.029) loss 2.3783 (2.3465) teacher_loss 0.8876 (0.8260) loss_zs_kd 1.5862 (1.4068) loss_oracle 1.0126 (1.0570) acc 71.8750 (72.8125) lr 1.5567e-04 eta 0:04:53
epoch [43/50] batch [40/319] time 0.074 (0.107) data 0.000 (0.015) loss 2.7783 (2.3414) teacher_loss 1.2857 (0.8242) loss_zs_kd 1.3527 (1.3904) loss_oracle 1.0838 (1.0603) acc 50.0000 (71.5625) lr 1.5567e-04 eta 0:04:28
epoch [43/50] batch [60/319] time 0.080 (0.096) data 0.000 (0.010) loss 2.5835 (2.3300) teacher_loss 1.0223 (0.8066) loss_zs_kd 1.4692 (1.3863) loss_oracle 1.0693 (1.0616) acc 62.5000 (71.1458) lr 1.5567e-04 eta 0:03:59
epoch [43/50] batch [80/319] time 0.077 (0.092) data 0.000 (0.007) loss 2.2059 (2.3430) teacher_loss 0.7540 (0.8202) loss_zs_kd 1.4016 (1.3846) loss_oracle 0.9828 (1.0578) acc 78.1250 (71.6016) lr 1.5567e-04 eta 0:03:47
epoch [43/50] batch [100/319] time 0.075 (0.089) data 0.000 (0.006) loss 2.3115 (2.3396) teacher_loss 0.7473 (0.8173) loss_zs_kd 1.2780 (1.3772) loss_oracle 1.1009 (1.0565) acc 68.7500 (71.5000) lr 1.5567e-04 eta 0:03:38
epoch [43/50] batch [120/319] time 0.076 (0.087) data 0.000 (0.005) loss 2.3781 (2.3390) teacher_loss 0.8506 (0.8173) loss_zs_kd 1.3644 (1.3708) loss_oracle 1.0378 (1.0551) acc 65.6250 (71.9531) lr 1.5567e-04 eta 0:03:31
epoch [43/50] batch [140/319] time 0.083 (0.086) data 0.000 (0.004) loss 2.7431 (2.3425) teacher_loss 1.2203 (0.8212) loss_zs_kd 1.2358 (1.3811) loss_oracle 1.0857 (1.0552) acc 62.5000 (71.8973) lr 1.5567e-04 eta 0:03:27
epoch [43/50] batch [160/319] time 0.076 (0.086) data 0.000 (0.004) loss 2.1201 (2.3401) teacher_loss 0.5836 (0.8168) loss_zs_kd 1.4897 (1.3852) loss_oracle 1.1053 (1.0584) acc 81.2500 (71.8164) lr 1.5567e-04 eta 0:03:24
epoch [43/50] batch [180/319] time 0.073 (0.085) data 0.000 (0.003) loss 2.1419 (2.3433) teacher_loss 0.6624 (0.8214) loss_zs_kd 1.3166 (1.3817) loss_oracle 1.0061 (1.0570) acc 75.0000 (71.7014) lr 1.5567e-04 eta 0:03:21
epoch [43/50] batch [200/319] time 0.082 (0.084) data 0.000 (0.003) loss 2.4200 (2.3482) teacher_loss 0.8957 (0.8263) loss_zs_kd 1.2479 (1.3825) loss_oracle 1.0743 (1.0576) acc 62.5000 (71.5781) lr 1.5567e-04 eta 0:03:18
epoch [43/50] batch [220/319] time 0.074 (0.084) data 0.000 (0.003) loss 2.4916 (2.3532) teacher_loss 0.9284 (0.8317) loss_zs_kd 1.5240 (1.3753) loss_oracle 1.0401 (1.0584) acc 68.7500 (71.2926) lr 1.5567e-04 eta 0:03:15
epoch [43/50] batch [240/319] time 0.085 (0.084) data 0.000 (0.003) loss 2.3816 (2.3518) teacher_loss 0.8530 (0.8309) loss_zs_kd 1.7208 (1.3778) loss_oracle 1.0059 (1.0583) acc 62.5000 (71.2630) lr 1.5567e-04 eta 0:03:13
epoch [43/50] batch [260/319] time 0.061 (0.083) data 0.000 (0.002) loss 2.1951 (2.3537) teacher_loss 0.6728 (0.8334) loss_zs_kd 1.4219 (1.3763) loss_oracle 1.0701 (1.0580) acc 75.0000 (71.2500) lr 1.5567e-04 eta 0:03:09
epoch [43/50] batch [280/319] time 0.077 (0.082) data 0.000 (0.002) loss 2.3170 (2.3503) teacher_loss 0.8339 (0.8299) loss_zs_kd 1.4363 (1.3756) loss_oracle 0.9652 (1.0585) acc 75.0000 (71.2835) lr 1.5567e-04 eta 0:03:06
epoch [43/50] batch [300/319] time 0.083 (0.082) data 0.000 (0.002) loss 2.3383 (2.3494) teacher_loss 0.8806 (0.8295) loss_zs_kd 1.1366 (1.3759) loss_oracle 1.0589 (1.0591) acc 78.1250 (71.2917) lr 1.5567e-04 eta 0:03:04
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,776
* accuracy: 63.4%
* error: 36.6%
* macro_f1: 57.7%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,783
* accuracy: 59.4%
* error: 40.6%
* macro_f1: 25.5%
******* Domain 2 best val acc:      64.1%, epoch: 39 *******
******* Domain 2 best val test acc: 58.2%, epoch: 39 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [44/50] batch [20/319] time 0.073 (0.109) data 0.000 (0.031) loss 2.6249 (2.3111) teacher_loss 1.1538 (0.8047) loss_zs_kd 1.3603 (1.3595) loss_oracle 1.0480 (1.0489) acc 62.5000 (73.5938) lr 1.2369e-04 eta 0:04:01
epoch [44/50] batch [40/319] time 0.098 (0.095) data 0.000 (0.016) loss 2.5372 (2.3450) teacher_loss 1.0177 (0.8273) loss_zs_kd 1.2263 (1.3843) loss_oracle 1.0218 (1.0565) acc 59.3750 (71.4844) lr 1.2369e-04 eta 0:03:28
epoch [44/50] batch [60/319] time 0.084 (0.089) data 0.000 (0.011) loss 2.1382 (2.3616) teacher_loss 0.6492 (0.8448) loss_zs_kd 1.5020 (1.3856) loss_oracle 1.0386 (1.0565) acc 81.2500 (70.4688) lr 1.2369e-04 eta 0:03:13
epoch [44/50] batch [80/319] time 0.080 (0.087) data 0.000 (0.008) loss 2.3469 (2.3575) teacher_loss 0.8296 (0.8417) loss_zs_kd 1.5023 (1.3880) loss_oracle 1.0929 (1.0587) acc 68.7500 (70.6250) lr 1.2369e-04 eta 0:03:07
epoch [44/50] batch [100/319] time 0.073 (0.086) data 0.000 (0.006) loss 2.2839 (2.3622) teacher_loss 0.7536 (0.8442) loss_zs_kd 1.1731 (1.3954) loss_oracle 1.0703 (1.0570) acc 68.7500 (70.9375) lr 1.2369e-04 eta 0:03:03
epoch [44/50] batch [120/319] time 0.075 (0.084) data 0.000 (0.005) loss 2.5001 (2.3759) teacher_loss 0.9987 (0.8603) loss_zs_kd 1.4499 (1.3858) loss_oracle 1.0574 (1.0575) acc 65.6250 (70.5469) lr 1.2369e-04 eta 0:02:58
epoch [44/50] batch [140/319] time 0.079 (0.084) data 0.000 (0.005) loss 2.4105 (2.3846) teacher_loss 0.8427 (0.8693) loss_zs_kd 1.3061 (1.3773) loss_oracle 1.0810 (1.0558) acc 71.8750 (70.1786) lr 1.2369e-04 eta 0:02:56
epoch [44/50] batch [160/319] time 0.079 (0.084) data 0.000 (0.004) loss 2.4453 (2.3844) teacher_loss 0.9256 (0.8687) loss_zs_kd 1.1619 (1.3788) loss_oracle 1.0421 (1.0568) acc 68.7500 (70.2148) lr 1.2369e-04 eta 0:02:54
epoch [44/50] batch [180/319] time 0.075 (0.084) data 0.000 (0.004) loss 2.4745 (2.3886) teacher_loss 0.9140 (0.8734) loss_zs_kd 1.0833 (1.3760) loss_oracle 1.0808 (1.0565) acc 71.8750 (70.2083) lr 1.2369e-04 eta 0:02:52
epoch [44/50] batch [200/319] time 0.087 (0.084) data 0.000 (0.003) loss 2.0573 (2.3867) teacher_loss 0.5414 (0.8725) loss_zs_kd 1.1343 (1.3809) loss_oracle 1.0064 (1.0562) acc 84.3750 (69.9688) lr 1.2369e-04 eta 0:02:50
epoch [44/50] batch [220/319] time 0.082 (0.084) data 0.000 (0.003) loss 2.3722 (2.3835) teacher_loss 0.8348 (0.8688) loss_zs_kd 1.5148 (1.3878) loss_oracle 1.0480 (1.0563) acc 68.7500 (70.0710) lr 1.2369e-04 eta 0:02:49
epoch [44/50] batch [240/319] time 0.175 (0.085) data 0.000 (0.003) loss 2.4281 (2.3789) teacher_loss 0.9209 (0.8651) loss_zs_kd 1.6513 (1.3919) loss_oracle 1.0434 (1.0550) acc 75.0000 (70.2083) lr 1.2369e-04 eta 0:02:49
epoch [44/50] batch [260/319] time 0.077 (0.085) data 0.000 (0.003) loss 2.3369 (2.3792) teacher_loss 0.7531 (0.8662) loss_zs_kd 1.2952 (1.3897) loss_oracle 1.0719 (1.0542) acc 71.8750 (70.0721) lr 1.2369e-04 eta 0:02:47
epoch [44/50] batch [280/319] time 0.085 (0.085) data 0.000 (0.002) loss 2.3553 (2.3791) teacher_loss 0.8541 (0.8649) loss_zs_kd 1.3787 (1.3889) loss_oracle 1.0645 (1.0552) acc 68.7500 (70.1004) lr 1.2369e-04 eta 0:02:45
epoch [44/50] batch [300/319] time 0.080 (0.084) data 0.000 (0.002) loss 2.3640 (2.3785) teacher_loss 0.8701 (0.8650) loss_zs_kd 1.5535 (1.3895) loss_oracle 1.0451 (1.0551) acc 65.6250 (70.1458) lr 1.2369e-04 eta 0:02:42
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,765
* accuracy: 63.2%
* error: 36.8%
* macro_f1: 58.0%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,766
* accuracy: 59.2%
* error: 40.8%
* macro_f1: 25.5%
******* Domain 2 best val acc:      64.1%, epoch: 39 *******
******* Domain 2 best val test acc: 58.2%, epoch: 39 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [45/50] batch [20/319] time 0.079 (0.117) data 0.000 (0.030) loss 2.1782 (2.3367) teacher_loss 0.6376 (0.8291) loss_zs_kd 0.9294 (1.3726) loss_oracle 1.0457 (1.0531) acc 81.2500 (70.3125) lr 9.5173e-05 eta 0:03:41
epoch [45/50] batch [40/319] time 0.070 (0.107) data 0.000 (0.015) loss 2.4709 (2.3459) teacher_loss 0.9646 (0.8322) loss_zs_kd 1.2837 (1.3653) loss_oracle 0.9934 (1.0541) acc 68.7500 (71.1719) lr 9.5173e-05 eta 0:03:19
epoch [45/50] batch [60/319] time 0.087 (0.098) data 0.001 (0.010) loss 2.6573 (2.3668) teacher_loss 1.2025 (0.8517) loss_zs_kd 1.5037 (1.3903) loss_oracle 0.9447 (1.0563) acc 68.7500 (71.1458) lr 9.5173e-05 eta 0:03:00
epoch [45/50] batch [80/319] time 0.084 (0.094) data 0.000 (0.008) loss 2.3377 (2.3663) teacher_loss 0.8242 (0.8502) loss_zs_kd 1.5661 (1.3734) loss_oracle 1.0286 (1.0567) acc 68.7500 (70.8594) lr 9.5173e-05 eta 0:02:52
epoch [45/50] batch [100/319] time 0.082 (0.092) data 0.000 (0.006) loss 2.2763 (2.3603) teacher_loss 0.7542 (0.8417) loss_zs_kd 1.3992 (1.3755) loss_oracle 1.0851 (1.0545) acc 62.5000 (70.7812) lr 9.5173e-05 eta 0:02:46
epoch [45/50] batch [120/319] time 0.081 (0.089) data 0.000 (0.005) loss 2.5931 (2.3605) teacher_loss 1.0785 (0.8420) loss_zs_kd 1.1019 (1.3651) loss_oracle 1.0775 (1.0533) acc 56.2500 (70.5729) lr 9.5173e-05 eta 0:02:39
epoch [45/50] batch [140/319] time 0.079 (0.088) data 0.000 (0.005) loss 2.2324 (2.3534) teacher_loss 0.7946 (0.8371) loss_zs_kd 1.1725 (1.3641) loss_oracle 1.0279 (1.0528) acc 75.0000 (70.7812) lr 9.5173e-05 eta 0:02:35
epoch [45/50] batch [160/319] time 0.076 (0.087) data 0.000 (0.004) loss 2.2995 (2.3497) teacher_loss 0.8080 (0.8335) loss_zs_kd 1.1731 (1.3531) loss_oracle 1.0399 (1.0538) acc 65.6250 (70.6445) lr 9.5173e-05 eta 0:02:32
epoch [45/50] batch [180/319] time 0.078 (0.085) data 0.000 (0.004) loss 2.4037 (2.3466) teacher_loss 0.7703 (0.8311) loss_zs_kd 1.3416 (1.3553) loss_oracle 1.1151 (1.0550) acc 68.7500 (70.8160) lr 9.5173e-05 eta 0:02:28
epoch [45/50] batch [200/319] time 0.087 (0.085) data 0.000 (0.003) loss 2.4140 (2.3494) teacher_loss 0.8397 (0.8322) loss_zs_kd 1.3853 (1.3485) loss_oracle 1.0985 (1.0566) acc 62.5000 (70.9531) lr 9.5173e-05 eta 0:02:25
epoch [45/50] batch [220/319] time 0.083 (0.085) data 0.000 (0.003) loss 2.2531 (2.3510) teacher_loss 0.6848 (0.8344) loss_zs_kd 1.1607 (1.3417) loss_oracle 1.0978 (1.0559) acc 75.0000 (70.8239) lr 9.5173e-05 eta 0:02:23
epoch [45/50] batch [240/319] time 0.075 (0.084) data 0.000 (0.003) loss 2.4951 (2.3498) teacher_loss 0.9670 (0.8324) loss_zs_kd 1.6205 (1.3417) loss_oracle 1.0906 (1.0563) acc 71.8750 (70.8594) lr 9.5173e-05 eta 0:02:20
epoch [45/50] batch [260/319] time 0.077 (0.083) data 0.000 (0.003) loss 2.3493 (2.3615) teacher_loss 0.7964 (0.8437) loss_zs_kd 1.3253 (1.3401) loss_oracle 1.0813 (1.0572) acc 65.6250 (70.4688) lr 9.5173e-05 eta 0:02:18
epoch [45/50] batch [280/319] time 0.079 (0.083) data 0.000 (0.002) loss 2.1290 (2.3651) teacher_loss 0.5802 (0.8479) loss_zs_kd 1.3583 (1.3392) loss_oracle 1.0678 (1.0569) acc 81.2500 (70.2455) lr 9.5173e-05 eta 0:02:15
epoch [45/50] batch [300/319] time 0.076 (0.083) data 0.000 (0.002) loss 2.3326 (2.3620) teacher_loss 0.8393 (0.8447) loss_zs_kd 1.1178 (1.3406) loss_oracle 1.0979 (1.0568) acc 78.1250 (70.4479) lr 9.5173e-05 eta 0:02:13
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,769
* accuracy: 63.2%
* error: 36.8%
* macro_f1: 57.7%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,763
* accuracy: 59.2%
* error: 40.8%
* macro_f1: 25.8%
******* Domain 2 best val acc:      64.1%, epoch: 39 *******
******* Domain 2 best val test acc: 58.2%, epoch: 39 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [46/50] batch [20/319] time 0.079 (0.115) data 0.000 (0.029) loss 2.2371 (2.3163) teacher_loss 0.6622 (0.8069) loss_zs_kd 1.3890 (1.3550) loss_oracle 1.0736 (1.0519) acc 75.0000 (72.0312) lr 7.0224e-05 eta 0:03:00
epoch [46/50] batch [40/319] time 0.086 (0.098) data 0.000 (0.014) loss 2.5729 (2.3444) teacher_loss 1.0470 (0.8318) loss_zs_kd 1.2385 (1.3384) loss_oracle 1.0615 (1.0561) acc 62.5000 (71.4844) lr 7.0224e-05 eta 0:02:32
epoch [46/50] batch [60/319] time 0.072 (0.091) data 0.000 (0.010) loss 2.3512 (2.3364) teacher_loss 0.7821 (0.8191) loss_zs_kd 1.3805 (1.3410) loss_oracle 1.0247 (1.0526) acc 81.2500 (71.8229) lr 7.0224e-05 eta 0:02:19
epoch [46/50] batch [80/319] time 0.099 (0.090) data 0.000 (0.007) loss 2.2324 (2.3457) teacher_loss 0.7206 (0.8259) loss_zs_kd 1.2025 (1.3489) loss_oracle 1.0419 (1.0566) acc 78.1250 (71.7969) lr 7.0224e-05 eta 0:02:15
epoch [46/50] batch [100/319] time 0.077 (0.088) data 0.000 (0.006) loss 2.0526 (2.3330) teacher_loss 0.5112 (0.8154) loss_zs_kd 1.2801 (1.3520) loss_oracle 1.0606 (1.0549) acc 81.2500 (71.9375) lr 7.0224e-05 eta 0:02:11
epoch [46/50] batch [120/319] time 0.081 (0.087) data 0.000 (0.005) loss 2.2706 (2.3268) teacher_loss 0.7478 (0.8082) loss_zs_kd 1.1921 (1.3470) loss_oracle 1.0465 (1.0559) acc 68.7500 (72.1354) lr 7.0224e-05 eta 0:02:07
epoch [46/50] batch [140/319] time 0.087 (0.086) data 0.000 (0.004) loss 2.3856 (2.3431) teacher_loss 0.8576 (0.8251) loss_zs_kd 1.0562 (1.3438) loss_oracle 1.0600 (1.0548) acc 71.8750 (71.8080) lr 7.0224e-05 eta 0:02:05
epoch [46/50] batch [160/319] time 0.080 (0.086) data 0.000 (0.004) loss 2.0881 (2.3368) teacher_loss 0.6799 (0.8206) loss_zs_kd 1.1713 (1.3460) loss_oracle 1.0650 (1.0531) acc 81.2500 (72.0703) lr 7.0224e-05 eta 0:02:03
epoch [46/50] batch [180/319] time 0.076 (0.086) data 0.000 (0.003) loss 2.5644 (2.3401) teacher_loss 1.0148 (0.8267) loss_zs_kd 1.1182 (1.3501) loss_oracle 1.1211 (1.0531) acc 65.6250 (71.7882) lr 7.0224e-05 eta 0:02:01
epoch [46/50] batch [200/319] time 0.082 (0.086) data 0.000 (0.003) loss 2.4568 (2.3410) teacher_loss 0.9231 (0.8264) loss_zs_kd 1.2057 (1.3511) loss_oracle 1.1114 (1.0548) acc 68.7500 (71.6094) lr 7.0224e-05 eta 0:01:59
epoch [46/50] batch [220/319] time 0.080 (0.086) data 0.000 (0.003) loss 2.6376 (2.3500) teacher_loss 1.1463 (0.8345) loss_zs_kd 1.4066 (1.3498) loss_oracle 1.0234 (1.0554) acc 59.3750 (71.3068) lr 7.0224e-05 eta 0:01:57
epoch [46/50] batch [240/319] time 0.077 (0.087) data 0.000 (0.003) loss 2.1909 (2.3524) teacher_loss 0.6661 (0.8379) loss_zs_kd 1.3400 (1.3517) loss_oracle 1.0381 (1.0557) acc 68.7500 (71.1068) lr 7.0224e-05 eta 0:01:57
epoch [46/50] batch [260/319] time 0.082 (0.086) data 0.000 (0.002) loss 2.3760 (2.3513) teacher_loss 0.8729 (0.8370) loss_zs_kd 1.4266 (1.3540) loss_oracle 1.0116 (1.0549) acc 65.6250 (71.2019) lr 7.0224e-05 eta 0:01:55
epoch [46/50] batch [280/319] time 0.081 (0.086) data 0.000 (0.002) loss 2.5373 (2.3562) teacher_loss 1.0035 (0.8411) loss_zs_kd 1.1848 (1.3541) loss_oracle 1.0349 (1.0548) acc 62.5000 (71.1384) lr 7.0224e-05 eta 0:01:53
epoch [46/50] batch [300/319] time 0.082 (0.086) data 0.000 (0.002) loss 2.4827 (2.3545) teacher_loss 0.9726 (0.8391) loss_zs_kd 1.5611 (1.3553) loss_oracle 1.0532 (1.0551) acc 62.5000 (71.1667) lr 7.0224e-05 eta 0:01:51
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,768
* accuracy: 63.2%
* error: 36.8%
* macro_f1: 57.9%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,732
* accuracy: 58.9%
* error: 41.1%
* macro_f1: 25.6%
******* Domain 2 best val acc:      64.1%, epoch: 39 *******
******* Domain 2 best val test acc: 58.2%, epoch: 39 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [47/50] batch [20/319] time 0.060 (0.100) data 0.000 (0.024) loss 2.1534 (2.4079) teacher_loss 0.6982 (0.8829) loss_zs_kd 1.2095 (1.3390) loss_oracle 1.0285 (1.0577) acc 75.0000 (68.2812) lr 4.8943e-05 eta 0:02:05
epoch [47/50] batch [40/319] time 0.106 (0.097) data 0.000 (0.012) loss 2.3805 (2.3985) teacher_loss 0.8864 (0.8767) loss_zs_kd 1.3448 (1.3338) loss_oracle 1.0791 (1.0578) acc 68.7500 (69.3750) lr 4.8943e-05 eta 0:01:59
epoch [47/50] batch [60/319] time 0.079 (0.092) data 0.000 (0.008) loss 2.4567 (2.3822) teacher_loss 0.9808 (0.8660) loss_zs_kd 1.5111 (1.3664) loss_oracle 1.0256 (1.0579) acc 68.7500 (69.5312) lr 4.8943e-05 eta 0:01:52
epoch [47/50] batch [80/319] time 0.075 (0.089) data 0.000 (0.006) loss 2.4753 (2.3850) teacher_loss 0.9565 (0.8678) loss_zs_kd 1.2492 (1.3493) loss_oracle 1.0933 (1.0564) acc 65.6250 (69.4922) lr 4.8943e-05 eta 0:01:46
epoch [47/50] batch [100/319] time 0.091 (0.087) data 0.000 (0.005) loss 2.2576 (2.3844) teacher_loss 0.6917 (0.8655) loss_zs_kd 1.2429 (1.3549) loss_oracle 1.0564 (1.0579) acc 68.7500 (69.8438) lr 4.8943e-05 eta 0:01:42
epoch [47/50] batch [120/319] time 0.083 (0.086) data 0.000 (0.004) loss 2.2403 (2.3716) teacher_loss 0.7388 (0.8559) loss_zs_kd 1.2976 (1.3634) loss_oracle 1.0411 (1.0554) acc 62.5000 (70.0781) lr 4.8943e-05 eta 0:01:39
epoch [47/50] batch [140/319] time 0.070 (0.086) data 0.000 (0.004) loss 2.6744 (2.3619) teacher_loss 1.1545 (0.8461) loss_zs_kd 1.4338 (1.3637) loss_oracle 1.1234 (1.0554) acc 59.3750 (70.5357) lr 4.8943e-05 eta 0:01:37
epoch [47/50] batch [160/319] time 0.079 (0.085) data 0.000 (0.003) loss 2.1989 (2.3560) teacher_loss 0.7304 (0.8394) loss_zs_kd 1.0507 (1.3639) loss_oracle 1.0593 (1.0575) acc 68.7500 (70.7617) lr 4.8943e-05 eta 0:01:34
epoch [47/50] batch [180/319] time 0.086 (0.084) data 0.000 (0.003) loss 2.2969 (2.3597) teacher_loss 0.7807 (0.8417) loss_zs_kd 1.4505 (1.3658) loss_oracle 1.0861 (1.0568) acc 71.8750 (70.6424) lr 4.8943e-05 eta 0:01:32
epoch [47/50] batch [200/319] time 0.090 (0.083) data 0.000 (0.003) loss 2.4339 (2.3515) teacher_loss 0.9286 (0.8344) loss_zs_kd 1.4533 (1.3627) loss_oracle 0.9918 (1.0557) acc 68.7500 (70.8594) lr 4.8943e-05 eta 0:01:29
epoch [47/50] batch [220/319] time 0.090 (0.084) data 0.000 (0.002) loss 2.2645 (2.3533) teacher_loss 0.7659 (0.8345) loss_zs_kd 1.2408 (1.3668) loss_oracle 1.0180 (1.0563) acc 68.7500 (70.7670) lr 4.8943e-05 eta 0:01:28
epoch [47/50] batch [240/319] time 0.075 (0.083) data 0.000 (0.002) loss 2.0738 (2.3565) teacher_loss 0.5855 (0.8380) loss_zs_kd 1.4271 (1.3674) loss_oracle 1.0791 (1.0567) acc 78.1250 (70.7292) lr 4.8943e-05 eta 0:01:26
epoch [47/50] batch [260/319] time 0.081 (0.083) data 0.000 (0.002) loss 2.5005 (2.3570) teacher_loss 0.9161 (0.8388) loss_zs_kd 1.3156 (1.3705) loss_oracle 1.1046 (1.0576) acc 62.5000 (70.5529) lr 4.8943e-05 eta 0:01:24
epoch [47/50] batch [280/319] time 0.095 (0.083) data 0.000 (0.002) loss 2.3132 (2.3561) teacher_loss 0.8603 (0.8386) loss_zs_kd 1.1841 (1.3717) loss_oracle 1.0190 (1.0582) acc 65.6250 (70.5134) lr 4.8943e-05 eta 0:01:23
epoch [47/50] batch [300/319] time 0.084 (0.083) data 0.000 (0.002) loss 2.1393 (2.3546) teacher_loss 0.6439 (0.8375) loss_zs_kd 1.2420 (1.3740) loss_oracle 0.9706 (1.0572) acc 75.0000 (70.6146) lr 4.8943e-05 eta 0:01:21
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,766
* accuracy: 63.2%
* error: 36.8%
* macro_f1: 57.8%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,750
* accuracy: 59.1%
* error: 40.9%
* macro_f1: 25.6%
******* Domain 2 best val acc:      64.1%, epoch: 39 *******
******* Domain 2 best val test acc: 58.2%, epoch: 39 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [48/50] batch [20/319] time 0.073 (0.108) data 0.000 (0.027) loss 2.3117 (2.3951) teacher_loss 0.8170 (0.8857) loss_zs_kd 1.2452 (1.3379) loss_oracle 1.0965 (1.0550) acc 65.6250 (69.2188) lr 3.1417e-05 eta 0:01:40
epoch [48/50] batch [40/319] time 0.079 (0.094) data 0.000 (0.013) loss 2.2523 (2.4039) teacher_loss 0.7248 (0.8880) loss_zs_kd 1.2717 (1.3595) loss_oracle 1.0174 (1.0610) acc 62.5000 (67.9688) lr 3.1417e-05 eta 0:01:25
epoch [48/50] batch [60/319] time 0.080 (0.090) data 0.000 (0.009) loss 2.3284 (2.3847) teacher_loss 0.7823 (0.8665) loss_zs_kd 1.0704 (1.3728) loss_oracle 1.0788 (1.0617) acc 75.0000 (69.2188) lr 3.1417e-05 eta 0:01:20
epoch [48/50] batch [80/319] time 0.087 (0.088) data 0.000 (0.007) loss 2.2975 (2.3658) teacher_loss 0.8070 (0.8466) loss_zs_kd 1.3022 (1.3818) loss_oracle 1.0612 (1.0600) acc 81.2500 (70.7812) lr 3.1417e-05 eta 0:01:17
epoch [48/50] batch [100/319] time 0.074 (0.087) data 0.000 (0.006) loss 2.2970 (2.3535) teacher_loss 0.8058 (0.8347) loss_zs_kd 1.2253 (1.3840) loss_oracle 1.0296 (1.0619) acc 62.5000 (70.7188) lr 3.1417e-05 eta 0:01:14
epoch [48/50] batch [120/319] time 0.077 (0.086) data 0.000 (0.005) loss 2.4965 (2.3502) teacher_loss 0.9409 (0.8300) loss_zs_kd 1.2056 (1.3824) loss_oracle 1.0870 (1.0610) acc 71.8750 (70.9896) lr 3.1417e-05 eta 0:01:11
epoch [48/50] batch [140/319] time 0.070 (0.084) data 0.000 (0.004) loss 2.5380 (2.3493) teacher_loss 1.0453 (0.8281) loss_zs_kd 1.4009 (1.3846) loss_oracle 1.0838 (1.0602) acc 59.3750 (71.2500) lr 3.1417e-05 eta 0:01:08
epoch [48/50] batch [160/319] time 0.077 (0.083) data 0.000 (0.004) loss 2.1727 (2.3478) teacher_loss 0.6864 (0.8274) loss_zs_kd 1.4629 (1.3796) loss_oracle 1.0525 (1.0595) acc 81.2500 (71.2695) lr 3.1417e-05 eta 0:01:06
epoch [48/50] batch [180/319] time 0.077 (0.083) data 0.000 (0.003) loss 2.1453 (2.3514) teacher_loss 0.6118 (0.8320) loss_zs_kd 1.4649 (1.3742) loss_oracle 1.0223 (1.0585) acc 78.1250 (71.1285) lr 3.1417e-05 eta 0:01:04
epoch [48/50] batch [200/319] time 0.078 (0.083) data 0.000 (0.003) loss 2.7663 (2.3548) teacher_loss 1.2470 (0.8359) loss_zs_kd 1.3804 (1.3811) loss_oracle 1.1045 (1.0584) acc 68.7500 (70.9688) lr 3.1417e-05 eta 0:01:02
epoch [48/50] batch [220/319] time 0.088 (0.083) data 0.000 (0.003) loss 2.0727 (2.3497) teacher_loss 0.5907 (0.8322) loss_zs_kd 1.2222 (1.3805) loss_oracle 1.0533 (1.0574) acc 75.0000 (71.0227) lr 3.1417e-05 eta 0:01:01
epoch [48/50] batch [240/319] time 0.076 (0.084) data 0.000 (0.002) loss 2.5510 (2.3544) teacher_loss 1.0399 (0.8372) loss_zs_kd 1.3071 (1.3784) loss_oracle 1.0911 (1.0579) acc 68.7500 (70.9245) lr 3.1417e-05 eta 0:01:00
epoch [48/50] batch [260/319] time 0.086 (0.084) data 0.000 (0.002) loss 2.4880 (2.3550) teacher_loss 0.9567 (0.8375) loss_zs_kd 1.4698 (1.3811) loss_oracle 1.0160 (1.0567) acc 68.7500 (70.9014) lr 3.1417e-05 eta 0:00:58
epoch [48/50] batch [280/319] time 0.083 (0.084) data 0.000 (0.002) loss 2.9049 (2.3541) teacher_loss 1.4029 (0.8373) loss_zs_kd 1.3705 (1.3762) loss_oracle 1.0519 (1.0560) acc 53.1250 (71.0379) lr 3.1417e-05 eta 0:00:56
epoch [48/50] batch [300/319] time 0.082 (0.084) data 0.000 (0.002) loss 2.1941 (2.3534) teacher_loss 0.6982 (0.8358) loss_zs_kd 1.2905 (1.3729) loss_oracle 1.0427 (1.0566) acc 68.7500 (71.0729) lr 3.1417e-05 eta 0:00:55
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,761
* accuracy: 63.1%
* error: 36.9%
* macro_f1: 57.7%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,753
* accuracy: 59.1%
* error: 40.9%
* macro_f1: 25.6%
******* Domain 2 best val acc:      64.1%, epoch: 39 *******
******* Domain 2 best val test acc: 58.2%, epoch: 39 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [49/50] batch [20/319] time 0.078 (0.115) data 0.000 (0.030) loss 2.4030 (2.3192) teacher_loss 0.9254 (0.8170) loss_zs_kd 1.2201 (1.3836) loss_oracle 1.0492 (1.0501) acc 71.8750 (71.2500) lr 1.7713e-05 eta 0:01:11
epoch [49/50] batch [40/319] time 0.082 (0.105) data 0.000 (0.015) loss 2.2885 (2.3401) teacher_loss 0.7702 (0.8263) loss_zs_kd 1.3977 (1.3777) loss_oracle 1.0749 (1.0523) acc 71.8750 (71.1719) lr 1.7713e-05 eta 0:01:02
epoch [49/50] batch [60/319] time 0.075 (0.096) data 0.000 (0.010) loss 1.9700 (2.3394) teacher_loss 0.4507 (0.8228) loss_zs_kd 1.1318 (1.3687) loss_oracle 1.0432 (1.0514) acc 90.6250 (71.4062) lr 1.7713e-05 eta 0:00:55
epoch [49/50] batch [80/319] time 0.080 (0.092) data 0.000 (0.008) loss 2.5399 (2.3456) teacher_loss 0.9877 (0.8310) loss_zs_kd 1.4990 (1.3590) loss_oracle 1.1307 (1.0524) acc 68.7500 (71.4453) lr 1.7713e-05 eta 0:00:51
epoch [49/50] batch [100/319] time 0.087 (0.090) data 0.000 (0.006) loss 2.4327 (2.3368) teacher_loss 0.8653 (0.8234) loss_zs_kd 1.6423 (1.3543) loss_oracle 1.0605 (1.0521) acc 65.6250 (71.2812) lr 1.7713e-05 eta 0:00:48
epoch [49/50] batch [120/319] time 0.081 (0.089) data 0.000 (0.005) loss 2.1895 (2.3488) teacher_loss 0.6925 (0.8337) loss_zs_kd 1.3899 (1.3540) loss_oracle 1.0942 (1.0539) acc 78.1250 (70.9115) lr 1.7713e-05 eta 0:00:46
epoch [49/50] batch [140/319] time 0.072 (0.088) data 0.000 (0.005) loss 2.2170 (2.3428) teacher_loss 0.7327 (0.8280) loss_zs_kd 1.2297 (1.3579) loss_oracle 1.0133 (1.0548) acc 81.2500 (71.0268) lr 1.7713e-05 eta 0:00:43
epoch [49/50] batch [160/319] time 0.083 (0.087) data 0.000 (0.004) loss 2.0766 (2.3406) teacher_loss 0.6202 (0.8272) loss_zs_kd 1.4082 (1.3651) loss_oracle 1.0570 (1.0545) acc 84.3750 (70.8984) lr 1.7713e-05 eta 0:00:41
epoch [49/50] batch [180/319] time 0.079 (0.086) data 0.000 (0.004) loss 2.2737 (2.3485) teacher_loss 0.7162 (0.8330) loss_zs_kd 1.3219 (1.3689) loss_oracle 1.0725 (1.0554) acc 81.2500 (70.9028) lr 1.7713e-05 eta 0:00:39
epoch [49/50] batch [200/319] time 0.074 (0.085) data 0.000 (0.003) loss 2.3684 (2.3538) teacher_loss 0.8372 (0.8374) loss_zs_kd 1.5244 (1.3760) loss_oracle 1.1228 (1.0565) acc 71.8750 (70.6094) lr 1.7713e-05 eta 0:00:37
epoch [49/50] batch [220/319] time 0.083 (0.085) data 0.000 (0.003) loss 2.6344 (2.3546) teacher_loss 1.0858 (0.8386) loss_zs_kd 1.3151 (1.3772) loss_oracle 1.0696 (1.0568) acc 56.2500 (70.4545) lr 1.7713e-05 eta 0:00:35
epoch [49/50] batch [240/319] time 0.089 (0.085) data 0.000 (0.003) loss 2.1501 (2.3575) teacher_loss 0.5847 (0.8424) loss_zs_kd 1.5433 (1.3770) loss_oracle 1.0326 (1.0559) acc 81.2500 (70.6510) lr 1.7713e-05 eta 0:00:33
epoch [49/50] batch [260/319] time 0.093 (0.085) data 0.000 (0.003) loss 2.3160 (2.3580) teacher_loss 0.8042 (0.8419) loss_zs_kd 1.4189 (1.3762) loss_oracle 0.9839 (1.0556) acc 68.7500 (70.7332) lr 1.7713e-05 eta 0:00:32
epoch [49/50] batch [280/319] time 0.080 (0.085) data 0.000 (0.002) loss 2.1850 (2.3566) teacher_loss 0.6534 (0.8405) loss_zs_kd 1.3860 (1.3750) loss_oracle 1.0474 (1.0552) acc 78.1250 (70.7812) lr 1.7713e-05 eta 0:00:30
epoch [49/50] batch [300/319] time 0.080 (0.084) data 0.000 (0.002) loss 2.2936 (2.3496) teacher_loss 0.8833 (0.8339) loss_zs_kd 1.4439 (1.3779) loss_oracle 1.0165 (1.0551) acc 71.8750 (71.0833) lr 1.7713e-05 eta 0:00:28
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,762
* accuracy: 63.1%
* error: 36.9%
* macro_f1: 57.8%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,755
* accuracy: 59.1%
* error: 40.9%
* macro_f1: 25.6%
******* Domain 2 best val acc:      64.1%, epoch: 39 *******
******* Domain 2 best val test acc: 58.2%, epoch: 39 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
epoch [50/50] batch [20/319] time 0.082 (0.115) data 0.000 (0.032) loss 2.2163 (2.4152) teacher_loss 0.6622 (0.8994) loss_zs_kd 1.3510 (1.3406) loss_oracle 1.0421 (1.0566) acc 68.7500 (69.6875) lr 7.8853e-06 eta 0:00:34
epoch [50/50] batch [40/319] time 0.081 (0.097) data 0.000 (0.016) loss 2.2260 (2.3843) teacher_loss 0.6335 (0.8691) loss_zs_kd 1.8478 (1.3623) loss_oracle 1.1100 (1.0552) acc 75.0000 (69.7656) lr 7.8853e-06 eta 0:00:27
epoch [50/50] batch [60/319] time 0.076 (0.092) data 0.000 (0.011) loss 2.1472 (2.3692) teacher_loss 0.6459 (0.8553) loss_zs_kd 1.3962 (1.3687) loss_oracle 1.0736 (1.0510) acc 71.8750 (70.0000) lr 7.8853e-06 eta 0:00:23
epoch [50/50] batch [80/319] time 0.083 (0.090) data 0.000 (0.008) loss 2.3021 (2.3508) teacher_loss 0.8317 (0.8362) loss_zs_kd 1.1994 (1.3626) loss_oracle 1.0221 (1.0539) acc 62.5000 (70.7812) lr 7.8853e-06 eta 0:00:21
epoch [50/50] batch [100/319] time 0.085 (0.090) data 0.000 (0.007) loss 2.4599 (2.3584) teacher_loss 0.9559 (0.8418) loss_zs_kd 1.4201 (1.3690) loss_oracle 1.1162 (1.0589) acc 71.8750 (71.0000) lr 7.8853e-06 eta 0:00:19
epoch [50/50] batch [120/319] time 0.087 (0.089) data 0.000 (0.006) loss 2.4268 (2.3595) teacher_loss 0.9509 (0.8428) loss_zs_kd 1.1807 (1.3635) loss_oracle 1.0655 (1.0598) acc 75.0000 (70.8333) lr 7.8853e-06 eta 0:00:17
epoch [50/50] batch [140/319] time 0.082 (0.088) data 0.000 (0.005) loss 2.1025 (2.3623) teacher_loss 0.5789 (0.8440) loss_zs_kd 1.2651 (1.3627) loss_oracle 1.1222 (1.0592) acc 81.2500 (70.9375) lr 7.8853e-06 eta 0:00:15
epoch [50/50] batch [160/319] time 0.080 (0.087) data 0.000 (0.004) loss 2.2995 (2.3646) teacher_loss 0.7869 (0.8472) loss_zs_kd 1.2576 (1.3682) loss_oracle 1.1196 (1.0583) acc 71.8750 (70.7031) lr 7.8853e-06 eta 0:00:13
epoch [50/50] batch [180/319] time 0.075 (0.086) data 0.000 (0.004) loss 2.2596 (2.3610) teacher_loss 0.7881 (0.8446) loss_zs_kd 1.4346 (1.3716) loss_oracle 1.0255 (1.0571) acc 68.7500 (70.6597) lr 7.8853e-06 eta 0:00:11
epoch [50/50] batch [200/319] time 0.072 (0.086) data 0.000 (0.003) loss 2.4735 (2.3698) teacher_loss 0.8888 (0.8518) loss_zs_kd 1.4071 (1.3719) loss_oracle 1.0228 (1.0562) acc 68.7500 (70.6406) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [220/319] time 0.077 (0.085) data 0.000 (0.003) loss 2.2034 (2.3680) teacher_loss 0.7267 (0.8490) loss_zs_kd 1.2472 (1.3752) loss_oracle 1.0164 (1.0571) acc 81.2500 (70.9943) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [240/319] time 0.085 (0.087) data 0.000 (0.003) loss 2.6165 (2.3631) teacher_loss 1.0729 (0.8430) loss_zs_kd 1.2999 (1.3739) loss_oracle 1.0966 (1.0580) acc 65.6250 (71.1979) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [260/319] time 0.089 (0.086) data 0.000 (0.003) loss 2.1787 (2.3641) teacher_loss 0.6455 (0.8444) loss_zs_kd 1.3484 (1.3748) loss_oracle 0.9909 (1.0566) acc 81.2500 (71.1418) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [280/319] time 0.084 (0.086) data 0.000 (0.003) loss 3.1391 (2.3608) teacher_loss 1.5556 (0.8404) loss_zs_kd 1.2908 (1.3713) loss_oracle 1.0943 (1.0571) acc 53.1250 (71.2054) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [300/319] time 0.085 (0.086) data 0.000 (0.002) loss 2.4122 (2.3570) teacher_loss 0.9386 (0.8375) loss_zs_kd 1.5055 (1.3726) loss_oracle 1.0386 (1.0565) acc 59.3750 (71.2396) lr 7.8853e-06 eta 0:00:01
Evaluate on the *val* set
=> result
* total: 4,378
* correct: 2,763
* accuracy: 63.1%
* error: 36.9%
* macro_f1: 57.8%
Evaluate on the *test* set
=> result
* total: 9,736
* correct: 5,751
* accuracy: 59.1%
* error: 40.9%
* macro_f1: 25.6%
******* Domain 2 best val acc:      64.1%, epoch: 39 *******
******* Domain 2 best val test acc: 58.2%, epoch: 39 *******
******* Domain 2 best test acc:     60.0%, epoch: 35 *******
Checkpoint saved to icml/multi-dg/tuning/16_seperate_prompt3/TRIP/terra_incognita/b32_ep50/ViT-B16/2/seed_1/warmup_1/prompt_learner/model.pth.tar-50
Finish the whole training
Elapsed: 0:41:39
