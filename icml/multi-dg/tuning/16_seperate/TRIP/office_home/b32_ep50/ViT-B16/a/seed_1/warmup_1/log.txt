Loading trainer: TRIP
Loading dataset: SPG_OfficeHome
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------------------------------
Dataset    SPG_OfficeHome
Source     ['clipart', 'product', 'real_world']
Target     ['art']
# classes  65
# train_x  9,222
# val      3,939
# test     2,427
---------  ------------------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
alpha_logit                                        1
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,196
[Info] Hyperparameters saved to: icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/hyperparameters.json
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/288] time 0.152 (0.203) data 0.000 (0.021) loss 1.4455 (1.5894) teacher_loss 1.1104 (1.2380) loss_zs_kd 0.0000 (0.0000) loss_oracle -0.0001 (-0.0001) acc 68.7500 (68.4375) lr 1.0000e-05 eta 0:48:44
epoch [1/50] batch [40/288] time 0.155 (0.181) data 0.000 (0.011) loss 1.6057 (1.6022) teacher_loss 1.2151 (1.2593) loss_zs_kd 0.0002 (0.0001) loss_oracle -0.0000 (-0.0001) acc 65.6250 (68.2812) lr 1.0000e-05 eta 0:43:16
epoch [1/50] batch [60/288] time 0.154 (0.171) data 0.001 (0.007) loss 1.7000 (1.6237) teacher_loss 1.4283 (1.2663) loss_zs_kd 0.0006 (0.0001) loss_oracle 0.0001 (-0.0000) acc 59.3750 (67.6042) lr 1.0000e-05 eta 0:40:57
epoch [1/50] batch [80/288] time 0.149 (0.167) data 0.000 (0.005) loss 1.5910 (1.6452) teacher_loss 1.2554 (1.2704) loss_zs_kd 0.0008 (0.0002) loss_oracle 0.0001 (-0.0000) acc 62.5000 (67.3828) lr 1.0000e-05 eta 0:39:44
epoch [1/50] batch [100/288] time 0.166 (0.165) data 0.001 (0.004) loss 1.4797 (1.6132) teacher_loss 1.0902 (1.2427) loss_zs_kd 0.0006 (0.0003) loss_oracle 0.0001 (-0.0000) acc 62.5000 (68.0938) lr 1.0000e-05 eta 0:39:14
epoch [1/50] batch [120/288] time 0.148 (0.164) data 0.000 (0.004) loss 1.4421 (1.6109) teacher_loss 1.0340 (1.2365) loss_zs_kd 0.0007 (0.0004) loss_oracle 0.0000 (-0.0000) acc 75.0000 (68.1771) lr 1.0000e-05 eta 0:39:03
epoch [1/50] batch [140/288] time 0.152 (0.163) data 0.000 (0.003) loss 1.9117 (1.6017) teacher_loss 1.4873 (1.2226) loss_zs_kd 0.0010 (0.0005) loss_oracle 0.0000 (-0.0000) acc 56.2500 (68.2589) lr 1.0000e-05 eta 0:38:51
epoch [1/50] batch [160/288] time 0.161 (0.162) data 0.000 (0.003) loss 1.2101 (1.5966) teacher_loss 1.0578 (1.2202) loss_zs_kd 0.0018 (0.0006) loss_oracle 0.0001 (0.0000) acc 71.8750 (68.5938) lr 1.0000e-05 eta 0:38:31
epoch [1/50] batch [180/288] time 0.154 (0.161) data 0.000 (0.003) loss 1.3095 (1.5976) teacher_loss 0.9947 (1.2238) loss_zs_kd 0.0021 (0.0007) loss_oracle 0.0001 (0.0000) acc 71.8750 (68.3681) lr 1.0000e-05 eta 0:38:10
epoch [1/50] batch [200/288] time 0.152 (0.161) data 0.000 (0.002) loss 1.7898 (1.5935) teacher_loss 1.3374 (1.2206) loss_zs_kd 0.0018 (0.0008) loss_oracle -0.0000 (0.0000) acc 62.5000 (68.2656) lr 1.0000e-05 eta 0:38:08
epoch [1/50] batch [220/288] time 0.166 (0.160) data 0.000 (0.002) loss 1.4902 (1.5866) teacher_loss 1.4196 (1.2163) loss_zs_kd 0.0019 (0.0009) loss_oracle 0.0001 (0.0000) acc 65.6250 (68.5369) lr 1.0000e-05 eta 0:37:54
epoch [1/50] batch [240/288] time 0.149 (0.160) data 0.000 (0.002) loss 1.3261 (1.5824) teacher_loss 0.9867 (1.2140) loss_zs_kd 0.0009 (0.0010) loss_oracle 0.0001 (0.0000) acc 81.2500 (68.8021) lr 1.0000e-05 eta 0:37:40
epoch [1/50] batch [260/288] time 0.086 (0.157) data 0.000 (0.002) loss 1.8178 (1.5880) teacher_loss 1.3072 (1.2197) loss_zs_kd 0.0046 (0.0011) loss_oracle 0.0000 (0.0000) acc 71.8750 (68.6659) lr 1.0000e-05 eta 0:36:55
epoch [1/50] batch [280/288] time 0.204 (0.156) data 0.000 (0.002) loss 1.7434 (1.5878) teacher_loss 1.5206 (1.2196) loss_zs_kd 0.0045 (0.0013) loss_oracle -0.0000 (0.0000) acc 62.5000 (68.7165) lr 1.0000e-05 eta 0:36:44
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,266
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 82.0%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 1,962
* accuracy: 80.8%
* error: 19.2%
* macro_f1: 76.6%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      82.9%, epoch: 1 *******
******* Domain a best val test acc: 80.8%, epoch: 1 *******
******* Domain a best test acc:     80.8%, epoch: 1 *******
epoch [2/50] batch [20/288] time 0.083 (0.191) data 0.000 (0.017) loss 1.6982 (1.8055) teacher_loss 1.0560 (1.1992) loss_zs_kd 0.0688 (0.0645) loss_oracle 0.1719 (0.1082) acc 65.6250 (67.9688) lr 2.0000e-03 eta 0:44:46
epoch [2/50] batch [40/288] time 0.262 (0.172) data 0.000 (0.009) loss 1.2430 (1.7639) teacher_loss 0.7073 (1.1363) loss_zs_kd 0.0717 (0.0770) loss_oracle 0.0780 (0.1214) acc 81.2500 (69.8438) lr 2.0000e-03 eta 0:40:22
epoch [2/50] batch [60/288] time 0.091 (0.174) data 0.001 (0.006) loss 1.9714 (1.7517) teacher_loss 1.3769 (1.1413) loss_zs_kd 0.0570 (0.0746) loss_oracle 0.0708 (0.1069) acc 65.6250 (70.0000) lr 2.0000e-03 eta 0:40:42
epoch [2/50] batch [80/288] time 0.150 (0.164) data 0.000 (0.005) loss 1.4189 (1.7315) teacher_loss 0.7877 (1.1265) loss_zs_kd 0.0696 (0.0714) loss_oracle 0.0931 (0.1024) acc 68.7500 (70.5078) lr 2.0000e-03 eta 0:38:27
epoch [2/50] batch [100/288] time 0.169 (0.163) data 0.000 (0.004) loss 2.0156 (1.7225) teacher_loss 1.5233 (1.1264) loss_zs_kd 0.0497 (0.0703) loss_oracle 0.0563 (0.0976) acc 65.6250 (70.5938) lr 2.0000e-03 eta 0:37:57
epoch [2/50] batch [120/288] time 0.152 (0.162) data 0.000 (0.003) loss 1.6750 (1.7114) teacher_loss 1.0878 (1.1237) loss_zs_kd 0.0399 (0.0683) loss_oracle 0.0589 (0.0937) acc 71.8750 (70.7292) lr 2.0000e-03 eta 0:37:51
epoch [2/50] batch [140/288] time 0.167 (0.162) data 0.000 (0.003) loss 1.5332 (1.7092) teacher_loss 1.1703 (1.1293) loss_zs_kd 0.0909 (0.0661) loss_oracle 0.1172 (0.0917) acc 71.8750 (70.5357) lr 2.0000e-03 eta 0:37:45
epoch [2/50] batch [160/288] time 0.165 (0.162) data 0.000 (0.002) loss 1.7275 (1.7192) teacher_loss 1.1485 (1.1315) loss_zs_kd 0.0961 (0.0658) loss_oracle 0.1346 (0.0960) acc 71.8750 (70.3711) lr 2.0000e-03 eta 0:37:38
epoch [2/50] batch [180/288] time 0.152 (0.161) data 0.000 (0.002) loss 1.7276 (1.7229) teacher_loss 0.9817 (1.1270) loss_zs_kd 0.0665 (0.0653) loss_oracle 0.1914 (0.1036) acc 78.1250 (70.5556) lr 2.0000e-03 eta 0:37:25
epoch [2/50] batch [200/288] time 0.168 (0.160) data 0.000 (0.002) loss 2.5632 (1.7334) teacher_loss 1.7899 (1.1298) loss_zs_kd 0.0868 (0.0661) loss_oracle 0.1348 (0.1109) acc 56.2500 (70.2969) lr 2.0000e-03 eta 0:37:11
epoch [2/50] batch [220/288] time 0.149 (0.160) data 0.000 (0.002) loss 1.7108 (1.7231) teacher_loss 1.1558 (1.1188) loss_zs_kd 0.0684 (0.0656) loss_oracle 0.1100 (0.1127) acc 71.8750 (70.6108) lr 2.0000e-03 eta 0:37:04
epoch [2/50] batch [240/288] time 0.155 (0.159) data 0.000 (0.002) loss 1.9545 (1.7160) teacher_loss 1.5088 (1.1150) loss_zs_kd 0.0569 (0.0650) loss_oracle 0.1000 (0.1114) acc 68.7500 (70.7031) lr 2.0000e-03 eta 0:36:50
epoch [2/50] batch [260/288] time 0.152 (0.159) data 0.000 (0.002) loss 1.7683 (1.7138) teacher_loss 1.1659 (1.1123) loss_zs_kd 0.0908 (0.0645) loss_oracle 0.0783 (0.1111) acc 71.8750 (70.6851) lr 2.0000e-03 eta 0:36:39
epoch [2/50] batch [280/288] time 0.148 (0.158) data 0.000 (0.001) loss 1.5645 (1.7066) teacher_loss 0.8902 (1.1081) loss_zs_kd 0.0645 (0.0640) loss_oracle 0.0849 (0.1078) acc 68.7500 (70.7031) lr 2.0000e-03 eta 0:36:27
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,349
* accuracy: 85.0%
* error: 15.0%
* macro_f1: 84.3%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,009
* accuracy: 82.8%
* error: 17.2%
* macro_f1: 79.0%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      85.0%, epoch: 2 *******
******* Domain a best val test acc: 82.8%, epoch: 2 *******
******* Domain a best test acc:     82.8%, epoch: 2 *******
epoch [3/50] batch [20/288] time 0.195 (0.122) data 0.000 (0.014) loss 1.9424 (1.6600) teacher_loss 1.5098 (1.1147) loss_zs_kd 0.0788 (0.0648) loss_oracle 0.0756 (0.0614) acc 50.0000 (71.0938) lr 1.9980e-03 eta 0:28:10
epoch [3/50] batch [40/288] time 0.338 (0.139) data 0.000 (0.007) loss 1.5318 (1.5753) teacher_loss 0.8998 (1.0399) loss_zs_kd 0.0411 (0.0625) loss_oracle 0.0625 (0.0649) acc 78.1250 (73.2812) lr 1.9980e-03 eta 0:31:56
epoch [3/50] batch [60/288] time 0.098 (0.144) data 0.000 (0.005) loss 1.3464 (1.6314) teacher_loss 0.9267 (1.0819) loss_zs_kd 0.0481 (0.0617) loss_oracle 0.0518 (0.0646) acc 75.0000 (72.4479) lr 1.9980e-03 eta 0:33:08
epoch [3/50] batch [80/288] time 0.103 (0.135) data 0.000 (0.004) loss 1.6550 (1.6398) teacher_loss 1.0855 (1.0848) loss_zs_kd 0.0532 (0.0604) loss_oracle 0.1096 (0.0688) acc 75.0000 (72.0703) lr 1.9980e-03 eta 0:30:57
epoch [3/50] batch [100/288] time 0.097 (0.137) data 0.000 (0.003) loss 1.5642 (1.6663) teacher_loss 1.0029 (1.0961) loss_zs_kd 0.0544 (0.0604) loss_oracle 0.0932 (0.0797) acc 75.0000 (71.9688) lr 1.9980e-03 eta 0:31:22
epoch [3/50] batch [120/288] time 0.102 (0.138) data 0.000 (0.003) loss 1.4521 (1.6655) teacher_loss 0.7543 (1.0841) loss_zs_kd 0.0731 (0.0627) loss_oracle 0.1484 (0.0921) acc 75.0000 (72.2135) lr 1.9980e-03 eta 0:31:32
epoch [3/50] batch [140/288] time 0.083 (0.142) data 0.000 (0.002) loss 1.5287 (1.6617) teacher_loss 0.9715 (1.0783) loss_zs_kd 0.0418 (0.0629) loss_oracle 0.1024 (0.0944) acc 75.0000 (72.0759) lr 1.9980e-03 eta 0:32:29
epoch [3/50] batch [160/288] time 0.101 (0.146) data 0.000 (0.002) loss 2.0082 (1.6706) teacher_loss 1.4907 (1.0818) loss_zs_kd 0.0824 (0.0645) loss_oracle 0.0821 (0.0966) acc 62.5000 (72.0117) lr 1.9980e-03 eta 0:33:15
epoch [3/50] batch [180/288] time 0.151 (0.147) data 0.000 (0.002) loss 1.6901 (1.6699) teacher_loss 1.1611 (1.0829) loss_zs_kd 0.0655 (0.0641) loss_oracle 0.0970 (0.0934) acc 71.8750 (72.0139) lr 1.9980e-03 eta 0:33:24
epoch [3/50] batch [200/288] time 0.158 (0.149) data 0.000 (0.002) loss 1.6551 (1.6762) teacher_loss 1.0172 (1.0815) loss_zs_kd 0.0518 (0.0645) loss_oracle 0.1479 (0.0986) acc 75.0000 (71.9844) lr 1.9980e-03 eta 0:33:43
epoch [3/50] batch [220/288] time 0.156 (0.149) data 0.000 (0.002) loss 1.5301 (1.6858) teacher_loss 0.8773 (1.0909) loss_zs_kd 0.0610 (0.0646) loss_oracle 0.0819 (0.0996) acc 75.0000 (71.6619) lr 1.9980e-03 eta 0:33:52
epoch [3/50] batch [240/288] time 0.158 (0.150) data 0.000 (0.001) loss 1.9381 (1.6873) teacher_loss 1.3307 (1.0945) loss_zs_kd 0.0537 (0.0643) loss_oracle 0.0866 (0.0985) acc 62.5000 (71.5495) lr 1.9980e-03 eta 0:34:00
epoch [3/50] batch [260/288] time 0.148 (0.151) data 0.000 (0.001) loss 1.1883 (1.6796) teacher_loss 0.7101 (1.0907) loss_zs_kd 0.0552 (0.0638) loss_oracle 0.1031 (0.0970) acc 84.3750 (71.5264) lr 1.9980e-03 eta 0:34:08
epoch [3/50] batch [280/288] time 0.153 (0.151) data 0.000 (0.001) loss 1.7813 (1.6753) teacher_loss 1.1356 (1.0869) loss_zs_kd 0.1076 (0.0639) loss_oracle 0.1471 (0.0974) acc 65.6250 (71.6183) lr 1.9980e-03 eta 0:34:09
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,362
* accuracy: 85.4%
* error: 14.6%
* macro_f1: 84.7%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,012
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.1%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      85.4%, epoch: 3 *******
******* Domain a best val test acc: 82.9%, epoch: 3 *******
******* Domain a best test acc:     82.9%, epoch: 3 *******
epoch [4/50] batch [20/288] time 0.150 (0.169) data 0.000 (0.017) loss 1.4374 (1.5849) teacher_loss 0.8982 (1.0255) loss_zs_kd 0.0626 (0.0676) loss_oracle 0.0716 (0.0716) acc 81.2500 (72.0312) lr 1.9921e-03 eta 0:38:05
epoch [4/50] batch [40/288] time 0.150 (0.160) data 0.000 (0.008) loss 1.5995 (1.5894) teacher_loss 1.1553 (1.0418) loss_zs_kd 0.1005 (0.0673) loss_oracle 0.0739 (0.0666) acc 68.7500 (72.5000) lr 1.9921e-03 eta 0:36:01
epoch [4/50] batch [60/288] time 0.146 (0.158) data 0.001 (0.006) loss 1.8079 (1.5716) teacher_loss 1.1975 (1.0312) loss_zs_kd 0.0503 (0.0645) loss_oracle 0.0502 (0.0636) acc 68.7500 (72.6042) lr 1.9921e-03 eta 0:35:26
epoch [4/50] batch [80/288] time 0.155 (0.157) data 0.000 (0.004) loss 1.9173 (1.5874) teacher_loss 1.5188 (1.0489) loss_zs_kd 0.0487 (0.0626) loss_oracle 0.0310 (0.0620) acc 65.6250 (72.4219) lr 1.9921e-03 eta 0:35:06
epoch [4/50] batch [100/288] time 0.177 (0.159) data 0.000 (0.004) loss 1.4836 (1.6056) teacher_loss 0.9232 (1.0688) loss_zs_kd 0.0514 (0.0603) loss_oracle 0.0923 (0.0618) acc 81.2500 (71.9688) lr 1.9921e-03 eta 0:35:30
epoch [4/50] batch [120/288] time 0.108 (0.153) data 0.000 (0.003) loss 1.7420 (1.6208) teacher_loss 0.9438 (1.0753) loss_zs_kd 0.0693 (0.0601) loss_oracle 0.0853 (0.0680) acc 78.1250 (71.7448) lr 1.9921e-03 eta 0:34:06
epoch [4/50] batch [140/288] time 0.302 (0.155) data 0.001 (0.003) loss 1.5178 (1.6344) teacher_loss 0.8805 (1.0831) loss_zs_kd 0.0496 (0.0604) loss_oracle 0.0499 (0.0672) acc 75.0000 (71.5179) lr 1.9921e-03 eta 0:34:31
epoch [4/50] batch [160/288] time 0.087 (0.153) data 0.000 (0.002) loss 1.6921 (1.6262) teacher_loss 1.2815 (1.0778) loss_zs_kd 0.0568 (0.0609) loss_oracle 0.0495 (0.0658) acc 65.6250 (71.5234) lr 1.9921e-03 eta 0:34:02
epoch [4/50] batch [180/288] time 0.359 (0.151) data 0.000 (0.002) loss 2.0744 (1.6237) teacher_loss 1.4237 (1.0750) loss_zs_kd 0.0825 (0.0617) loss_oracle 0.0959 (0.0660) acc 65.6250 (71.6146) lr 1.9921e-03 eta 0:33:42
epoch [4/50] batch [200/288] time 0.096 (0.150) data 0.000 (0.002) loss 1.9055 (1.6273) teacher_loss 1.1745 (1.0726) loss_zs_kd 0.0533 (0.0630) loss_oracle 0.1073 (0.0699) acc 68.7500 (71.9062) lr 1.9921e-03 eta 0:33:26
epoch [4/50] batch [220/288] time 0.102 (0.152) data 0.000 (0.002) loss 1.7582 (1.6273) teacher_loss 1.1707 (1.0745) loss_zs_kd 0.0592 (0.0633) loss_oracle 0.0490 (0.0695) acc 71.8750 (71.7330) lr 1.9921e-03 eta 0:33:48
epoch [4/50] batch [240/288] time 0.085 (0.154) data 0.000 (0.002) loss 1.9108 (1.6231) teacher_loss 1.1922 (1.0718) loss_zs_kd 0.0652 (0.0635) loss_oracle 0.0621 (0.0692) acc 68.7500 (71.9792) lr 1.9921e-03 eta 0:34:01
epoch [4/50] batch [260/288] time 0.094 (0.154) data 0.000 (0.002) loss 1.4571 (1.6270) teacher_loss 0.8532 (1.0759) loss_zs_kd 0.0572 (0.0637) loss_oracle 0.0549 (0.0696) acc 71.8750 (71.8630) lr 1.9921e-03 eta 0:33:58
epoch [4/50] batch [280/288] time 0.144 (0.152) data 0.000 (0.001) loss 1.3759 (1.6205) teacher_loss 0.7664 (1.0715) loss_zs_kd 0.0526 (0.0632) loss_oracle 0.0565 (0.0693) acc 75.0000 (72.0982) lr 1.9921e-03 eta 0:33:39
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,385
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.2%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.6%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      85.9%, epoch: 4 *******
******* Domain a best val test acc: 83.3%, epoch: 4 *******
******* Domain a best test acc:     83.3%, epoch: 4 *******
epoch [5/50] batch [20/288] time 0.162 (0.182) data 0.000 (0.015) loss 2.0621 (1.7853) teacher_loss 1.4557 (1.2145) loss_zs_kd 0.0766 (0.0829) loss_oracle 0.0750 (0.0771) acc 71.8750 (68.9062) lr 1.9823e-03 eta 0:40:04
epoch [5/50] batch [40/288] time 0.148 (0.167) data 0.000 (0.008) loss 1.1527 (1.6862) teacher_loss 0.5179 (1.1139) loss_zs_kd 0.0853 (0.0773) loss_oracle 0.0555 (0.0789) acc 90.6250 (71.4844) lr 1.9823e-03 eta 0:36:50
epoch [5/50] batch [60/288] time 0.151 (0.162) data 0.000 (0.005) loss 1.4784 (1.6545) teacher_loss 1.1531 (1.0827) loss_zs_kd 0.0444 (0.0719) loss_oracle 0.0651 (0.0755) acc 62.5000 (72.3438) lr 1.9823e-03 eta 0:35:38
epoch [5/50] batch [80/288] time 0.175 (0.160) data 0.000 (0.004) loss 1.9711 (1.6316) teacher_loss 1.4660 (1.0661) loss_zs_kd 0.0559 (0.0720) loss_oracle 0.0682 (0.0735) acc 71.8750 (72.5000) lr 1.9823e-03 eta 0:35:12
epoch [5/50] batch [100/288] time 0.185 (0.162) data 0.000 (0.003) loss 1.3296 (1.6369) teacher_loss 0.7392 (1.0734) loss_zs_kd 0.0623 (0.0712) loss_oracle 0.0722 (0.0720) acc 81.2500 (72.2812) lr 1.9823e-03 eta 0:35:33
epoch [5/50] batch [120/288] time 0.172 (0.166) data 0.000 (0.003) loss 1.6555 (1.6235) teacher_loss 1.0263 (1.0634) loss_zs_kd 0.0767 (0.0699) loss_oracle 0.0586 (0.0711) acc 75.0000 (72.3698) lr 1.9823e-03 eta 0:36:13
epoch [5/50] batch [140/288] time 0.153 (0.165) data 0.000 (0.002) loss 1.4099 (1.6247) teacher_loss 1.0759 (1.0679) loss_zs_kd 0.0570 (0.0682) loss_oracle 0.0629 (0.0684) acc 75.0000 (72.3661) lr 1.9823e-03 eta 0:36:00
epoch [5/50] batch [160/288] time 0.184 (0.166) data 0.000 (0.002) loss 1.9283 (1.6249) teacher_loss 1.3333 (1.0704) loss_zs_kd 0.0624 (0.0677) loss_oracle 0.0436 (0.0675) acc 65.6250 (72.0898) lr 1.9823e-03 eta 0:36:06
epoch [5/50] batch [180/288] time 0.098 (0.160) data 0.000 (0.002) loss 1.5616 (1.6345) teacher_loss 1.0031 (1.0753) loss_zs_kd 0.0631 (0.0678) loss_oracle 0.0840 (0.0674) acc 68.7500 (71.8924) lr 1.9823e-03 eta 0:34:56
epoch [5/50] batch [200/288] time 0.092 (0.160) data 0.000 (0.002) loss 1.6217 (1.6363) teacher_loss 1.0504 (1.0783) loss_zs_kd 0.0586 (0.0669) loss_oracle 0.0507 (0.0662) acc 81.2500 (71.9219) lr 1.9823e-03 eta 0:34:52
epoch [5/50] batch [220/288] time 0.280 (0.160) data 0.000 (0.002) loss 1.8959 (1.6365) teacher_loss 1.2571 (1.0803) loss_zs_kd 0.0629 (0.0666) loss_oracle 0.0444 (0.0648) acc 78.1250 (71.7472) lr 1.9823e-03 eta 0:34:47
epoch [5/50] batch [240/288] time 0.090 (0.157) data 0.000 (0.001) loss 1.4371 (1.6304) teacher_loss 0.8778 (1.0772) loss_zs_kd 0.0521 (0.0664) loss_oracle 0.0605 (0.0646) acc 81.2500 (71.8229) lr 1.9823e-03 eta 0:34:08
epoch [5/50] batch [260/288] time 0.084 (0.156) data 0.000 (0.001) loss 1.8138 (1.6249) teacher_loss 1.2156 (1.0722) loss_zs_kd 0.0375 (0.0659) loss_oracle 0.0934 (0.0657) acc 62.5000 (71.8389) lr 1.9823e-03 eta 0:33:48
epoch [5/50] batch [280/288] time 0.083 (0.157) data 0.000 (0.001) loss 1.3403 (1.6137) teacher_loss 0.7787 (1.0630) loss_zs_kd 0.0824 (0.0656) loss_oracle 0.0639 (0.0658) acc 81.2500 (72.1094) lr 1.9823e-03 eta 0:33:51
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,366
* accuracy: 85.5%
* error: 14.5%
* macro_f1: 84.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.7%
******* Domain a best val acc:      85.9%, epoch: 4 *******
******* Domain a best val test acc: 83.3%, epoch: 4 *******
******* Domain a best test acc:     83.3%, epoch: 4 *******
epoch [6/50] batch [20/288] time 0.153 (0.185) data 0.000 (0.020) loss 1.6971 (1.4812) teacher_loss 1.1263 (0.9533) loss_zs_kd 0.0483 (0.0587) loss_oracle 0.0476 (0.0536) acc 68.7500 (73.5938) lr 1.9686e-03 eta 0:39:56
epoch [6/50] batch [40/288] time 0.149 (0.173) data 0.000 (0.010) loss 1.6388 (1.5420) teacher_loss 1.0004 (1.0115) loss_zs_kd 0.0631 (0.0616) loss_oracle 0.0454 (0.0495) acc 75.0000 (72.8906) lr 1.9686e-03 eta 0:37:18
epoch [6/50] batch [60/288] time 0.150 (0.169) data 0.000 (0.007) loss 1.6676 (1.5285) teacher_loss 1.1134 (1.0025) loss_zs_kd 0.0765 (0.0607) loss_oracle 0.0663 (0.0517) acc 59.3750 (72.9167) lr 1.9686e-03 eta 0:36:20
epoch [6/50] batch [80/288] time 0.176 (0.167) data 0.000 (0.005) loss 1.5369 (1.5371) teacher_loss 1.0435 (1.0108) loss_zs_kd 0.0712 (0.0616) loss_oracle 0.0731 (0.0527) acc 68.7500 (72.7344) lr 1.9686e-03 eta 0:35:53
epoch [6/50] batch [100/288] time 0.177 (0.168) data 0.000 (0.004) loss 2.0304 (1.5624) teacher_loss 1.3868 (1.0318) loss_zs_kd 0.0769 (0.0621) loss_oracle 0.0464 (0.0514) acc 62.5000 (72.5000) lr 1.9686e-03 eta 0:35:58
epoch [6/50] batch [120/288] time 0.165 (0.168) data 0.000 (0.004) loss 1.6872 (1.5540) teacher_loss 1.1941 (1.0256) loss_zs_kd 0.0795 (0.0621) loss_oracle 0.0546 (0.0514) acc 68.7500 (73.0208) lr 1.9686e-03 eta 0:36:00
epoch [6/50] batch [140/288] time 0.154 (0.167) data 0.000 (0.003) loss 1.3173 (1.5462) teacher_loss 0.8730 (1.0268) loss_zs_kd 0.0466 (0.0619) loss_oracle 0.0481 (0.0516) acc 78.1250 (72.8795) lr 1.9686e-03 eta 0:35:34
epoch [6/50] batch [160/288] time 0.149 (0.165) data 0.000 (0.003) loss 1.3220 (1.5604) teacher_loss 0.7366 (1.0305) loss_zs_kd 0.0716 (0.0640) loss_oracle 0.0609 (0.0524) acc 78.1250 (72.7734) lr 1.9686e-03 eta 0:35:06
epoch [6/50] batch [180/288] time 0.152 (0.163) data 0.000 (0.002) loss 1.4088 (1.5676) teacher_loss 0.8037 (1.0345) loss_zs_kd 0.0880 (0.0648) loss_oracle 0.0773 (0.0558) acc 81.2500 (72.8472) lr 1.9686e-03 eta 0:34:43
epoch [6/50] batch [200/288] time 0.156 (0.162) data 0.000 (0.002) loss 1.4514 (1.5710) teacher_loss 1.0542 (1.0383) loss_zs_kd 0.0595 (0.0647) loss_oracle 0.0909 (0.0564) acc 68.7500 (72.6719) lr 1.9686e-03 eta 0:34:25
epoch [6/50] batch [220/288] time 0.149 (0.161) data 0.000 (0.002) loss 1.8025 (1.5750) teacher_loss 1.2678 (1.0419) loss_zs_kd 0.0747 (0.0652) loss_oracle 0.0612 (0.0573) acc 68.7500 (72.6420) lr 1.9686e-03 eta 0:34:16
epoch [6/50] batch [240/288] time 0.099 (0.158) data 0.000 (0.002) loss 1.3484 (1.5781) teacher_loss 1.0247 (1.0459) loss_zs_kd 0.0436 (0.0654) loss_oracle 0.0596 (0.0575) acc 75.0000 (72.4349) lr 1.9686e-03 eta 0:33:34
epoch [6/50] batch [260/288] time 0.107 (0.156) data 0.000 (0.002) loss 1.9817 (1.5867) teacher_loss 1.2976 (1.0485) loss_zs_kd 0.0597 (0.0662) loss_oracle 0.0480 (0.0574) acc 71.8750 (72.3798) lr 1.9686e-03 eta 0:33:00
epoch [6/50] batch [280/288] time 0.084 (0.157) data 0.000 (0.002) loss 1.8147 (1.5859) teacher_loss 1.2499 (1.0471) loss_zs_kd 0.0878 (0.0665) loss_oracle 0.0574 (0.0574) acc 68.7500 (72.4888) lr 1.9686e-03 eta 0:33:12
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,367
* accuracy: 85.5%
* error: 14.5%
* macro_f1: 84.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.9%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      85.9%, epoch: 4 *******
******* Domain a best val test acc: 83.3%, epoch: 4 *******
******* Domain a best test acc:     83.4%, epoch: 6 *******
epoch [7/50] batch [20/288] time 0.092 (0.179) data 0.000 (0.016) loss 1.9138 (1.6744) teacher_loss 1.2745 (1.0998) loss_zs_kd 0.0539 (0.0607) loss_oracle 0.0896 (0.0813) acc 68.7500 (71.2500) lr 1.9511e-03 eta 0:37:50
epoch [7/50] batch [40/288] time 0.082 (0.176) data 0.000 (0.008) loss 1.3926 (1.6214) teacher_loss 0.7369 (1.0330) loss_zs_kd 0.0571 (0.0642) loss_oracle 0.0851 (0.0823) acc 90.6250 (73.2812) lr 1.9511e-03 eta 0:37:03
epoch [7/50] batch [60/288] time 0.151 (0.162) data 0.000 (0.005) loss 1.6696 (1.5970) teacher_loss 1.2085 (1.0186) loss_zs_kd 0.0571 (0.0636) loss_oracle 0.0540 (0.0782) acc 65.6250 (73.5417) lr 1.9511e-03 eta 0:34:04
epoch [7/50] batch [80/288] time 0.170 (0.160) data 0.000 (0.004) loss 1.5238 (1.5914) teacher_loss 0.9973 (1.0221) loss_zs_kd 0.0767 (0.0663) loss_oracle 0.0738 (0.0764) acc 68.7500 (73.0859) lr 1.9511e-03 eta 0:33:29
epoch [7/50] batch [100/288] time 0.150 (0.158) data 0.000 (0.003) loss 1.2328 (1.5880) teacher_loss 0.6246 (1.0175) loss_zs_kd 0.0455 (0.0666) loss_oracle 0.0622 (0.0752) acc 84.3750 (72.7188) lr 1.9511e-03 eta 0:33:08
epoch [7/50] batch [120/288] time 0.149 (0.157) data 0.000 (0.003) loss 1.8673 (1.5873) teacher_loss 1.2567 (1.0162) loss_zs_kd 0.0675 (0.0659) loss_oracle 0.0667 (0.0712) acc 75.0000 (72.7604) lr 1.9511e-03 eta 0:32:47
epoch [7/50] batch [140/288] time 0.148 (0.156) data 0.000 (0.002) loss 1.7762 (1.5939) teacher_loss 1.1280 (1.0275) loss_zs_kd 0.0637 (0.0666) loss_oracle 0.0520 (0.0684) acc 68.7500 (72.3884) lr 1.9511e-03 eta 0:32:33
epoch [7/50] batch [160/288] time 0.147 (0.155) data 0.000 (0.002) loss 1.7445 (1.6106) teacher_loss 1.0585 (1.0433) loss_zs_kd 0.0625 (0.0666) loss_oracle 0.0616 (0.0669) acc 78.1250 (72.2070) lr 1.9511e-03 eta 0:32:22
epoch [7/50] batch [180/288] time 0.155 (0.155) data 0.000 (0.002) loss 1.6570 (1.6088) teacher_loss 1.1495 (1.0470) loss_zs_kd 0.0789 (0.0668) loss_oracle 0.0672 (0.0654) acc 71.8750 (72.2222) lr 1.9511e-03 eta 0:32:13
epoch [7/50] batch [200/288] time 0.152 (0.154) data 0.000 (0.002) loss 1.3077 (1.6022) teacher_loss 0.8081 (1.0411) loss_zs_kd 0.0647 (0.0672) loss_oracle 0.0671 (0.0653) acc 71.8750 (72.3594) lr 1.9511e-03 eta 0:32:05
epoch [7/50] batch [220/288] time 0.172 (0.154) data 0.000 (0.002) loss 1.9045 (1.5993) teacher_loss 1.2891 (1.0408) loss_zs_kd 0.0660 (0.0668) loss_oracle 0.0643 (0.0637) acc 68.7500 (72.2443) lr 1.9511e-03 eta 0:32:01
epoch [7/50] batch [240/288] time 0.151 (0.154) data 0.000 (0.001) loss 2.0438 (1.6035) teacher_loss 1.4723 (1.0461) loss_zs_kd 0.0650 (0.0671) loss_oracle 0.0426 (0.0630) acc 59.3750 (72.2135) lr 1.9511e-03 eta 0:31:55
epoch [7/50] batch [260/288] time 0.180 (0.155) data 0.000 (0.001) loss 1.5532 (1.6059) teacher_loss 1.0193 (1.0475) loss_zs_kd 0.0624 (0.0667) loss_oracle 0.0706 (0.0628) acc 71.8750 (72.1394) lr 1.9511e-03 eta 0:32:09
epoch [7/50] batch [280/288] time 0.148 (0.156) data 0.000 (0.001) loss 1.3166 (1.6006) teacher_loss 0.7507 (1.0428) loss_zs_kd 0.0513 (0.0664) loss_oracle 0.0551 (0.0624) acc 75.0000 (72.3326) lr 1.9511e-03 eta 0:32:14
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,384
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.8%
******* Domain a best val acc:      85.9%, epoch: 4 *******
******* Domain a best val test acc: 83.3%, epoch: 4 *******
******* Domain a best test acc:     83.4%, epoch: 6 *******
epoch [8/50] batch [20/288] time 0.101 (0.144) data 0.000 (0.015) loss 1.5534 (1.6163) teacher_loss 0.9423 (1.0417) loss_zs_kd 0.1342 (0.0734) loss_oracle 0.0506 (0.0775) acc 84.3750 (74.0625) lr 1.9298e-03 eta 0:29:35
epoch [8/50] batch [40/288] time 0.097 (0.139) data 0.000 (0.008) loss 1.4230 (1.5706) teacher_loss 0.8864 (1.0200) loss_zs_kd 0.0958 (0.0707) loss_oracle 0.0509 (0.0692) acc 81.2500 (73.2031) lr 1.9298e-03 eta 0:28:33
epoch [8/50] batch [60/288] time 0.095 (0.147) data 0.000 (0.005) loss 2.3973 (1.5647) teacher_loss 1.6488 (1.0092) loss_zs_kd 0.0380 (0.0690) loss_oracle 0.0610 (0.0664) acc 56.2500 (73.1771) lr 1.9298e-03 eta 0:30:16
epoch [8/50] batch [80/288] time 0.097 (0.135) data 0.000 (0.004) loss 1.7837 (1.5481) teacher_loss 1.1584 (1.0037) loss_zs_kd 0.0424 (0.0677) loss_oracle 0.0776 (0.0656) acc 68.7500 (73.5156) lr 1.9298e-03 eta 0:27:34
epoch [8/50] batch [100/288] time 0.093 (0.136) data 0.000 (0.003) loss 1.3253 (1.5531) teacher_loss 0.8295 (1.0121) loss_zs_kd 0.0949 (0.0678) loss_oracle 0.0463 (0.0653) acc 84.3750 (73.7188) lr 1.9298e-03 eta 0:27:51
epoch [8/50] batch [120/288] time 0.097 (0.141) data 0.000 (0.003) loss 1.7770 (1.5417) teacher_loss 1.0418 (1.0022) loss_zs_kd 0.0777 (0.0689) loss_oracle 0.0747 (0.0636) acc 75.0000 (73.9062) lr 1.9298e-03 eta 0:28:52
epoch [8/50] batch [140/288] time 0.084 (0.145) data 0.000 (0.002) loss 1.3291 (1.5541) teacher_loss 0.9054 (1.0106) loss_zs_kd 0.0983 (0.0694) loss_oracle 0.0828 (0.0640) acc 75.0000 (73.6384) lr 1.9298e-03 eta 0:29:34
epoch [8/50] batch [160/288] time 0.294 (0.148) data 0.000 (0.002) loss 1.7279 (1.5778) teacher_loss 1.2028 (1.0289) loss_zs_kd 0.0849 (0.0688) loss_oracle 0.1128 (0.0681) acc 65.6250 (73.2031) lr 1.9298e-03 eta 0:30:04
epoch [8/50] batch [180/288] time 0.155 (0.145) data 0.000 (0.002) loss 1.9119 (1.5852) teacher_loss 1.4109 (1.0289) loss_zs_kd 0.0575 (0.0681) loss_oracle 0.1100 (0.0727) acc 65.6250 (73.2118) lr 1.9298e-03 eta 0:29:31
epoch [8/50] batch [200/288] time 0.155 (0.147) data 0.000 (0.002) loss 1.9210 (1.5941) teacher_loss 1.2828 (1.0329) loss_zs_kd 0.0609 (0.0680) loss_oracle 0.1385 (0.0759) acc 65.6250 (73.0938) lr 1.9298e-03 eta 0:29:52
epoch [8/50] batch [220/288] time 0.167 (0.149) data 0.000 (0.002) loss 1.2586 (1.5931) teacher_loss 0.7437 (1.0270) loss_zs_kd 0.0631 (0.0683) loss_oracle 0.0890 (0.0792) acc 81.2500 (73.3239) lr 1.9298e-03 eta 0:30:07
epoch [8/50] batch [240/288] time 0.177 (0.151) data 0.000 (0.001) loss 1.4785 (1.5928) teacher_loss 1.0029 (1.0296) loss_zs_kd 0.0952 (0.0689) loss_oracle 0.0764 (0.0794) acc 75.0000 (73.1641) lr 1.9298e-03 eta 0:30:28
epoch [8/50] batch [260/288] time 0.170 (0.152) data 0.000 (0.001) loss 1.5358 (1.5935) teacher_loss 1.0325 (1.0303) loss_zs_kd 0.0998 (0.0687) loss_oracle 0.0694 (0.0782) acc 71.8750 (73.1851) lr 1.9298e-03 eta 0:30:37
epoch [8/50] batch [280/288] time 0.149 (0.152) data 0.000 (0.001) loss 2.1084 (1.5982) teacher_loss 1.5388 (1.0364) loss_zs_kd 0.0811 (0.0683) loss_oracle 0.1066 (0.0778) acc 56.2500 (72.9799) lr 1.9298e-03 eta 0:30:38
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,384
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,014
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.3%
******* Domain a best val acc:      85.9%, epoch: 4 *******
******* Domain a best val test acc: 83.3%, epoch: 4 *******
******* Domain a best test acc:     83.4%, epoch: 6 *******
epoch [9/50] batch [20/288] time 0.175 (0.178) data 0.000 (0.016) loss 1.4685 (1.5580) teacher_loss 1.0601 (1.0527) loss_zs_kd 0.0652 (0.0646) loss_oracle 0.0729 (0.0524) acc 71.8750 (71.4062) lr 1.9048e-03 eta 0:35:50
epoch [9/50] batch [40/288] time 0.175 (0.172) data 0.000 (0.008) loss 1.0137 (1.5246) teacher_loss 0.6906 (0.9996) loss_zs_kd 0.0379 (0.0669) loss_oracle 0.0350 (0.0511) acc 78.1250 (73.0469) lr 1.9048e-03 eta 0:34:38
epoch [9/50] batch [60/288] time 0.186 (0.168) data 0.000 (0.005) loss 1.6845 (1.5356) teacher_loss 1.1515 (1.0073) loss_zs_kd 0.0655 (0.0671) loss_oracle 0.0466 (0.0507) acc 62.5000 (72.8646) lr 1.9048e-03 eta 0:33:47
epoch [9/50] batch [80/288] time 0.102 (0.160) data 0.000 (0.004) loss 1.3871 (1.5666) teacher_loss 0.8429 (1.0387) loss_zs_kd 0.0657 (0.0693) loss_oracle 0.0720 (0.0530) acc 87.5000 (72.3047) lr 1.9048e-03 eta 0:32:01
epoch [9/50] batch [100/288] time 0.097 (0.157) data 0.000 (0.003) loss 1.3908 (1.5736) teacher_loss 0.7892 (1.0395) loss_zs_kd 0.0638 (0.0685) loss_oracle 0.0969 (0.0595) acc 71.8750 (72.3438) lr 1.9048e-03 eta 0:31:20
epoch [9/50] batch [120/288] time 0.116 (0.159) data 0.000 (0.003) loss 1.5747 (1.5850) teacher_loss 1.0838 (1.0501) loss_zs_kd 0.0609 (0.0688) loss_oracle 0.0603 (0.0631) acc 68.7500 (71.8490) lr 1.9048e-03 eta 0:31:39
epoch [9/50] batch [140/288] time 0.089 (0.155) data 0.000 (0.002) loss 1.4210 (1.5918) teacher_loss 0.9801 (1.0563) loss_zs_kd 0.0332 (0.0689) loss_oracle 0.0796 (0.0626) acc 71.8750 (71.8750) lr 1.9048e-03 eta 0:30:51
epoch [9/50] batch [160/288] time 0.088 (0.153) data 0.000 (0.002) loss 1.6199 (1.5922) teacher_loss 0.8350 (1.0506) loss_zs_kd 0.0795 (0.0703) loss_oracle 0.0525 (0.0631) acc 75.0000 (71.9531) lr 1.9048e-03 eta 0:30:28
epoch [9/50] batch [180/288] time 0.084 (0.154) data 0.000 (0.002) loss 2.1028 (1.5909) teacher_loss 1.6872 (1.0516) loss_zs_kd 0.0780 (0.0695) loss_oracle 0.0635 (0.0631) acc 65.6250 (71.9271) lr 1.9048e-03 eta 0:30:38
epoch [9/50] batch [200/288] time 0.085 (0.155) data 0.000 (0.002) loss 1.5598 (1.5958) teacher_loss 1.2195 (1.0584) loss_zs_kd 0.0670 (0.0700) loss_oracle 0.0472 (0.0627) acc 65.6250 (71.7188) lr 1.9048e-03 eta 0:30:48
epoch [9/50] batch [220/288] time 0.153 (0.156) data 0.000 (0.002) loss 1.3376 (1.5968) teacher_loss 0.6866 (1.0585) loss_zs_kd 0.1061 (0.0697) loss_oracle 0.0598 (0.0623) acc 81.2500 (71.6761) lr 1.9048e-03 eta 0:30:54
epoch [9/50] batch [240/288] time 0.150 (0.154) data 0.000 (0.002) loss 1.5733 (1.5966) teacher_loss 1.1542 (1.0569) loss_zs_kd 0.1043 (0.0690) loss_oracle 0.0814 (0.0624) acc 68.7500 (71.7448) lr 1.9048e-03 eta 0:30:27
epoch [9/50] batch [260/288] time 0.149 (0.154) data 0.000 (0.001) loss 1.4226 (1.5991) teacher_loss 0.9569 (1.0603) loss_zs_kd 0.0513 (0.0688) loss_oracle 0.0485 (0.0625) acc 68.7500 (71.6346) lr 1.9048e-03 eta 0:30:22
epoch [9/50] batch [280/288] time 0.160 (0.154) data 0.000 (0.001) loss 1.4118 (1.5924) teacher_loss 0.9360 (1.0536) loss_zs_kd 0.0391 (0.0689) loss_oracle 0.0815 (0.0626) acc 75.0000 (71.7634) lr 1.9048e-03 eta 0:30:16
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,378
* accuracy: 85.8%
* error: 14.2%
* macro_f1: 85.1%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,017
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.6%
******* Domain a best val acc:      85.9%, epoch: 4 *******
******* Domain a best val test acc: 83.3%, epoch: 4 *******
******* Domain a best test acc:     83.4%, epoch: 6 *******
epoch [10/50] batch [20/288] time 0.154 (0.165) data 0.000 (0.014) loss 1.3899 (1.6054) teacher_loss 0.8250 (1.0716) loss_zs_kd 0.0411 (0.0675) loss_oracle 0.0567 (0.0609) acc 75.0000 (72.8125) lr 1.8763e-03 eta 0:32:20
epoch [10/50] batch [40/288] time 0.154 (0.159) data 0.000 (0.007) loss 1.7859 (1.5634) teacher_loss 1.1898 (1.0286) loss_zs_kd 0.0926 (0.0697) loss_oracle 0.0417 (0.0612) acc 65.6250 (73.8281) lr 1.8763e-03 eta 0:31:05
epoch [10/50] batch [60/288] time 0.150 (0.156) data 0.000 (0.005) loss 1.3954 (1.5354) teacher_loss 0.9007 (0.9925) loss_zs_kd 0.0512 (0.0705) loss_oracle 0.0526 (0.0599) acc 78.1250 (74.2708) lr 1.8763e-03 eta 0:30:30
epoch [10/50] batch [80/288] time 0.147 (0.156) data 0.000 (0.004) loss 1.6172 (1.5632) teacher_loss 1.1341 (1.0185) loss_zs_kd 0.0509 (0.0714) loss_oracle 0.0501 (0.0608) acc 81.2500 (73.8672) lr 1.8763e-03 eta 0:30:34
epoch [10/50] batch [100/288] time 0.151 (0.155) data 0.000 (0.003) loss 1.9348 (1.5465) teacher_loss 1.4180 (1.0100) loss_zs_kd 0.0615 (0.0697) loss_oracle 0.0579 (0.0607) acc 68.7500 (74.0312) lr 1.8763e-03 eta 0:30:20
epoch [10/50] batch [120/288] time 0.156 (0.155) data 0.000 (0.002) loss 1.6283 (1.5504) teacher_loss 0.9307 (1.0082) loss_zs_kd 0.0678 (0.0694) loss_oracle 0.0872 (0.0654) acc 81.2500 (74.1667) lr 1.8763e-03 eta 0:30:08
epoch [10/50] batch [140/288] time 0.146 (0.154) data 0.000 (0.002) loss 1.7704 (1.5666) teacher_loss 1.2521 (1.0231) loss_zs_kd 0.0666 (0.0697) loss_oracle 0.0709 (0.0667) acc 68.7500 (73.7946) lr 1.8763e-03 eta 0:29:56
epoch [10/50] batch [160/288] time 0.203 (0.155) data 0.000 (0.002) loss 1.1242 (1.5667) teacher_loss 0.7347 (1.0216) loss_zs_kd 0.0542 (0.0689) loss_oracle 0.1034 (0.0683) acc 87.5000 (73.7695) lr 1.8763e-03 eta 0:30:02
epoch [10/50] batch [180/288] time 0.087 (0.155) data 0.000 (0.002) loss 1.6154 (1.5799) teacher_loss 1.1286 (1.0313) loss_zs_kd 0.0512 (0.0685) loss_oracle 0.0458 (0.0695) acc 71.8750 (73.4028) lr 1.8763e-03 eta 0:30:05
epoch [10/50] batch [200/288] time 0.327 (0.152) data 0.000 (0.002) loss 1.9725 (1.5865) teacher_loss 1.3078 (1.0379) loss_zs_kd 0.0545 (0.0681) loss_oracle 0.0427 (0.0682) acc 62.5000 (73.1875) lr 1.8763e-03 eta 0:29:22
epoch [10/50] batch [220/288] time 0.130 (0.151) data 0.000 (0.001) loss 1.5857 (1.5848) teacher_loss 1.1739 (1.0386) loss_zs_kd 0.0733 (0.0683) loss_oracle 0.0623 (0.0672) acc 62.5000 (73.2102) lr 1.8763e-03 eta 0:29:12
epoch [10/50] batch [240/288] time 0.097 (0.151) data 0.000 (0.001) loss 1.4786 (1.5911) teacher_loss 1.0214 (1.0419) loss_zs_kd 0.0777 (0.0688) loss_oracle 0.0573 (0.0664) acc 75.0000 (73.1250) lr 1.8763e-03 eta 0:29:10
epoch [10/50] batch [260/288] time 0.097 (0.151) data 0.000 (0.001) loss 1.3371 (1.5911) teacher_loss 1.1037 (1.0421) loss_zs_kd 0.0819 (0.0696) loss_oracle 0.0605 (0.0666) acc 75.0000 (72.9808) lr 1.8763e-03 eta 0:29:00
epoch [10/50] batch [280/288] time 0.344 (0.151) data 0.000 (0.001) loss 2.2633 (1.6018) teacher_loss 1.5637 (1.0502) loss_zs_kd 0.0984 (0.0703) loss_oracle 0.0863 (0.0682) acc 65.6250 (72.7902) lr 1.8763e-03 eta 0:28:57
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,394
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,027
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.1%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.2%, epoch: 10 *******
******* Domain a best val test acc: 83.5%, epoch: 10 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [11/50] batch [20/288] time 0.154 (0.190) data 0.000 (0.016) loss 2.1836 (1.5953) teacher_loss 1.6243 (1.0575) loss_zs_kd 0.0573 (0.0760) loss_oracle 0.0631 (0.0693) acc 62.5000 (72.0312) lr 1.8443e-03 eta 0:36:20
epoch [11/50] batch [40/288] time 0.161 (0.174) data 0.000 (0.008) loss 1.8503 (1.6330) teacher_loss 1.2060 (1.0643) loss_zs_kd 0.0903 (0.0734) loss_oracle 0.0696 (0.0729) acc 68.7500 (71.1719) lr 1.8443e-03 eta 0:33:18
epoch [11/50] batch [60/288] time 0.159 (0.167) data 0.000 (0.006) loss 1.2710 (1.6219) teacher_loss 0.9220 (1.0615) loss_zs_kd 0.0786 (0.0712) loss_oracle 0.0588 (0.0689) acc 75.0000 (71.5625) lr 1.8443e-03 eta 0:31:53
epoch [11/50] batch [80/288] time 0.153 (0.163) data 0.000 (0.004) loss 1.6628 (1.5942) teacher_loss 1.0892 (1.0365) loss_zs_kd 0.0422 (0.0695) loss_oracle 0.0536 (0.0679) acc 71.8750 (72.3828) lr 1.8443e-03 eta 0:31:08
epoch [11/50] batch [100/288] time 0.155 (0.161) data 0.000 (0.003) loss 1.7336 (1.5750) teacher_loss 1.2309 (1.0233) loss_zs_kd 0.0851 (0.0689) loss_oracle 0.0657 (0.0665) acc 71.8750 (72.7812) lr 1.8443e-03 eta 0:30:40
epoch [11/50] batch [120/288] time 0.169 (0.161) data 0.000 (0.003) loss 2.1112 (1.5773) teacher_loss 1.5318 (1.0254) loss_zs_kd 0.0662 (0.0696) loss_oracle 0.0796 (0.0665) acc 59.3750 (72.7604) lr 1.8443e-03 eta 0:30:38
epoch [11/50] batch [140/288] time 0.149 (0.162) data 0.000 (0.003) loss 1.4076 (1.5754) teacher_loss 0.8422 (1.0267) loss_zs_kd 0.0508 (0.0698) loss_oracle 0.0586 (0.0676) acc 84.3750 (72.8795) lr 1.8443e-03 eta 0:30:43
epoch [11/50] batch [160/288] time 0.153 (0.161) data 0.000 (0.002) loss 1.7743 (1.5886) teacher_loss 1.2456 (1.0397) loss_zs_kd 0.0686 (0.0695) loss_oracle 0.0681 (0.0675) acc 62.5000 (72.5195) lr 1.8443e-03 eta 0:30:29
epoch [11/50] batch [180/288] time 0.151 (0.161) data 0.000 (0.002) loss 1.6764 (1.5846) teacher_loss 1.1220 (1.0355) loss_zs_kd 0.0872 (0.0701) loss_oracle 0.0456 (0.0674) acc 71.8750 (72.6389) lr 1.8443e-03 eta 0:30:25
epoch [11/50] batch [200/288] time 0.153 (0.160) data 0.000 (0.002) loss 1.6516 (1.5807) teacher_loss 1.1355 (1.0314) loss_zs_kd 0.0964 (0.0701) loss_oracle 0.0799 (0.0665) acc 71.8750 (72.7656) lr 1.8443e-03 eta 0:30:10
epoch [11/50] batch [220/288] time 0.146 (0.159) data 0.000 (0.002) loss 1.5009 (1.5869) teacher_loss 0.9813 (1.0321) loss_zs_kd 0.0738 (0.0711) loss_oracle 0.0608 (0.0655) acc 84.3750 (72.7131) lr 1.8443e-03 eta 0:29:57
epoch [11/50] batch [240/288] time 0.152 (0.158) data 0.000 (0.002) loss 1.7744 (1.5879) teacher_loss 1.0556 (1.0287) loss_zs_kd 0.0689 (0.0715) loss_oracle 0.0723 (0.0655) acc 71.8750 (72.7734) lr 1.8443e-03 eta 0:29:45
epoch [11/50] batch [260/288] time 0.088 (0.158) data 0.000 (0.001) loss 1.4996 (1.5896) teacher_loss 0.9940 (1.0307) loss_zs_kd 0.0592 (0.0714) loss_oracle 0.0591 (0.0649) acc 75.0000 (72.6562) lr 1.8443e-03 eta 0:29:34
epoch [11/50] batch [280/288] time 0.148 (0.155) data 0.000 (0.001) loss 1.2497 (1.5801) teacher_loss 0.7935 (1.0234) loss_zs_kd 0.0433 (0.0710) loss_oracle 0.0664 (0.0649) acc 81.2500 (72.8460) lr 1.8443e-03 eta 0:29:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,396
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.6%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.8%
******* Domain a best val acc:      86.2%, epoch: 11 *******
******* Domain a best val test acc: 83.3%, epoch: 11 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [12/50] batch [20/288] time 0.379 (0.202) data 0.000 (0.014) loss 1.3283 (1.6093) teacher_loss 0.9769 (1.0378) loss_zs_kd 0.0450 (0.0738) loss_oracle 0.0771 (0.0622) acc 81.2500 (71.0938) lr 1.8090e-03 eta 0:37:41
epoch [12/50] batch [40/288] time 0.084 (0.178) data 0.000 (0.007) loss 1.6695 (1.5980) teacher_loss 1.0589 (1.0204) loss_zs_kd 0.0502 (0.0741) loss_oracle 0.0592 (0.0634) acc 71.8750 (71.7969) lr 1.8090e-03 eta 0:33:08
epoch [12/50] batch [60/288] time 0.093 (0.159) data 0.000 (0.005) loss 1.7409 (1.6182) teacher_loss 1.2372 (1.0481) loss_zs_kd 0.0663 (0.0726) loss_oracle 0.0483 (0.0632) acc 68.7500 (71.5625) lr 1.8090e-03 eta 0:29:36
epoch [12/50] batch [80/288] time 0.390 (0.159) data 0.000 (0.004) loss 2.5599 (1.6110) teacher_loss 1.9694 (1.0498) loss_zs_kd 0.0839 (0.0746) loss_oracle 0.0598 (0.0617) acc 53.1250 (72.1094) lr 1.8090e-03 eta 0:29:33
epoch [12/50] batch [100/288] time 0.158 (0.154) data 0.000 (0.003) loss 1.7239 (1.5920) teacher_loss 1.1784 (1.0312) loss_zs_kd 0.0700 (0.0728) loss_oracle 0.0618 (0.0612) acc 59.3750 (72.5000) lr 1.8090e-03 eta 0:28:29
epoch [12/50] batch [120/288] time 0.147 (0.155) data 0.000 (0.003) loss 1.9330 (1.5827) teacher_loss 1.2579 (1.0245) loss_zs_kd 0.0687 (0.0718) loss_oracle 0.0604 (0.0625) acc 68.7500 (72.8906) lr 1.8090e-03 eta 0:28:46
epoch [12/50] batch [140/288] time 0.179 (0.156) data 0.000 (0.002) loss 1.0585 (1.5869) teacher_loss 0.5323 (1.0311) loss_zs_kd 0.0552 (0.0722) loss_oracle 0.0722 (0.0633) acc 90.6250 (72.8571) lr 1.8090e-03 eta 0:28:55
epoch [12/50] batch [160/288] time 0.171 (0.157) data 0.000 (0.002) loss 1.6926 (1.5867) teacher_loss 1.0569 (1.0298) loss_zs_kd 0.1050 (0.0727) loss_oracle 0.0843 (0.0655) acc 68.7500 (72.6953) lr 1.8090e-03 eta 0:29:00
epoch [12/50] batch [180/288] time 0.175 (0.158) data 0.000 (0.002) loss 1.4774 (1.6057) teacher_loss 0.8783 (1.0438) loss_zs_kd 0.0928 (0.0733) loss_oracle 0.0932 (0.0680) acc 81.2500 (72.3438) lr 1.8090e-03 eta 0:29:02
epoch [12/50] batch [200/288] time 0.152 (0.157) data 0.000 (0.002) loss 1.9139 (1.6131) teacher_loss 1.1540 (1.0444) loss_zs_kd 0.0721 (0.0732) loss_oracle 0.0813 (0.0705) acc 71.8750 (72.4062) lr 1.8090e-03 eta 0:28:55
epoch [12/50] batch [220/288] time 0.149 (0.157) data 0.000 (0.001) loss 1.5704 (1.6151) teacher_loss 0.7917 (1.0409) loss_zs_kd 0.1064 (0.0737) loss_oracle 0.0909 (0.0717) acc 75.0000 (72.5710) lr 1.8090e-03 eta 0:28:45
epoch [12/50] batch [240/288] time 0.150 (0.156) data 0.000 (0.001) loss 1.8562 (1.6168) teacher_loss 1.4539 (1.0430) loss_zs_kd 0.0876 (0.0737) loss_oracle 0.0904 (0.0720) acc 59.3750 (72.6432) lr 1.8090e-03 eta 0:28:37
epoch [12/50] batch [260/288] time 0.152 (0.156) data 0.000 (0.001) loss 1.6291 (1.6139) teacher_loss 0.9215 (1.0405) loss_zs_kd 0.0884 (0.0733) loss_oracle 0.1103 (0.0726) acc 78.1250 (72.6322) lr 1.8090e-03 eta 0:28:28
epoch [12/50] batch [280/288] time 0.148 (0.155) data 0.000 (0.001) loss 1.5549 (1.6105) teacher_loss 0.9176 (1.0383) loss_zs_kd 0.0560 (0.0730) loss_oracle 0.0937 (0.0746) acc 78.1250 (72.6562) lr 1.8090e-03 eta 0:28:22
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,380
* accuracy: 85.8%
* error: 14.2%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,015
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.6%
******* Domain a best val acc:      86.2%, epoch: 11 *******
******* Domain a best val test acc: 83.3%, epoch: 11 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [13/50] batch [20/288] time 0.092 (0.167) data 0.000 (0.014) loss 1.3390 (1.5555) teacher_loss 0.7995 (0.9955) loss_zs_kd 0.0800 (0.0757) loss_oracle 0.1042 (0.0971) acc 81.2500 (72.5000) lr 1.7705e-03 eta 0:30:26
epoch [13/50] batch [40/288] time 0.288 (0.149) data 0.000 (0.007) loss 1.3279 (1.6285) teacher_loss 0.7821 (1.0604) loss_zs_kd 0.1013 (0.0770) loss_oracle 0.1024 (0.0972) acc 84.3750 (71.7188) lr 1.7705e-03 eta 0:27:09
epoch [13/50] batch [60/288] time 0.333 (0.150) data 0.000 (0.005) loss 2.4579 (1.7002) teacher_loss 1.6769 (1.1132) loss_zs_kd 0.0836 (0.0751) loss_oracle 0.0825 (0.0904) acc 53.1250 (70.3646) lr 1.7705e-03 eta 0:27:09
epoch [13/50] batch [80/288] time 0.093 (0.148) data 0.000 (0.004) loss 1.9385 (1.6781) teacher_loss 1.3812 (1.0954) loss_zs_kd 0.1191 (0.0742) loss_oracle 0.0793 (0.0884) acc 62.5000 (70.6641) lr 1.7705e-03 eta 0:26:47
epoch [13/50] batch [100/288] time 0.105 (0.145) data 0.000 (0.003) loss 1.1384 (1.6403) teacher_loss 0.6462 (1.0604) loss_zs_kd 0.0381 (0.0731) loss_oracle 0.0641 (0.0830) acc 87.5000 (71.6562) lr 1.7705e-03 eta 0:26:08
epoch [13/50] batch [120/288] time 0.110 (0.146) data 0.000 (0.003) loss 1.6986 (1.6256) teacher_loss 1.0131 (1.0497) loss_zs_kd 0.0774 (0.0738) loss_oracle 0.0649 (0.0808) acc 78.1250 (72.2917) lr 1.7705e-03 eta 0:26:18
epoch [13/50] batch [140/288] time 0.111 (0.151) data 0.000 (0.002) loss 1.6159 (1.6345) teacher_loss 1.0206 (1.0592) loss_zs_kd 0.0624 (0.0732) loss_oracle 0.0506 (0.0800) acc 75.0000 (72.1652) lr 1.7705e-03 eta 0:27:07
epoch [13/50] batch [160/288] time 0.104 (0.154) data 0.000 (0.002) loss 1.7055 (1.6181) teacher_loss 1.1453 (1.0454) loss_zs_kd 0.0543 (0.0723) loss_oracle 0.0693 (0.0780) acc 75.0000 (72.6172) lr 1.7705e-03 eta 0:27:37
epoch [13/50] batch [180/288] time 0.110 (0.156) data 0.000 (0.002) loss 1.0811 (1.6254) teacher_loss 0.6612 (1.0544) loss_zs_kd 0.0501 (0.0728) loss_oracle 0.0705 (0.0775) acc 78.1250 (72.3090) lr 1.7705e-03 eta 0:27:59
epoch [13/50] batch [200/288] time 0.168 (0.157) data 0.000 (0.002) loss 1.9293 (1.6220) teacher_loss 1.4400 (1.0524) loss_zs_kd 0.0545 (0.0732) loss_oracle 0.0613 (0.0768) acc 62.5000 (72.3750) lr 1.7705e-03 eta 0:28:11
epoch [13/50] batch [220/288] time 0.139 (0.158) data 0.000 (0.002) loss 1.6703 (1.6149) teacher_loss 0.8352 (1.0464) loss_zs_kd 0.0955 (0.0730) loss_oracle 0.0602 (0.0755) acc 81.2500 (72.5000) lr 1.7705e-03 eta 0:28:12
epoch [13/50] batch [240/288] time 0.177 (0.158) data 0.000 (0.001) loss 1.6567 (1.6092) teacher_loss 1.2513 (1.0430) loss_zs_kd 0.0761 (0.0723) loss_oracle 0.0334 (0.0740) acc 68.7500 (72.5521) lr 1.7705e-03 eta 0:28:09
epoch [13/50] batch [260/288] time 0.155 (0.158) data 0.000 (0.001) loss 1.5691 (1.5945) teacher_loss 1.0524 (1.0317) loss_zs_kd 0.0713 (0.0714) loss_oracle 0.0652 (0.0725) acc 75.0000 (72.8726) lr 1.7705e-03 eta 0:28:09
epoch [13/50] batch [280/288] time 0.149 (0.158) data 0.000 (0.001) loss 1.2598 (1.5929) teacher_loss 0.7562 (1.0328) loss_zs_kd 0.0512 (0.0711) loss_oracle 0.0633 (0.0718) acc 84.3750 (72.9129) lr 1.7705e-03 eta 0:28:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,389
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,018
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.8%
******* Domain a best val acc:      86.2%, epoch: 11 *******
******* Domain a best val test acc: 83.3%, epoch: 11 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [14/50] batch [20/288] time 0.157 (0.172) data 0.000 (0.014) loss 1.4337 (1.5897) teacher_loss 1.0122 (1.0472) loss_zs_kd 0.0667 (0.0682) loss_oracle 0.0638 (0.0582) acc 68.7500 (72.5000) lr 1.7290e-03 eta 0:30:26
epoch [14/50] batch [40/288] time 0.146 (0.166) data 0.000 (0.007) loss 1.5288 (1.5698) teacher_loss 0.7918 (1.0142) loss_zs_kd 0.1009 (0.0679) loss_oracle 0.0828 (0.0622) acc 75.0000 (73.4375) lr 1.7290e-03 eta 0:29:26
epoch [14/50] batch [60/288] time 0.151 (0.165) data 0.000 (0.005) loss 1.4098 (1.5636) teacher_loss 0.9218 (0.9993) loss_zs_kd 0.0622 (0.0690) loss_oracle 0.0802 (0.0633) acc 71.8750 (73.4375) lr 1.7290e-03 eta 0:29:10
epoch [14/50] batch [80/288] time 0.150 (0.163) data 0.000 (0.004) loss 1.9308 (1.5956) teacher_loss 1.2208 (1.0192) loss_zs_kd 0.0619 (0.0714) loss_oracle 0.0605 (0.0677) acc 75.0000 (72.5000) lr 1.7290e-03 eta 0:28:38
epoch [14/50] batch [100/288] time 0.094 (0.156) data 0.000 (0.003) loss 1.6697 (1.5917) teacher_loss 1.2168 (1.0176) loss_zs_kd 0.0762 (0.0735) loss_oracle 0.0539 (0.0670) acc 71.8750 (72.6875) lr 1.7290e-03 eta 0:27:29
epoch [14/50] batch [120/288] time 0.176 (0.150) data 0.000 (0.002) loss 1.1818 (1.5996) teacher_loss 0.7825 (1.0265) loss_zs_kd 0.0649 (0.0735) loss_oracle 0.0489 (0.0657) acc 81.2500 (72.5781) lr 1.7290e-03 eta 0:26:24
epoch [14/50] batch [140/288] time 0.258 (0.152) data 0.000 (0.002) loss 1.3419 (1.6061) teacher_loss 0.8802 (1.0365) loss_zs_kd 0.0569 (0.0731) loss_oracle 0.0555 (0.0642) acc 75.0000 (72.2991) lr 1.7290e-03 eta 0:26:39
epoch [14/50] batch [160/288] time 0.094 (0.149) data 0.000 (0.002) loss 1.2656 (1.5930) teacher_loss 0.7472 (1.0240) loss_zs_kd 0.0690 (0.0726) loss_oracle 0.0586 (0.0639) acc 78.1250 (72.6562) lr 1.7290e-03 eta 0:26:05
epoch [14/50] batch [180/288] time 0.092 (0.143) data 0.000 (0.002) loss 1.0001 (1.5791) teacher_loss 0.6589 (1.0126) loss_zs_kd 0.0474 (0.0716) loss_oracle 0.0619 (0.0627) acc 84.3750 (72.8646) lr 1.7290e-03 eta 0:25:01
epoch [14/50] batch [200/288] time 0.100 (0.143) data 0.000 (0.002) loss 2.0260 (1.5787) teacher_loss 1.3064 (1.0199) loss_zs_kd 0.0775 (0.0715) loss_oracle 0.0897 (0.0624) acc 68.7500 (72.7969) lr 1.7290e-03 eta 0:24:59
epoch [14/50] batch [220/288] time 0.087 (0.143) data 0.000 (0.001) loss 1.0032 (1.5733) teacher_loss 0.4462 (1.0143) loss_zs_kd 0.0601 (0.0717) loss_oracle 0.0895 (0.0632) acc 93.7500 (73.0682) lr 1.7290e-03 eta 0:24:56
epoch [14/50] batch [240/288] time 0.097 (0.145) data 0.000 (0.001) loss 1.5347 (1.5692) teacher_loss 0.8359 (1.0092) loss_zs_kd 0.0991 (0.0725) loss_oracle 0.0692 (0.0631) acc 78.1250 (73.2161) lr 1.7290e-03 eta 0:25:11
epoch [14/50] batch [260/288] time 0.084 (0.147) data 0.000 (0.001) loss 1.9221 (1.5697) teacher_loss 1.3938 (1.0117) loss_zs_kd 0.0763 (0.0727) loss_oracle 0.0521 (0.0620) acc 65.6250 (73.1851) lr 1.7290e-03 eta 0:25:25
epoch [14/50] batch [280/288] time 0.082 (0.146) data 0.000 (0.001) loss 1.0641 (1.5670) teacher_loss 0.5522 (1.0107) loss_zs_kd 0.0628 (0.0727) loss_oracle 0.0686 (0.0623) acc 87.5000 (73.2701) lr 1.7290e-03 eta 0:25:18
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,369
* accuracy: 85.5%
* error: 14.5%
* macro_f1: 84.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,014
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.4%
******* Domain a best val acc:      86.2%, epoch: 11 *******
******* Domain a best val test acc: 83.3%, epoch: 11 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [15/50] batch [20/288] time 0.157 (0.182) data 0.000 (0.017) loss 1.7758 (1.5933) teacher_loss 1.1466 (1.0177) loss_zs_kd 0.0562 (0.0778) loss_oracle 0.0505 (0.0674) acc 75.0000 (74.5312) lr 1.6845e-03 eta 0:31:26
epoch [15/50] batch [40/288] time 0.151 (0.168) data 0.000 (0.008) loss 1.8193 (1.6260) teacher_loss 1.1758 (1.0821) loss_zs_kd 0.0671 (0.0750) loss_oracle 0.0500 (0.0617) acc 71.8750 (73.0469) lr 1.6845e-03 eta 0:28:53
epoch [15/50] batch [60/288] time 0.171 (0.166) data 0.000 (0.006) loss 1.2948 (1.5764) teacher_loss 0.7252 (1.0340) loss_zs_kd 0.0712 (0.0748) loss_oracle 0.0532 (0.0611) acc 75.0000 (73.4896) lr 1.6845e-03 eta 0:28:35
epoch [15/50] batch [80/288] time 0.164 (0.164) data 0.000 (0.004) loss 1.5476 (1.5416) teacher_loss 0.9683 (0.9982) loss_zs_kd 0.0609 (0.0737) loss_oracle 0.0448 (0.0609) acc 78.1250 (74.1016) lr 1.6845e-03 eta 0:28:09
epoch [15/50] batch [100/288] time 0.149 (0.164) data 0.000 (0.003) loss 1.5741 (1.5282) teacher_loss 1.0537 (0.9890) loss_zs_kd 0.0556 (0.0738) loss_oracle 0.0630 (0.0613) acc 75.0000 (74.1875) lr 1.6845e-03 eta 0:27:59
epoch [15/50] batch [120/288] time 0.151 (0.164) data 0.000 (0.003) loss 1.9389 (1.5306) teacher_loss 1.3801 (0.9931) loss_zs_kd 0.0819 (0.0724) loss_oracle 0.0578 (0.0620) acc 68.7500 (74.0885) lr 1.6845e-03 eta 0:27:56
epoch [15/50] batch [140/288] time 0.160 (0.163) data 0.000 (0.003) loss 1.5549 (1.5412) teacher_loss 1.1129 (1.0036) loss_zs_kd 0.0591 (0.0718) loss_oracle 0.0578 (0.0624) acc 68.7500 (73.5491) lr 1.6845e-03 eta 0:27:51
epoch [15/50] batch [160/288] time 0.178 (0.164) data 0.000 (0.002) loss 1.5417 (1.5466) teacher_loss 1.0830 (1.0097) loss_zs_kd 0.0516 (0.0720) loss_oracle 0.0450 (0.0621) acc 68.7500 (73.3984) lr 1.6845e-03 eta 0:27:51
epoch [15/50] batch [180/288] time 0.097 (0.162) data 0.000 (0.002) loss 1.9038 (1.5502) teacher_loss 1.4085 (1.0164) loss_zs_kd 0.0822 (0.0718) loss_oracle 0.0571 (0.0615) acc 71.8750 (73.2465) lr 1.6845e-03 eta 0:27:35
epoch [15/50] batch [200/288] time 0.087 (0.160) data 0.000 (0.002) loss 1.3208 (1.5546) teacher_loss 0.8349 (1.0223) loss_zs_kd 0.1027 (0.0725) loss_oracle 0.0733 (0.0613) acc 68.7500 (73.0000) lr 1.6845e-03 eta 0:27:09
epoch [15/50] batch [220/288] time 0.098 (0.159) data 0.000 (0.002) loss 1.8627 (1.5575) teacher_loss 1.1933 (1.0204) loss_zs_kd 0.0826 (0.0733) loss_oracle 0.0913 (0.0621) acc 71.8750 (72.9688) lr 1.6845e-03 eta 0:26:55
epoch [15/50] batch [240/288] time 0.093 (0.157) data 0.000 (0.002) loss 1.5638 (1.5556) teacher_loss 0.8118 (1.0139) loss_zs_kd 0.1293 (0.0740) loss_oracle 0.1010 (0.0651) acc 71.8750 (73.0469) lr 1.6845e-03 eta 0:26:25
epoch [15/50] batch [260/288] time 0.097 (0.156) data 0.000 (0.001) loss 1.4861 (1.5603) teacher_loss 1.0624 (1.0161) loss_zs_kd 0.0520 (0.0745) loss_oracle 0.0808 (0.0672) acc 78.1250 (72.9928) lr 1.6845e-03 eta 0:26:19
epoch [15/50] batch [280/288] time 0.231 (0.159) data 0.000 (0.001) loss 1.4627 (1.5581) teacher_loss 0.9662 (1.0127) loss_zs_kd 0.0697 (0.0739) loss_oracle 0.0461 (0.0679) acc 71.8750 (73.0692) lr 1.6845e-03 eta 0:26:39
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,389
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,007
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 79.1%
******* Domain a best val acc:      86.2%, epoch: 11 *******
******* Domain a best val test acc: 83.3%, epoch: 11 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [16/50] batch [20/288] time 0.150 (0.190) data 0.000 (0.016) loss 2.1467 (1.6071) teacher_loss 1.3362 (1.0594) loss_zs_kd 0.0849 (0.0702) loss_oracle 0.0621 (0.0668) acc 71.8750 (72.5000) lr 1.6374e-03 eta 0:31:48
epoch [16/50] batch [40/288] time 0.163 (0.176) data 0.000 (0.008) loss 1.8070 (1.5834) teacher_loss 1.2145 (1.0155) loss_zs_kd 0.0841 (0.0739) loss_oracle 0.0692 (0.0733) acc 68.7500 (72.8906) lr 1.6374e-03 eta 0:29:26
epoch [16/50] batch [60/288] time 0.164 (0.171) data 0.000 (0.006) loss 2.2494 (1.5850) teacher_loss 1.2341 (1.0147) loss_zs_kd 0.0895 (0.0730) loss_oracle 0.0826 (0.0715) acc 71.8750 (72.6042) lr 1.6374e-03 eta 0:28:28
epoch [16/50] batch [80/288] time 0.169 (0.168) data 0.000 (0.004) loss 1.9870 (1.5872) teacher_loss 1.3864 (1.0126) loss_zs_kd 0.0723 (0.0727) loss_oracle 0.0416 (0.0679) acc 62.5000 (72.9688) lr 1.6374e-03 eta 0:27:56
epoch [16/50] batch [100/288] time 0.162 (0.166) data 0.000 (0.003) loss 1.5427 (1.5936) teacher_loss 1.0944 (1.0244) loss_zs_kd 0.0928 (0.0741) loss_oracle 0.0596 (0.0659) acc 65.6250 (72.3438) lr 1.6374e-03 eta 0:27:36
epoch [16/50] batch [120/288] time 0.153 (0.165) data 0.000 (0.003) loss 1.4939 (1.5858) teacher_loss 0.8467 (1.0188) loss_zs_kd 0.0895 (0.0731) loss_oracle 0.0454 (0.0641) acc 75.0000 (72.3177) lr 1.6374e-03 eta 0:27:25
epoch [16/50] batch [140/288] time 0.155 (0.165) data 0.000 (0.003) loss 1.6392 (1.5569) teacher_loss 1.0958 (0.9991) loss_zs_kd 0.0807 (0.0729) loss_oracle 0.0735 (0.0625) acc 78.1250 (73.0134) lr 1.6374e-03 eta 0:27:18
epoch [16/50] batch [160/288] time 0.154 (0.164) data 0.000 (0.002) loss 1.6450 (1.5544) teacher_loss 1.0974 (0.9966) loss_zs_kd 0.0637 (0.0734) loss_oracle 0.0730 (0.0617) acc 75.0000 (73.2031) lr 1.6374e-03 eta 0:27:07
epoch [16/50] batch [180/288] time 0.154 (0.164) data 0.000 (0.002) loss 1.3179 (1.5607) teacher_loss 0.8084 (1.0067) loss_zs_kd 0.0569 (0.0733) loss_oracle 0.0524 (0.0617) acc 81.2500 (73.1944) lr 1.6374e-03 eta 0:27:01
epoch [16/50] batch [200/288] time 0.156 (0.164) data 0.000 (0.002) loss 1.4094 (1.5652) teacher_loss 0.8828 (1.0122) loss_zs_kd 0.0836 (0.0726) loss_oracle 0.0960 (0.0614) acc 75.0000 (73.0000) lr 1.6374e-03 eta 0:26:55
epoch [16/50] batch [220/288] time 0.103 (0.162) data 0.000 (0.002) loss 1.2066 (1.5672) teacher_loss 0.7710 (1.0101) loss_zs_kd 0.0810 (0.0732) loss_oracle 0.0526 (0.0621) acc 75.0000 (72.9403) lr 1.6374e-03 eta 0:26:33
epoch [16/50] batch [240/288] time 0.103 (0.159) data 0.000 (0.002) loss 1.4927 (1.5593) teacher_loss 0.8713 (1.0022) loss_zs_kd 0.0981 (0.0738) loss_oracle 0.0649 (0.0622) acc 71.8750 (73.0339) lr 1.6374e-03 eta 0:26:01
epoch [16/50] batch [260/288] time 0.095 (0.160) data 0.000 (0.001) loss 1.4782 (1.5636) teacher_loss 0.9366 (1.0076) loss_zs_kd 0.0692 (0.0740) loss_oracle 0.0605 (0.0623) acc 78.1250 (72.9567) lr 1.6374e-03 eta 0:26:08
epoch [16/50] batch [280/288] time 0.091 (0.158) data 0.000 (0.001) loss 2.1696 (1.5725) teacher_loss 1.5139 (1.0153) loss_zs_kd 0.0914 (0.0744) loss_oracle 0.0689 (0.0622) acc 62.5000 (72.8795) lr 1.6374e-03 eta 0:25:46
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,388
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.8%
******* Domain a best val acc:      86.2%, epoch: 11 *******
******* Domain a best val test acc: 83.3%, epoch: 11 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [17/50] batch [20/288] time 0.333 (0.177) data 0.000 (0.016) loss 1.9785 (1.4967) teacher_loss 1.4407 (0.8727) loss_zs_kd 0.0675 (0.0676) loss_oracle 0.0912 (0.0963) acc 71.8750 (77.5000) lr 1.5878e-03 eta 0:28:52
epoch [17/50] batch [40/288] time 0.178 (0.158) data 0.000 (0.008) loss 1.4479 (1.5614) teacher_loss 0.9155 (0.9619) loss_zs_kd 0.0854 (0.0700) loss_oracle 0.0797 (0.0836) acc 75.0000 (75.4688) lr 1.5878e-03 eta 0:25:42
epoch [17/50] batch [60/288] time 0.146 (0.160) data 0.000 (0.006) loss 1.9964 (1.5694) teacher_loss 1.3214 (0.9766) loss_zs_kd 0.0627 (0.0713) loss_oracle 0.0549 (0.0791) acc 62.5000 (74.4271) lr 1.5878e-03 eta 0:25:59
epoch [17/50] batch [80/288] time 0.148 (0.159) data 0.000 (0.004) loss 1.4501 (1.5684) teacher_loss 0.8119 (0.9731) loss_zs_kd 0.0878 (0.0736) loss_oracle 0.0832 (0.0759) acc 78.1250 (74.4922) lr 1.5878e-03 eta 0:25:42
epoch [17/50] batch [100/288] time 0.150 (0.157) data 0.000 (0.003) loss 1.6122 (1.5642) teacher_loss 1.1445 (0.9758) loss_zs_kd 0.0644 (0.0739) loss_oracle 0.0558 (0.0749) acc 59.3750 (74.3125) lr 1.5878e-03 eta 0:25:24
epoch [17/50] batch [120/288] time 0.151 (0.156) data 0.000 (0.003) loss 1.4234 (1.5668) teacher_loss 1.0681 (0.9843) loss_zs_kd 0.0514 (0.0736) loss_oracle 0.0959 (0.0730) acc 71.8750 (73.6458) lr 1.5878e-03 eta 0:25:10
epoch [17/50] batch [140/288] time 0.153 (0.156) data 0.000 (0.002) loss 1.7020 (1.5812) teacher_loss 1.1743 (1.0071) loss_zs_kd 0.0714 (0.0743) loss_oracle 0.0778 (0.0734) acc 68.7500 (73.2366) lr 1.5878e-03 eta 0:25:01
epoch [17/50] batch [160/288] time 0.168 (0.156) data 0.000 (0.002) loss 1.6900 (1.5813) teacher_loss 1.1327 (1.0084) loss_zs_kd 0.0679 (0.0749) loss_oracle 0.0531 (0.0745) acc 68.7500 (73.2422) lr 1.5878e-03 eta 0:24:59
epoch [17/50] batch [180/288] time 0.151 (0.155) data 0.000 (0.002) loss 1.5552 (1.5754) teacher_loss 0.9570 (1.0008) loss_zs_kd 0.0530 (0.0742) loss_oracle 0.0770 (0.0765) acc 68.7500 (73.4028) lr 1.5878e-03 eta 0:24:49
epoch [17/50] batch [200/288] time 0.170 (0.155) data 0.000 (0.002) loss 1.3017 (1.5747) teacher_loss 0.8326 (0.9998) loss_zs_kd 0.0455 (0.0740) loss_oracle 0.0833 (0.0771) acc 71.8750 (73.4219) lr 1.5878e-03 eta 0:24:51
epoch [17/50] batch [220/288] time 0.171 (0.155) data 0.000 (0.002) loss 1.3317 (1.5783) teacher_loss 0.8440 (0.9989) loss_zs_kd 0.0376 (0.0741) loss_oracle 0.0945 (0.0786) acc 81.2500 (73.4801) lr 1.5878e-03 eta 0:24:46
epoch [17/50] batch [240/288] time 0.148 (0.155) data 0.000 (0.002) loss 1.3976 (1.5755) teacher_loss 0.9247 (0.9967) loss_zs_kd 0.0679 (0.0738) loss_oracle 0.1115 (0.0794) acc 75.0000 (73.5547) lr 1.5878e-03 eta 0:24:44
epoch [17/50] batch [260/288] time 0.149 (0.155) data 0.000 (0.001) loss 2.1122 (1.5788) teacher_loss 1.4087 (0.9999) loss_zs_kd 0.0755 (0.0741) loss_oracle 0.0522 (0.0789) acc 65.6250 (73.5337) lr 1.5878e-03 eta 0:24:39
epoch [17/50] batch [280/288] time 0.149 (0.155) data 0.000 (0.001) loss 1.9454 (1.5784) teacher_loss 1.3427 (0.9991) loss_zs_kd 0.0880 (0.0744) loss_oracle 0.0767 (0.0780) acc 65.6250 (73.6272) lr 1.5878e-03 eta 0:24:33
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,387
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.2%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,016
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.6%
******* Domain a best val acc:      86.2%, epoch: 11 *******
******* Domain a best val test acc: 83.3%, epoch: 11 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [18/50] batch [20/288] time 0.210 (0.182) data 0.000 (0.015) loss 2.0187 (1.5726) teacher_loss 1.3287 (1.0052) loss_zs_kd 0.0673 (0.0720) loss_oracle 0.0921 (0.0859) acc 65.6250 (74.5312) lr 1.5358e-03 eta 0:28:44
epoch [18/50] batch [40/288] time 0.193 (0.143) data 0.000 (0.008) loss 2.1449 (1.6083) teacher_loss 1.6135 (1.0486) loss_zs_kd 0.0929 (0.0738) loss_oracle 0.0619 (0.0776) acc 62.5000 (72.7344) lr 1.5358e-03 eta 0:22:34
epoch [18/50] batch [60/288] time 0.093 (0.147) data 0.000 (0.005) loss 1.3280 (1.6061) teacher_loss 0.7418 (1.0401) loss_zs_kd 0.0821 (0.0731) loss_oracle 0.0671 (0.0706) acc 75.0000 (72.8125) lr 1.5358e-03 eta 0:23:11
epoch [18/50] batch [80/288] time 0.382 (0.158) data 0.000 (0.004) loss 2.4285 (1.6199) teacher_loss 1.7830 (1.0600) loss_zs_kd 0.0901 (0.0732) loss_oracle 0.0955 (0.0709) acc 56.2500 (72.1484) lr 1.5358e-03 eta 0:24:44
epoch [18/50] batch [100/288] time 0.099 (0.157) data 0.000 (0.003) loss 1.5462 (1.6063) teacher_loss 0.8442 (1.0457) loss_zs_kd 0.0583 (0.0725) loss_oracle 0.0939 (0.0737) acc 84.3750 (72.3125) lr 1.5358e-03 eta 0:24:35
epoch [18/50] batch [120/288] time 0.085 (0.158) data 0.000 (0.003) loss 1.5236 (1.5924) teacher_loss 1.1219 (1.0333) loss_zs_kd 0.0638 (0.0720) loss_oracle 0.0729 (0.0756) acc 78.1250 (72.6302) lr 1.5358e-03 eta 0:24:43
epoch [18/50] batch [140/288] time 0.176 (0.154) data 0.000 (0.002) loss 1.4922 (1.5982) teacher_loss 1.0359 (1.0398) loss_zs_kd 0.0811 (0.0725) loss_oracle 0.0805 (0.0764) acc 75.0000 (72.5223) lr 1.5358e-03 eta 0:23:59
epoch [18/50] batch [160/288] time 0.148 (0.155) data 0.000 (0.002) loss 1.6658 (1.5912) teacher_loss 0.9149 (1.0292) loss_zs_kd 0.0899 (0.0732) loss_oracle 0.1606 (0.0790) acc 71.8750 (72.8516) lr 1.5358e-03 eta 0:24:03
epoch [18/50] batch [180/288] time 0.155 (0.154) data 0.000 (0.002) loss 1.4375 (1.5925) teacher_loss 0.8411 (1.0231) loss_zs_kd 0.1033 (0.0738) loss_oracle 0.1284 (0.0855) acc 71.8750 (72.9514) lr 1.5358e-03 eta 0:23:57
epoch [18/50] batch [200/288] time 0.178 (0.155) data 0.000 (0.002) loss 1.8326 (1.5844) teacher_loss 1.3402 (1.0142) loss_zs_kd 0.0685 (0.0734) loss_oracle 0.0940 (0.0865) acc 65.6250 (73.0625) lr 1.5358e-03 eta 0:23:57
epoch [18/50] batch [220/288] time 0.179 (0.156) data 0.000 (0.002) loss 2.1833 (1.5917) teacher_loss 1.5792 (1.0202) loss_zs_kd 0.0626 (0.0731) loss_oracle 0.1091 (0.0877) acc 59.3750 (72.7557) lr 1.5358e-03 eta 0:24:12
epoch [18/50] batch [240/288] time 0.181 (0.158) data 0.000 (0.001) loss 1.2203 (1.5940) teacher_loss 0.7991 (1.0208) loss_zs_kd 0.0778 (0.0737) loss_oracle 0.0625 (0.0880) acc 81.2500 (72.7344) lr 1.5358e-03 eta 0:24:22
epoch [18/50] batch [260/288] time 0.175 (0.159) data 0.000 (0.001) loss 1.4626 (1.5921) teacher_loss 0.9957 (1.0206) loss_zs_kd 0.0429 (0.0732) loss_oracle 0.0757 (0.0868) acc 75.0000 (72.8846) lr 1.5358e-03 eta 0:24:32
epoch [18/50] batch [280/288] time 0.150 (0.159) data 0.000 (0.001) loss 1.8572 (1.5955) teacher_loss 1.2401 (1.0242) loss_zs_kd 0.0993 (0.0732) loss_oracle 0.1141 (0.0866) acc 65.6250 (72.8571) lr 1.5358e-03 eta 0:24:29
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,397
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,019
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.9%
******* Domain a best val acc:      86.2%, epoch: 18 *******
******* Domain a best val test acc: 83.2%, epoch: 18 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [19/50] batch [20/288] time 0.100 (0.172) data 0.000 (0.014) loss 1.7026 (1.5554) teacher_loss 1.0827 (0.9582) loss_zs_kd 0.0570 (0.0745) loss_oracle 0.0767 (0.0841) acc 68.7500 (75.6250) lr 1.4818e-03 eta 0:26:19
epoch [19/50] batch [40/288] time 0.098 (0.146) data 0.000 (0.007) loss 1.7490 (1.6136) teacher_loss 1.1056 (1.0199) loss_zs_kd 0.0835 (0.0745) loss_oracle 0.0681 (0.0820) acc 62.5000 (72.7344) lr 1.4818e-03 eta 0:22:23
epoch [19/50] batch [60/288] time 0.333 (0.144) data 0.000 (0.005) loss 1.8676 (1.6594) teacher_loss 1.2287 (1.0682) loss_zs_kd 0.0952 (0.0773) loss_oracle 0.0764 (0.0783) acc 68.7500 (71.2500) lr 1.4818e-03 eta 0:21:54
epoch [19/50] batch [80/288] time 0.211 (0.147) data 0.000 (0.004) loss 1.6238 (1.6145) teacher_loss 0.9538 (1.0261) loss_zs_kd 0.0704 (0.0773) loss_oracle 0.0656 (0.0768) acc 68.7500 (72.2656) lr 1.4818e-03 eta 0:22:27
epoch [19/50] batch [100/288] time 0.083 (0.142) data 0.000 (0.003) loss 1.5503 (1.6031) teacher_loss 1.1468 (1.0205) loss_zs_kd 0.0639 (0.0762) loss_oracle 0.1010 (0.0782) acc 65.6250 (72.6250) lr 1.4818e-03 eta 0:21:32
epoch [19/50] batch [120/288] time 0.359 (0.144) data 0.000 (0.003) loss 1.9910 (1.6034) teacher_loss 1.4834 (1.0181) loss_zs_kd 0.0665 (0.0753) loss_oracle 0.1156 (0.0830) acc 65.6250 (72.8125) lr 1.4818e-03 eta 0:21:48
epoch [19/50] batch [140/288] time 0.097 (0.145) data 0.000 (0.002) loss 1.9146 (1.6090) teacher_loss 1.3264 (1.0164) loss_zs_kd 0.0642 (0.0750) loss_oracle 0.1057 (0.0898) acc 75.0000 (72.9688) lr 1.4818e-03 eta 0:21:55
epoch [19/50] batch [160/288] time 0.087 (0.148) data 0.000 (0.002) loss 1.6478 (1.6131) teacher_loss 1.0322 (1.0176) loss_zs_kd 0.0813 (0.0738) loss_oracle 0.0965 (0.0910) acc 75.0000 (72.8125) lr 1.4818e-03 eta 0:22:19
epoch [19/50] batch [180/288] time 0.096 (0.150) data 0.000 (0.002) loss 1.7621 (1.6133) teacher_loss 1.0988 (1.0202) loss_zs_kd 0.0680 (0.0740) loss_oracle 0.0627 (0.0881) acc 71.8750 (72.6215) lr 1.4818e-03 eta 0:22:37
epoch [19/50] batch [200/288] time 0.149 (0.149) data 0.000 (0.002) loss 1.1630 (1.6026) teacher_loss 0.6670 (1.0129) loss_zs_kd 0.0835 (0.0742) loss_oracle 0.0741 (0.0865) acc 81.2500 (72.7500) lr 1.4818e-03 eta 0:22:19
epoch [19/50] batch [220/288] time 0.160 (0.150) data 0.000 (0.001) loss 1.3569 (1.6076) teacher_loss 0.9836 (1.0207) loss_zs_kd 0.0801 (0.0755) loss_oracle 0.0610 (0.0853) acc 78.1250 (72.6705) lr 1.4818e-03 eta 0:22:27
epoch [19/50] batch [240/288] time 0.152 (0.150) data 0.000 (0.001) loss 1.3387 (1.6059) teacher_loss 0.6986 (1.0192) loss_zs_kd 0.1372 (0.0757) loss_oracle 0.0526 (0.0842) acc 84.3750 (72.7734) lr 1.4818e-03 eta 0:22:27
epoch [19/50] batch [260/288] time 0.154 (0.150) data 0.000 (0.001) loss 1.6298 (1.6018) teacher_loss 1.0542 (1.0166) loss_zs_kd 0.0774 (0.0758) loss_oracle 0.0604 (0.0830) acc 75.0000 (72.7885) lr 1.4818e-03 eta 0:22:25
epoch [19/50] batch [280/288] time 0.167 (0.151) data 0.000 (0.001) loss 2.1898 (1.6023) teacher_loss 1.5055 (1.0181) loss_zs_kd 0.1056 (0.0758) loss_oracle 0.0810 (0.0825) acc 62.5000 (72.7902) lr 1.4818e-03 eta 0:22:31
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,395
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,016
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.7%
******* Domain a best val acc:      86.2%, epoch: 18 *******
******* Domain a best val test acc: 83.2%, epoch: 18 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [20/50] batch [20/288] time 0.148 (0.179) data 0.000 (0.014) loss 2.2240 (1.5575) teacher_loss 1.4214 (0.9962) loss_zs_kd 0.0666 (0.0732) loss_oracle 0.0502 (0.0614) acc 56.2500 (72.6562) lr 1.4258e-03 eta 0:26:30
epoch [20/50] batch [40/288] time 0.156 (0.169) data 0.000 (0.007) loss 1.5949 (1.5656) teacher_loss 0.9957 (1.0064) loss_zs_kd 0.1428 (0.0794) loss_oracle 0.1003 (0.0672) acc 75.0000 (72.0312) lr 1.4258e-03 eta 0:25:02
epoch [20/50] batch [60/288] time 0.153 (0.163) data 0.000 (0.005) loss 1.2717 (1.5769) teacher_loss 0.8355 (0.9962) loss_zs_kd 0.0445 (0.0793) loss_oracle 0.0572 (0.0724) acc 84.3750 (72.8646) lr 1.4258e-03 eta 0:24:05
epoch [20/50] batch [80/288] time 0.153 (0.160) data 0.000 (0.004) loss 1.3940 (1.5620) teacher_loss 0.8565 (0.9946) loss_zs_kd 0.0577 (0.0782) loss_oracle 0.0517 (0.0686) acc 75.0000 (73.1641) lr 1.4258e-03 eta 0:23:35
epoch [20/50] batch [100/288] time 0.154 (0.158) data 0.000 (0.003) loss 1.5805 (1.5534) teacher_loss 1.1894 (0.9864) loss_zs_kd 0.0700 (0.0766) loss_oracle 0.0518 (0.0665) acc 62.5000 (73.2500) lr 1.4258e-03 eta 0:23:17
epoch [20/50] batch [120/288] time 0.096 (0.157) data 0.000 (0.003) loss 1.2313 (1.5720) teacher_loss 0.6377 (1.0058) loss_zs_kd 0.0344 (0.0760) loss_oracle 0.0442 (0.0648) acc 81.2500 (72.7604) lr 1.4258e-03 eta 0:23:00
epoch [20/50] batch [140/288] time 0.101 (0.153) data 0.000 (0.002) loss 1.4307 (1.5775) teacher_loss 0.8597 (1.0123) loss_zs_kd 0.0732 (0.0763) loss_oracle 0.0495 (0.0636) acc 78.1250 (72.6562) lr 1.4258e-03 eta 0:22:25
epoch [20/50] batch [160/288] time 0.126 (0.156) data 0.000 (0.002) loss 1.4618 (1.5681) teacher_loss 1.0019 (1.0071) loss_zs_kd 0.0543 (0.0758) loss_oracle 0.0543 (0.0630) acc 75.0000 (72.9883) lr 1.4258e-03 eta 0:22:45
epoch [20/50] batch [180/288] time 0.119 (0.153) data 0.000 (0.002) loss 1.2066 (1.5576) teacher_loss 0.7573 (0.9991) loss_zs_kd 0.0592 (0.0752) loss_oracle 0.0539 (0.0633) acc 81.2500 (73.2639) lr 1.4258e-03 eta 0:22:18
epoch [20/50] batch [200/288] time 0.128 (0.153) data 0.000 (0.002) loss 2.0359 (1.5562) teacher_loss 1.4731 (1.0027) loss_zs_kd 0.1439 (0.0751) loss_oracle 0.0786 (0.0637) acc 65.6250 (73.1875) lr 1.4258e-03 eta 0:22:16
epoch [20/50] batch [220/288] time 0.160 (0.155) data 0.000 (0.002) loss 1.3339 (1.5471) teacher_loss 0.8828 (0.9925) loss_zs_kd 0.0708 (0.0753) loss_oracle 0.0637 (0.0642) acc 75.0000 (73.3949) lr 1.4258e-03 eta 0:22:26
epoch [20/50] batch [240/288] time 0.113 (0.156) data 0.000 (0.001) loss 1.6730 (1.5440) teacher_loss 1.0701 (0.9883) loss_zs_kd 0.0667 (0.0755) loss_oracle 0.0582 (0.0639) acc 65.6250 (73.4505) lr 1.4258e-03 eta 0:22:33
epoch [20/50] batch [260/288] time 0.154 (0.157) data 0.000 (0.001) loss 1.8439 (1.5380) teacher_loss 1.3260 (0.9844) loss_zs_kd 0.0513 (0.0752) loss_oracle 0.0691 (0.0643) acc 68.7500 (73.5216) lr 1.4258e-03 eta 0:22:43
epoch [20/50] batch [280/288] time 0.153 (0.156) data 0.000 (0.001) loss 1.2400 (1.5369) teacher_loss 0.7319 (0.9825) loss_zs_kd 0.0772 (0.0749) loss_oracle 0.0620 (0.0649) acc 78.1250 (73.4821) lr 1.4258e-03 eta 0:22:28
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,401
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,013
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.6%
******* Domain a best val acc:      86.3%, epoch: 20 *******
******* Domain a best val test acc: 82.9%, epoch: 20 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [21/50] batch [20/288] time 0.152 (0.170) data 0.000 (0.014) loss 1.2084 (1.5079) teacher_loss 0.5275 (0.9403) loss_zs_kd 0.0789 (0.0838) loss_oracle 0.0589 (0.0668) acc 87.5000 (75.3125) lr 1.3681e-03 eta 0:24:26
epoch [21/50] batch [40/288] time 0.150 (0.172) data 0.000 (0.007) loss 1.1783 (1.5164) teacher_loss 0.6265 (0.9426) loss_zs_kd 0.0765 (0.0806) loss_oracle 0.0623 (0.0622) acc 84.3750 (74.2188) lr 1.3681e-03 eta 0:24:40
epoch [21/50] batch [60/288] time 0.149 (0.172) data 0.000 (0.005) loss 1.5142 (1.5353) teacher_loss 0.8860 (0.9654) loss_zs_kd 0.0922 (0.0786) loss_oracle 0.0620 (0.0589) acc 75.0000 (74.3229) lr 1.3681e-03 eta 0:24:35
epoch [21/50] batch [80/288] time 0.154 (0.167) data 0.000 (0.004) loss 1.3506 (1.5638) teacher_loss 0.6526 (0.9936) loss_zs_kd 0.0497 (0.0753) loss_oracle 0.0519 (0.0595) acc 78.1250 (73.4375) lr 1.3681e-03 eta 0:23:47
epoch [21/50] batch [100/288] time 0.168 (0.165) data 0.000 (0.003) loss 1.4461 (1.5517) teacher_loss 0.9551 (0.9826) loss_zs_kd 0.0529 (0.0742) loss_oracle 0.0699 (0.0634) acc 71.8750 (73.6562) lr 1.3681e-03 eta 0:23:32
epoch [21/50] batch [120/288] time 0.162 (0.164) data 0.000 (0.003) loss 1.5596 (1.5648) teacher_loss 0.7589 (0.9887) loss_zs_kd 0.0633 (0.0751) loss_oracle 0.0680 (0.0650) acc 84.3750 (73.8542) lr 1.3681e-03 eta 0:23:18
epoch [21/50] batch [140/288] time 0.154 (0.163) data 0.000 (0.002) loss 1.6554 (1.5754) teacher_loss 1.0196 (1.0005) loss_zs_kd 0.0832 (0.0747) loss_oracle 0.0637 (0.0648) acc 78.1250 (73.4152) lr 1.3681e-03 eta 0:23:07
epoch [21/50] batch [160/288] time 0.162 (0.162) data 0.000 (0.002) loss 1.4233 (1.5899) teacher_loss 0.9739 (1.0153) loss_zs_kd 0.0626 (0.0759) loss_oracle 0.0513 (0.0643) acc 68.7500 (72.9492) lr 1.3681e-03 eta 0:22:51
epoch [21/50] batch [180/288] time 0.166 (0.162) data 0.000 (0.002) loss 1.8971 (1.5951) teacher_loss 1.2189 (1.0202) loss_zs_kd 0.0619 (0.0764) loss_oracle 0.0575 (0.0633) acc 75.0000 (72.7257) lr 1.3681e-03 eta 0:22:48
epoch [21/50] batch [200/288] time 0.096 (0.156) data 0.000 (0.002) loss 1.8143 (1.5868) teacher_loss 1.1728 (1.0120) loss_zs_kd 0.0630 (0.0764) loss_oracle 0.0463 (0.0632) acc 68.7500 (72.9375) lr 1.3681e-03 eta 0:21:54
epoch [21/50] batch [220/288] time 0.237 (0.157) data 0.000 (0.002) loss 1.4443 (1.5857) teacher_loss 0.8627 (1.0097) loss_zs_kd 0.0685 (0.0761) loss_oracle 0.0629 (0.0628) acc 78.1250 (72.9972) lr 1.3681e-03 eta 0:21:58
epoch [21/50] batch [240/288] time 0.103 (0.156) data 0.000 (0.001) loss 1.6044 (1.5952) teacher_loss 1.0289 (1.0204) loss_zs_kd 0.0926 (0.0762) loss_oracle 0.1012 (0.0637) acc 81.2500 (72.9297) lr 1.3681e-03 eta 0:21:54
epoch [21/50] batch [260/288] time 0.095 (0.154) data 0.000 (0.001) loss 1.5148 (1.5925) teacher_loss 0.9152 (1.0170) loss_zs_kd 0.0640 (0.0763) loss_oracle 0.0595 (0.0651) acc 75.0000 (72.9207) lr 1.3681e-03 eta 0:21:33
epoch [21/50] batch [280/288] time 0.089 (0.155) data 0.000 (0.001) loss 1.5125 (1.5937) teacher_loss 0.9023 (1.0152) loss_zs_kd 0.0557 (0.0760) loss_oracle 0.0975 (0.0665) acc 78.1250 (72.9129) lr 1.3681e-03 eta 0:21:38
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,393
* accuracy: 86.1%
* error: 13.9%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,017
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.7%
******* Domain a best val acc:      86.3%, epoch: 20 *******
******* Domain a best val test acc: 82.9%, epoch: 20 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [22/50] batch [20/288] time 0.151 (0.173) data 0.000 (0.014) loss 2.3217 (1.6167) teacher_loss 1.5828 (0.9949) loss_zs_kd 0.0728 (0.0815) loss_oracle 0.0656 (0.0894) acc 62.5000 (72.8125) lr 1.3090e-03 eta 0:24:00
epoch [22/50] batch [40/288] time 0.154 (0.165) data 0.000 (0.007) loss 1.3180 (1.5663) teacher_loss 0.9048 (0.9843) loss_zs_kd 0.0690 (0.0754) loss_oracle 0.0710 (0.0829) acc 84.3750 (72.6562) lr 1.3090e-03 eta 0:22:52
epoch [22/50] batch [60/288] time 0.152 (0.161) data 0.000 (0.005) loss 1.3751 (1.5733) teacher_loss 0.7082 (1.0046) loss_zs_kd 0.1055 (0.0763) loss_oracle 0.0787 (0.0792) acc 78.1250 (72.9167) lr 1.3090e-03 eta 0:22:14
epoch [22/50] batch [80/288] time 0.149 (0.158) data 0.000 (0.004) loss 1.9794 (1.5575) teacher_loss 1.3018 (0.9910) loss_zs_kd 0.0852 (0.0771) loss_oracle 0.0541 (0.0741) acc 68.7500 (73.7109) lr 1.3090e-03 eta 0:21:50
epoch [22/50] batch [100/288] time 0.150 (0.157) data 0.000 (0.003) loss 1.1377 (1.5560) teacher_loss 0.7103 (0.9902) loss_zs_kd 0.0428 (0.0749) loss_oracle 0.0416 (0.0696) acc 78.1250 (73.6250) lr 1.3090e-03 eta 0:21:33
epoch [22/50] batch [120/288] time 0.175 (0.156) data 0.000 (0.003) loss 1.6533 (1.5675) teacher_loss 1.0385 (1.0003) loss_zs_kd 0.0933 (0.0757) loss_oracle 0.1032 (0.0700) acc 71.8750 (73.6198) lr 1.3090e-03 eta 0:21:28
epoch [22/50] batch [140/288] time 0.155 (0.157) data 0.000 (0.002) loss 1.5722 (1.5601) teacher_loss 0.9924 (0.9937) loss_zs_kd 0.0873 (0.0746) loss_oracle 0.0728 (0.0727) acc 78.1250 (73.9286) lr 1.3090e-03 eta 0:21:27
epoch [22/50] batch [160/288] time 0.151 (0.157) data 0.000 (0.002) loss 1.0043 (1.5744) teacher_loss 0.5843 (1.0026) loss_zs_kd 0.0575 (0.0749) loss_oracle 0.0613 (0.0736) acc 84.3750 (73.7305) lr 1.3090e-03 eta 0:21:23
epoch [22/50] batch [180/288] time 0.158 (0.156) data 0.000 (0.002) loss 1.4271 (1.5696) teacher_loss 0.8921 (0.9979) loss_zs_kd 0.0493 (0.0752) loss_oracle 0.0722 (0.0740) acc 81.2500 (73.8715) lr 1.3090e-03 eta 0:21:15
epoch [22/50] batch [200/288] time 0.149 (0.156) data 0.000 (0.002) loss 1.4830 (1.5731) teacher_loss 0.9075 (1.0017) loss_zs_kd 0.0710 (0.0753) loss_oracle 0.0705 (0.0730) acc 71.8750 (73.6875) lr 1.3090e-03 eta 0:21:08
epoch [22/50] batch [220/288] time 0.159 (0.156) data 0.000 (0.001) loss 0.9671 (1.5724) teacher_loss 0.5715 (1.0011) loss_zs_kd 0.0807 (0.0762) loss_oracle 0.0498 (0.0720) acc 81.2500 (73.4943) lr 1.3090e-03 eta 0:21:08
epoch [22/50] batch [240/288] time 0.154 (0.157) data 0.000 (0.001) loss 1.5902 (1.5724) teacher_loss 1.0657 (1.0038) loss_zs_kd 0.0744 (0.0762) loss_oracle 0.0605 (0.0709) acc 75.0000 (73.4635) lr 1.3090e-03 eta 0:21:11
epoch [22/50] batch [260/288] time 0.171 (0.157) data 0.000 (0.001) loss 1.5723 (1.5706) teacher_loss 0.9229 (1.0036) loss_zs_kd 0.0687 (0.0757) loss_oracle 0.0747 (0.0705) acc 81.2500 (73.5817) lr 1.3090e-03 eta 0:21:11
epoch [22/50] batch [280/288] time 0.087 (0.153) data 0.000 (0.001) loss 1.0406 (1.5760) teacher_loss 0.5761 (1.0046) loss_zs_kd 0.0750 (0.0757) loss_oracle 0.0752 (0.0712) acc 87.5000 (73.4375) lr 1.3090e-03 eta 0:20:36
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,384
* accuracy: 85.9%
* error: 14.1%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,013
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.6%
******* Domain a best val acc:      86.3%, epoch: 20 *******
******* Domain a best val test acc: 82.9%, epoch: 20 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [23/50] batch [20/288] time 0.336 (0.174) data 0.000 (0.015) loss 1.6813 (1.5466) teacher_loss 1.1845 (0.9678) loss_zs_kd 0.0684 (0.0718) loss_oracle 0.0842 (0.0894) acc 68.7500 (75.1562) lr 1.2487e-03 eta 0:23:20
epoch [23/50] batch [40/288] time 0.104 (0.173) data 0.000 (0.008) loss 1.3114 (1.5422) teacher_loss 0.8955 (0.9684) loss_zs_kd 0.0698 (0.0719) loss_oracle 0.0649 (0.0869) acc 75.0000 (74.2188) lr 1.2487e-03 eta 0:23:07
epoch [23/50] batch [60/288] time 0.268 (0.167) data 0.000 (0.005) loss 2.0434 (1.5721) teacher_loss 1.3627 (1.0011) loss_zs_kd 0.0952 (0.0737) loss_oracle 0.0657 (0.0812) acc 65.6250 (73.1771) lr 1.2487e-03 eta 0:22:19
epoch [23/50] batch [80/288] time 0.100 (0.160) data 0.000 (0.004) loss 1.9209 (1.5606) teacher_loss 1.3441 (0.9939) loss_zs_kd 0.0803 (0.0748) loss_oracle 0.0658 (0.0763) acc 62.5000 (73.2422) lr 1.2487e-03 eta 0:21:18
epoch [23/50] batch [100/288] time 0.150 (0.157) data 0.000 (0.003) loss 1.3262 (1.5465) teacher_loss 0.7758 (0.9826) loss_zs_kd 0.0731 (0.0761) loss_oracle 0.0554 (0.0740) acc 84.3750 (73.5312) lr 1.2487e-03 eta 0:20:54
epoch [23/50] batch [120/288] time 0.174 (0.158) data 0.000 (0.003) loss 1.1674 (1.5523) teacher_loss 0.6383 (0.9866) loss_zs_kd 0.0912 (0.0772) loss_oracle 0.0695 (0.0724) acc 87.5000 (73.5417) lr 1.2487e-03 eta 0:20:58
epoch [23/50] batch [140/288] time 0.152 (0.157) data 0.000 (0.002) loss 1.1582 (1.5417) teacher_loss 0.9612 (0.9822) loss_zs_kd 0.0636 (0.0766) loss_oracle 0.0471 (0.0706) acc 81.2500 (73.6830) lr 1.2487e-03 eta 0:20:45
epoch [23/50] batch [160/288] time 0.149 (0.156) data 0.000 (0.002) loss 1.6715 (1.5476) teacher_loss 1.0292 (0.9926) loss_zs_kd 0.0696 (0.0765) loss_oracle 0.0545 (0.0687) acc 71.8750 (73.3398) lr 1.2487e-03 eta 0:20:35
epoch [23/50] batch [180/288] time 0.151 (0.156) data 0.000 (0.002) loss 1.3310 (1.5404) teacher_loss 0.8796 (0.9871) loss_zs_kd 0.0449 (0.0763) loss_oracle 0.0495 (0.0670) acc 78.1250 (73.5938) lr 1.2487e-03 eta 0:20:28
epoch [23/50] batch [200/288] time 0.154 (0.155) data 0.000 (0.002) loss 1.3936 (1.5424) teacher_loss 0.8734 (0.9884) loss_zs_kd 0.0791 (0.0762) loss_oracle 0.0488 (0.0662) acc 75.0000 (73.7188) lr 1.2487e-03 eta 0:20:21
epoch [23/50] batch [220/288] time 0.151 (0.155) data 0.000 (0.002) loss 1.7810 (1.5493) teacher_loss 1.1807 (0.9904) loss_zs_kd 0.0669 (0.0763) loss_oracle 0.0705 (0.0659) acc 71.8750 (73.5227) lr 1.2487e-03 eta 0:20:15
epoch [23/50] batch [240/288] time 0.147 (0.155) data 0.000 (0.001) loss 1.2513 (1.5552) teacher_loss 0.7903 (0.9976) loss_zs_kd 0.0757 (0.0757) loss_oracle 0.0488 (0.0654) acc 78.1250 (73.5156) lr 1.2487e-03 eta 0:20:09
epoch [23/50] batch [260/288] time 0.146 (0.154) data 0.000 (0.001) loss 1.3254 (1.5685) teacher_loss 0.7076 (1.0074) loss_zs_kd 0.0731 (0.0759) loss_oracle 0.0718 (0.0649) acc 84.3750 (73.2091) lr 1.2487e-03 eta 0:20:03
epoch [23/50] batch [280/288] time 0.150 (0.154) data 0.000 (0.001) loss 1.6084 (1.5602) teacher_loss 0.9006 (0.9997) loss_zs_kd 0.1015 (0.0758) loss_oracle 0.0731 (0.0647) acc 78.1250 (73.4487) lr 1.2487e-03 eta 0:19:58
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,388
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.3%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.8%
******* Domain a best val acc:      86.3%, epoch: 20 *******
******* Domain a best val test acc: 82.9%, epoch: 20 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [24/50] batch [20/288] time 0.146 (0.183) data 0.000 (0.015) loss 1.4797 (1.5816) teacher_loss 1.0341 (0.9820) loss_zs_kd 0.0683 (0.0689) loss_oracle 0.0823 (0.0585) acc 75.0000 (72.9688) lr 1.1874e-03 eta 0:23:41
epoch [24/50] batch [40/288] time 0.086 (0.165) data 0.000 (0.008) loss 1.5059 (1.6097) teacher_loss 1.0777 (1.0235) loss_zs_kd 0.1018 (0.0736) loss_oracle 0.0488 (0.0562) acc 68.7500 (71.9531) lr 1.1874e-03 eta 0:21:20
epoch [24/50] batch [60/288] time 0.104 (0.150) data 0.000 (0.005) loss 1.6303 (1.6094) teacher_loss 1.0059 (1.0324) loss_zs_kd 0.0647 (0.0774) loss_oracle 0.0378 (0.0568) acc 81.2500 (72.2396) lr 1.1874e-03 eta 0:19:19
epoch [24/50] batch [80/288] time 0.164 (0.156) data 0.000 (0.004) loss 1.4121 (1.5792) teacher_loss 0.9198 (1.0059) loss_zs_kd 0.0905 (0.0761) loss_oracle 0.0687 (0.0561) acc 75.0000 (72.6562) lr 1.1874e-03 eta 0:19:57
epoch [24/50] batch [100/288] time 0.103 (0.151) data 0.000 (0.003) loss 1.3734 (1.5752) teacher_loss 0.8487 (1.0037) loss_zs_kd 0.1071 (0.0754) loss_oracle 0.0636 (0.0570) acc 81.2500 (73.0312) lr 1.1874e-03 eta 0:19:15
epoch [24/50] batch [120/288] time 0.093 (0.153) data 0.000 (0.003) loss 1.5383 (1.5632) teacher_loss 1.0712 (0.9959) loss_zs_kd 0.0625 (0.0757) loss_oracle 0.0447 (0.0576) acc 65.6250 (73.4375) lr 1.1874e-03 eta 0:19:29
epoch [24/50] batch [140/288] time 0.105 (0.152) data 0.000 (0.002) loss 1.5937 (1.5545) teacher_loss 1.0123 (0.9908) loss_zs_kd 0.0631 (0.0767) loss_oracle 0.0753 (0.0581) acc 78.1250 (73.6607) lr 1.1874e-03 eta 0:19:20
epoch [24/50] batch [160/288] time 0.084 (0.151) data 0.000 (0.002) loss 1.7029 (1.5572) teacher_loss 1.1633 (0.9950) loss_zs_kd 0.0678 (0.0783) loss_oracle 0.0611 (0.0593) acc 65.6250 (73.4961) lr 1.1874e-03 eta 0:19:08
epoch [24/50] batch [180/288] time 0.097 (0.149) data 0.000 (0.002) loss 1.5709 (1.5693) teacher_loss 0.8732 (1.0067) loss_zs_kd 0.0591 (0.0775) loss_oracle 0.0633 (0.0595) acc 75.0000 (73.1250) lr 1.1874e-03 eta 0:18:55
epoch [24/50] batch [200/288] time 0.106 (0.151) data 0.000 (0.002) loss 1.6914 (1.5740) teacher_loss 0.9637 (1.0070) loss_zs_kd 0.0685 (0.0772) loss_oracle 0.0685 (0.0593) acc 71.8750 (73.0312) lr 1.1874e-03 eta 0:19:07
epoch [24/50] batch [220/288] time 0.152 (0.151) data 0.000 (0.002) loss 1.4167 (1.5709) teacher_loss 0.8445 (1.0065) loss_zs_kd 0.0810 (0.0766) loss_oracle 0.0824 (0.0608) acc 71.8750 (73.1250) lr 1.1874e-03 eta 0:18:57
epoch [24/50] batch [240/288] time 0.143 (0.151) data 0.000 (0.001) loss 1.5456 (1.5713) teacher_loss 0.8286 (1.0065) loss_zs_kd 0.0797 (0.0765) loss_oracle 0.0767 (0.0622) acc 84.3750 (73.0990) lr 1.1874e-03 eta 0:18:54
epoch [24/50] batch [260/288] time 0.154 (0.151) data 0.000 (0.001) loss 1.1008 (1.5742) teacher_loss 0.5449 (1.0089) loss_zs_kd 0.0830 (0.0763) loss_oracle 0.0824 (0.0634) acc 87.5000 (73.0288) lr 1.1874e-03 eta 0:18:52
epoch [24/50] batch [280/288] time 0.150 (0.151) data 0.000 (0.001) loss 1.3497 (1.5770) teacher_loss 0.9222 (1.0115) loss_zs_kd 0.0802 (0.0760) loss_oracle 0.1201 (0.0653) acc 75.0000 (73.0022) lr 1.1874e-03 eta 0:18:49
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,393
* accuracy: 86.1%
* error: 13.9%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,017
* accuracy: 83.1%
* error: 16.9%
* macro_f1: 79.8%
******* Domain a best val acc:      86.3%, epoch: 20 *******
******* Domain a best val test acc: 82.9%, epoch: 20 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [25/50] batch [20/288] time 0.171 (0.179) data 0.000 (0.014) loss 1.8385 (1.5702) teacher_loss 1.2952 (1.0180) loss_zs_kd 0.0775 (0.0783) loss_oracle 0.0899 (0.0871) acc 65.6250 (72.3438) lr 1.1253e-03 eta 0:22:16
epoch [25/50] batch [40/288] time 0.155 (0.165) data 0.000 (0.007) loss 1.3985 (1.5917) teacher_loss 0.8029 (1.0252) loss_zs_kd 0.0577 (0.0759) loss_oracle 0.0621 (0.0815) acc 81.2500 (72.8125) lr 1.1253e-03 eta 0:20:31
epoch [25/50] batch [60/288] time 0.150 (0.160) data 0.000 (0.005) loss 1.9384 (1.5970) teacher_loss 1.4169 (1.0400) loss_zs_kd 0.0752 (0.0739) loss_oracle 0.0699 (0.0775) acc 59.3750 (72.6562) lr 1.1253e-03 eta 0:19:52
epoch [25/50] batch [80/288] time 0.158 (0.159) data 0.000 (0.004) loss 1.4460 (1.5975) teacher_loss 1.0281 (1.0396) loss_zs_kd 0.1332 (0.0744) loss_oracle 0.0579 (0.0776) acc 71.8750 (73.0859) lr 1.1253e-03 eta 0:19:40
epoch [25/50] batch [100/288] time 0.149 (0.158) data 0.000 (0.003) loss 1.4243 (1.6007) teacher_loss 0.8801 (1.0354) loss_zs_kd 0.0930 (0.0772) loss_oracle 0.0684 (0.0755) acc 71.8750 (73.3750) lr 1.1253e-03 eta 0:19:27
epoch [25/50] batch [120/288] time 0.168 (0.158) data 0.000 (0.003) loss 1.9398 (1.5932) teacher_loss 1.2529 (1.0238) loss_zs_kd 0.0631 (0.0761) loss_oracle 0.0804 (0.0746) acc 65.6250 (73.5156) lr 1.1253e-03 eta 0:19:25
epoch [25/50] batch [140/288] time 0.154 (0.157) data 0.000 (0.002) loss 0.9684 (1.5973) teacher_loss 0.4748 (1.0231) loss_zs_kd 0.0534 (0.0757) loss_oracle 0.0886 (0.0759) acc 84.3750 (73.5491) lr 1.1253e-03 eta 0:19:14
epoch [25/50] batch [160/288] time 0.084 (0.155) data 0.000 (0.002) loss 1.5128 (1.5881) teacher_loss 0.8968 (1.0087) loss_zs_kd 0.0706 (0.0758) loss_oracle 0.0713 (0.0760) acc 78.1250 (73.6523) lr 1.1253e-03 eta 0:18:52
epoch [25/50] batch [180/288] time 0.161 (0.151) data 0.000 (0.002) loss 1.7548 (1.5907) teacher_loss 1.0833 (1.0117) loss_zs_kd 0.0731 (0.0759) loss_oracle 0.0533 (0.0763) acc 68.7500 (73.5417) lr 1.1253e-03 eta 0:18:26
epoch [25/50] batch [200/288] time 0.149 (0.154) data 0.000 (0.002) loss 1.2918 (1.5810) teacher_loss 0.7277 (1.0080) loss_zs_kd 0.0991 (0.0762) loss_oracle 0.0870 (0.0752) acc 81.2500 (73.5938) lr 1.1253e-03 eta 0:18:38
epoch [25/50] batch [220/288] time 0.102 (0.151) data 0.000 (0.002) loss 1.5679 (1.5738) teacher_loss 0.9183 (1.0026) loss_zs_kd 0.0926 (0.0762) loss_oracle 0.0458 (0.0738) acc 75.0000 (73.6080) lr 1.1253e-03 eta 0:18:18
epoch [25/50] batch [240/288] time 0.087 (0.150) data 0.000 (0.001) loss 2.0942 (1.5729) teacher_loss 1.5167 (1.0042) loss_zs_kd 0.0734 (0.0761) loss_oracle 0.0670 (0.0728) acc 65.6250 (73.5677) lr 1.1253e-03 eta 0:18:08
epoch [25/50] batch [260/288] time 0.392 (0.151) data 0.000 (0.001) loss 1.3889 (1.5711) teacher_loss 0.8788 (1.0021) loss_zs_kd 0.0753 (0.0764) loss_oracle 0.0693 (0.0719) acc 75.0000 (73.6779) lr 1.1253e-03 eta 0:18:12
epoch [25/50] batch [280/288] time 0.121 (0.151) data 0.000 (0.001) loss 1.3926 (1.5703) teacher_loss 1.0127 (0.9999) loss_zs_kd 0.0545 (0.0763) loss_oracle 0.0531 (0.0704) acc 75.0000 (73.8504) lr 1.1253e-03 eta 0:18:09
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,389
* accuracy: 86.0%
* error: 14.0%
* macro_f1: 85.4%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,015
* accuracy: 83.0%
* error: 17.0%
* macro_f1: 79.6%
******* Domain a best val acc:      86.3%, epoch: 20 *******
******* Domain a best val test acc: 82.9%, epoch: 20 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [26/50] batch [20/288] time 0.148 (0.178) data 0.000 (0.012) loss 1.2542 (1.4230) teacher_loss 0.7248 (0.8671) loss_zs_kd 0.0454 (0.0746) loss_oracle 0.0330 (0.0540) acc 78.1250 (77.3438) lr 1.0628e-03 eta 0:21:16
epoch [26/50] batch [40/288] time 0.169 (0.169) data 0.000 (0.006) loss 1.6144 (1.4906) teacher_loss 1.0312 (0.9390) loss_zs_kd 0.0433 (0.0727) loss_oracle 0.0394 (0.0549) acc 75.0000 (74.5312) lr 1.0628e-03 eta 0:20:10
epoch [26/50] batch [60/288] time 0.146 (0.166) data 0.000 (0.004) loss 1.4784 (1.5076) teacher_loss 1.0407 (0.9541) loss_zs_kd 0.0611 (0.0734) loss_oracle 0.0439 (0.0574) acc 65.6250 (74.4271) lr 1.0628e-03 eta 0:19:46
epoch [26/50] batch [80/288] time 0.173 (0.165) data 0.000 (0.003) loss 1.8936 (1.5224) teacher_loss 1.3975 (0.9615) loss_zs_kd 0.0790 (0.0747) loss_oracle 0.0560 (0.0596) acc 75.0000 (74.2578) lr 1.0628e-03 eta 0:19:37
epoch [26/50] batch [100/288] time 0.152 (0.165) data 0.000 (0.002) loss 1.2151 (1.5353) teacher_loss 0.8115 (0.9728) loss_zs_kd 0.0588 (0.0750) loss_oracle 0.0677 (0.0607) acc 78.1250 (73.7812) lr 1.0628e-03 eta 0:19:34
epoch [26/50] batch [120/288] time 0.152 (0.164) data 0.000 (0.002) loss 1.5982 (1.5364) teacher_loss 1.0190 (0.9763) loss_zs_kd 0.1134 (0.0751) loss_oracle 0.0870 (0.0621) acc 68.7500 (73.7500) lr 1.0628e-03 eta 0:19:21
epoch [26/50] batch [140/288] time 0.151 (0.162) data 0.000 (0.002) loss 1.5759 (1.5364) teacher_loss 1.0251 (0.9719) loss_zs_kd 0.0739 (0.0753) loss_oracle 0.0700 (0.0625) acc 78.1250 (73.8839) lr 1.0628e-03 eta 0:19:03
epoch [26/50] batch [160/288] time 0.162 (0.161) data 0.000 (0.002) loss 1.6594 (1.5298) teacher_loss 1.0950 (0.9713) loss_zs_kd 0.1026 (0.0761) loss_oracle 0.0677 (0.0629) acc 71.8750 (73.9648) lr 1.0628e-03 eta 0:18:54
epoch [26/50] batch [180/288] time 0.151 (0.160) data 0.000 (0.001) loss 1.3243 (1.5305) teacher_loss 0.8135 (0.9700) loss_zs_kd 0.0522 (0.0758) loss_oracle 0.0566 (0.0620) acc 81.2500 (73.8542) lr 1.0628e-03 eta 0:18:45
epoch [26/50] batch [200/288] time 0.152 (0.159) data 0.000 (0.001) loss 1.5533 (1.5369) teacher_loss 0.9612 (0.9740) loss_zs_kd 0.0815 (0.0760) loss_oracle 0.0681 (0.0624) acc 84.3750 (73.6875) lr 1.0628e-03 eta 0:18:36
epoch [26/50] batch [220/288] time 0.169 (0.159) data 0.000 (0.001) loss 0.9947 (1.5415) teacher_loss 0.3893 (0.9762) loss_zs_kd 0.0650 (0.0761) loss_oracle 0.0574 (0.0628) acc 90.6250 (73.6080) lr 1.0628e-03 eta 0:18:26
epoch [26/50] batch [240/288] time 0.111 (0.155) data 0.000 (0.001) loss 1.6151 (1.5474) teacher_loss 1.0548 (0.9826) loss_zs_kd 0.0832 (0.0762) loss_oracle 0.0553 (0.0628) acc 71.8750 (73.4766) lr 1.0628e-03 eta 0:18:01
epoch [26/50] batch [260/288] time 0.196 (0.157) data 0.001 (0.001) loss 1.5991 (1.5599) teacher_loss 0.9558 (0.9905) loss_zs_kd 0.0932 (0.0772) loss_oracle 0.0673 (0.0633) acc 71.8750 (73.2091) lr 1.0628e-03 eta 0:18:12
epoch [26/50] batch [280/288] time 0.082 (0.156) data 0.000 (0.001) loss 1.9285 (1.5647) teacher_loss 1.1669 (0.9942) loss_zs_kd 0.0844 (0.0778) loss_oracle 0.0540 (0.0637) acc 65.6250 (73.1808) lr 1.0628e-03 eta 0:18:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,397
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 80.0%
******* Domain a best val acc:      86.3%, epoch: 20 *******
******* Domain a best val test acc: 82.9%, epoch: 20 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [27/50] batch [20/288] time 0.223 (0.188) data 0.000 (0.017) loss 0.9470 (1.5078) teacher_loss 0.6566 (0.9585) loss_zs_kd 0.1005 (0.0889) loss_oracle 0.0876 (0.0742) acc 78.1250 (75.4688) lr 1.0000e-03 eta 0:21:34
epoch [27/50] batch [40/288] time 0.153 (0.161) data 0.000 (0.008) loss 1.0821 (1.5468) teacher_loss 0.4789 (1.0051) loss_zs_kd 0.0469 (0.0810) loss_oracle 0.0590 (0.0730) acc 87.5000 (74.1406) lr 1.0000e-03 eta 0:18:23
epoch [27/50] batch [60/288] time 0.162 (0.161) data 0.000 (0.006) loss 2.0024 (1.5281) teacher_loss 1.3820 (0.9817) loss_zs_kd 0.1130 (0.0808) loss_oracle 0.1000 (0.0740) acc 65.6250 (74.4271) lr 1.0000e-03 eta 0:18:23
epoch [27/50] batch [80/288] time 0.149 (0.161) data 0.000 (0.004) loss 1.6565 (1.5374) teacher_loss 0.9158 (0.9750) loss_zs_kd 0.0788 (0.0774) loss_oracle 0.0762 (0.0748) acc 68.7500 (74.2578) lr 1.0000e-03 eta 0:18:22
epoch [27/50] batch [100/288] time 0.151 (0.161) data 0.000 (0.004) loss 1.4597 (1.5463) teacher_loss 0.8469 (0.9843) loss_zs_kd 0.1005 (0.0769) loss_oracle 0.0771 (0.0745) acc 78.1250 (74.0000) lr 1.0000e-03 eta 0:18:18
epoch [27/50] batch [120/288] time 0.153 (0.160) data 0.000 (0.003) loss 1.1431 (1.5221) teacher_loss 0.8068 (0.9652) loss_zs_kd 0.0654 (0.0761) loss_oracle 0.0618 (0.0731) acc 84.3750 (74.5833) lr 1.0000e-03 eta 0:18:03
epoch [27/50] batch [140/288] time 0.189 (0.159) data 0.000 (0.003) loss 1.5689 (1.5372) teacher_loss 1.0680 (0.9772) loss_zs_kd 0.1329 (0.0776) loss_oracle 0.0663 (0.0729) acc 68.7500 (74.3080) lr 1.0000e-03 eta 0:17:54
epoch [27/50] batch [160/288] time 0.167 (0.159) data 0.000 (0.002) loss 1.7851 (1.5439) teacher_loss 1.0516 (0.9750) loss_zs_kd 0.0890 (0.0779) loss_oracle 0.0697 (0.0733) acc 68.7500 (74.3164) lr 1.0000e-03 eta 0:17:50
epoch [27/50] batch [180/288] time 0.182 (0.159) data 0.000 (0.002) loss 1.3338 (1.5476) teacher_loss 0.7835 (0.9780) loss_zs_kd 0.0785 (0.0780) loss_oracle 0.0554 (0.0738) acc 71.8750 (74.2361) lr 1.0000e-03 eta 0:17:51
epoch [27/50] batch [200/288] time 0.174 (0.159) data 0.000 (0.002) loss 1.5762 (1.5387) teacher_loss 1.0023 (0.9710) loss_zs_kd 0.0739 (0.0781) loss_oracle 0.0654 (0.0735) acc 78.1250 (74.2969) lr 1.0000e-03 eta 0:17:47
epoch [27/50] batch [220/288] time 0.168 (0.159) data 0.000 (0.002) loss 1.6556 (1.5398) teacher_loss 0.9652 (0.9689) loss_zs_kd 0.1751 (0.0781) loss_oracle 0.1120 (0.0740) acc 71.8750 (74.3182) lr 1.0000e-03 eta 0:17:43
epoch [27/50] batch [240/288] time 0.167 (0.159) data 0.000 (0.002) loss 1.1023 (1.5538) teacher_loss 0.6988 (0.9812) loss_zs_kd 0.0474 (0.0784) loss_oracle 0.0677 (0.0740) acc 81.2500 (73.9453) lr 1.0000e-03 eta 0:17:40
epoch [27/50] batch [260/288] time 0.147 (0.159) data 0.000 (0.002) loss 1.2563 (1.5559) teacher_loss 0.7737 (0.9833) loss_zs_kd 0.0590 (0.0783) loss_oracle 0.0863 (0.0741) acc 75.0000 (73.8942) lr 1.0000e-03 eta 0:17:36
epoch [27/50] batch [280/288] time 0.165 (0.159) data 0.000 (0.001) loss 1.9829 (1.5604) teacher_loss 1.2893 (0.9878) loss_zs_kd 0.1097 (0.0785) loss_oracle 0.0845 (0.0739) acc 59.3750 (73.7723) lr 1.0000e-03 eta 0:17:34
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,400
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,011
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 79.4%
******* Domain a best val acc:      86.3%, epoch: 20 *******
******* Domain a best val test acc: 82.9%, epoch: 20 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [28/50] batch [20/288] time 0.083 (0.131) data 0.000 (0.014) loss 1.2077 (1.5615) teacher_loss 0.7550 (0.9600) loss_zs_kd 0.0675 (0.0873) loss_oracle 0.0796 (0.0736) acc 84.3750 (75.0000) lr 9.3721e-04 eta 0:14:27
epoch [28/50] batch [40/288] time 0.233 (0.140) data 0.000 (0.007) loss 1.1787 (1.5743) teacher_loss 0.6694 (0.9968) loss_zs_kd 0.0570 (0.0770) loss_oracle 0.0590 (0.0701) acc 84.3750 (74.1406) lr 9.3721e-04 eta 0:15:20
epoch [28/50] batch [60/288] time 0.087 (0.149) data 0.000 (0.005) loss 1.3998 (1.5812) teacher_loss 0.7908 (1.0072) loss_zs_kd 0.1053 (0.0768) loss_oracle 0.0778 (0.0689) acc 75.0000 (73.4896) lr 9.3721e-04 eta 0:16:17
epoch [28/50] batch [80/288] time 0.282 (0.152) data 0.000 (0.004) loss 1.1736 (1.5929) teacher_loss 0.7664 (1.0148) loss_zs_kd 0.0667 (0.0777) loss_oracle 0.0577 (0.0709) acc 78.1250 (73.3594) lr 9.3721e-04 eta 0:16:32
epoch [28/50] batch [100/288] time 0.104 (0.145) data 0.000 (0.003) loss 1.4647 (1.5846) teacher_loss 0.8313 (1.0011) loss_zs_kd 0.0498 (0.0777) loss_oracle 0.0581 (0.0702) acc 81.2500 (73.8750) lr 9.3721e-04 eta 0:15:45
epoch [28/50] batch [120/288] time 0.102 (0.146) data 0.000 (0.002) loss 1.4676 (1.5684) teacher_loss 0.9121 (0.9874) loss_zs_kd 0.0606 (0.0762) loss_oracle 0.0692 (0.0686) acc 81.2500 (74.0104) lr 9.3721e-04 eta 0:15:51
epoch [28/50] batch [140/288] time 0.163 (0.147) data 0.000 (0.002) loss 0.9679 (1.5599) teacher_loss 0.4756 (0.9804) loss_zs_kd 0.0581 (0.0767) loss_oracle 0.0712 (0.0687) acc 90.6250 (74.1295) lr 9.3721e-04 eta 0:15:54
epoch [28/50] batch [160/288] time 0.166 (0.149) data 0.000 (0.002) loss 1.8352 (1.5455) teacher_loss 1.3348 (0.9719) loss_zs_kd 0.0889 (0.0768) loss_oracle 0.1115 (0.0697) acc 62.5000 (74.4727) lr 9.3721e-04 eta 0:16:02
epoch [28/50] batch [180/288] time 0.173 (0.150) data 0.000 (0.002) loss 1.3794 (1.5444) teacher_loss 0.8105 (0.9674) loss_zs_kd 0.1182 (0.0778) loss_oracle 0.1047 (0.0734) acc 75.0000 (74.6354) lr 9.3721e-04 eta 0:16:08
epoch [28/50] batch [200/288] time 0.155 (0.151) data 0.000 (0.002) loss 2.0337 (1.5574) teacher_loss 1.4111 (0.9759) loss_zs_kd 0.1089 (0.0786) loss_oracle 0.0947 (0.0765) acc 65.6250 (74.4531) lr 9.3721e-04 eta 0:16:09
epoch [28/50] batch [220/288] time 0.154 (0.152) data 0.000 (0.001) loss 1.5131 (1.5553) teacher_loss 0.9647 (0.9746) loss_zs_kd 0.0608 (0.0788) loss_oracle 0.0883 (0.0775) acc 78.1250 (74.5455) lr 9.3721e-04 eta 0:16:13
epoch [28/50] batch [240/288] time 0.176 (0.153) data 0.000 (0.001) loss 0.9428 (1.5579) teacher_loss 0.4208 (0.9754) loss_zs_kd 0.0657 (0.0790) loss_oracle 0.1097 (0.0787) acc 93.7500 (74.5443) lr 9.3721e-04 eta 0:16:14
epoch [28/50] batch [260/288] time 0.152 (0.153) data 0.000 (0.001) loss 1.3214 (1.5635) teacher_loss 0.6880 (0.9801) loss_zs_kd 0.0672 (0.0790) loss_oracle 0.1062 (0.0804) acc 84.3750 (74.4591) lr 9.3721e-04 eta 0:16:13
epoch [28/50] batch [280/288] time 0.150 (0.153) data 0.000 (0.001) loss 1.4023 (1.5644) teacher_loss 0.8536 (0.9785) loss_zs_kd 0.0781 (0.0794) loss_oracle 0.0530 (0.0812) acc 68.7500 (74.4196) lr 9.3721e-04 eta 0:16:09
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,393
* accuracy: 86.1%
* error: 13.9%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,021
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 79.9%
******* Domain a best val acc:      86.3%, epoch: 20 *******
******* Domain a best val test acc: 82.9%, epoch: 20 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [29/50] batch [20/288] time 0.153 (0.172) data 0.000 (0.014) loss 1.3812 (1.4407) teacher_loss 0.6773 (0.8625) loss_zs_kd 0.1016 (0.0769) loss_oracle 0.0677 (0.0646) acc 81.2500 (77.5000) lr 8.7467e-04 eta 0:18:09
epoch [29/50] batch [40/288] time 0.098 (0.155) data 0.000 (0.007) loss 1.3139 (1.5374) teacher_loss 0.8351 (0.9781) loss_zs_kd 0.0674 (0.0739) loss_oracle 0.0854 (0.0702) acc 81.2500 (74.5312) lr 8.7467e-04 eta 0:16:18
epoch [29/50] batch [60/288] time 0.167 (0.152) data 0.000 (0.005) loss 2.2316 (1.5673) teacher_loss 1.7170 (1.0131) loss_zs_kd 0.0755 (0.0712) loss_oracle 0.0724 (0.0732) acc 62.5000 (73.6979) lr 8.7467e-04 eta 0:15:52
epoch [29/50] batch [80/288] time 0.083 (0.152) data 0.000 (0.004) loss 1.3214 (1.5757) teacher_loss 0.7657 (1.0118) loss_zs_kd 0.1002 (0.0753) loss_oracle 0.0751 (0.0747) acc 87.5000 (73.2422) lr 8.7467e-04 eta 0:15:49
epoch [29/50] batch [100/288] time 0.099 (0.145) data 0.000 (0.003) loss 1.9733 (1.5967) teacher_loss 1.4141 (1.0288) loss_zs_kd 0.0838 (0.0774) loss_oracle 0.0698 (0.0762) acc 65.6250 (73.0625) lr 8.7467e-04 eta 0:15:03
epoch [29/50] batch [120/288] time 0.394 (0.149) data 0.000 (0.003) loss 1.5098 (1.5863) teacher_loss 0.9342 (1.0128) loss_zs_kd 0.0483 (0.0767) loss_oracle 0.0418 (0.0764) acc 78.1250 (73.6719) lr 8.7467e-04 eta 0:15:23
epoch [29/50] batch [140/288] time 0.160 (0.151) data 0.000 (0.002) loss 1.7842 (1.5760) teacher_loss 1.0469 (1.0029) loss_zs_kd 0.0454 (0.0761) loss_oracle 0.0491 (0.0760) acc 65.6250 (73.7946) lr 8.7467e-04 eta 0:15:36
epoch [29/50] batch [160/288] time 0.098 (0.151) data 0.000 (0.002) loss 1.4193 (1.5735) teacher_loss 0.8147 (0.9999) loss_zs_kd 0.0976 (0.0770) loss_oracle 0.0755 (0.0764) acc 71.8750 (73.8281) lr 8.7467e-04 eta 0:15:29
epoch [29/50] batch [180/288] time 0.093 (0.152) data 0.000 (0.002) loss 1.7394 (1.5769) teacher_loss 1.1961 (1.0067) loss_zs_kd 0.0781 (0.0778) loss_oracle 0.0752 (0.0765) acc 68.7500 (73.7153) lr 8.7467e-04 eta 0:15:37
epoch [29/50] batch [200/288] time 0.152 (0.151) data 0.000 (0.002) loss 1.7945 (1.5752) teacher_loss 1.1623 (1.0030) loss_zs_kd 0.0713 (0.0781) loss_oracle 0.0759 (0.0771) acc 68.7500 (73.7344) lr 8.7467e-04 eta 0:15:29
epoch [29/50] batch [220/288] time 0.149 (0.151) data 0.000 (0.001) loss 1.9072 (1.5835) teacher_loss 1.0718 (1.0056) loss_zs_kd 0.1012 (0.0778) loss_oracle 0.0570 (0.0771) acc 71.8750 (73.6932) lr 8.7467e-04 eta 0:15:25
epoch [29/50] batch [240/288] time 0.153 (0.151) data 0.000 (0.001) loss 1.4954 (1.5864) teacher_loss 0.9454 (1.0073) loss_zs_kd 0.0705 (0.0775) loss_oracle 0.0583 (0.0765) acc 78.1250 (73.6198) lr 8.7467e-04 eta 0:15:22
epoch [29/50] batch [260/288] time 0.185 (0.152) data 0.000 (0.001) loss 1.4835 (1.5854) teacher_loss 0.8998 (1.0076) loss_zs_kd 0.0965 (0.0777) loss_oracle 0.0638 (0.0753) acc 75.0000 (73.6298) lr 8.7467e-04 eta 0:15:26
epoch [29/50] batch [280/288] time 0.155 (0.153) data 0.000 (0.001) loss 2.1901 (1.5866) teacher_loss 1.5950 (1.0088) loss_zs_kd 0.0656 (0.0779) loss_oracle 0.0472 (0.0744) acc 56.2500 (73.5826) lr 8.7467e-04 eta 0:15:29
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,400
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 79.9%
******* Domain a best val acc:      86.3%, epoch: 20 *******
******* Domain a best val test acc: 82.9%, epoch: 20 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [30/50] batch [20/288] time 0.174 (0.181) data 0.000 (0.015) loss 1.4071 (1.5374) teacher_loss 0.8054 (0.9723) loss_zs_kd 0.0684 (0.0749) loss_oracle 0.0916 (0.0687) acc 75.0000 (72.8125) lr 8.1262e-04 eta 0:18:13
epoch [30/50] batch [40/288] time 0.173 (0.176) data 0.000 (0.008) loss 1.4967 (1.4722) teacher_loss 0.9079 (0.9065) loss_zs_kd 0.0697 (0.0732) loss_oracle 0.1022 (0.0702) acc 65.6250 (74.7656) lr 8.1262e-04 eta 0:17:38
epoch [30/50] batch [60/288] time 0.152 (0.172) data 0.001 (0.005) loss 1.7021 (1.5246) teacher_loss 1.1531 (0.9388) loss_zs_kd 0.1146 (0.0758) loss_oracle 0.1296 (0.0774) acc 68.7500 (74.8958) lr 8.1262e-04 eta 0:17:07
epoch [30/50] batch [80/288] time 0.149 (0.168) data 0.001 (0.004) loss 1.5776 (1.5454) teacher_loss 0.8661 (0.9591) loss_zs_kd 0.0708 (0.0753) loss_oracle 0.0841 (0.0781) acc 78.1250 (74.4531) lr 8.1262e-04 eta 0:16:43
epoch [30/50] batch [100/288] time 0.099 (0.161) data 0.000 (0.003) loss 1.2637 (1.5642) teacher_loss 0.6182 (0.9773) loss_zs_kd 0.0449 (0.0748) loss_oracle 0.0560 (0.0778) acc 87.5000 (74.0000) lr 8.1262e-04 eta 0:15:59
epoch [30/50] batch [120/288] time 0.352 (0.159) data 0.000 (0.003) loss 1.6002 (1.5674) teacher_loss 1.1161 (0.9873) loss_zs_kd 0.0547 (0.0749) loss_oracle 0.0724 (0.0763) acc 71.8750 (73.7240) lr 8.1262e-04 eta 0:15:45
epoch [30/50] batch [140/288] time 0.234 (0.161) data 0.000 (0.002) loss 1.7878 (1.5805) teacher_loss 1.3668 (1.0027) loss_zs_kd 0.0527 (0.0758) loss_oracle 0.0756 (0.0758) acc 56.2500 (73.4598) lr 8.1262e-04 eta 0:15:52
epoch [30/50] batch [160/288] time 0.100 (0.154) data 0.000 (0.002) loss 1.3948 (1.5736) teacher_loss 0.8110 (0.9988) loss_zs_kd 0.0682 (0.0762) loss_oracle 0.0482 (0.0747) acc 75.0000 (73.6719) lr 8.1262e-04 eta 0:15:07
epoch [30/50] batch [180/288] time 0.091 (0.156) data 0.000 (0.002) loss 1.5843 (1.5825) teacher_loss 1.1518 (1.0087) loss_zs_kd 0.0756 (0.0763) loss_oracle 0.0458 (0.0740) acc 71.8750 (73.3681) lr 8.1262e-04 eta 0:15:12
epoch [30/50] batch [200/288] time 0.361 (0.158) data 0.001 (0.002) loss 1.6824 (1.5793) teacher_loss 1.2600 (1.0058) loss_zs_kd 0.0851 (0.0763) loss_oracle 0.0850 (0.0736) acc 62.5000 (73.3594) lr 8.1262e-04 eta 0:15:26
epoch [30/50] batch [220/288] time 0.094 (0.157) data 0.000 (0.002) loss 1.8090 (1.5786) teacher_loss 1.0781 (1.0055) loss_zs_kd 0.0771 (0.0767) loss_oracle 0.0853 (0.0737) acc 71.8750 (73.1676) lr 8.1262e-04 eta 0:15:16
epoch [30/50] batch [240/288] time 0.123 (0.159) data 0.000 (0.001) loss 1.1643 (1.5769) teacher_loss 0.6431 (1.0056) loss_zs_kd 0.0743 (0.0770) loss_oracle 0.1105 (0.0749) acc 84.3750 (73.3073) lr 8.1262e-04 eta 0:15:20
epoch [30/50] batch [260/288] time 0.088 (0.154) data 0.000 (0.001) loss 1.4336 (1.5836) teacher_loss 0.7252 (1.0125) loss_zs_kd 0.0808 (0.0772) loss_oracle 0.0849 (0.0752) acc 75.0000 (73.1731) lr 8.1262e-04 eta 0:14:50
epoch [30/50] batch [280/288] time 0.084 (0.150) data 0.000 (0.001) loss 1.3514 (1.5760) teacher_loss 0.7238 (1.0052) loss_zs_kd 0.0636 (0.0768) loss_oracle 0.0770 (0.0746) acc 81.2500 (73.3929) lr 8.1262e-04 eta 0:14:22
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,402
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,020
* accuracy: 83.2%
* error: 16.8%
* macro_f1: 79.8%
******* Domain a best val acc:      86.4%, epoch: 30 *******
******* Domain a best val test acc: 83.2%, epoch: 30 *******
******* Domain a best test acc:     83.5%, epoch: 10 *******
epoch [31/50] batch [20/288] time 0.110 (0.119) data 0.000 (0.014) loss 0.8172 (1.5336) teacher_loss 0.3966 (0.9491) loss_zs_kd 0.0638 (0.0873) loss_oracle 0.0635 (0.0838) acc 90.6250 (75.0000) lr 7.5131e-04 eta 0:11:20
epoch [31/50] batch [40/288] time 0.099 (0.115) data 0.000 (0.007) loss 1.2286 (1.5756) teacher_loss 0.6862 (0.9961) loss_zs_kd 0.0859 (0.0810) loss_oracle 0.0618 (0.0767) acc 87.5000 (73.7500) lr 7.5131e-04 eta 0:10:55
epoch [31/50] batch [60/288] time 0.106 (0.110) data 0.000 (0.005) loss 1.5625 (1.5819) teacher_loss 1.0257 (1.0104) loss_zs_kd 0.0697 (0.0782) loss_oracle 0.0602 (0.0766) acc 71.8750 (73.3854) lr 7.5131e-04 eta 0:10:25
epoch [31/50] batch [80/288] time 0.096 (0.107) data 0.000 (0.004) loss 1.4341 (1.5608) teacher_loss 0.8939 (0.9882) loss_zs_kd 0.0682 (0.0784) loss_oracle 0.0872 (0.0767) acc 81.2500 (73.7109) lr 7.5131e-04 eta 0:10:10
epoch [31/50] batch [100/288] time 0.107 (0.107) data 0.000 (0.003) loss 1.7066 (1.5479) teacher_loss 1.0317 (0.9788) loss_zs_kd 0.1259 (0.0778) loss_oracle 0.0941 (0.0783) acc 78.1250 (74.1875) lr 7.5131e-04 eta 0:10:04
epoch [31/50] batch [120/288] time 0.099 (0.106) data 0.000 (0.003) loss 1.2492 (1.5622) teacher_loss 0.5628 (0.9801) loss_zs_kd 0.0685 (0.0799) loss_oracle 0.1244 (0.0812) acc 84.3750 (74.1667) lr 7.5131e-04 eta 0:09:57
epoch [31/50] batch [140/288] time 0.118 (0.106) data 0.000 (0.002) loss 1.3077 (1.5604) teacher_loss 0.7982 (0.9789) loss_zs_kd 0.1062 (0.0802) loss_oracle 0.1127 (0.0820) acc 75.0000 (74.1295) lr 7.5131e-04 eta 0:09:53
epoch [31/50] batch [160/288] time 0.114 (0.106) data 0.000 (0.002) loss 1.5759 (1.5672) teacher_loss 0.8424 (0.9844) loss_zs_kd 0.0887 (0.0797) loss_oracle 0.0733 (0.0828) acc 84.3750 (74.0430) lr 7.5131e-04 eta 0:09:56
epoch [31/50] batch [180/288] time 0.112 (0.106) data 0.000 (0.002) loss 1.7220 (1.5661) teacher_loss 1.2293 (0.9860) loss_zs_kd 0.0756 (0.0787) loss_oracle 0.0717 (0.0833) acc 71.8750 (73.8194) lr 7.5131e-04 eta 0:09:52
epoch [31/50] batch [200/288] time 0.097 (0.106) data 0.000 (0.002) loss 1.4556 (1.5664) teacher_loss 0.9378 (0.9877) loss_zs_kd 0.0665 (0.0793) loss_oracle 0.0977 (0.0835) acc 81.2500 (73.7969) lr 7.5131e-04 eta 0:09:48
epoch [31/50] batch [220/288] time 0.109 (0.106) data 0.000 (0.002) loss 1.7194 (1.5685) teacher_loss 0.9989 (0.9909) loss_zs_kd 0.0834 (0.0793) loss_oracle 0.0912 (0.0829) acc 68.7500 (73.7500) lr 7.5131e-04 eta 0:09:45
epoch [31/50] batch [240/288] time 0.094 (0.105) data 0.000 (0.001) loss 1.7509 (1.5669) teacher_loss 1.0132 (0.9891) loss_zs_kd 0.0831 (0.0793) loss_oracle 0.0705 (0.0818) acc 78.1250 (73.7370) lr 7.5131e-04 eta 0:09:40
epoch [31/50] batch [260/288] time 0.107 (0.105) data 0.000 (0.001) loss 1.2368 (1.5673) teacher_loss 0.6044 (0.9892) loss_zs_kd 0.0423 (0.0788) loss_oracle 0.1004 (0.0808) acc 87.5000 (73.7260) lr 7.5131e-04 eta 0:09:36
epoch [31/50] batch [280/288] time 0.103 (0.105) data 0.000 (0.001) loss 1.4522 (1.5556) teacher_loss 0.8498 (0.9797) loss_zs_kd 0.0539 (0.0783) loss_oracle 0.0645 (0.0799) acc 84.3750 (74.1406) lr 7.5131e-04 eta 0:09:34
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,398
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,029
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.3%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.4%, epoch: 30 *******
******* Domain a best val test acc: 83.2%, epoch: 30 *******
******* Domain a best test acc:     83.6%, epoch: 31 *******
epoch [32/50] batch [20/288] time 0.109 (0.122) data 0.000 (0.017) loss 1.4188 (1.6146) teacher_loss 0.9164 (0.9999) loss_zs_kd 0.0463 (0.0751) loss_oracle 0.0451 (0.0729) acc 78.1250 (74.2188) lr 6.9098e-04 eta 0:11:06
epoch [32/50] batch [40/288] time 0.098 (0.112) data 0.000 (0.009) loss 0.8841 (1.5907) teacher_loss 0.4931 (1.0030) loss_zs_kd 0.0443 (0.0783) loss_oracle 0.0591 (0.0747) acc 87.5000 (73.8281) lr 6.9098e-04 eta 0:10:05
epoch [32/50] batch [60/288] time 0.110 (0.108) data 0.000 (0.006) loss 1.4062 (1.6197) teacher_loss 0.8729 (1.0184) loss_zs_kd 0.0916 (0.0806) loss_oracle 0.1117 (0.0760) acc 75.0000 (73.2292) lr 6.9098e-04 eta 0:09:43
epoch [32/50] batch [80/288] time 0.096 (0.107) data 0.000 (0.005) loss 2.0235 (1.5848) teacher_loss 1.4443 (0.9943) loss_zs_kd 0.0689 (0.0801) loss_oracle 0.0776 (0.0738) acc 59.3750 (73.9062) lr 6.9098e-04 eta 0:09:35
epoch [32/50] batch [100/288] time 0.103 (0.105) data 0.000 (0.004) loss 1.0829 (1.5818) teacher_loss 0.5274 (0.9956) loss_zs_kd 0.0807 (0.0789) loss_oracle 0.0864 (0.0727) acc 84.3750 (74.0000) lr 6.9098e-04 eta 0:09:25
epoch [32/50] batch [120/288] time 0.101 (0.105) data 0.001 (0.003) loss 1.0810 (1.5733) teacher_loss 0.6180 (0.9919) loss_zs_kd 0.0580 (0.0791) loss_oracle 0.0676 (0.0731) acc 78.1250 (73.8021) lr 6.9098e-04 eta 0:09:19
epoch [32/50] batch [140/288] time 0.099 (0.104) data 0.000 (0.003) loss 1.5644 (1.5717) teacher_loss 0.9805 (0.9902) loss_zs_kd 0.1196 (0.0790) loss_oracle 0.0902 (0.0731) acc 78.1250 (73.7723) lr 6.9098e-04 eta 0:09:13
epoch [32/50] batch [160/288] time 0.098 (0.104) data 0.001 (0.002) loss 1.2737 (1.5697) teacher_loss 0.7426 (0.9855) loss_zs_kd 0.0799 (0.0787) loss_oracle 0.0771 (0.0733) acc 84.3750 (73.7891) lr 6.9098e-04 eta 0:09:10
epoch [32/50] batch [180/288] time 0.108 (0.103) data 0.000 (0.002) loss 1.4655 (1.5784) teacher_loss 0.9659 (0.9938) loss_zs_kd 0.0882 (0.0788) loss_oracle 0.0706 (0.0732) acc 75.0000 (73.3681) lr 6.9098e-04 eta 0:09:06
epoch [32/50] batch [200/288] time 0.095 (0.103) data 0.000 (0.002) loss 1.6637 (1.5754) teacher_loss 1.0094 (0.9913) loss_zs_kd 0.0771 (0.0784) loss_oracle 0.0586 (0.0730) acc 75.0000 (73.5156) lr 6.9098e-04 eta 0:09:03
epoch [32/50] batch [220/288] time 0.107 (0.103) data 0.000 (0.002) loss 1.4886 (1.5729) teacher_loss 0.8307 (0.9962) loss_zs_kd 0.0487 (0.0777) loss_oracle 0.0701 (0.0725) acc 71.8750 (73.5511) lr 6.9098e-04 eta 0:09:00
epoch [32/50] batch [240/288] time 0.103 (0.103) data 0.001 (0.002) loss 1.4768 (1.5634) teacher_loss 1.0064 (0.9886) loss_zs_kd 0.0573 (0.0771) loss_oracle 0.0623 (0.0726) acc 78.1250 (73.8281) lr 6.9098e-04 eta 0:08:58
epoch [32/50] batch [260/288] time 0.097 (0.103) data 0.000 (0.002) loss 1.5164 (1.5694) teacher_loss 0.9903 (0.9954) loss_zs_kd 0.0400 (0.0770) loss_oracle 0.0784 (0.0733) acc 78.1250 (73.7139) lr 6.9098e-04 eta 0:08:54
epoch [32/50] batch [280/288] time 0.085 (0.102) data 0.000 (0.002) loss 1.2068 (1.5665) teacher_loss 0.8002 (0.9933) loss_zs_kd 0.0608 (0.0769) loss_oracle 0.0757 (0.0741) acc 71.8750 (73.7723) lr 6.9098e-04 eta 0:08:47
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,404
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,030
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.3%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain a best val acc:      86.4%, epoch: 32 *******
******* Domain a best val test acc: 83.6%, epoch: 32 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [33/50] batch [20/288] time 0.101 (0.113) data 0.000 (0.017) loss 1.3916 (1.6356) teacher_loss 1.0585 (1.0467) loss_zs_kd 0.0705 (0.0830) loss_oracle 0.0422 (0.0827) acc 78.1250 (72.3438) lr 6.3188e-04 eta 0:09:44
epoch [33/50] batch [40/288] time 0.090 (0.103) data 0.000 (0.008) loss 1.5774 (1.6574) teacher_loss 1.0556 (1.0558) loss_zs_kd 0.0683 (0.0814) loss_oracle 0.0701 (0.0798) acc 71.8750 (71.7188) lr 6.3188e-04 eta 0:08:50
epoch [33/50] batch [60/288] time 0.096 (0.100) data 0.000 (0.006) loss 1.5965 (1.6476) teacher_loss 0.9731 (1.0398) loss_zs_kd 0.0865 (0.0810) loss_oracle 0.0718 (0.0819) acc 68.7500 (72.1354) lr 6.3188e-04 eta 0:08:33
epoch [33/50] batch [80/288] time 0.089 (0.099) data 0.000 (0.004) loss 1.4529 (1.6367) teacher_loss 0.7665 (1.0361) loss_zs_kd 0.0614 (0.0792) loss_oracle 0.0940 (0.0806) acc 81.2500 (72.5391) lr 6.3188e-04 eta 0:08:23
epoch [33/50] batch [100/288] time 0.090 (0.098) data 0.000 (0.003) loss 1.5152 (1.6050) teacher_loss 0.9117 (1.0138) loss_zs_kd 0.0619 (0.0800) loss_oracle 0.0685 (0.0804) acc 81.2500 (73.1250) lr 6.3188e-04 eta 0:08:17
epoch [33/50] batch [120/288] time 0.105 (0.098) data 0.000 (0.003) loss 2.2085 (1.5999) teacher_loss 1.7201 (1.0124) loss_zs_kd 0.0697 (0.0798) loss_oracle 0.1075 (0.0817) acc 56.2500 (73.2552) lr 6.3188e-04 eta 0:08:14
epoch [33/50] batch [140/288] time 0.095 (0.098) data 0.000 (0.003) loss 1.5248 (1.5938) teacher_loss 1.0967 (1.0062) loss_zs_kd 0.1001 (0.0800) loss_oracle 0.0932 (0.0828) acc 75.0000 (73.3259) lr 6.3188e-04 eta 0:08:11
epoch [33/50] batch [160/288] time 0.100 (0.097) data 0.000 (0.002) loss 1.7131 (1.5928) teacher_loss 1.0040 (1.0011) loss_zs_kd 0.0989 (0.0795) loss_oracle 0.1042 (0.0840) acc 71.8750 (73.5742) lr 6.3188e-04 eta 0:08:09
epoch [33/50] batch [180/288] time 0.090 (0.097) data 0.000 (0.002) loss 1.6993 (1.5946) teacher_loss 1.1397 (1.0000) loss_zs_kd 0.0789 (0.0789) loss_oracle 0.0795 (0.0849) acc 68.7500 (73.6285) lr 6.3188e-04 eta 0:08:05
epoch [33/50] batch [200/288] time 0.105 (0.097) data 0.000 (0.002) loss 1.5452 (1.5891) teacher_loss 1.0460 (0.9961) loss_zs_kd 0.0611 (0.0784) loss_oracle 0.0633 (0.0845) acc 71.8750 (73.7031) lr 6.3188e-04 eta 0:08:04
epoch [33/50] batch [220/288] time 0.097 (0.097) data 0.000 (0.002) loss 1.5615 (1.5848) teacher_loss 1.0661 (0.9928) loss_zs_kd 0.0347 (0.0782) loss_oracle 0.0859 (0.0838) acc 75.0000 (73.8636) lr 6.3188e-04 eta 0:08:00
epoch [33/50] batch [240/288] time 0.091 (0.096) data 0.000 (0.002) loss 1.2301 (1.5760) teacher_loss 0.6103 (0.9840) loss_zs_kd 0.0875 (0.0783) loss_oracle 0.1053 (0.0842) acc 84.3750 (74.0625) lr 6.3188e-04 eta 0:07:56
epoch [33/50] batch [260/288] time 0.095 (0.096) data 0.001 (0.001) loss 1.4656 (1.5783) teacher_loss 0.9785 (0.9909) loss_zs_kd 0.0477 (0.0781) loss_oracle 0.0499 (0.0839) acc 78.1250 (73.9904) lr 6.3188e-04 eta 0:07:53
epoch [33/50] batch [280/288] time 0.086 (0.096) data 0.000 (0.001) loss 1.3675 (1.5780) teacher_loss 0.9123 (0.9920) loss_zs_kd 0.0647 (0.0780) loss_oracle 0.0667 (0.0825) acc 78.1250 (73.9286) lr 6.3188e-04 eta 0:07:50
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,396
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.1%
******* Domain a best val acc:      86.4%, epoch: 32 *******
******* Domain a best val test acc: 83.6%, epoch: 32 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [34/50] batch [20/288] time 0.089 (0.113) data 0.000 (0.016) loss 1.7205 (1.4529) teacher_loss 1.2708 (0.8920) loss_zs_kd 0.0819 (0.0766) loss_oracle 0.0379 (0.0681) acc 68.7500 (76.0938) lr 5.7422e-04 eta 0:09:10
epoch [34/50] batch [40/288] time 0.096 (0.104) data 0.000 (0.008) loss 1.5068 (1.5110) teacher_loss 0.8556 (0.9564) loss_zs_kd 0.0488 (0.0745) loss_oracle 0.0562 (0.0695) acc 81.2500 (73.8281) lr 5.7422e-04 eta 0:08:24
epoch [34/50] batch [60/288] time 0.095 (0.101) data 0.000 (0.005) loss 1.4771 (1.5205) teacher_loss 0.8847 (0.9675) loss_zs_kd 0.0430 (0.0741) loss_oracle 0.0674 (0.0691) acc 75.0000 (73.3333) lr 5.7422e-04 eta 0:08:09
epoch [34/50] batch [80/288] time 0.101 (0.100) data 0.000 (0.004) loss 1.4693 (1.5173) teacher_loss 0.9275 (0.9702) loss_zs_kd 0.0895 (0.0745) loss_oracle 0.0908 (0.0690) acc 78.1250 (73.7891) lr 5.7422e-04 eta 0:08:01
epoch [34/50] batch [100/288] time 0.088 (0.098) data 0.000 (0.003) loss 1.7247 (1.5153) teacher_loss 1.1541 (0.9591) loss_zs_kd 0.0936 (0.0745) loss_oracle 0.0443 (0.0693) acc 65.6250 (74.1562) lr 5.7422e-04 eta 0:07:48
epoch [34/50] batch [120/288] time 0.089 (0.097) data 0.000 (0.003) loss 1.6784 (1.5305) teacher_loss 0.9633 (0.9699) loss_zs_kd 0.0645 (0.0742) loss_oracle 0.0740 (0.0703) acc 75.0000 (74.0365) lr 5.7422e-04 eta 0:07:44
epoch [34/50] batch [140/288] time 0.095 (0.097) data 0.000 (0.002) loss 1.8827 (1.5278) teacher_loss 1.1233 (0.9646) loss_zs_kd 0.0749 (0.0750) loss_oracle 0.0837 (0.0709) acc 65.6250 (74.2411) lr 5.7422e-04 eta 0:07:41
epoch [34/50] batch [160/288] time 0.093 (0.097) data 0.000 (0.002) loss 0.8731 (1.5287) teacher_loss 0.6163 (0.9643) loss_zs_kd 0.0922 (0.0762) loss_oracle 0.0614 (0.0714) acc 84.3750 (74.0234) lr 5.7422e-04 eta 0:07:37
epoch [34/50] batch [180/288] time 0.098 (0.097) data 0.000 (0.002) loss 1.6921 (1.5348) teacher_loss 1.0396 (0.9698) loss_zs_kd 0.0726 (0.0771) loss_oracle 0.0811 (0.0728) acc 68.7500 (73.8194) lr 5.7422e-04 eta 0:07:35
epoch [34/50] batch [200/288] time 0.099 (0.097) data 0.000 (0.002) loss 2.1867 (1.5506) teacher_loss 1.4933 (0.9815) loss_zs_kd 0.0909 (0.0772) loss_oracle 0.0753 (0.0733) acc 62.5000 (73.6250) lr 5.7422e-04 eta 0:07:35
epoch [34/50] batch [220/288] time 0.089 (0.097) data 0.000 (0.002) loss 1.6997 (1.5537) teacher_loss 1.1091 (0.9829) loss_zs_kd 0.0650 (0.0776) loss_oracle 0.0851 (0.0739) acc 65.6250 (73.6932) lr 5.7422e-04 eta 0:07:34
epoch [34/50] batch [240/288] time 0.099 (0.097) data 0.000 (0.002) loss 1.8270 (1.5615) teacher_loss 1.1117 (0.9890) loss_zs_kd 0.0901 (0.0780) loss_oracle 0.0922 (0.0754) acc 68.7500 (73.6328) lr 5.7422e-04 eta 0:07:31
epoch [34/50] batch [260/288] time 0.094 (0.097) data 0.000 (0.001) loss 1.3133 (1.5593) teacher_loss 0.8315 (0.9864) loss_zs_kd 0.0881 (0.0782) loss_oracle 0.0821 (0.0758) acc 78.1250 (73.8341) lr 5.7422e-04 eta 0:07:29
epoch [34/50] batch [280/288] time 0.084 (0.097) data 0.000 (0.001) loss 1.9781 (1.5562) teacher_loss 1.1827 (0.9831) loss_zs_kd 0.1145 (0.0777) loss_oracle 0.1185 (0.0764) acc 65.6250 (73.8951) lr 5.7422e-04 eta 0:07:26
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,396
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,028
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.3%
******* Domain a best val acc:      86.4%, epoch: 32 *******
******* Domain a best val test acc: 83.6%, epoch: 32 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [35/50] batch [20/288] time 0.108 (0.112) data 0.000 (0.014) loss 1.8581 (1.6419) teacher_loss 1.3308 (1.0633) loss_zs_kd 0.0998 (0.0802) loss_oracle 0.1102 (0.1024) acc 71.8750 (72.0312) lr 5.1825e-04 eta 0:08:32
epoch [35/50] batch [40/288] time 0.089 (0.106) data 0.000 (0.007) loss 1.6870 (1.6845) teacher_loss 1.1256 (1.0855) loss_zs_kd 0.0631 (0.0803) loss_oracle 0.0979 (0.1071) acc 71.8750 (71.4844) lr 5.1825e-04 eta 0:08:05
epoch [35/50] batch [60/288] time 0.102 (0.103) data 0.000 (0.005) loss 2.1947 (1.6700) teacher_loss 1.4646 (1.0533) loss_zs_kd 0.0665 (0.0785) loss_oracle 0.1362 (0.1094) acc 68.7500 (73.0729) lr 5.1825e-04 eta 0:07:48
epoch [35/50] batch [80/288] time 0.092 (0.101) data 0.000 (0.004) loss 1.8398 (1.6652) teacher_loss 1.1217 (1.0502) loss_zs_kd 0.0861 (0.0773) loss_oracle 0.1554 (0.1108) acc 71.8750 (72.7344) lr 5.1825e-04 eta 0:07:39
epoch [35/50] batch [100/288] time 0.102 (0.100) data 0.000 (0.003) loss 1.6999 (1.6568) teacher_loss 1.2066 (1.0446) loss_zs_kd 0.0678 (0.0778) loss_oracle 0.0935 (0.1107) acc 68.7500 (72.7500) lr 5.1825e-04 eta 0:07:30
epoch [35/50] batch [120/288] time 0.096 (0.099) data 0.000 (0.003) loss 1.5273 (1.6443) teacher_loss 0.9200 (1.0320) loss_zs_kd 0.1136 (0.0782) loss_oracle 0.0996 (0.1087) acc 71.8750 (73.1771) lr 5.1825e-04 eta 0:07:24
epoch [35/50] batch [140/288] time 0.096 (0.098) data 0.000 (0.002) loss 1.0997 (1.6422) teacher_loss 0.5252 (1.0266) loss_zs_kd 0.0389 (0.0779) loss_oracle 0.1048 (0.1074) acc 87.5000 (73.3259) lr 5.1825e-04 eta 0:07:19
epoch [35/50] batch [160/288] time 0.095 (0.098) data 0.000 (0.002) loss 1.5044 (1.6343) teacher_loss 0.9469 (1.0216) loss_zs_kd 0.0825 (0.0783) loss_oracle 0.0738 (0.1062) acc 68.7500 (73.3984) lr 5.1825e-04 eta 0:07:16
epoch [35/50] batch [180/288] time 0.100 (0.098) data 0.000 (0.002) loss 1.1908 (1.6209) teacher_loss 0.7018 (1.0041) loss_zs_kd 0.0449 (0.0775) loss_oracle 0.0932 (0.1049) acc 81.2500 (73.8021) lr 5.1825e-04 eta 0:07:13
epoch [35/50] batch [200/288] time 0.098 (0.098) data 0.001 (0.002) loss 1.0862 (1.6164) teacher_loss 0.6288 (0.9996) loss_zs_kd 0.0728 (0.0778) loss_oracle 0.0841 (0.1032) acc 81.2500 (73.7812) lr 5.1825e-04 eta 0:07:10
epoch [35/50] batch [220/288] time 0.096 (0.097) data 0.000 (0.001) loss 1.2915 (1.6073) teacher_loss 0.6303 (0.9944) loss_zs_kd 0.0615 (0.0776) loss_oracle 0.0800 (0.1011) acc 81.2500 (73.8494) lr 5.1825e-04 eta 0:07:07
epoch [35/50] batch [240/288] time 0.094 (0.097) data 0.000 (0.001) loss 1.2515 (1.5999) teacher_loss 0.4677 (0.9878) loss_zs_kd 0.1030 (0.0778) loss_oracle 0.0837 (0.1001) acc 84.3750 (73.9453) lr 5.1825e-04 eta 0:07:04
epoch [35/50] batch [260/288] time 0.092 (0.097) data 0.000 (0.001) loss 1.3061 (1.5885) teacher_loss 0.7483 (0.9756) loss_zs_kd 0.0641 (0.0778) loss_oracle 0.0948 (0.0996) acc 87.5000 (74.2909) lr 5.1825e-04 eta 0:07:01
epoch [35/50] batch [280/288] time 0.082 (0.097) data 0.000 (0.001) loss 1.4847 (1.5900) teacher_loss 1.0209 (0.9793) loss_zs_kd 0.0648 (0.0783) loss_oracle 0.0984 (0.1003) acc 78.1250 (74.1741) lr 5.1825e-04 eta 0:06:59
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,396
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,028
* accuracy: 83.6%
* error: 16.4%
* macro_f1: 80.3%
******* Domain a best val acc:      86.4%, epoch: 32 *******
******* Domain a best val test acc: 83.6%, epoch: 32 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [36/50] batch [20/288] time 0.106 (0.111) data 0.000 (0.013) loss 2.0571 (1.5766) teacher_loss 1.3452 (0.9305) loss_zs_kd 0.0640 (0.0766) loss_oracle 0.1232 (0.1147) acc 62.5000 (75.9375) lr 4.6417e-04 eta 0:07:55
epoch [36/50] batch [40/288] time 0.106 (0.104) data 0.000 (0.007) loss 1.3769 (1.6325) teacher_loss 0.9028 (0.9873) loss_zs_kd 0.0605 (0.0813) loss_oracle 0.1091 (0.1168) acc 75.0000 (74.4531) lr 4.6417e-04 eta 0:07:27
epoch [36/50] batch [60/288] time 0.101 (0.103) data 0.000 (0.005) loss 1.3709 (1.6144) teacher_loss 0.7781 (0.9867) loss_zs_kd 0.0970 (0.0811) loss_oracle 0.0746 (0.1128) acc 84.3750 (74.2708) lr 4.6417e-04 eta 0:07:19
epoch [36/50] batch [80/288] time 0.119 (0.104) data 0.000 (0.004) loss 1.5617 (1.5832) teacher_loss 0.9228 (0.9651) loss_zs_kd 0.0691 (0.0798) loss_oracle 0.1036 (0.1107) acc 75.0000 (74.8047) lr 4.6417e-04 eta 0:07:20
epoch [36/50] batch [100/288] time 0.101 (0.106) data 0.000 (0.003) loss 1.7643 (1.5596) teacher_loss 1.1200 (0.9385) loss_zs_kd 0.0675 (0.0798) loss_oracle 0.0788 (0.1107) acc 65.6250 (75.2812) lr 4.6417e-04 eta 0:07:27
epoch [36/50] batch [120/288] time 0.101 (0.106) data 0.000 (0.002) loss 1.1960 (1.5695) teacher_loss 0.6661 (0.9525) loss_zs_kd 0.0688 (0.0784) loss_oracle 0.0849 (0.1092) acc 87.5000 (74.8958) lr 4.6417e-04 eta 0:07:25
epoch [36/50] batch [140/288] time 0.091 (0.106) data 0.000 (0.002) loss 1.4660 (1.5790) teacher_loss 1.0109 (0.9590) loss_zs_kd 0.0677 (0.0789) loss_oracle 0.1013 (0.1081) acc 71.8750 (74.6875) lr 4.6417e-04 eta 0:07:21
epoch [36/50] batch [160/288] time 0.099 (0.105) data 0.000 (0.002) loss 1.3967 (1.5697) teacher_loss 0.8587 (0.9521) loss_zs_kd 0.0536 (0.0800) loss_oracle 0.1003 (0.1075) acc 75.0000 (74.8828) lr 4.6417e-04 eta 0:07:14
epoch [36/50] batch [180/288] time 0.103 (0.104) data 0.000 (0.002) loss 2.0035 (1.5752) teacher_loss 1.2995 (0.9585) loss_zs_kd 0.0694 (0.0798) loss_oracle 0.1232 (0.1051) acc 71.8750 (74.6354) lr 4.6417e-04 eta 0:07:12
epoch [36/50] batch [200/288] time 0.117 (0.105) data 0.000 (0.002) loss 1.6565 (1.5881) teacher_loss 1.0542 (0.9673) loss_zs_kd 0.0847 (0.0797) loss_oracle 0.1335 (0.1043) acc 71.8750 (74.5625) lr 4.6417e-04 eta 0:07:11
epoch [36/50] batch [220/288] time 0.100 (0.104) data 0.000 (0.001) loss 1.7514 (1.5859) teacher_loss 1.0636 (0.9669) loss_zs_kd 0.0783 (0.0792) loss_oracle 0.1285 (0.1044) acc 71.8750 (74.5312) lr 4.6417e-04 eta 0:07:07
epoch [36/50] batch [240/288] time 0.095 (0.104) data 0.000 (0.001) loss 1.5941 (1.5836) teacher_loss 0.8112 (0.9648) loss_zs_kd 0.0769 (0.0788) loss_oracle 0.1035 (0.1040) acc 78.1250 (74.5182) lr 4.6417e-04 eta 0:07:03
epoch [36/50] batch [260/288] time 0.095 (0.104) data 0.000 (0.001) loss 1.2966 (1.5761) teacher_loss 0.6735 (0.9596) loss_zs_kd 0.0739 (0.0785) loss_oracle 0.0670 (0.1031) acc 75.0000 (74.5433) lr 4.6417e-04 eta 0:07:02
epoch [36/50] batch [280/288] time 0.099 (0.104) data 0.000 (0.001) loss 1.4836 (1.5800) teacher_loss 0.7526 (0.9644) loss_zs_kd 0.0754 (0.0792) loss_oracle 0.0717 (0.1028) acc 87.5000 (74.4643) lr 4.6417e-04 eta 0:06:58
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,399
* accuracy: 86.3%
* error: 13.7%
* macro_f1: 85.6%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.1%
******* Domain a best val acc:      86.4%, epoch: 32 *******
******* Domain a best val test acc: 83.6%, epoch: 32 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [37/50] batch [20/288] time 0.116 (0.125) data 0.000 (0.014) loss 1.5384 (1.6051) teacher_loss 0.9451 (0.9926) loss_zs_kd 0.0631 (0.0810) loss_oracle 0.0957 (0.0982) acc 78.1250 (75.3125) lr 4.1221e-04 eta 0:08:23
epoch [37/50] batch [40/288] time 0.100 (0.114) data 0.000 (0.007) loss 1.6448 (1.6394) teacher_loss 1.0039 (1.0292) loss_zs_kd 0.0917 (0.0759) loss_oracle 0.1096 (0.0966) acc 75.0000 (73.9062) lr 4.1221e-04 eta 0:07:35
epoch [37/50] batch [60/288] time 0.101 (0.110) data 0.001 (0.005) loss 1.4869 (1.6124) teacher_loss 0.8355 (1.0092) loss_zs_kd 0.0768 (0.0737) loss_oracle 0.0867 (0.0978) acc 81.2500 (74.0625) lr 4.1221e-04 eta 0:07:15
epoch [37/50] batch [80/288] time 0.096 (0.107) data 0.000 (0.004) loss 1.1068 (1.6084) teacher_loss 0.6207 (0.9933) loss_zs_kd 0.0590 (0.0759) loss_oracle 0.1057 (0.1004) acc 84.3750 (74.6875) lr 4.1221e-04 eta 0:07:02
epoch [37/50] batch [100/288] time 0.092 (0.105) data 0.000 (0.003) loss 1.4289 (1.5876) teacher_loss 0.8612 (0.9762) loss_zs_kd 0.0691 (0.0767) loss_oracle 0.0891 (0.0993) acc 81.2500 (75.1250) lr 4.1221e-04 eta 0:06:53
epoch [37/50] batch [120/288] time 0.090 (0.104) data 0.000 (0.002) loss 1.7999 (1.6029) teacher_loss 1.3435 (0.9909) loss_zs_kd 0.0421 (0.0773) loss_oracle 0.0995 (0.0973) acc 62.5000 (74.4792) lr 4.1221e-04 eta 0:06:45
epoch [37/50] batch [140/288] time 0.099 (0.103) data 0.000 (0.002) loss 1.8365 (1.6028) teacher_loss 1.1656 (0.9917) loss_zs_kd 0.0524 (0.0773) loss_oracle 0.0922 (0.0956) acc 68.7500 (74.3973) lr 4.1221e-04 eta 0:06:40
epoch [37/50] batch [160/288] time 0.096 (0.103) data 0.001 (0.002) loss 1.6239 (1.5950) teacher_loss 0.9004 (0.9851) loss_zs_kd 0.0566 (0.0774) loss_oracle 0.0810 (0.0969) acc 78.1250 (74.3555) lr 4.1221e-04 eta 0:06:36
epoch [37/50] batch [180/288] time 0.096 (0.102) data 0.000 (0.002) loss 1.5624 (1.5827) teacher_loss 0.9671 (0.9730) loss_zs_kd 0.0726 (0.0773) loss_oracle 0.0897 (0.0967) acc 75.0000 (74.6528) lr 4.1221e-04 eta 0:06:33
epoch [37/50] batch [200/288] time 0.090 (0.101) data 0.000 (0.002) loss 1.3619 (1.5866) teacher_loss 0.8322 (0.9735) loss_zs_kd 0.0767 (0.0774) loss_oracle 0.0946 (0.0973) acc 75.0000 (74.7812) lr 4.1221e-04 eta 0:06:28
epoch [37/50] batch [220/288] time 0.155 (0.102) data 0.001 (0.001) loss 1.9226 (1.5887) teacher_loss 1.3181 (0.9764) loss_zs_kd 0.0949 (0.0774) loss_oracle 0.1156 (0.0975) acc 68.7500 (74.6449) lr 4.1221e-04 eta 0:06:27
epoch [37/50] batch [240/288] time 0.100 (0.101) data 0.000 (0.001) loss 1.2372 (1.5887) teacher_loss 0.7479 (0.9763) loss_zs_kd 0.0844 (0.0772) loss_oracle 0.1134 (0.0980) acc 84.3750 (74.5833) lr 4.1221e-04 eta 0:06:23
epoch [37/50] batch [260/288] time 0.097 (0.101) data 0.001 (0.001) loss 1.7110 (1.5932) teacher_loss 1.0595 (0.9803) loss_zs_kd 0.0978 (0.0773) loss_oracle 0.1037 (0.0987) acc 75.0000 (74.3750) lr 4.1221e-04 eta 0:06:20
epoch [37/50] batch [280/288] time 0.104 (0.101) data 0.000 (0.001) loss 1.4162 (1.5923) teacher_loss 0.7473 (0.9751) loss_zs_kd 0.0884 (0.0779) loss_oracle 0.0816 (0.0992) acc 84.3750 (74.5201) lr 4.1221e-04 eta 0:06:18
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,395
* accuracy: 86.2%
* error: 13.8%
* macro_f1: 85.5%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,025
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.0%
******* Domain a best val acc:      86.4%, epoch: 32 *******
******* Domain a best val test acc: 83.6%, epoch: 32 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [38/50] batch [20/288] time 0.088 (0.103) data 0.000 (0.015) loss 1.7038 (1.6620) teacher_loss 1.1786 (1.0323) loss_zs_kd 0.0605 (0.0820) loss_oracle 0.0901 (0.0979) acc 62.5000 (72.0312) lr 3.6258e-04 eta 0:06:23
epoch [38/50] batch [40/288] time 0.095 (0.098) data 0.000 (0.007) loss 1.0778 (1.5874) teacher_loss 0.3722 (0.9578) loss_zs_kd 0.1299 (0.0805) loss_oracle 0.1109 (0.0949) acc 90.6250 (74.5312) lr 3.6258e-04 eta 0:06:02
epoch [38/50] batch [60/288] time 0.089 (0.095) data 0.000 (0.005) loss 1.2803 (1.5884) teacher_loss 0.7127 (0.9648) loss_zs_kd 0.0895 (0.0800) loss_oracle 0.0691 (0.0902) acc 78.1250 (74.7917) lr 3.6258e-04 eta 0:05:49
epoch [38/50] batch [80/288] time 0.090 (0.094) data 0.000 (0.004) loss 1.6379 (1.5952) teacher_loss 1.0128 (0.9750) loss_zs_kd 0.0583 (0.0794) loss_oracle 0.0861 (0.0889) acc 78.1250 (74.3359) lr 3.6258e-04 eta 0:05:43
epoch [38/50] batch [100/288] time 0.089 (0.093) data 0.000 (0.003) loss 1.7321 (1.6130) teacher_loss 1.0193 (0.9971) loss_zs_kd 0.0654 (0.0789) loss_oracle 0.0941 (0.0886) acc 81.2500 (74.0312) lr 3.6258e-04 eta 0:05:40
epoch [38/50] batch [120/288] time 0.092 (0.093) data 0.000 (0.003) loss 1.9204 (1.6251) teacher_loss 1.4465 (1.0067) loss_zs_kd 0.0863 (0.0783) loss_oracle 0.1001 (0.0880) acc 65.6250 (73.4635) lr 3.6258e-04 eta 0:05:36
epoch [38/50] batch [140/288] time 0.091 (0.092) data 0.000 (0.002) loss 1.4920 (1.6306) teacher_loss 0.8736 (1.0124) loss_zs_kd 0.0745 (0.0792) loss_oracle 0.0747 (0.0870) acc 81.2500 (73.1027) lr 3.6258e-04 eta 0:05:33
epoch [38/50] batch [160/288] time 0.095 (0.092) data 0.000 (0.002) loss 1.2590 (1.6356) teacher_loss 0.6488 (1.0225) loss_zs_kd 0.0625 (0.0799) loss_oracle 0.0629 (0.0868) acc 84.3750 (72.7539) lr 3.6258e-04 eta 0:05:31
epoch [38/50] batch [180/288] time 0.091 (0.092) data 0.000 (0.002) loss 1.8766 (1.6408) teacher_loss 1.0899 (1.0290) loss_zs_kd 0.1109 (0.0813) loss_oracle 0.0888 (0.0869) acc 78.1250 (72.7257) lr 3.6258e-04 eta 0:05:29
epoch [38/50] batch [200/288] time 0.095 (0.093) data 0.000 (0.002) loss 1.3357 (1.6249) teacher_loss 0.7079 (1.0163) loss_zs_kd 0.0565 (0.0806) loss_oracle 0.1020 (0.0871) acc 75.0000 (73.0625) lr 3.6258e-04 eta 0:05:29
epoch [38/50] batch [220/288] time 0.102 (0.095) data 0.000 (0.002) loss 1.4403 (1.6209) teacher_loss 0.6489 (1.0132) loss_zs_kd 0.0781 (0.0803) loss_oracle 0.0950 (0.0873) acc 78.1250 (73.1250) lr 3.6258e-04 eta 0:05:33
epoch [38/50] batch [240/288] time 0.108 (0.095) data 0.000 (0.001) loss 1.3175 (1.6204) teacher_loss 0.5792 (1.0122) loss_zs_kd 0.0642 (0.0807) loss_oracle 0.0771 (0.0871) acc 81.2500 (73.3203) lr 3.6258e-04 eta 0:05:32
epoch [38/50] batch [260/288] time 0.097 (0.095) data 0.001 (0.001) loss 1.6874 (1.6196) teacher_loss 0.9971 (1.0094) loss_zs_kd 0.0573 (0.0805) loss_oracle 0.0855 (0.0871) acc 81.2500 (73.4375) lr 3.6258e-04 eta 0:05:32
epoch [38/50] batch [280/288] time 0.098 (0.096) data 0.000 (0.001) loss 1.2314 (1.6118) teacher_loss 0.6463 (1.0027) loss_zs_kd 0.0619 (0.0798) loss_oracle 0.0511 (0.0864) acc 84.3750 (73.6049) lr 3.6258e-04 eta 0:05:31
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,406
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.1%
******* Domain a best val acc:      86.5%, epoch: 38 *******
******* Domain a best val test acc: 83.4%, epoch: 38 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [39/50] batch [20/288] time 0.094 (0.115) data 0.000 (0.016) loss 1.6208 (1.5482) teacher_loss 0.9847 (0.9539) loss_zs_kd 0.0697 (0.0749) loss_oracle 0.0994 (0.0739) acc 75.0000 (73.2812) lr 3.1545e-04 eta 0:06:34
epoch [39/50] batch [40/288] time 0.095 (0.103) data 0.000 (0.008) loss 1.8054 (1.5559) teacher_loss 1.1852 (0.9791) loss_zs_kd 0.0925 (0.0781) loss_oracle 0.0726 (0.0734) acc 62.5000 (73.7500) lr 3.1545e-04 eta 0:05:53
epoch [39/50] batch [60/288] time 0.092 (0.100) data 0.000 (0.006) loss 1.4242 (1.5521) teacher_loss 0.8664 (0.9769) loss_zs_kd 0.0460 (0.0774) loss_oracle 0.0797 (0.0753) acc 75.0000 (73.4896) lr 3.1545e-04 eta 0:05:40
epoch [39/50] batch [80/288] time 0.100 (0.100) data 0.000 (0.004) loss 1.7761 (1.5534) teacher_loss 1.3130 (0.9728) loss_zs_kd 0.0689 (0.0785) loss_oracle 0.0684 (0.0769) acc 62.5000 (73.8672) lr 3.1545e-04 eta 0:05:37
epoch [39/50] batch [100/288] time 0.095 (0.100) data 0.000 (0.003) loss 1.2617 (1.5556) teacher_loss 0.8815 (0.9747) loss_zs_kd 0.0713 (0.0785) loss_oracle 0.0800 (0.0771) acc 75.0000 (73.8438) lr 3.1545e-04 eta 0:05:34
epoch [39/50] batch [120/288] time 0.096 (0.099) data 0.000 (0.003) loss 1.1668 (1.5648) teacher_loss 0.6202 (0.9825) loss_zs_kd 0.0601 (0.0790) loss_oracle 0.0435 (0.0762) acc 84.3750 (73.8281) lr 3.1545e-04 eta 0:05:31
epoch [39/50] batch [140/288] time 0.093 (0.100) data 0.000 (0.003) loss 1.6843 (1.5609) teacher_loss 1.1354 (0.9829) loss_zs_kd 0.0958 (0.0807) loss_oracle 0.0683 (0.0762) acc 65.6250 (73.8839) lr 3.1545e-04 eta 0:05:30
epoch [39/50] batch [160/288] time 0.089 (0.099) data 0.000 (0.002) loss 1.7432 (1.5499) teacher_loss 1.0793 (0.9777) loss_zs_kd 0.0930 (0.0807) loss_oracle 0.0954 (0.0770) acc 75.0000 (74.3164) lr 3.1545e-04 eta 0:05:26
epoch [39/50] batch [180/288] time 0.098 (0.099) data 0.000 (0.002) loss 1.3549 (1.5481) teacher_loss 0.7622 (0.9751) loss_zs_kd 0.1143 (0.0803) loss_oracle 0.1089 (0.0778) acc 81.2500 (74.2708) lr 3.1545e-04 eta 0:05:23
epoch [39/50] batch [200/288] time 0.101 (0.099) data 0.000 (0.002) loss 1.5161 (1.5673) teacher_loss 0.7517 (0.9861) loss_zs_kd 0.0804 (0.0802) loss_oracle 0.0581 (0.0778) acc 78.1250 (74.0156) lr 3.1545e-04 eta 0:05:23
epoch [39/50] batch [220/288] time 0.098 (0.099) data 0.000 (0.002) loss 1.8403 (1.5626) teacher_loss 1.2950 (0.9840) loss_zs_kd 0.0810 (0.0797) loss_oracle 0.0740 (0.0770) acc 71.8750 (74.1903) lr 3.1545e-04 eta 0:05:21
epoch [39/50] batch [240/288] time 0.105 (0.100) data 0.001 (0.002) loss 1.5276 (1.5668) teacher_loss 0.7828 (0.9867) loss_zs_kd 0.0865 (0.0803) loss_oracle 0.0718 (0.0770) acc 68.7500 (73.9193) lr 3.1545e-04 eta 0:05:21
epoch [39/50] batch [260/288] time 0.096 (0.100) data 0.000 (0.001) loss 1.0845 (1.5652) teacher_loss 0.5554 (0.9868) loss_zs_kd 0.0543 (0.0797) loss_oracle 0.0743 (0.0769) acc 84.3750 (73.8582) lr 3.1545e-04 eta 0:05:18
epoch [39/50] batch [280/288] time 0.085 (0.099) data 0.000 (0.001) loss 1.5281 (1.5644) teacher_loss 0.8938 (0.9862) loss_zs_kd 0.0598 (0.0796) loss_oracle 0.0913 (0.0772) acc 78.1250 (73.8616) lr 3.1545e-04 eta 0:05:15
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,410
* accuracy: 86.6%
* error: 13.4%
* macro_f1: 85.9%
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,027
* accuracy: 83.5%
* error: 16.5%
* macro_f1: 80.2%
******* Domain a best val acc:      86.6%, epoch: 39 *******
******* Domain a best val test acc: 83.5%, epoch: 39 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [40/50] batch [20/288] time 0.097 (0.124) data 0.000 (0.017) loss 1.6313 (1.6082) teacher_loss 0.8963 (0.9719) loss_zs_kd 0.0727 (0.0773) loss_oracle 0.0786 (0.0802) acc 68.7500 (73.9062) lr 2.7103e-04 eta 0:06:31
epoch [40/50] batch [40/288] time 0.099 (0.114) data 0.000 (0.009) loss 1.9550 (1.6228) teacher_loss 1.2699 (1.0018) loss_zs_kd 0.1044 (0.0813) loss_oracle 0.0905 (0.0811) acc 68.7500 (73.2031) lr 2.7103e-04 eta 0:05:57
epoch [40/50] batch [60/288] time 0.099 (0.110) data 0.000 (0.006) loss 1.6428 (1.5827) teacher_loss 0.8171 (0.9768) loss_zs_kd 0.0855 (0.0789) loss_oracle 0.0611 (0.0798) acc 78.1250 (73.8542) lr 2.7103e-04 eta 0:05:42
epoch [40/50] batch [80/288] time 0.097 (0.108) data 0.000 (0.005) loss 1.9056 (1.5876) teacher_loss 1.2771 (0.9863) loss_zs_kd 0.0859 (0.0796) loss_oracle 0.0898 (0.0774) acc 68.7500 (73.7500) lr 2.7103e-04 eta 0:05:34
epoch [40/50] batch [100/288] time 0.100 (0.107) data 0.000 (0.004) loss 1.5963 (1.5918) teacher_loss 1.0052 (0.9924) loss_zs_kd 0.0627 (0.0786) loss_oracle 0.0883 (0.0774) acc 65.6250 (73.3750) lr 2.7103e-04 eta 0:05:29
epoch [40/50] batch [120/288] time 0.105 (0.107) data 0.000 (0.003) loss 1.5201 (1.5815) teacher_loss 0.9681 (0.9906) loss_zs_kd 0.0906 (0.0788) loss_oracle 0.0523 (0.0774) acc 71.8750 (73.3073) lr 2.7103e-04 eta 0:05:25
epoch [40/50] batch [140/288] time 0.096 (0.106) data 0.000 (0.003) loss 1.5963 (1.5859) teacher_loss 0.9724 (0.9899) loss_zs_kd 0.0628 (0.0789) loss_oracle 0.0725 (0.0780) acc 68.7500 (73.4598) lr 2.7103e-04 eta 0:05:20
epoch [40/50] batch [160/288] time 0.090 (0.107) data 0.000 (0.002) loss 1.9242 (1.5803) teacher_loss 1.3087 (0.9905) loss_zs_kd 0.0917 (0.0796) loss_oracle 0.0913 (0.0781) acc 75.0000 (73.5352) lr 2.7103e-04 eta 0:05:21
epoch [40/50] batch [180/288] time 0.111 (0.106) data 0.000 (0.002) loss 1.3840 (1.5949) teacher_loss 0.7589 (1.0017) loss_zs_kd 0.0885 (0.0797) loss_oracle 0.0798 (0.0791) acc 81.2500 (73.3854) lr 2.7103e-04 eta 0:05:16
epoch [40/50] batch [200/288] time 0.098 (0.105) data 0.001 (0.002) loss 1.6303 (1.5864) teacher_loss 1.0054 (0.9946) loss_zs_kd 0.0964 (0.0794) loss_oracle 0.0802 (0.0799) acc 68.7500 (73.5156) lr 2.7103e-04 eta 0:05:12
epoch [40/50] batch [220/288] time 0.103 (0.105) data 0.000 (0.002) loss 1.2689 (1.5856) teacher_loss 0.8041 (0.9955) loss_zs_kd 0.0345 (0.0797) loss_oracle 0.0762 (0.0811) acc 81.2500 (73.5369) lr 2.7103e-04 eta 0:05:08
epoch [40/50] batch [240/288] time 0.102 (0.104) data 0.000 (0.002) loss 1.2458 (1.5738) teacher_loss 0.6350 (0.9865) loss_zs_kd 0.0991 (0.0795) loss_oracle 0.0811 (0.0811) acc 81.2500 (73.7760) lr 2.7103e-04 eta 0:05:05
epoch [40/50] batch [260/288] time 0.106 (0.104) data 0.000 (0.002) loss 1.8342 (1.5708) teacher_loss 1.1502 (0.9815) loss_zs_kd 0.1010 (0.0799) loss_oracle 0.0943 (0.0818) acc 65.6250 (73.8221) lr 2.7103e-04 eta 0:05:02
epoch [40/50] batch [280/288] time 0.084 (0.103) data 0.000 (0.002) loss 1.8002 (1.5703) teacher_loss 1.0967 (0.9784) loss_zs_kd 0.1085 (0.0796) loss_oracle 0.0967 (0.0818) acc 78.1250 (73.8170) lr 2.7103e-04 eta 0:04:57
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,406
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.0%
******* Domain a best val acc:      86.6%, epoch: 39 *******
******* Domain a best val test acc: 83.5%, epoch: 39 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [41/50] batch [20/288] time 0.089 (0.114) data 0.000 (0.015) loss 1.3084 (1.6074) teacher_loss 0.7774 (0.9794) loss_zs_kd 0.0882 (0.0847) loss_oracle 0.0688 (0.0815) acc 78.1250 (73.7500) lr 2.2949e-04 eta 0:05:26
epoch [41/50] batch [40/288] time 0.089 (0.104) data 0.001 (0.008) loss 1.5889 (1.5911) teacher_loss 0.8666 (0.9845) loss_zs_kd 0.0751 (0.0789) loss_oracle 0.0647 (0.0805) acc 75.0000 (73.9844) lr 2.2949e-04 eta 0:04:54
epoch [41/50] batch [60/288] time 0.096 (0.101) data 0.000 (0.005) loss 1.4678 (1.5907) teacher_loss 1.1287 (0.9954) loss_zs_kd 0.0803 (0.0770) loss_oracle 0.0506 (0.0785) acc 71.8750 (73.9062) lr 2.2949e-04 eta 0:04:45
epoch [41/50] batch [80/288] time 0.095 (0.100) data 0.000 (0.004) loss 1.1733 (1.5995) teacher_loss 0.5942 (1.0015) loss_zs_kd 0.0690 (0.0759) loss_oracle 0.0833 (0.0793) acc 75.0000 (73.9453) lr 2.2949e-04 eta 0:04:40
epoch [41/50] batch [100/288] time 0.088 (0.099) data 0.000 (0.003) loss 2.2554 (1.6016) teacher_loss 1.4490 (1.0056) loss_zs_kd 0.0985 (0.0774) loss_oracle 0.0608 (0.0776) acc 56.2500 (73.5938) lr 2.2949e-04 eta 0:04:35
epoch [41/50] batch [120/288] time 0.099 (0.098) data 0.000 (0.003) loss 1.7687 (1.6007) teacher_loss 1.2447 (1.0048) loss_zs_kd 0.1036 (0.0769) loss_oracle 0.0786 (0.0786) acc 65.6250 (73.6458) lr 2.2949e-04 eta 0:04:31
epoch [41/50] batch [140/288] time 0.103 (0.099) data 0.000 (0.002) loss 1.5186 (1.5941) teacher_loss 1.1199 (0.9976) loss_zs_kd 0.0637 (0.0770) loss_oracle 0.0754 (0.0778) acc 71.8750 (73.8839) lr 2.2949e-04 eta 0:04:31
epoch [41/50] batch [160/288] time 0.094 (0.099) data 0.000 (0.002) loss 1.4533 (1.5849) teacher_loss 0.6820 (0.9860) loss_zs_kd 0.0506 (0.0768) loss_oracle 0.0746 (0.0774) acc 78.1250 (74.0039) lr 2.2949e-04 eta 0:04:29
epoch [41/50] batch [180/288] time 0.090 (0.099) data 0.000 (0.002) loss 2.3661 (1.5830) teacher_loss 1.6939 (0.9845) loss_zs_kd 0.1242 (0.0773) loss_oracle 0.0848 (0.0771) acc 59.3750 (74.1146) lr 2.2949e-04 eta 0:04:26
epoch [41/50] batch [200/288] time 0.096 (0.098) data 0.000 (0.002) loss 1.7180 (1.5862) teacher_loss 1.0341 (0.9861) loss_zs_kd 0.0740 (0.0776) loss_oracle 0.0800 (0.0772) acc 65.6250 (74.0938) lr 2.2949e-04 eta 0:04:23
epoch [41/50] batch [220/288] time 0.096 (0.098) data 0.000 (0.002) loss 1.4780 (1.5848) teacher_loss 0.9430 (0.9846) loss_zs_kd 0.1012 (0.0781) loss_oracle 0.0902 (0.0775) acc 78.1250 (74.0909) lr 2.2949e-04 eta 0:04:21
epoch [41/50] batch [240/288] time 0.101 (0.098) data 0.000 (0.001) loss 1.5910 (1.5809) teacher_loss 0.9409 (0.9828) loss_zs_kd 0.0905 (0.0774) loss_oracle 0.0990 (0.0773) acc 78.1250 (74.0625) lr 2.2949e-04 eta 0:04:18
epoch [41/50] batch [260/288] time 0.092 (0.098) data 0.000 (0.001) loss 2.0854 (1.5824) teacher_loss 1.5583 (0.9839) loss_zs_kd 0.0885 (0.0777) loss_oracle 0.0516 (0.0774) acc 68.7500 (74.0986) lr 2.2949e-04 eta 0:04:16
epoch [41/50] batch [280/288] time 0.085 (0.097) data 0.000 (0.001) loss 1.4085 (1.5811) teacher_loss 0.7131 (0.9838) loss_zs_kd 0.0772 (0.0775) loss_oracle 0.0398 (0.0773) acc 81.2500 (74.0290) lr 2.2949e-04 eta 0:04:13
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,405
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 80.0%
******* Domain a best val acc:      86.6%, epoch: 39 *******
******* Domain a best val test acc: 83.5%, epoch: 39 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [42/50] batch [20/288] time 0.096 (0.120) data 0.000 (0.015) loss 1.3760 (1.4961) teacher_loss 0.8255 (0.9497) loss_zs_kd 0.0627 (0.0723) loss_oracle 0.0725 (0.0781) acc 81.2500 (75.4688) lr 1.9098e-04 eta 0:05:09
epoch [42/50] batch [40/288] time 0.102 (0.110) data 0.000 (0.008) loss 1.5225 (1.5324) teacher_loss 0.9241 (0.9743) loss_zs_kd 0.0723 (0.0755) loss_oracle 0.0842 (0.0800) acc 75.0000 (74.9219) lr 1.9098e-04 eta 0:04:40
epoch [42/50] batch [60/288] time 0.095 (0.106) data 0.001 (0.005) loss 1.9083 (1.5813) teacher_loss 1.3412 (1.0028) loss_zs_kd 0.0812 (0.0792) loss_oracle 0.0770 (0.0846) acc 71.8750 (73.9583) lr 1.9098e-04 eta 0:04:27
epoch [42/50] batch [80/288] time 0.110 (0.105) data 0.000 (0.004) loss 1.4774 (1.5852) teacher_loss 0.8211 (0.9888) loss_zs_kd 0.0741 (0.0801) loss_oracle 0.0957 (0.0842) acc 78.1250 (73.8281) lr 1.9098e-04 eta 0:04:24
epoch [42/50] batch [100/288] time 0.097 (0.105) data 0.001 (0.003) loss 1.6463 (1.5942) teacher_loss 0.9229 (1.0006) loss_zs_kd 0.0855 (0.0809) loss_oracle 0.0975 (0.0844) acc 71.8750 (73.5000) lr 1.9098e-04 eta 0:04:22
epoch [42/50] batch [120/288] time 0.097 (0.106) data 0.000 (0.003) loss 1.3362 (1.5710) teacher_loss 0.7021 (0.9733) loss_zs_kd 0.0883 (0.0793) loss_oracle 0.0773 (0.0862) acc 78.1250 (74.1927) lr 1.9098e-04 eta 0:04:22
epoch [42/50] batch [140/288] time 0.102 (0.105) data 0.000 (0.003) loss 1.8676 (1.5679) teacher_loss 1.1368 (0.9698) loss_zs_kd 0.0788 (0.0801) loss_oracle 0.0933 (0.0872) acc 62.5000 (74.1964) lr 1.9098e-04 eta 0:04:17
epoch [42/50] batch [160/288] time 0.097 (0.104) data 0.000 (0.002) loss 1.7474 (1.5785) teacher_loss 1.1345 (0.9743) loss_zs_kd 0.1309 (0.0811) loss_oracle 0.1299 (0.0883) acc 75.0000 (74.1211) lr 1.9098e-04 eta 0:04:12
epoch [42/50] batch [180/288] time 0.108 (0.104) data 0.000 (0.002) loss 1.6837 (1.5721) teacher_loss 1.1016 (0.9712) loss_zs_kd 0.0997 (0.0810) loss_oracle 0.0803 (0.0872) acc 75.0000 (74.1493) lr 1.9098e-04 eta 0:04:09
epoch [42/50] batch [200/288] time 0.105 (0.103) data 0.000 (0.002) loss 1.8142 (1.5728) teacher_loss 0.9667 (0.9718) loss_zs_kd 0.0613 (0.0809) loss_oracle 0.0753 (0.0868) acc 75.0000 (74.2656) lr 1.9098e-04 eta 0:04:06
epoch [42/50] batch [220/288] time 0.114 (0.103) data 0.000 (0.002) loss 1.2542 (1.5754) teacher_loss 0.9455 (0.9758) loss_zs_kd 0.0729 (0.0807) loss_oracle 0.0717 (0.0860) acc 75.0000 (74.1619) lr 1.9098e-04 eta 0:04:04
epoch [42/50] batch [240/288] time 0.099 (0.103) data 0.000 (0.002) loss 1.5949 (1.5736) teacher_loss 1.0342 (0.9751) loss_zs_kd 0.0735 (0.0803) loss_oracle 0.0802 (0.0851) acc 71.8750 (74.2839) lr 1.9098e-04 eta 0:04:02
epoch [42/50] batch [260/288] time 0.108 (0.103) data 0.000 (0.002) loss 1.9629 (1.5803) teacher_loss 1.2144 (0.9805) loss_zs_kd 0.0797 (0.0801) loss_oracle 0.0698 (0.0849) acc 65.6250 (74.0745) lr 1.9098e-04 eta 0:03:59
epoch [42/50] batch [280/288] time 0.084 (0.102) data 0.000 (0.001) loss 1.6288 (1.5744) teacher_loss 0.8877 (0.9754) loss_zs_kd 0.0656 (0.0796) loss_oracle 0.0589 (0.0837) acc 81.2500 (74.2969) lr 1.9098e-04 eta 0:03:55
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,406
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,024
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.1%
******* Domain a best val acc:      86.6%, epoch: 39 *******
******* Domain a best val test acc: 83.5%, epoch: 39 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [43/50] batch [20/288] time 0.082 (0.107) data 0.000 (0.014) loss 1.9793 (1.7020) teacher_loss 1.2721 (1.0585) loss_zs_kd 0.0697 (0.0797) loss_oracle 0.0832 (0.0810) acc 68.7500 (70.7812) lr 1.5567e-04 eta 0:04:03
epoch [43/50] batch [40/288] time 0.098 (0.097) data 0.000 (0.007) loss 1.6527 (1.6217) teacher_loss 1.0848 (1.0270) loss_zs_kd 0.0732 (0.0811) loss_oracle 0.0549 (0.0789) acc 68.7500 (72.1094) lr 1.5567e-04 eta 0:03:39
epoch [43/50] batch [60/288] time 0.089 (0.096) data 0.000 (0.005) loss 2.5025 (1.6468) teacher_loss 1.7209 (1.0453) loss_zs_kd 0.0829 (0.0828) loss_oracle 0.1184 (0.0798) acc 56.2500 (71.8750) lr 1.5567e-04 eta 0:03:35
epoch [43/50] batch [80/288] time 0.097 (0.096) data 0.000 (0.004) loss 1.1058 (1.6043) teacher_loss 0.5858 (0.9982) loss_zs_kd 0.0328 (0.0785) loss_oracle 0.0739 (0.0789) acc 87.5000 (73.4766) lr 1.5567e-04 eta 0:03:32
epoch [43/50] batch [100/288] time 0.120 (0.097) data 0.001 (0.003) loss 1.7422 (1.6107) teacher_loss 1.3382 (1.0054) loss_zs_kd 0.0874 (0.0778) loss_oracle 0.0764 (0.0791) acc 65.6250 (73.0312) lr 1.5567e-04 eta 0:03:34
epoch [43/50] batch [120/288] time 0.094 (0.097) data 0.000 (0.002) loss 1.3911 (1.6060) teacher_loss 0.7444 (1.0006) loss_zs_kd 0.0683 (0.0792) loss_oracle 0.0754 (0.0795) acc 75.0000 (73.2552) lr 1.5567e-04 eta 0:03:31
epoch [43/50] batch [140/288] time 0.101 (0.096) data 0.000 (0.002) loss 1.2834 (1.5975) teacher_loss 0.8173 (0.9982) loss_zs_kd 0.0813 (0.0793) loss_oracle 0.0771 (0.0793) acc 81.2500 (73.4598) lr 1.5567e-04 eta 0:03:28
epoch [43/50] batch [160/288] time 0.084 (0.096) data 0.001 (0.002) loss 1.4588 (1.5877) teacher_loss 0.8747 (0.9906) loss_zs_kd 0.0818 (0.0798) loss_oracle 0.0819 (0.0784) acc 78.1250 (73.5547) lr 1.5567e-04 eta 0:03:24
epoch [43/50] batch [180/288] time 0.090 (0.095) data 0.000 (0.002) loss 1.2152 (1.5747) teacher_loss 0.7279 (0.9799) loss_zs_kd 0.0648 (0.0792) loss_oracle 0.1039 (0.0778) acc 78.1250 (73.8021) lr 1.5567e-04 eta 0:03:21
epoch [43/50] batch [200/288] time 0.089 (0.095) data 0.000 (0.002) loss 1.3608 (1.5717) teacher_loss 0.9171 (0.9807) loss_zs_kd 0.0863 (0.0793) loss_oracle 0.1085 (0.0777) acc 78.1250 (73.8750) lr 1.5567e-04 eta 0:03:19
epoch [43/50] batch [220/288] time 0.097 (0.094) data 0.000 (0.001) loss 1.4132 (1.5723) teacher_loss 0.8947 (0.9817) loss_zs_kd 0.0965 (0.0793) loss_oracle 0.0953 (0.0780) acc 71.8750 (73.9205) lr 1.5567e-04 eta 0:03:16
epoch [43/50] batch [240/288] time 0.085 (0.094) data 0.000 (0.001) loss 1.6378 (1.5681) teacher_loss 1.0996 (0.9776) loss_zs_kd 0.1099 (0.0792) loss_oracle 0.0727 (0.0784) acc 65.6250 (73.8672) lr 1.5567e-04 eta 0:03:13
epoch [43/50] batch [260/288] time 0.095 (0.094) data 0.000 (0.001) loss 1.4842 (1.5669) teacher_loss 0.7702 (0.9772) loss_zs_kd 0.0767 (0.0794) loss_oracle 0.0897 (0.0789) acc 78.1250 (73.8702) lr 1.5567e-04 eta 0:03:11
epoch [43/50] batch [280/288] time 0.084 (0.093) data 0.000 (0.001) loss 1.7296 (1.5674) teacher_loss 1.0321 (0.9760) loss_zs_kd 0.0788 (0.0793) loss_oracle 0.0641 (0.0789) acc 78.1250 (73.9955) lr 1.5567e-04 eta 0:03:08
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,406
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.0%
******* Domain a best val acc:      86.6%, epoch: 39 *******
******* Domain a best val test acc: 83.5%, epoch: 39 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [44/50] batch [20/288] time 0.103 (0.117) data 0.000 (0.013) loss 1.5835 (1.5546) teacher_loss 1.1210 (0.9822) loss_zs_kd 0.0662 (0.0776) loss_oracle 0.0753 (0.0877) acc 68.7500 (73.2812) lr 1.2369e-04 eta 0:03:53
epoch [44/50] batch [40/288] time 0.093 (0.108) data 0.000 (0.006) loss 1.4148 (1.5876) teacher_loss 0.7824 (1.0120) loss_zs_kd 0.0747 (0.0770) loss_oracle 0.0686 (0.0825) acc 81.2500 (72.4219) lr 1.2369e-04 eta 0:03:33
epoch [44/50] batch [60/288] time 0.099 (0.104) data 0.000 (0.004) loss 1.4092 (1.5851) teacher_loss 0.7672 (1.0021) loss_zs_kd 0.0504 (0.0800) loss_oracle 0.0718 (0.0816) acc 84.3750 (72.7604) lr 1.2369e-04 eta 0:03:24
epoch [44/50] batch [80/288] time 0.088 (0.102) data 0.000 (0.003) loss 1.8000 (1.5789) teacher_loss 1.1241 (0.9950) loss_zs_kd 0.1101 (0.0800) loss_oracle 0.0908 (0.0804) acc 71.8750 (72.6953) lr 1.2369e-04 eta 0:03:17
epoch [44/50] batch [100/288] time 0.093 (0.103) data 0.000 (0.003) loss 1.7086 (1.5723) teacher_loss 1.0907 (0.9858) loss_zs_kd 0.0658 (0.0797) loss_oracle 0.1067 (0.0808) acc 65.6250 (73.2812) lr 1.2369e-04 eta 0:03:16
epoch [44/50] batch [120/288] time 0.093 (0.101) data 0.000 (0.002) loss 1.6839 (1.5640) teacher_loss 0.9653 (0.9752) loss_zs_kd 0.1219 (0.0796) loss_oracle 0.1104 (0.0805) acc 68.7500 (73.6979) lr 1.2369e-04 eta 0:03:11
epoch [44/50] batch [140/288] time 0.097 (0.100) data 0.000 (0.002) loss 1.6890 (1.5730) teacher_loss 0.9325 (0.9779) loss_zs_kd 0.0958 (0.0797) loss_oracle 0.0839 (0.0804) acc 68.7500 (73.6830) lr 1.2369e-04 eta 0:03:06
epoch [44/50] batch [160/288] time 0.093 (0.099) data 0.000 (0.002) loss 1.7841 (1.5758) teacher_loss 1.1549 (0.9820) loss_zs_kd 0.0765 (0.0798) loss_oracle 0.1023 (0.0803) acc 62.5000 (73.5742) lr 1.2369e-04 eta 0:03:03
epoch [44/50] batch [180/288] time 0.102 (0.098) data 0.001 (0.002) loss 1.6787 (1.5858) teacher_loss 0.8855 (0.9904) loss_zs_kd 0.0676 (0.0798) loss_oracle 0.0512 (0.0800) acc 78.1250 (73.4896) lr 1.2369e-04 eta 0:03:00
epoch [44/50] batch [200/288] time 0.095 (0.098) data 0.000 (0.001) loss 1.4104 (1.5902) teacher_loss 0.8404 (0.9899) loss_zs_kd 0.0800 (0.0810) loss_oracle 0.0809 (0.0802) acc 78.1250 (73.4688) lr 1.2369e-04 eta 0:02:58
epoch [44/50] batch [220/288] time 0.091 (0.098) data 0.000 (0.001) loss 1.6738 (1.5933) teacher_loss 0.9795 (0.9935) loss_zs_kd 0.0768 (0.0805) loss_oracle 0.0896 (0.0797) acc 71.8750 (73.4091) lr 1.2369e-04 eta 0:02:56
epoch [44/50] batch [240/288] time 0.098 (0.099) data 0.000 (0.001) loss 1.6631 (1.6036) teacher_loss 1.0572 (1.0060) loss_zs_kd 0.0722 (0.0802) loss_oracle 0.0871 (0.0792) acc 68.7500 (73.0729) lr 1.2369e-04 eta 0:02:55
epoch [44/50] batch [260/288] time 0.093 (0.098) data 0.000 (0.001) loss 2.0619 (1.5997) teacher_loss 1.3983 (1.0024) loss_zs_kd 0.0642 (0.0798) loss_oracle 0.0838 (0.0793) acc 68.7500 (73.0649) lr 1.2369e-04 eta 0:02:52
epoch [44/50] batch [280/288] time 0.085 (0.098) data 0.000 (0.001) loss 1.7181 (1.5960) teacher_loss 1.2572 (1.0012) loss_zs_kd 0.0630 (0.0803) loss_oracle 0.0466 (0.0795) acc 65.6250 (72.9799) lr 1.2369e-04 eta 0:02:49
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,409
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.9%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.1%
******* Domain a best val acc:      86.6%, epoch: 39 *******
******* Domain a best val test acc: 83.5%, epoch: 39 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [45/50] batch [20/288] time 0.100 (0.112) data 0.000 (0.015) loss 1.8401 (1.5964) teacher_loss 1.2117 (0.9893) loss_zs_kd 0.0626 (0.0838) loss_oracle 0.0476 (0.0807) acc 59.3750 (73.9062) lr 9.5173e-05 eta 0:03:11
epoch [45/50] batch [40/288] time 0.093 (0.103) data 0.000 (0.008) loss 1.2363 (1.5412) teacher_loss 0.7349 (0.9382) loss_zs_kd 0.0410 (0.0804) loss_oracle 0.0494 (0.0781) acc 78.1250 (75.0781) lr 9.5173e-05 eta 0:02:54
epoch [45/50] batch [60/288] time 0.098 (0.101) data 0.000 (0.005) loss 1.2415 (1.5231) teacher_loss 0.5433 (0.9350) loss_zs_kd 0.0694 (0.0788) loss_oracle 0.0982 (0.0787) acc 84.3750 (75.2604) lr 9.5173e-05 eta 0:02:48
epoch [45/50] batch [80/288] time 0.092 (0.102) data 0.000 (0.004) loss 0.9854 (1.5234) teacher_loss 0.6165 (0.9486) loss_zs_kd 0.0570 (0.0779) loss_oracle 0.0590 (0.0782) acc 84.3750 (74.8828) lr 9.5173e-05 eta 0:02:47
epoch [45/50] batch [100/288] time 0.099 (0.101) data 0.000 (0.003) loss 2.1327 (1.5149) teacher_loss 1.4505 (0.9422) loss_zs_kd 0.0669 (0.0775) loss_oracle 0.0556 (0.0781) acc 62.5000 (75.0312) lr 9.5173e-05 eta 0:02:44
epoch [45/50] batch [120/288] time 0.098 (0.100) data 0.000 (0.003) loss 1.4845 (1.5359) teacher_loss 0.8807 (0.9532) loss_zs_kd 0.0698 (0.0786) loss_oracle 0.0689 (0.0788) acc 75.0000 (74.6875) lr 9.5173e-05 eta 0:02:41
epoch [45/50] batch [140/288] time 0.093 (0.100) data 0.000 (0.002) loss 1.0738 (1.5415) teacher_loss 0.4219 (0.9572) loss_zs_kd 0.0642 (0.0784) loss_oracle 0.0869 (0.0785) acc 87.5000 (74.7768) lr 9.5173e-05 eta 0:02:38
epoch [45/50] batch [160/288] time 0.097 (0.099) data 0.000 (0.002) loss 1.5319 (1.5468) teacher_loss 1.0171 (0.9613) loss_zs_kd 0.0994 (0.0795) loss_oracle 0.0766 (0.0791) acc 75.0000 (74.5312) lr 9.5173e-05 eta 0:02:35
epoch [45/50] batch [180/288] time 0.110 (0.100) data 0.000 (0.002) loss 1.8390 (1.5493) teacher_loss 1.2231 (0.9670) loss_zs_kd 0.1100 (0.0798) loss_oracle 0.1239 (0.0791) acc 62.5000 (74.2188) lr 9.5173e-05 eta 0:02:34
epoch [45/50] batch [200/288] time 0.100 (0.100) data 0.000 (0.002) loss 1.2485 (1.5540) teacher_loss 0.6076 (0.9701) loss_zs_kd 0.0705 (0.0803) loss_oracle 0.0745 (0.0790) acc 87.5000 (74.2656) lr 9.5173e-05 eta 0:02:32
epoch [45/50] batch [220/288] time 0.096 (0.100) data 0.000 (0.002) loss 1.5171 (1.5506) teacher_loss 0.8341 (0.9657) loss_zs_kd 0.1124 (0.0801) loss_oracle 0.0872 (0.0787) acc 81.2500 (74.3608) lr 9.5173e-05 eta 0:02:30
epoch [45/50] batch [240/288] time 0.093 (0.100) data 0.000 (0.002) loss 1.5787 (1.5453) teacher_loss 0.9688 (0.9574) loss_zs_kd 0.0647 (0.0796) loss_oracle 0.0817 (0.0786) acc 71.8750 (74.6615) lr 9.5173e-05 eta 0:02:28
epoch [45/50] batch [260/288] time 0.100 (0.100) data 0.000 (0.001) loss 1.7158 (1.5450) teacher_loss 1.2598 (0.9560) loss_zs_kd 0.1135 (0.0802) loss_oracle 0.1203 (0.0790) acc 68.7500 (74.7837) lr 9.5173e-05 eta 0:02:26
epoch [45/50] batch [280/288] time 0.085 (0.099) data 0.000 (0.001) loss 1.5663 (1.5567) teacher_loss 0.9571 (0.9650) loss_zs_kd 0.0889 (0.0806) loss_oracle 0.0733 (0.0791) acc 84.3750 (74.6205) lr 9.5173e-05 eta 0:02:23
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,407
* accuracy: 86.5%
* error: 13.5%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 80.0%
******* Domain a best val acc:      86.6%, epoch: 39 *******
******* Domain a best val test acc: 83.5%, epoch: 39 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [46/50] batch [20/288] time 0.097 (0.117) data 0.000 (0.017) loss 1.3030 (1.5593) teacher_loss 0.7997 (0.9599) loss_zs_kd 0.0643 (0.0758) loss_oracle 0.0632 (0.0792) acc 78.1250 (75.1562) lr 7.0224e-05 eta 0:02:46
epoch [46/50] batch [40/288] time 0.102 (0.108) data 0.000 (0.009) loss 1.5263 (1.6108) teacher_loss 1.0309 (1.0006) loss_zs_kd 0.0906 (0.0804) loss_oracle 0.0757 (0.0806) acc 71.8750 (73.7500) lr 7.0224e-05 eta 0:02:31
epoch [46/50] batch [60/288] time 0.104 (0.109) data 0.000 (0.006) loss 1.6074 (1.6216) teacher_loss 0.9029 (1.0120) loss_zs_kd 0.0727 (0.0831) loss_oracle 0.0624 (0.0798) acc 75.0000 (73.0208) lr 7.0224e-05 eta 0:02:30
epoch [46/50] batch [80/288] time 0.096 (0.106) data 0.000 (0.004) loss 1.2927 (1.5987) teacher_loss 0.7542 (0.9882) loss_zs_kd 0.0645 (0.0807) loss_oracle 0.0570 (0.0788) acc 81.2500 (73.7500) lr 7.0224e-05 eta 0:02:24
epoch [46/50] batch [100/288] time 0.097 (0.104) data 0.000 (0.004) loss 1.4143 (1.5715) teacher_loss 0.8637 (0.9750) loss_zs_kd 0.0843 (0.0805) loss_oracle 0.0680 (0.0788) acc 68.7500 (74.2812) lr 7.0224e-05 eta 0:02:19
epoch [46/50] batch [120/288] time 0.090 (0.102) data 0.000 (0.003) loss 1.1273 (1.5742) teacher_loss 0.5162 (0.9789) loss_zs_kd 0.0845 (0.0809) loss_oracle 0.0643 (0.0782) acc 84.3750 (74.0365) lr 7.0224e-05 eta 0:02:14
epoch [46/50] batch [140/288] time 0.101 (0.101) data 0.000 (0.003) loss 1.4930 (1.5765) teacher_loss 0.9129 (0.9797) loss_zs_kd 0.0810 (0.0807) loss_oracle 0.0985 (0.0784) acc 78.1250 (73.8393) lr 7.0224e-05 eta 0:02:11
epoch [46/50] batch [160/288] time 0.103 (0.100) data 0.000 (0.002) loss 1.5448 (1.5654) teacher_loss 1.0020 (0.9733) loss_zs_kd 0.1057 (0.0801) loss_oracle 0.0861 (0.0781) acc 75.0000 (73.9453) lr 7.0224e-05 eta 0:02:08
epoch [46/50] batch [180/288] time 0.097 (0.100) data 0.000 (0.002) loss 1.6200 (1.5791) teacher_loss 1.0233 (0.9813) loss_zs_kd 0.1185 (0.0806) loss_oracle 0.1035 (0.0784) acc 75.0000 (73.8021) lr 7.0224e-05 eta 0:02:05
epoch [46/50] batch [200/288] time 0.094 (0.099) data 0.000 (0.002) loss 1.8412 (1.5873) teacher_loss 1.2695 (0.9899) loss_zs_kd 0.0542 (0.0806) loss_oracle 0.0526 (0.0783) acc 75.0000 (73.6875) lr 7.0224e-05 eta 0:02:03
epoch [46/50] batch [220/288] time 0.152 (0.099) data 0.000 (0.002) loss 1.5837 (1.5873) teacher_loss 0.8516 (0.9912) loss_zs_kd 0.0802 (0.0801) loss_oracle 0.0795 (0.0783) acc 75.0000 (73.6364) lr 7.0224e-05 eta 0:02:01
epoch [46/50] batch [240/288] time 0.102 (0.099) data 0.000 (0.002) loss 1.6512 (1.5874) teacher_loss 0.8729 (0.9918) loss_zs_kd 0.0882 (0.0802) loss_oracle 0.0602 (0.0780) acc 75.0000 (73.7109) lr 7.0224e-05 eta 0:01:59
epoch [46/50] batch [260/288] time 0.109 (0.099) data 0.000 (0.002) loss 1.3820 (1.5827) teacher_loss 0.8054 (0.9875) loss_zs_kd 0.0629 (0.0804) loss_oracle 0.0854 (0.0780) acc 87.5000 (73.8942) lr 7.0224e-05 eta 0:01:57
epoch [46/50] batch [280/288] time 0.099 (0.099) data 0.000 (0.001) loss 2.0290 (1.5866) teacher_loss 1.2815 (0.9905) loss_zs_kd 0.0788 (0.0807) loss_oracle 0.0948 (0.0782) acc 68.7500 (73.7835) lr 7.0224e-05 eta 0:01:55
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 80.0%
******* Domain a best val acc:      86.6%, epoch: 39 *******
******* Domain a best val test acc: 83.5%, epoch: 39 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [47/50] batch [20/288] time 0.112 (0.133) data 0.000 (0.018) loss 1.8752 (1.6172) teacher_loss 1.2944 (1.0091) loss_zs_kd 0.0664 (0.0784) loss_oracle 0.0904 (0.0810) acc 68.7500 (73.2812) lr 4.8943e-05 eta 0:02:29
epoch [47/50] batch [40/288] time 0.102 (0.121) data 0.000 (0.009) loss 1.8688 (1.6001) teacher_loss 1.2947 (1.0000) loss_zs_kd 0.0669 (0.0750) loss_oracle 0.0529 (0.0790) acc 71.8750 (73.4375) lr 4.8943e-05 eta 0:02:15
epoch [47/50] batch [60/288] time 0.096 (0.113) data 0.000 (0.006) loss 2.1575 (1.5935) teacher_loss 1.6188 (1.0034) loss_zs_kd 0.0712 (0.0760) loss_oracle 0.0678 (0.0776) acc 62.5000 (73.2812) lr 4.8943e-05 eta 0:02:03
epoch [47/50] batch [80/288] time 0.102 (0.109) data 0.000 (0.005) loss 1.9056 (1.5832) teacher_loss 1.1333 (0.9944) loss_zs_kd 0.0613 (0.0779) loss_oracle 0.0831 (0.0788) acc 65.6250 (73.8281) lr 4.8943e-05 eta 0:01:56
epoch [47/50] batch [100/288] time 0.090 (0.106) data 0.000 (0.004) loss 1.4130 (1.5729) teacher_loss 0.6880 (0.9871) loss_zs_kd 0.0596 (0.0787) loss_oracle 0.0639 (0.0805) acc 78.1250 (73.8750) lr 4.8943e-05 eta 0:01:51
epoch [47/50] batch [120/288] time 0.091 (0.105) data 0.000 (0.003) loss 1.7250 (1.5607) teacher_loss 1.2107 (0.9761) loss_zs_kd 0.0618 (0.0792) loss_oracle 0.0633 (0.0803) acc 75.0000 (74.1146) lr 4.8943e-05 eta 0:01:48
epoch [47/50] batch [140/288] time 0.095 (0.103) data 0.000 (0.003) loss 1.4131 (1.5656) teacher_loss 0.8345 (0.9779) loss_zs_kd 0.0974 (0.0799) loss_oracle 0.0704 (0.0799) acc 78.1250 (74.1295) lr 4.8943e-05 eta 0:01:44
epoch [47/50] batch [160/288] time 0.094 (0.102) data 0.000 (0.002) loss 1.7592 (1.5745) teacher_loss 0.9561 (0.9814) loss_zs_kd 0.1353 (0.0806) loss_oracle 0.0980 (0.0800) acc 71.8750 (74.0039) lr 4.8943e-05 eta 0:01:41
epoch [47/50] batch [180/288] time 0.091 (0.102) data 0.000 (0.002) loss 1.5439 (1.5800) teacher_loss 0.9109 (0.9843) loss_zs_kd 0.0872 (0.0809) loss_oracle 0.0822 (0.0803) acc 68.7500 (73.7674) lr 4.8943e-05 eta 0:01:38
epoch [47/50] batch [200/288] time 0.107 (0.102) data 0.000 (0.002) loss 0.8659 (1.5816) teacher_loss 0.4212 (0.9827) loss_zs_kd 0.0584 (0.0801) loss_oracle 0.0855 (0.0801) acc 84.3750 (73.8281) lr 4.8943e-05 eta 0:01:36
epoch [47/50] batch [220/288] time 0.104 (0.102) data 0.000 (0.002) loss 1.5319 (1.5870) teacher_loss 0.8601 (0.9886) loss_zs_kd 0.0833 (0.0806) loss_oracle 0.0702 (0.0803) acc 71.8750 (73.7642) lr 4.8943e-05 eta 0:01:34
epoch [47/50] batch [240/288] time 0.111 (0.102) data 0.001 (0.002) loss 1.2092 (1.5852) teacher_loss 0.6756 (0.9882) loss_zs_kd 0.0678 (0.0808) loss_oracle 0.0505 (0.0799) acc 81.2500 (73.6979) lr 4.8943e-05 eta 0:01:32
epoch [47/50] batch [260/288] time 0.101 (0.102) data 0.001 (0.002) loss 2.1330 (1.5850) teacher_loss 1.4130 (0.9908) loss_zs_kd 0.1224 (0.0808) loss_oracle 0.0965 (0.0792) acc 65.6250 (73.6298) lr 4.8943e-05 eta 0:01:30
epoch [47/50] batch [280/288] time 0.084 (0.101) data 0.000 (0.001) loss 1.1750 (1.5886) teacher_loss 0.5555 (0.9945) loss_zs_kd 0.0641 (0.0810) loss_oracle 0.0833 (0.0795) acc 84.3750 (73.5379) lr 4.8943e-05 eta 0:01:28
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,403
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.1%
******* Domain a best val acc:      86.6%, epoch: 39 *******
******* Domain a best val test acc: 83.5%, epoch: 39 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [48/50] batch [20/288] time 0.098 (0.124) data 0.001 (0.014) loss 1.5509 (1.6141) teacher_loss 1.0524 (1.0520) loss_zs_kd 0.0809 (0.0844) loss_oracle 0.0553 (0.0773) acc 65.6250 (71.7188) lr 3.1417e-05 eta 0:01:45
epoch [48/50] batch [40/288] time 0.099 (0.112) data 0.000 (0.007) loss 2.1295 (1.6028) teacher_loss 1.2856 (1.0283) loss_zs_kd 0.0948 (0.0815) loss_oracle 0.0787 (0.0783) acc 65.6250 (72.1094) lr 3.1417e-05 eta 0:01:32
epoch [48/50] batch [60/288] time 0.096 (0.108) data 0.000 (0.005) loss 1.7228 (1.5894) teacher_loss 1.1021 (1.0101) loss_zs_kd 0.0744 (0.0850) loss_oracle 0.0736 (0.0791) acc 71.8750 (73.0208) lr 3.1417e-05 eta 0:01:26
epoch [48/50] batch [80/288] time 0.099 (0.106) data 0.000 (0.004) loss 1.6919 (1.5861) teacher_loss 1.0571 (1.0034) loss_zs_kd 0.0843 (0.0828) loss_oracle 0.1021 (0.0805) acc 75.0000 (73.1250) lr 3.1417e-05 eta 0:01:23
epoch [48/50] batch [100/288] time 0.106 (0.105) data 0.000 (0.003) loss 1.5399 (1.5717) teacher_loss 0.8935 (0.9884) loss_zs_kd 0.0654 (0.0819) loss_oracle 0.0768 (0.0793) acc 68.7500 (73.5625) lr 3.1417e-05 eta 0:01:20
epoch [48/50] batch [120/288] time 0.099 (0.104) data 0.001 (0.003) loss 1.8465 (1.5885) teacher_loss 1.1423 (0.9897) loss_zs_kd 0.0965 (0.0811) loss_oracle 0.0973 (0.0792) acc 68.7500 (73.4115) lr 3.1417e-05 eta 0:01:17
epoch [48/50] batch [140/288] time 0.094 (0.104) data 0.000 (0.002) loss 1.6328 (1.5916) teacher_loss 0.9142 (0.9883) loss_zs_kd 0.0769 (0.0808) loss_oracle 0.0643 (0.0794) acc 75.0000 (73.5714) lr 3.1417e-05 eta 0:01:14
epoch [48/50] batch [160/288] time 0.100 (0.103) data 0.000 (0.002) loss 1.5660 (1.5986) teacher_loss 0.9850 (0.9972) loss_zs_kd 0.0650 (0.0813) loss_oracle 0.0564 (0.0794) acc 71.8750 (73.4180) lr 3.1417e-05 eta 0:01:12
epoch [48/50] batch [180/288] time 0.099 (0.103) data 0.000 (0.002) loss 1.4794 (1.5909) teacher_loss 0.7715 (0.9892) loss_zs_kd 0.0751 (0.0818) loss_oracle 0.0795 (0.0797) acc 78.1250 (73.5938) lr 3.1417e-05 eta 0:01:10
epoch [48/50] batch [200/288] time 0.105 (0.103) data 0.001 (0.002) loss 1.7237 (1.5923) teacher_loss 0.8959 (0.9896) loss_zs_kd 0.0841 (0.0817) loss_oracle 0.0644 (0.0798) acc 71.8750 (73.6094) lr 3.1417e-05 eta 0:01:08
epoch [48/50] batch [220/288] time 0.107 (0.102) data 0.000 (0.002) loss 1.4436 (1.5865) teacher_loss 0.9161 (0.9869) loss_zs_kd 0.0764 (0.0816) loss_oracle 0.0940 (0.0798) acc 75.0000 (73.7500) lr 3.1417e-05 eta 0:01:05
epoch [48/50] batch [240/288] time 0.102 (0.102) data 0.000 (0.001) loss 1.6432 (1.5892) teacher_loss 1.0958 (0.9888) loss_zs_kd 0.0726 (0.0817) loss_oracle 0.0661 (0.0798) acc 71.8750 (73.6328) lr 3.1417e-05 eta 0:01:03
epoch [48/50] batch [260/288] time 0.098 (0.102) data 0.000 (0.001) loss 1.6498 (1.5843) teacher_loss 1.0133 (0.9861) loss_zs_kd 0.0800 (0.0813) loss_oracle 0.0815 (0.0796) acc 78.1250 (73.6298) lr 3.1417e-05 eta 0:01:01
epoch [48/50] batch [280/288] time 0.103 (0.102) data 0.000 (0.001) loss 1.4409 (1.5819) teacher_loss 0.9007 (0.9841) loss_zs_kd 0.0784 (0.0814) loss_oracle 0.0609 (0.0796) acc 81.2500 (73.6161) lr 3.1417e-05 eta 0:00:59
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,405
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.8%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.1%
******* Domain a best val acc:      86.6%, epoch: 39 *******
******* Domain a best val test acc: 83.5%, epoch: 39 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [49/50] batch [20/288] time 0.109 (0.122) data 0.000 (0.017) loss 1.6500 (1.5198) teacher_loss 1.1054 (0.9360) loss_zs_kd 0.0719 (0.0839) loss_oracle 0.0958 (0.0883) acc 65.6250 (74.6875) lr 1.7713e-05 eta 0:01:07
epoch [49/50] batch [40/288] time 0.102 (0.111) data 0.000 (0.009) loss 1.9619 (1.5272) teacher_loss 1.3568 (0.9610) loss_zs_kd 0.0751 (0.0802) loss_oracle 0.0804 (0.0845) acc 62.5000 (74.3750) lr 1.7713e-05 eta 0:00:59
epoch [49/50] batch [60/288] time 0.099 (0.106) data 0.000 (0.006) loss 1.1118 (1.5384) teacher_loss 0.5473 (0.9583) loss_zs_kd 0.0782 (0.0814) loss_oracle 0.0665 (0.0828) acc 84.3750 (74.8438) lr 1.7713e-05 eta 0:00:54
epoch [49/50] batch [80/288] time 0.105 (0.106) data 0.000 (0.005) loss 1.5577 (1.5243) teacher_loss 1.0323 (0.9450) loss_zs_kd 0.0761 (0.0814) loss_oracle 0.0799 (0.0811) acc 75.0000 (75.5859) lr 1.7713e-05 eta 0:00:52
epoch [49/50] batch [100/288] time 0.101 (0.106) data 0.000 (0.004) loss 1.1520 (1.5383) teacher_loss 0.5884 (0.9584) loss_zs_kd 0.0753 (0.0824) loss_oracle 0.0846 (0.0816) acc 84.3750 (75.2500) lr 1.7713e-05 eta 0:00:50
epoch [49/50] batch [120/288] time 0.105 (0.105) data 0.000 (0.003) loss 1.5731 (1.5454) teacher_loss 0.9784 (0.9659) loss_zs_kd 0.0789 (0.0817) loss_oracle 0.0423 (0.0814) acc 78.1250 (75.0260) lr 1.7713e-05 eta 0:00:47
epoch [49/50] batch [140/288] time 0.095 (0.105) data 0.000 (0.003) loss 1.8924 (1.5521) teacher_loss 1.3045 (0.9674) loss_zs_kd 0.0985 (0.0819) loss_oracle 0.1044 (0.0815) acc 62.5000 (74.8214) lr 1.7713e-05 eta 0:00:45
epoch [49/50] batch [160/288] time 0.100 (0.104) data 0.000 (0.002) loss 1.1852 (1.5509) teacher_loss 0.7594 (0.9656) loss_zs_kd 0.0769 (0.0812) loss_oracle 0.0776 (0.0814) acc 81.2500 (74.7070) lr 1.7713e-05 eta 0:00:43
epoch [49/50] batch [180/288] time 0.091 (0.103) data 0.000 (0.002) loss 1.8103 (1.5421) teacher_loss 1.2765 (0.9596) loss_zs_kd 0.1205 (0.0809) loss_oracle 0.1119 (0.0809) acc 68.7500 (74.8438) lr 1.7713e-05 eta 0:00:40
epoch [49/50] batch [200/288] time 0.089 (0.103) data 0.000 (0.002) loss 2.1195 (1.5575) teacher_loss 1.6526 (0.9728) loss_zs_kd 0.0820 (0.0809) loss_oracle 0.0754 (0.0810) acc 56.2500 (74.4844) lr 1.7713e-05 eta 0:00:38
epoch [49/50] batch [220/288] time 0.092 (0.102) data 0.000 (0.002) loss 1.7966 (1.5651) teacher_loss 1.1209 (0.9784) loss_zs_kd 0.0947 (0.0812) loss_oracle 0.0875 (0.0810) acc 68.7500 (74.3750) lr 1.7713e-05 eta 0:00:36
epoch [49/50] batch [240/288] time 0.092 (0.102) data 0.000 (0.002) loss 1.2815 (1.5656) teacher_loss 0.6278 (0.9766) loss_zs_kd 0.0737 (0.0812) loss_oracle 0.0663 (0.0807) acc 81.2500 (74.3490) lr 1.7713e-05 eta 0:00:34
epoch [49/50] batch [260/288] time 0.100 (0.101) data 0.000 (0.002) loss 1.6330 (1.5577) teacher_loss 1.0574 (0.9675) loss_zs_kd 0.1044 (0.0806) loss_oracle 0.1052 (0.0805) acc 78.1250 (74.7356) lr 1.7713e-05 eta 0:00:31
epoch [49/50] batch [280/288] time 0.083 (0.100) data 0.000 (0.001) loss 1.2807 (1.5488) teacher_loss 0.5988 (0.9601) loss_zs_kd 0.0599 (0.0802) loss_oracle 0.0552 (0.0806) acc 90.6250 (74.8772) lr 1.7713e-05 eta 0:00:29
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,404
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,022
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 80.0%
******* Domain a best val acc:      86.6%, epoch: 39 *******
******* Domain a best val test acc: 83.5%, epoch: 39 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
epoch [50/50] batch [20/288] time 0.090 (0.112) data 0.000 (0.014) loss 1.5763 (1.4326) teacher_loss 1.0591 (0.8749) loss_zs_kd 0.0644 (0.0807) loss_oracle 0.0802 (0.0827) acc 71.8750 (76.4062) lr 7.8853e-06 eta 0:00:29
epoch [50/50] batch [40/288] time 0.100 (0.104) data 0.000 (0.007) loss 1.7768 (1.5310) teacher_loss 1.2194 (0.9536) loss_zs_kd 0.0799 (0.0806) loss_oracle 0.0680 (0.0801) acc 62.5000 (74.3750) lr 7.8853e-06 eta 0:00:25
epoch [50/50] batch [60/288] time 0.094 (0.101) data 0.000 (0.005) loss 1.3468 (1.5370) teacher_loss 0.8858 (0.9538) loss_zs_kd 0.0728 (0.0788) loss_oracle 0.1113 (0.0806) acc 78.1250 (74.4271) lr 7.8853e-06 eta 0:00:22
epoch [50/50] batch [80/288] time 0.097 (0.100) data 0.000 (0.004) loss 1.3755 (1.5487) teacher_loss 0.8102 (0.9681) loss_zs_kd 0.0745 (0.0792) loss_oracle 0.0751 (0.0817) acc 71.8750 (73.7109) lr 7.8853e-06 eta 0:00:20
epoch [50/50] batch [100/288] time 0.093 (0.099) data 0.000 (0.003) loss 1.7359 (1.5625) teacher_loss 1.2472 (0.9863) loss_zs_kd 0.0783 (0.0796) loss_oracle 0.0878 (0.0814) acc 65.6250 (73.5000) lr 7.8853e-06 eta 0:00:18
epoch [50/50] batch [120/288] time 0.092 (0.098) data 0.000 (0.002) loss 1.4483 (1.5589) teacher_loss 0.8310 (0.9821) loss_zs_kd 0.0690 (0.0793) loss_oracle 0.0603 (0.0809) acc 78.1250 (73.6719) lr 7.8853e-06 eta 0:00:16
epoch [50/50] batch [140/288] time 0.101 (0.098) data 0.000 (0.002) loss 1.7950 (1.5675) teacher_loss 1.2599 (0.9802) loss_zs_kd 0.0914 (0.0809) loss_oracle 0.0747 (0.0809) acc 65.6250 (73.8839) lr 7.8853e-06 eta 0:00:14
epoch [50/50] batch [160/288] time 0.096 (0.098) data 0.000 (0.002) loss 1.6414 (1.5543) teacher_loss 0.8543 (0.9701) loss_zs_kd 0.0597 (0.0802) loss_oracle 0.0938 (0.0811) acc 78.1250 (74.1406) lr 7.8853e-06 eta 0:00:12
epoch [50/50] batch [180/288] time 0.090 (0.097) data 0.000 (0.002) loss 1.7174 (1.5687) teacher_loss 1.0444 (0.9865) loss_zs_kd 0.0844 (0.0804) loss_oracle 0.0841 (0.0808) acc 75.0000 (74.0625) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [200/288] time 0.094 (0.097) data 0.000 (0.002) loss 1.8142 (1.5769) teacher_loss 1.1901 (0.9916) loss_zs_kd 0.0702 (0.0807) loss_oracle 0.0957 (0.0807) acc 59.3750 (73.9062) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [220/288] time 0.098 (0.097) data 0.000 (0.001) loss 1.1267 (1.5737) teacher_loss 0.6993 (0.9856) loss_zs_kd 0.0657 (0.0812) loss_oracle 0.1060 (0.0807) acc 75.0000 (74.0057) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [240/288] time 0.089 (0.097) data 0.000 (0.001) loss 1.4562 (1.5865) teacher_loss 0.7861 (0.9949) loss_zs_kd 0.0672 (0.0812) loss_oracle 0.0798 (0.0808) acc 78.1250 (73.8932) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [260/288] time 0.096 (0.097) data 0.000 (0.001) loss 1.2697 (1.5933) teacher_loss 0.7443 (1.0020) loss_zs_kd 0.0656 (0.0813) loss_oracle 0.0699 (0.0808) acc 75.0000 (73.6779) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [280/288] time 0.084 (0.096) data 0.000 (0.001) loss 2.0517 (1.5899) teacher_loss 1.2908 (0.9965) loss_zs_kd 0.0783 (0.0812) loss_oracle 0.0674 (0.0808) acc 53.1250 (73.8058) lr 7.8853e-06 eta 0:00:00
Evaluate on the *val* set
=> result
* total: 3,939
* correct: 3,404
* accuracy: 86.4%
* error: 13.6%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 2,427
* correct: 2,023
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 80.1%
******* Domain a best val acc:      86.6%, epoch: 39 *******
******* Domain a best val test acc: 83.5%, epoch: 39 *******
******* Domain a best test acc:     83.6%, epoch: 32 *******
Checkpoint saved to icml/multi-dg/tuning/16_seperate/TRIP/office_home/b32_ep50/ViT-B16/a/seed_1/warmup_1/prompt_learner/model.pth.tar-50
Finish the whole training
Elapsed: 0:37:51
