Loading trainer: TRIP
Loading dataset: SPG_VLCS
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------------------------
Dataset    SPG_VLCS
Source     ['labelme', 'pascal', 'sun']
Target     ['caltech']
# classes  5
# train_x  6,519
# val      2,795
# test     1,415
---------  ----------------------------
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Initial context: "a photo of a"
Number of context words (tokens): 4
Turning off gradients in both the image and the text encoder
=== Trainable Parameters by Module ===
prompt_learner.0.ctx                               2,048
prompt_learner.1.ctx                               2,048
prompt_learner.2.ctx                               2,048
gate.mlp.0.weight                                  65,536
gate.mlp.0.bias                                    128
gate.mlp.2.weight                                  384
gate.mlp.2.bias                                    3
Total trainable params: 72,195
[Info] Hyperparameters saved to: icml/multi-dg/tuning/19_ema_teacherupdate/TRIP/vlcs/b32_ep50/ViT-B16/c/seed_1/warmup_1/hyperparameters.json
Loading evaluator: Classification
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=icml/multi-dg/tuning/19_ema_teacherupdate/TRIP/vlcs/b32_ep50/ViT-B16/c/seed_1/warmup_1/tensorboard)
epoch [1/50] batch [20/203] time 0.170 (0.140) data 0.001 (0.021) loss 0.8932 (1.1148) teacher_loss 0.6211 (0.7934) loss_zs_kd 0.0000 (0.0000) loss_oracle 0.0001 (-0.0001) kd_loss 0.5442 (0.6430) acc 68.7500 (71.0938) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3348 (0.3348) gate/usage_min 0.3312 (0.3312) gate/usage_std 0.0015 (0.0015) teacher/entropy 0.5543 (0.4558) teacher/usage_max 0.4701 (0.4262) teacher/usage_min 0.2316 (0.2358) teacher/usage_std 0.1005 (0.0814) nleep/row_max_mean 1531.1741 (1519.6300) nleep/row_max_std 44.7928 (55.2309) nleep/row_min_mean 1526.9183 (1512.6109) lr 1.0000e-05 eta 0:23:34
epoch [1/50] batch [40/203] time 0.084 (0.128) data 0.000 (0.011) loss 0.9945 (1.0391) teacher_loss 0.7834 (0.7583) loss_zs_kd 0.0001 (0.0000) loss_oracle 0.0003 (0.0001) kd_loss 0.4217 (0.5614) acc 75.0000 (72.2656) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3349 (0.3348) gate/usage_min 0.3311 (0.3312) gate/usage_std 0.0016 (0.0015) teacher/entropy 0.6768 (0.5370) teacher/usage_max 0.4276 (0.4209) teacher/usage_min 0.2849 (0.2448) teacher/usage_std 0.0667 (0.0756) nleep/row_max_mean 1526.6866 (1524.2237) nleep/row_max_std 43.8665 (50.8244) nleep/row_min_mean 1523.7378 (1518.9067) lr 1.0000e-05 eta 0:21:36
epoch [1/50] batch [60/203] time 0.089 (0.117) data 0.000 (0.007) loss 0.9564 (1.0044) teacher_loss 0.7729 (0.7554) loss_zs_kd 0.0004 (0.0001) loss_oracle 0.0010 (0.0004) kd_loss 0.3655 (0.4975) acc 75.0000 (73.2812) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3349 (0.3348) gate/usage_min 0.3312 (0.3312) gate/usage_std 0.0016 (0.0015) teacher/entropy 0.7321 (0.6008) teacher/usage_max 0.4544 (0.4126) teacher/usage_min 0.2546 (0.2523) teacher/usage_std 0.0869 (0.0689) nleep/row_max_mean 1543.1497 (1525.3919) nleep/row_max_std 47.2553 (49.0676) nleep/row_min_mean 1540.5750 (1520.9598) lr 1.0000e-05 eta 0:19:37
epoch [1/50] batch [80/203] time 0.091 (0.113) data 0.000 (0.006) loss 0.8815 (0.9471) teacher_loss 0.6891 (0.7252) loss_zs_kd 0.0007 (0.0002) loss_oracle 0.0013 (0.0006) kd_loss 0.3828 (0.4431) acc 71.8750 (74.2188) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3347 (0.3348) gate/usage_min 0.3313 (0.3312) gate/usage_std 0.0015 (0.0015) teacher/entropy 0.7153 (0.6551) teacher/usage_max 0.4352 (0.4095) teacher/usage_min 0.2573 (0.2575) teacher/usage_std 0.0749 (0.0650) nleep/row_max_mean 1546.1899 (1527.2491) nleep/row_max_std 51.0364 (48.5059) nleep/row_min_mean 1543.4214 (1523.4083) lr 1.0000e-05 eta 0:18:59
epoch [1/50] batch [100/203] time 0.098 (0.111) data 0.000 (0.004) loss 0.9766 (0.9301) teacher_loss 0.8731 (0.7305) loss_zs_kd 0.0012 (0.0004) loss_oracle 0.0018 (0.0008) kd_loss 0.2040 (0.3980) acc 71.8750 (74.0000) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3348 (0.3348) gate/usage_min 0.3311 (0.3312) gate/usage_std 0.0016 (0.0015) teacher/entropy 0.8940 (0.7003) teacher/usage_max 0.3909 (0.4045) teacher/usage_min 0.2971 (0.2636) teacher/usage_std 0.0412 (0.0601) nleep/row_max_mean 1532.9598 (1526.5296) nleep/row_max_std 42.5947 (48.8026) nleep/row_min_mean 1531.4607 (1523.1234) lr 1.0000e-05 eta 0:18:39
epoch [1/50] batch [120/203] time 0.109 (0.111) data 0.000 (0.004) loss 0.9342 (0.9099) teacher_loss 0.8498 (0.7289) loss_zs_kd 0.0013 (0.0005) loss_oracle 0.0028 (0.0010) kd_loss 0.1648 (0.3603) acc 65.6250 (73.5938) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3347 (0.3348) gate/usage_min 0.3313 (0.3312) gate/usage_std 0.0015 (0.0015) teacher/entropy 0.9331 (0.7379) teacher/usage_max 0.3987 (0.4033) teacher/usage_min 0.2609 (0.2661) teacher/usage_std 0.0565 (0.0586) nleep/row_max_mean 1538.0400 (1526.7609) nleep/row_max_std 50.1589 (49.1859) nleep/row_min_mean 1536.6873 (1523.6878) lr 1.0000e-05 eta 0:18:30
epoch [1/50] batch [140/203] time 0.109 (0.109) data 0.000 (0.003) loss 0.6557 (0.8881) teacher_loss 0.5859 (0.7229) loss_zs_kd 0.0015 (0.0006) loss_oracle 0.0021 (0.0012) kd_loss 0.1361 (0.3287) acc 78.1250 (73.8839) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3348 (0.3348) gate/usage_min 0.3314 (0.3312) gate/usage_std 0.0015 (0.0015) teacher/entropy 0.9625 (0.7695) teacher/usage_max 0.3759 (0.4019) teacher/usage_min 0.3070 (0.2692) teacher/usage_std 0.0304 (0.0567) nleep/row_max_mean 1516.0940 (1526.1056) nleep/row_max_std 58.3580 (50.0968) nleep/row_min_mean 1514.9565 (1523.2992) lr 1.0000e-05 eta 0:18:10
epoch [1/50] batch [160/203] time 0.110 (0.108) data 0.001 (0.003) loss 0.6875 (0.8737) teacher_loss 0.6134 (0.7216) loss_zs_kd 0.0003 (0.0006) loss_oracle 0.0026 (0.0014) kd_loss 0.1455 (0.3022) acc 78.1250 (73.9648) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3348 (0.3348) gate/usage_min 0.3311 (0.3312) gate/usage_std 0.0016 (0.0015) teacher/entropy 0.9525 (0.7960) teacher/usage_max 0.4478 (0.4022) teacher/usage_min 0.2697 (0.2715) teacher/usage_std 0.0811 (0.0561) nleep/row_max_mean 1510.6954 (1525.6106) nleep/row_max_std 69.3414 (50.7037) nleep/row_min_mean 1509.4104 (1523.0211) lr 1.0000e-05 eta 0:18:01
epoch [1/50] batch [180/203] time 0.119 (0.108) data 0.000 (0.003) loss 0.8923 (0.8696) teacher_loss 0.8489 (0.7290) loss_zs_kd 0.0022 (0.0007) loss_oracle 0.0029 (0.0015) kd_loss 0.0817 (0.2789) acc 68.7500 (73.8194) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3348 (0.3348) gate/usage_min 0.3312 (0.3312) gate/usage_std 0.0015 (0.0015) teacher/entropy 1.0162 (0.8192) teacher/usage_max 0.4024 (0.4035) teacher/usage_min 0.2727 (0.2723) teacher/usage_std 0.0533 (0.0565) nleep/row_max_mean 1520.9456 (1525.1566) nleep/row_max_std 65.0683 (51.5192) nleep/row_min_mean 1520.0720 (1522.7521) lr 1.0000e-05 eta 0:17:59
epoch [1/50] batch [200/203] time 0.085 (0.107) data 0.000 (0.002) loss 0.6701 (0.8512) teacher_loss 0.6383 (0.7210) loss_zs_kd 0.0009 (0.0008) loss_oracle 0.0021 (0.0016) kd_loss 0.0606 (0.2580) acc 75.0000 (74.0156) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3347 (0.3348) gate/usage_min 0.3313 (0.3312) gate/usage_std 0.0015 (0.0015) teacher/entropy 1.0380 (0.8402) teacher/usage_max 0.3814 (0.4031) teacher/usage_min 0.2834 (0.2736) teacher/usage_std 0.0400 (0.0558) nleep/row_max_mean 1520.7224 (1524.7290) nleep/row_max_std 42.3358 (52.1343) nleep/row_min_mean 1519.9492 (1522.4865) lr 1.0000e-05 eta 0:17:42
Evaluate on the *val* set
=> result
* total: 2,795
* correct: 2,199
* accuracy: 78.7%
* error: 21.3%
* macro_f1: 82.4%
Checkpoint saved to icml/multi-dg/tuning/19_ema_teacherupdate/TRIP/vlcs/b32_ep50/ViT-B16/c/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 1,415
* correct: 1,415
* accuracy: 100.0%
* error: 0.0%
* macro_f1: 100.0%
Checkpoint saved to icml/multi-dg/tuning/19_ema_teacherupdate/TRIP/vlcs/b32_ep50/ViT-B16/c/seed_1/warmup_1/prompt_learner/model-best-test.pth.tar
******* Domain c best val acc:      78.7%, epoch: 1 *******
******* Domain c best val test acc: 100.0%, epoch: 1 *******
******* Domain c best test acc:     100.0%, epoch: 1 *******
epoch [2/50] batch [20/203] time 0.106 (0.122) data 0.000 (0.021) loss 1.0928 (0.8057) teacher_loss 0.8178 (0.6582) loss_zs_kd 0.0083 (0.0088) loss_oracle 0.3237 (0.1553) kd_loss 0.2181 (0.1309) acc 68.7500 (77.3438) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3353 (0.3351) gate/usage_min 0.3307 (0.3311) gate/usage_std 0.0020 (0.0017) teacher/entropy 0.8780 (0.9666) teacher/usage_max 0.4482 (0.4433) teacher/usage_min 0.1142 (0.2258) teacher/usage_std 0.1550 (0.0931) nleep/row_max_mean 1520.8929 (1516.8455) nleep/row_max_std 71.0065 (62.2240) nleep/row_min_mean 1519.0187 (1515.6844) lr 2.0000e-03 eta 0:20:08
epoch [2/50] batch [40/203] time 0.099 (0.115) data 0.000 (0.011) loss 0.9015 (0.8634) teacher_loss 0.4113 (0.5958) loss_zs_kd 0.0166 (0.0124) loss_oracle 0.3462 (0.2180) kd_loss 0.6176 (0.3050) acc 81.2500 (78.9844) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3353 (0.3352) gate/usage_min 0.3296 (0.3306) gate/usage_std 0.0026 (0.0020) teacher/entropy 0.4769 (0.7912) teacher/usage_max 0.7509 (0.5450) teacher/usage_min 0.0808 (0.1487) teacher/usage_std 0.2974 (0.1677) nleep/row_max_mean 1502.1294 (1519.1134) nleep/row_max_std 74.1625 (59.6346) nleep/row_min_mean 1498.3333 (1516.9907) lr 2.0000e-03 eta 0:18:57
epoch [2/50] batch [60/203] time 0.097 (0.111) data 0.000 (0.007) loss 1.0285 (0.9352) teacher_loss 0.4627 (0.5842) loss_zs_kd 0.0355 (0.0158) loss_oracle 0.2474 (0.2411) kd_loss 0.8487 (0.4450) acc 78.1250 (79.5833) gate/entropy 1.0986 (1.0986) gate/usage_max 0.3366 (0.3354) gate/usage_min 0.3289 (0.3301) gate/usage_std 0.0033 (0.0023) teacher/entropy 0.2416 (0.6500) teacher/usage_max 0.9108 (0.6372) teacher/usage_min 0.0376 (0.1185) teacher/usage_std 0.4083 (0.2275) nleep/row_max_mean 1524.5442 (1520.2727) nleep/row_max_std 65.6502 (60.2033) nleep/row_min_mean 1517.7360 (1517.1424) lr 2.0000e-03 eta 0:18:17
epoch [2/50] batch [80/203] time 0.183 (0.109) data 0.002 (0.006) loss 1.1060 (0.9637) teacher_loss 0.5804 (0.5685) loss_zs_kd 0.0232 (0.0171) loss_oracle 0.2624 (0.2382) kd_loss 0.7656 (0.5351) acc 75.0000 (79.6484) gate/entropy 1.0985 (1.0986) gate/usage_max 0.3386 (0.3359) gate/usage_min 0.3280 (0.3297) gate/usage_std 0.0044 (0.0027) teacher/entropy 0.3218 (0.5582) teacher/usage_max 0.8047 (0.6924) teacher/usage_min 0.0888 (0.1022) teacher/usage_std 0.3334 (0.2635) nleep/row_max_mean 1503.2180 (1520.0974) nleep/row_max_std 75.6597 (61.2722) nleep/row_min_mean 1495.8005 (1515.9184) lr 2.0000e-03 eta 0:17:57
epoch [2/50] batch [100/203] time 0.095 (0.109) data 0.000 (0.005) loss 0.9290 (0.9940) teacher_loss 0.4199 (0.5665) loss_zs_kd 0.0231 (0.0189) loss_oracle 0.2415 (0.2510) kd_loss 0.7537 (0.5851) acc 87.5000 (79.6250) gate/entropy 1.0985 (1.0986) gate/usage_max 0.3411 (0.3367) gate/usage_min 0.3272 (0.3292) gate/usage_std 0.0058 (0.0032) teacher/entropy 0.3307 (0.5065) teacher/usage_max 0.7692 (0.7178) teacher/usage_min 0.0552 (0.0932) teacher/usage_std 0.3121 (0.2800) nleep/row_max_mean 1503.9714 (1518.8138) nleep/row_max_std 75.0364 (62.5023) nleep/row_min_mean 1494.0887 (1513.6624) lr 2.0000e-03 eta 0:17:54
epoch [2/50] batch [120/203] time 0.114 (0.107) data 0.000 (0.004) loss 1.0816 (1.0238) teacher_loss 0.4019 (0.5621) loss_zs_kd 0.0197 (0.0216) loss_oracle 0.5119 (0.2809) kd_loss 0.8277 (0.6210) acc 90.6250 (79.8958) gate/entropy 1.0984 (1.0985) gate/usage_max 0.3438 (0.3377) gate/usage_min 0.3262 (0.3288) gate/usage_std 0.0075 (0.0037) teacher/entropy 0.2510 (0.4689) teacher/usage_max 0.7885 (0.7296) teacher/usage_min 0.0185 (0.0845) teacher/usage_std 0.3297 (0.2878) nleep/row_max_mean 1513.7196 (1517.6443) nleep/row_max_std 66.5792 (63.5766) nleep/row_min_mean 1501.4230 (1511.4601) lr 2.0000e-03 eta 0:17:33
epoch [2/50] batch [140/203] time 0.102 (0.106) data 0.000 (0.003) loss 1.2823 (1.0432) teacher_loss 0.4875 (0.5458) loss_zs_kd 0.0550 (0.0240) loss_oracle 0.6680 (0.3202) kd_loss 0.8666 (0.6506) acc 84.3750 (80.9152) gate/entropy 1.0982 (1.0985) gate/usage_max 0.3471 (0.3388) gate/usage_min 0.3251 (0.3284) gate/usage_std 0.0098 (0.0045) teacher/entropy 0.2191 (0.4385) teacher/usage_max 0.5768 (0.7177) teacher/usage_min 0.0010 (0.0746) teacher/usage_std 0.2433 (0.2846) nleep/row_max_mean 1507.0715 (1516.5042) nleep/row_max_std 58.1912 (63.9085) nleep/row_min_mean 1492.0713 (1509.2670) lr 2.0000e-03 eta 0:17:19
epoch [2/50] batch [160/203] time 0.089 (0.105) data 0.000 (0.003) loss 0.9843 (1.0476) teacher_loss 0.2412 (0.5147) loss_zs_kd 0.0645 (0.0278) loss_oracle 0.5058 (0.3527) kd_loss 0.9157 (0.6852) acc 90.6250 (82.3633) gate/entropy 1.0980 (1.0984) gate/usage_max 0.3501 (0.3401) gate/usage_min 0.3242 (0.3279) gate/usage_std 0.0119 (0.0053) teacher/entropy 0.1776 (0.4037) teacher/usage_max 0.5714 (0.6981) teacher/usage_min 0.0027 (0.0666) teacher/usage_std 0.2412 (0.2787) nleep/row_max_mean 1507.6863 (1516.0893) nleep/row_max_std 85.7381 (64.0535) nleep/row_min_mean 1494.2017 (1507.8822) lr 2.0000e-03 eta 0:17:11
epoch [2/50] batch [180/203] time 0.093 (0.104) data 0.000 (0.003) loss 0.9381 (1.0495) teacher_loss 0.2212 (0.4912) loss_zs_kd 0.0467 (0.0300) loss_oracle 0.4455 (0.3745) kd_loss 0.9416 (0.7123) acc 93.7500 (83.4201) gate/entropy 1.0978 (1.0984) gate/usage_max 0.3526 (0.3413) gate/usage_min 0.3235 (0.3274) gate/usage_std 0.0136 (0.0061) teacher/entropy 0.1333 (0.3757) teacher/usage_max 0.6196 (0.6865) teacher/usage_min 0.0277 (0.0611) teacher/usage_std 0.2420 (0.2747) nleep/row_max_mean 1525.7302 (1515.7481) nleep/row_max_std 54.3745 (64.0556) nleep/row_min_mean 1511.2017 (1506.8042) lr 2.0000e-03 eta 0:16:59
epoch [2/50] batch [200/203] time 0.121 (0.103) data 0.000 (0.002) loss 1.4048 (1.0556) teacher_loss 0.6127 (0.4789) loss_zs_kd 0.0892 (0.0322) loss_oracle 0.6135 (0.3898) kd_loss 0.8816 (0.7314) acc 87.5000 (84.0938) gate/entropy 1.0976 (1.0983) gate/usage_max 0.3551 (0.3426) gate/usage_min 0.3224 (0.3270) gate/usage_std 0.0154 (0.0070) teacher/entropy 0.1837 (0.3555) teacher/usage_max 0.6865 (0.6751) teacher/usage_min 0.0599 (0.0605) teacher/usage_std 0.2619 (0.2689) nleep/row_max_mean 1524.8322 (1515.3676) nleep/row_max_std 54.0237 (64.3092) nleep/row_min_mean 1509.5638 (1505.9689) lr 2.0000e-03 eta 0:16:45
Evaluate on the *val* set
=> result
* total: 2,795
* correct: 2,112
* accuracy: 75.6%
* error: 24.4%
* macro_f1: 79.9%
Evaluate on the *test* set
=> result
* total: 1,415
* correct: 1,413
* accuracy: 99.9%
* error: 0.1%
* macro_f1: 99.7%
******* Domain c best val acc:      78.7%, epoch: 1 *******
******* Domain c best val test acc: 100.0%, epoch: 1 *******
******* Domain c best test acc:     100.0%, epoch: 1 *******
epoch [3/50] batch [20/203] time 0.104 (0.118) data 0.000 (0.013) loss 1.3967 (1.0238) teacher_loss 0.6361 (0.2670) loss_zs_kd 0.0664 (0.0499) loss_oracle 0.5490 (0.5721) kd_loss 0.9058 (0.8917) acc 78.1250 (92.0312) gate/entropy 1.0972 (1.0974) gate/usage_max 0.3581 (0.3569) gate/usage_min 0.3208 (0.3215) gate/usage_std 0.0175 (0.0166) teacher/entropy 0.1763 (0.1824) teacher/usage_max 0.4910 (0.5794) teacher/usage_min 0.1088 (0.0712) teacher/usage_std 0.1630 (0.2123) nleep/row_max_mean 1494.3097 (1514.0498) nleep/row_max_std 71.9917 (61.5254) nleep/row_min_mean 1479.8931 (1499.5615) lr 1.9980e-03 eta 0:19:03
epoch [3/50] batch [40/203] time 0.091 (0.114) data 0.000 (0.007) loss 1.1020 (1.0423) teacher_loss 0.3548 (0.2759) loss_zs_kd 0.0826 (0.0590) loss_oracle 0.4580 (0.5676) kd_loss 0.9538 (0.9063) acc 87.5000 (92.1094) gate/entropy 1.0969 (1.0972) gate/usage_max 0.3611 (0.3583) gate/usage_min 0.3188 (0.3206) gate/usage_std 0.0196 (0.0177) teacher/entropy 0.1234 (0.1671) teacher/usage_max 0.5153 (0.5786) teacher/usage_min 0.0055 (0.0603) teacher/usage_std 0.2323 (0.2168) nleep/row_max_mean 1503.2427 (1512.6084) nleep/row_max_std 75.1737 (63.2004) nleep/row_min_mean 1487.0204 (1497.1814) lr 1.9980e-03 eta 0:18:22
epoch [3/50] batch [60/203] time 0.113 (0.110) data 0.001 (0.005) loss 0.9438 (1.0342) teacher_loss 0.1973 (0.2721) loss_zs_kd 0.0443 (0.0589) loss_oracle 0.4854 (0.5458) kd_loss 0.9633 (0.9196) acc 96.8750 (92.3438) gate/entropy 1.0966 (1.0971) gate/usage_max 0.3635 (0.3597) gate/usage_min 0.3171 (0.3198) gate/usage_std 0.0214 (0.0186) teacher/entropy 0.1027 (0.1514) teacher/usage_max 0.5809 (0.5829) teacher/usage_min 0.0275 (0.0502) teacher/usage_std 0.2296 (0.2230) nleep/row_max_mean 1511.5139 (1513.8484) nleep/row_max_std 69.9729 (61.7351) nleep/row_min_mean 1493.1045 (1497.4180) lr 1.9980e-03 eta 0:17:42
epoch [3/50] batch [80/203] time 0.097 (0.106) data 0.000 (0.004) loss 1.0890 (1.0366) teacher_loss 0.3317 (0.2799) loss_zs_kd 0.0382 (0.0579) loss_oracle 0.5352 (0.5279) kd_loss 0.9412 (0.9276) acc 84.3750 (91.6016) gate/entropy 1.0962 (1.0969) gate/usage_max 0.3660 (0.3610) gate/usage_min 0.3155 (0.3189) gate/usage_std 0.0231 (0.0196) teacher/entropy 0.1116 (0.1407) teacher/usage_max 0.6585 (0.5904) teacher/usage_min 0.0351 (0.0401) teacher/usage_std 0.2552 (0.2305) nleep/row_max_mean 1516.5325 (1513.4928) nleep/row_max_std 55.5511 (61.8611) nleep/row_min_mean 1498.3330 (1496.4937) lr 1.9980e-03 eta 0:17:08
epoch [3/50] batch [100/203] time 0.095 (0.104) data 0.000 (0.003) loss 1.1471 (1.0421) teacher_loss 0.4067 (0.2863) loss_zs_kd 0.0820 (0.0567) loss_oracle 0.4687 (0.5277) kd_loss 0.9302 (0.9273) acc 87.5000 (91.5000) gate/entropy 1.0958 (1.0967) gate/usage_max 0.3688 (0.3623) gate/usage_min 0.3137 (0.3180) gate/usage_std 0.0251 (0.0205) teacher/entropy 0.1315 (0.1387) teacher/usage_max 0.5717 (0.5962) teacher/usage_min 0.0056 (0.0370) teacher/usage_std 0.2396 (0.2342) nleep/row_max_mean 1497.4653 (1512.2381) nleep/row_max_std 79.0705 (62.0572) nleep/row_min_mean 1479.7922 (1494.9075) lr 1.9980e-03 eta 0:16:44
epoch [3/50] batch [120/203] time 0.090 (0.103) data 0.000 (0.002) loss 1.1403 (1.0515) teacher_loss 0.3654 (0.2945) loss_zs_kd 0.0373 (0.0561) loss_oracle 0.5653 (0.5263) kd_loss 0.9473 (0.9315) acc 93.7500 (91.1979) gate/entropy 1.0954 (1.0965) gate/usage_max 0.3716 (0.3636) gate/usage_min 0.3117 (0.3171) gate/usage_std 0.0271 (0.0214) teacher/entropy 0.0966 (0.1323) teacher/usage_max 0.6622 (0.5999) teacher/usage_min 0.0017 (0.0333) teacher/usage_std 0.2697 (0.2371) nleep/row_max_mean 1499.6750 (1511.2834) nleep/row_max_std 63.4223 (62.3302) nleep/row_min_mean 1479.7598 (1493.6005) lr 1.9980e-03 eta 0:16:30
epoch [3/50] batch [140/203] time 0.096 (0.102) data 0.000 (0.002) loss 0.9445 (1.0594) teacher_loss 0.2037 (0.3028) loss_zs_kd 0.0403 (0.0551) loss_oracle 0.4561 (0.5210) kd_loss 0.9851 (0.9370) acc 93.7500 (91.0491) gate/entropy 1.0948 (1.0963) gate/usage_max 0.3746 (0.3650) gate/usage_min 0.3094 (0.3162) gate/usage_std 0.0293 (0.0224) teacher/entropy 0.0714 (0.1260) teacher/usage_max 0.5583 (0.5972) teacher/usage_min 0.0018 (0.0297) teacher/usage_std 0.2394 (0.2383) nleep/row_max_mean 1499.9556 (1510.2879) nleep/row_max_std 79.4387 (63.2027) nleep/row_min_mean 1482.1281 (1492.3802) lr 1.9980e-03 eta 0:16:17
epoch [3/50] batch [160/203] time 0.087 (0.101) data 0.000 (0.002) loss 1.1853 (1.0574) teacher_loss 0.3789 (0.3013) loss_zs_kd 0.0544 (0.0549) loss_oracle 0.5476 (0.5167) kd_loss 1.0108 (0.9406) acc 87.5000 (91.1914) gate/entropy 1.0943 (1.0961) gate/usage_max 0.3771 (0.3664) gate/usage_min 0.3073 (0.3152) gate/usage_std 0.0311 (0.0234) teacher/entropy 0.0202 (0.1216) teacher/usage_max 0.6863 (0.5944) teacher/usage_min 0.0003 (0.0277) teacher/usage_std 0.2804 (0.2385) nleep/row_max_mean 1514.5173 (1510.3510) nleep/row_max_std 53.2093 (62.8430) nleep/row_min_mean 1490.9364 (1492.1213) lr 1.9980e-03 eta 0:16:09
epoch [3/50] batch [180/203] time 0.100 (0.102) data 0.000 (0.002) loss 0.9854 (1.0576) teacher_loss 0.1809 (0.2979) loss_zs_kd 0.0333 (0.0550) loss_oracle 0.6361 (0.5184) kd_loss 0.9395 (0.9459) acc 96.8750 (91.3194) gate/entropy 1.0938 (1.0959) gate/usage_max 0.3794 (0.3677) gate/usage_min 0.3053 (0.3142) gate/usage_std 0.0329 (0.0244) teacher/entropy 0.1343 (0.1159) teacher/usage_max 0.5665 (0.5910) teacher/usage_min 0.0009 (0.0255) teacher/usage_std 0.2413 (0.2387) nleep/row_max_mean 1522.7115 (1510.2888) nleep/row_max_std 58.7172 (63.0864) nleep/row_min_mean 1503.4110 (1491.7197) lr 1.9980e-03 eta 0:16:16
epoch [3/50] batch [200/203] time 0.083 (0.101) data 0.000 (0.002) loss 1.1526 (1.0557) teacher_loss 0.3335 (0.2912) loss_zs_kd 0.0615 (0.0547) loss_oracle 0.6059 (0.5253) kd_loss 0.9708 (0.9491) acc 87.5000 (91.4531) gate/entropy 1.0933 (1.0956) gate/usage_max 0.3822 (0.3690) gate/usage_min 0.3033 (0.3132) gate/usage_std 0.0348 (0.0253) teacher/entropy 0.0597 (0.1116) teacher/usage_max 0.6469 (0.5895) teacher/usage_min 0.0001 (0.0237) teacher/usage_std 0.2644 (0.2391) nleep/row_max_mean 1518.1335 (1509.9084) nleep/row_max_std 49.3743 (63.4755) nleep/row_min_mean 1493.6677 (1491.0001) lr 1.9980e-03 eta 0:16:07
Evaluate on the *val* set
=> result
* total: 2,795
* correct: 2,312
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 86.3%
Checkpoint saved to icml/multi-dg/tuning/19_ema_teacherupdate/TRIP/vlcs/b32_ep50/ViT-B16/c/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 1,415
* correct: 1,415
* accuracy: 100.0%
* error: 0.0%
* macro_f1: 100.0%
******* Domain c best val acc:      82.7%, epoch: 3 *******
******* Domain c best val test acc: 100.0%, epoch: 3 *******
******* Domain c best test acc:     100.0%, epoch: 1 *******
epoch [4/50] batch [20/203] time 0.111 (0.121) data 0.000 (0.013) loss 0.8277 (1.0707) teacher_loss 0.1149 (0.2910) loss_zs_kd 0.0626 (0.0480) loss_oracle 0.3378 (0.5203) kd_loss 1.0251 (0.9909) acc 100.0000 (91.0938) gate/entropy 1.0926 (1.0929) gate/usage_max 0.3850 (0.3839) gate/usage_min 0.3011 (0.3020) gate/usage_std 0.0369 (0.0361) teacher/entropy 0.0462 (0.0652) teacher/usage_max 0.5740 (0.5492) teacher/usage_min 0.0003 (0.0123) teacher/usage_std 0.2431 (0.2328) nleep/row_max_mean 1503.2992 (1501.2961) nleep/row_max_std 65.4363 (66.9980) nleep/row_min_mean 1480.7495 (1479.6493) lr 1.9921e-03 eta 0:19:14
epoch [4/50] batch [40/203] time 0.105 (0.111) data 0.001 (0.007) loss 0.8818 (1.0096) teacher_loss 0.1170 (0.2377) loss_zs_kd 0.0573 (0.0579) loss_oracle 0.5195 (0.4848) kd_loss 0.9529 (1.0011) acc 96.8750 (93.0469) gate/entropy 1.0922 (1.0926) gate/usage_max 0.3868 (0.3850) gate/usage_min 0.2993 (0.3011) gate/usage_std 0.0383 (0.0369) teacher/entropy 0.0793 (0.0528) teacher/usage_max 0.6059 (0.5600) teacher/usage_min 0.0052 (0.0076) teacher/usage_std 0.2484 (0.2377) nleep/row_max_mean 1507.6854 (1505.4083) nleep/row_max_std 67.2138 (64.3824) nleep/row_min_mean 1486.8245 (1483.2586) lr 1.9921e-03 eta 0:17:30
epoch [4/50] batch [60/203] time 0.089 (0.107) data 0.001 (0.005) loss 0.9750 (0.9894) teacher_loss 0.2273 (0.2167) loss_zs_kd 0.0497 (0.0598) loss_oracle 0.4837 (0.4803) kd_loss 0.9618 (1.0054) acc 90.6250 (93.6979) gate/entropy 1.0918 (1.0924) gate/usage_max 0.3880 (0.3858) gate/usage_min 0.2980 (0.3003) gate/usage_std 0.0392 (0.0375) teacher/entropy 0.0789 (0.0480) teacher/usage_max 0.5542 (0.5594) teacher/usage_min 0.0004 (0.0077) teacher/usage_std 0.2395 (0.2384) nleep/row_max_mean 1490.8629 (1505.8190) nleep/row_max_std 65.8704 (63.6902) nleep/row_min_mean 1468.3218 (1483.3917) lr 1.9921e-03 eta 0:16:53
epoch [4/50] batch [80/203] time 0.101 (0.104) data 0.000 (0.004) loss 1.0861 (0.9815) teacher_loss 0.3110 (0.2076) loss_zs_kd 0.0609 (0.0602) loss_oracle 0.4919 (0.4788) kd_loss 0.9976 (1.0088) acc 84.3750 (93.8672) gate/entropy 1.0916 (1.0922) gate/usage_max 0.3889 (0.3864) gate/usage_min 0.2967 (0.2995) gate/usage_std 0.0400 (0.0380) teacher/entropy 0.0390 (0.0435) teacher/usage_max 0.5739 (0.5648) teacher/usage_min 0.0316 (0.0072) teacher/usage_std 0.2256 (0.2402) nleep/row_max_mean 1512.0980 (1504.8746) nleep/row_max_std 57.9419 (64.5003) nleep/row_min_mean 1488.9377 (1482.2265) lr 1.9921e-03 eta 0:16:20
epoch [4/50] batch [100/203] time 0.095 (0.102) data 0.000 (0.003) loss 0.9773 (0.9814) teacher_loss 0.1558 (0.2021) loss_zs_kd 0.0950 (0.0624) loss_oracle 0.5109 (0.4844) kd_loss 1.0371 (1.0118) acc 93.7500 (94.0000) gate/entropy 1.0912 (1.0921) gate/usage_max 0.3900 (0.3870) gate/usage_min 0.2950 (0.2988) gate/usage_std 0.0409 (0.0385) teacher/entropy 0.0176 (0.0415) teacher/usage_max 0.5306 (0.5635) teacher/usage_min 0.0003 (0.0071) teacher/usage_std 0.2368 (0.2401) nleep/row_max_mean 1498.7273 (1504.2058) nleep/row_max_std 63.7338 (65.2006) nleep/row_min_mean 1476.3850 (1481.5320) lr 1.9921e-03 eta 0:15:58
epoch [4/50] batch [120/203] time 0.117 (0.102) data 0.000 (0.002) loss 0.9427 (0.9829) teacher_loss 0.1520 (0.2015) loss_zs_kd 0.0515 (0.0644) loss_oracle 0.4781 (0.4852) kd_loss 1.0518 (1.0132) acc 96.8750 (94.1667) gate/entropy 1.0909 (1.0919) gate/usage_max 0.3912 (0.3876) gate/usage_min 0.2938 (0.2981) gate/usage_std 0.0418 (0.0390) teacher/entropy 0.0087 (0.0390) teacher/usage_max 0.5627 (0.5626) teacher/usage_min 0.0007 (0.0071) teacher/usage_std 0.2408 (0.2398) nleep/row_max_mean 1498.8379 (1503.7547) nleep/row_max_std 65.4606 (65.1040) nleep/row_min_mean 1474.7582 (1481.0756) lr 1.9921e-03 eta 0:16:04
epoch [4/50] batch [140/203] time 0.112 (0.105) data 0.000 (0.002) loss 1.0190 (0.9777) teacher_loss 0.2196 (0.1953) loss_zs_kd 0.0629 (0.0652) loss_oracle 0.5067 (0.4860) kd_loss 1.0292 (1.0137) acc 90.6250 (94.4196) gate/entropy 1.0907 (1.0917) gate/usage_max 0.3917 (0.3882) gate/usage_min 0.2926 (0.2974) gate/usage_std 0.0424 (0.0394) teacher/entropy 0.0147 (0.0375) teacher/usage_max 0.5046 (0.5624) teacher/usage_min 0.0001 (0.0071) teacher/usage_std 0.2356 (0.2398) nleep/row_max_mean 1496.6375 (1503.5756) nleep/row_max_std 65.9396 (64.4678) nleep/row_min_mean 1471.4861 (1480.8221) lr 1.9921e-03 eta 0:16:27
epoch [4/50] batch [160/203] time 0.111 (0.105) data 0.001 (0.002) loss 1.1595 (0.9732) teacher_loss 0.3460 (0.1912) loss_zs_kd 0.0596 (0.0649) loss_oracle 0.5063 (0.4830) kd_loss 1.0611 (1.0160) acc 90.6250 (94.5312) gate/entropy 1.0904 (1.0916) gate/usage_max 0.3926 (0.3887) gate/usage_min 0.2914 (0.2967) gate/usage_std 0.0431 (0.0398) teacher/entropy 0.0028 (0.0354) teacher/usage_max 0.5934 (0.5666) teacher/usage_min 0.0002 (0.0065) teacher/usage_std 0.2476 (0.2414) nleep/row_max_mean 1487.4594 (1503.1669) nleep/row_max_std 79.0150 (64.6739) nleep/row_min_mean 1462.3950 (1480.3718) lr 1.9921e-03 eta 0:16:24
epoch [4/50] batch [180/203] time 0.114 (0.105) data 0.000 (0.002) loss 0.8844 (0.9713) teacher_loss 0.1157 (0.1903) loss_zs_kd 0.0472 (0.0647) loss_oracle 0.4022 (0.4803) kd_loss 1.0880 (1.0170) acc 96.8750 (94.5486) gate/entropy 1.0902 (1.0914) gate/usage_max 0.3934 (0.3892) gate/usage_min 0.2902 (0.2960) gate/usage_std 0.0438 (0.0403) teacher/entropy 0.0011 (0.0341) teacher/usage_max 0.7187 (0.5699) teacher/usage_min 0.0001 (0.0069) teacher/usage_std 0.2957 (0.2420) nleep/row_max_mean 1494.2290 (1502.6922) nleep/row_max_std 70.7635 (64.7379) nleep/row_min_mean 1471.6926 (1479.8626) lr 1.9921e-03 eta 0:16:24
epoch [4/50] batch [200/203] time 0.089 (0.104) data 0.000 (0.002) loss 1.1608 (0.9706) teacher_loss 0.3902 (0.1896) loss_zs_kd 0.0706 (0.0650) loss_oracle 0.4662 (0.4787) kd_loss 1.0043 (1.0183) acc 93.7500 (94.6562) gate/entropy 1.0898 (1.0913) gate/usage_max 0.3943 (0.3897) gate/usage_min 0.2888 (0.2954) gate/usage_std 0.0446 (0.0407) teacher/entropy 0.0299 (0.0331) teacher/usage_max 0.5260 (0.5712) teacher/usage_min 0.0011 (0.0066) teacher/usage_std 0.2359 (0.2426) nleep/row_max_mean 1501.8011 (1501.9338) nleep/row_max_std 59.7128 (64.8954) nleep/row_min_mean 1478.7717 (1479.0908) lr 1.9921e-03 eta 0:16:16
Evaluate on the *val* set
=> result
* total: 2,795
* correct: 2,312
* accuracy: 82.7%
* error: 17.3%
* macro_f1: 86.9%
Evaluate on the *test* set
=> result
* total: 1,415
* correct: 1,415
* accuracy: 100.0%
* error: 0.0%
* macro_f1: 100.0%
******* Domain c best val acc:      82.7%, epoch: 3 *******
******* Domain c best val test acc: 100.0%, epoch: 3 *******
******* Domain c best test acc:     100.0%, epoch: 1 *******
epoch [5/50] batch [20/203] time 0.088 (0.142) data 0.000 (0.017) loss 0.8949 (0.9526) teacher_loss 0.1284 (0.1750) loss_zs_kd 0.0653 (0.0691) loss_oracle 0.4167 (0.4739) kd_loss 1.0509 (1.0123) acc 93.7500 (95.1562) gate/entropy 1.0895 (1.0896) gate/usage_max 0.3952 (0.3948) gate/usage_min 0.2874 (0.2880) gate/usage_std 0.0454 (0.0451) teacher/entropy 0.0068 (0.0299) teacher/usage_max 0.5926 (0.5792) teacher/usage_min 0.0007 (0.0071) teacher/usage_std 0.2472 (0.2443) nleep/row_max_mean 1499.4116 (1497.3217) nleep/row_max_std 75.1914 (63.6404) nleep/row_min_mean 1477.5063 (1475.0132) lr 1.9823e-03 eta 0:22:03
epoch [5/50] batch [40/203] time 0.165 (0.145) data 0.000 (0.009) loss 1.0615 (0.9561) teacher_loss 0.2687 (0.1723) loss_zs_kd 0.0846 (0.0676) loss_oracle 0.4793 (0.4866) kd_loss 1.0218 (1.0134) acc 93.7500 (95.3125) gate/entropy 1.0893 (1.0895) gate/usage_max 0.3957 (0.3951) gate/usage_min 0.2863 (0.2874) gate/usage_std 0.0460 (0.0454) teacher/entropy 0.0074 (0.0295) teacher/usage_max 0.5313 (0.5744) teacher/usage_min 0.0014 (0.0104) teacher/usage_std 0.2362 (0.2408) nleep/row_max_mean 1489.6947 (1497.2076) nleep/row_max_std 72.3497 (64.5506) nleep/row_min_mean 1468.3770 (1474.6385) lr 1.9823e-03 eta 0:22:31
epoch [5/50] batch [60/203] time 0.118 (0.132) data 0.001 (0.006) loss 1.0404 (0.9534) teacher_loss 0.2571 (0.1693) loss_zs_kd 0.0527 (0.0668) loss_oracle 0.4772 (0.4870) kd_loss 1.0367 (1.0143) acc 90.6250 (95.4167) gate/entropy 1.0891 (1.0894) gate/usage_max 0.3961 (0.3954) gate/usage_min 0.2854 (0.2869) gate/usage_std 0.0464 (0.0456) teacher/entropy 0.0269 (0.0291) teacher/usage_max 0.6099 (0.5650) teacher/usage_min 0.0151 (0.0120) teacher/usage_std 0.2446 (0.2379) nleep/row_max_mean 1490.7578 (1500.6669) nleep/row_max_std 81.6071 (61.7797) nleep/row_min_mean 1468.8417 (1477.8440) lr 1.9823e-03 eta 0:20:29
epoch [5/50] batch [80/203] time 0.113 (0.126) data 0.000 (0.005) loss 0.8653 (0.9489) teacher_loss 0.0534 (0.1613) loss_zs_kd 0.0688 (0.0685) loss_oracle 0.5124 (0.4923) kd_loss 1.0426 (1.0144) acc 100.0000 (95.5859) gate/entropy 1.0889 (1.0893) gate/usage_max 0.3963 (0.3956) gate/usage_min 0.2844 (0.2864) gate/usage_std 0.0467 (0.0459) teacher/entropy 0.0113 (0.0290) teacher/usage_max 0.5923 (0.5695) teacher/usage_min 0.0018 (0.0130) teacher/usage_std 0.2465 (0.2383) nleep/row_max_mean 1493.5051 (1500.5827) nleep/row_max_std 76.5307 (62.4916) nleep/row_min_mean 1469.3199 (1477.5127) lr 1.9823e-03 eta 0:19:27
epoch [5/50] batch [100/203] time 0.100 (0.121) data 0.000 (0.004) loss 0.8033 (0.9519) teacher_loss 0.0239 (0.1664) loss_zs_kd 0.0522 (0.0677) loss_oracle 0.5019 (0.4880) kd_loss 1.0048 (1.0153) acc 100.0000 (95.5000) gate/entropy 1.0887 (1.0892) gate/usage_max 0.3969 (0.3958) gate/usage_min 0.2834 (0.2859) gate/usage_std 0.0473 (0.0461) teacher/entropy 0.0260 (0.0291) teacher/usage_max 0.5362 (0.5684) teacher/usage_min 0.0586 (0.0148) teacher/usage_std 0.2015 (0.2369) nleep/row_max_mean 1503.1227 (1500.1184) nleep/row_max_std 65.4274 (63.2772) nleep/row_min_mean 1479.6069 (1477.0030) lr 1.9823e-03 eta 0:18:41
epoch [5/50] batch [120/203] time 0.103 (0.118) data 0.000 (0.003) loss 0.9312 (0.9534) teacher_loss 0.1707 (0.1653) loss_zs_kd 0.0628 (0.0690) loss_oracle 0.4766 (0.4927) kd_loss 0.9816 (1.0146) acc 93.7500 (95.5469) gate/entropy 1.0886 (1.0891) gate/usage_max 0.3970 (0.3960) gate/usage_min 0.2827 (0.2854) gate/usage_std 0.0476 (0.0464) teacher/entropy 0.0472 (0.0298) teacher/usage_max 0.5196 (0.5688) teacher/usage_min 0.0162 (0.0164) teacher/usage_std 0.2254 (0.2365) nleep/row_max_mean 1473.8251 (1498.9156) nleep/row_max_std 88.6045 (64.1191) nleep/row_min_mean 1452.7185 (1475.8386) lr 1.9823e-03 eta 0:18:10
epoch [5/50] batch [140/203] time 0.103 (0.116) data 0.000 (0.003) loss 0.8672 (0.9509) teacher_loss 0.0850 (0.1630) loss_zs_kd 0.0757 (0.0699) loss_oracle 0.4696 (0.4930) kd_loss 1.0192 (1.0130) acc 100.0000 (95.7366) gate/entropy 1.0884 (1.0890) gate/usage_max 0.3971 (0.3962) gate/usage_min 0.2820 (0.2849) gate/usage_std 0.0478 (0.0465) teacher/entropy 0.0280 (0.0327) teacher/usage_max 0.5297 (0.5687) teacher/usage_min 0.0332 (0.0196) teacher/usage_std 0.2156 (0.2347) nleep/row_max_mean 1511.4971 (1498.2932) nleep/row_max_std 47.1931 (64.9270) nleep/row_min_mean 1485.9744 (1475.2597) lr 1.9823e-03 eta 0:17:49
epoch [5/50] batch [160/203] time 0.099 (0.115) data 0.000 (0.002) loss 0.8342 (0.9529) teacher_loss 0.0816 (0.1670) loss_zs_kd 0.0605 (0.0695) loss_oracle 0.4117 (0.4911) kd_loss 1.0329 (1.0111) acc 100.0000 (95.6641) gate/entropy 1.0883 (1.0889) gate/usage_max 0.3975 (0.3963) gate/usage_min 0.2813 (0.2845) gate/usage_std 0.0482 (0.0467) teacher/entropy 0.0353 (0.0342) teacher/usage_max 0.6139 (0.5682) teacher/usage_min 0.0434 (0.0216) teacher/usage_std 0.2330 (0.2334) nleep/row_max_mean 1484.7822 (1497.1133) nleep/row_max_std 82.2009 (66.2787) nleep/row_min_mean 1466.5861 (1474.1087) lr 1.9823e-03 eta 0:17:33
epoch [5/50] batch [180/203] time 0.103 (0.113) data 0.000 (0.002) loss 0.8900 (0.9512) teacher_loss 0.1063 (0.1652) loss_zs_kd 0.0515 (0.0692) loss_oracle 0.5320 (0.4936) kd_loss 0.9840 (1.0092) acc 96.8750 (95.6944) gate/entropy 1.0881 (1.0889) gate/usage_max 0.3979 (0.3964) gate/usage_min 0.2807 (0.2841) gate/usage_std 0.0486 (0.0469) teacher/entropy 0.0685 (0.0360) teacher/usage_max 0.5583 (0.5682) teacher/usage_min 0.0344 (0.0227) teacher/usage_std 0.2202 (0.2328) nleep/row_max_mean 1497.4041 (1496.8740) nleep/row_max_std 71.6216 (66.3118) nleep/row_min_mean 1474.1340 (1473.8430) lr 1.9823e-03 eta 0:17:19
epoch [5/50] batch [200/203] time 0.095 (0.112) data 0.000 (0.002) loss 0.8558 (0.9512) teacher_loss 0.0558 (0.1646) loss_zs_kd 0.0735 (0.0696) loss_oracle 0.6187 (0.4958) kd_loss 0.9079 (1.0078) acc 96.8750 (95.6875) gate/entropy 1.0879 (1.0888) gate/usage_max 0.3985 (0.3966) gate/usage_min 0.2801 (0.2838) gate/usage_std 0.0490 (0.0471) teacher/entropy 0.1155 (0.0373) teacher/usage_max 0.5457 (0.5662) teacher/usage_min 0.0433 (0.0249) teacher/usage_std 0.2124 (0.2310) nleep/row_max_mean 1495.9814 (1497.0208) nleep/row_max_std 59.0239 (65.7857) nleep/row_min_mean 1473.4323 (1473.9769) lr 1.9823e-03 eta 0:17:05
Evaluate on the *val* set
=> result
* total: 2,795
* correct: 2,327
* accuracy: 83.3%
* error: 16.7%
* macro_f1: 87.2%
Checkpoint saved to icml/multi-dg/tuning/19_ema_teacherupdate/TRIP/vlcs/b32_ep50/ViT-B16/c/seed_1/warmup_1/prompt_learner/model-best.pth.tar
Evaluate on the *test* set
=> result
* total: 1,415
* correct: 1,415
* accuracy: 100.0%
* error: 0.0%
* macro_f1: 100.0%
******* Domain c best val acc:      83.3%, epoch: 5 *******
******* Domain c best val test acc: 100.0%, epoch: 5 *******
******* Domain c best test acc:     100.0%, epoch: 1 *******
epoch [6/50] batch [20/203] time 0.154 (0.130) data 0.001 (0.015) loss 0.8704 (0.9201) teacher_loss 0.1165 (0.1512) loss_zs_kd 0.0687 (0.0679) loss_oracle 0.4946 (0.4889) kd_loss 0.9446 (0.9810) acc 96.8750 (96.2500) gate/entropy 1.0877 (1.0878) gate/usage_max 0.3991 (0.3988) gate/usage_min 0.2794 (0.2796) gate/usage_std 0.0496 (0.0494) teacher/entropy 0.1006 (0.0580) teacher/usage_max 0.4709 (0.5393) teacher/usage_min 0.0911 (0.0536) teacher/usage_std 0.1718 (0.2091) nleep/row_max_mean 1514.0457 (1494.7558) nleep/row_max_std 41.5368 (63.7282) nleep/row_min_mean 1492.8358 (1471.8627) lr 1.9686e-03 eta 0:19:43
epoch [6/50] batch [40/203] time 0.103 (0.120) data 0.000 (0.008) loss 0.9316 (0.9403) teacher_loss 0.1242 (0.1599) loss_zs_kd 0.0884 (0.0730) loss_oracle 0.5590 (0.5102) kd_loss 0.9675 (0.9776) acc 96.8750 (95.5469) gate/entropy 1.0875 (1.0877) gate/usage_max 0.3996 (0.3991) gate/usage_min 0.2786 (0.2793) gate/usage_std 0.0501 (0.0496) teacher/entropy 0.0594 (0.0616) teacher/usage_max 0.5129 (0.5366) teacher/usage_min 0.0311 (0.0517) teacher/usage_std 0.2150 (0.2087) nleep/row_max_mean 1499.5295 (1496.2549) nleep/row_max_std 63.8051 (61.1547) nleep/row_min_mean 1475.2340 (1473.6127) lr 1.9686e-03 eta 0:18:11
epoch [6/50] batch [60/203] time 0.121 (0.115) data 0.001 (0.005) loss 0.9188 (0.9417) teacher_loss 0.1517 (0.1539) loss_zs_kd 0.0520 (0.0730) loss_oracle 0.5161 (0.5239) kd_loss 0.9660 (0.9789) acc 96.8750 (95.5208) gate/entropy 1.0874 (1.0876) gate/usage_max 0.3997 (0.3993) gate/usage_min 0.2780 (0.2789) gate/usage_std 0.0503 (0.0498) teacher/entropy 0.0551 (0.0618) teacher/usage_max 0.5220 (0.5343) teacher/usage_min 0.0111 (0.0513) teacher/usage_std 0.2289 (0.2082) nleep/row_max_mean 1502.4678 (1496.6194) nleep/row_max_std 54.0606 (60.9732) nleep/row_min_mean 1480.4152 (1474.0388) lr 1.9686e-03 eta 0:17:23
epoch [6/50] batch [80/203] time 0.103 (0.110) data 0.000 (0.004) loss 0.8591 (0.9369) teacher_loss 0.0605 (0.1534) loss_zs_kd 0.0605 (0.0709) loss_oracle 0.5176 (0.5144) kd_loss 1.0191 (0.9816) acc 100.0000 (95.7031) gate/entropy 1.0872 (1.0875) gate/usage_max 0.3998 (0.3994) gate/usage_min 0.2772 (0.2786) gate/usage_std 0.0506 (0.0499) teacher/entropy 0.0297 (0.0616) teacher/usage_max 0.5539 (0.5406) teacher/usage_min 0.0390 (0.0518) teacher/usage_std 0.2166 (0.2100) nleep/row_max_mean 1505.5828 (1495.4996) nleep/row_max_std 56.5019 (62.4217) nleep/row_min_mean 1480.5679 (1472.9286) lr 1.9686e-03 eta 0:16:38
epoch [6/50] batch [100/203] time 0.107 (0.108) data 0.000 (0.003) loss 0.9864 (0.9418) teacher_loss 0.2045 (0.1532) loss_zs_kd 0.0698 (0.0732) loss_oracle 0.5135 (0.5216) kd_loss 0.9806 (0.9825) acc 87.5000 (95.5625) gate/entropy 1.0872 (1.0874) gate/usage_max 0.3996 (0.3994) gate/usage_min 0.2769 (0.2783) gate/usage_std 0.0506 (0.0501) teacher/entropy 0.0541 (0.0608) teacher/usage_max 0.5441 (0.5406) teacher/usage_min 0.0076 (0.0506) teacher/usage_std 0.2336 (0.2105) nleep/row_max_mean 1476.4945 (1495.0968) nleep/row_max_std 84.3638 (62.8916) nleep/row_min_mean 1452.9305 (1472.4048) lr 1.9686e-03 eta 0:16:19
epoch [6/50] batch [120/203] time 0.097 (0.106) data 0.000 (0.003) loss 0.9920 (0.9461) teacher_loss 0.1734 (0.1544) loss_zs_kd 0.0919 (0.0731) loss_oracle 0.5392 (0.5254) kd_loss 1.0061 (0.9849) acc 93.7500 (95.6250) gate/entropy 1.0870 (1.0874) gate/usage_max 0.3999 (0.3995) gate/usage_min 0.2762 (0.2780) gate/usage_std 0.0510 (0.0502) teacher/entropy 0.0236 (0.0581) teacher/usage_max 0.5317 (0.5397) teacher/usage_min 0.0035 (0.0498) teacher/usage_std 0.2348 (0.2108) nleep/row_max_mean 1522.4948 (1495.2291) nleep/row_max_std 42.6815 (63.0296) nleep/row_min_mean 1497.3428 (1472.2398) lr 1.9686e-03 eta 0:15:59
epoch [6/50] batch [140/203] time 0.091 (0.105) data 0.000 (0.002) loss 1.0011 (0.9524) teacher_loss 0.1972 (0.1598) loss_zs_kd 0.0723 (0.0726) loss_oracle 0.5480 (0.5285) kd_loss 0.9875 (0.9842) acc 90.6250 (95.4241) gate/entropy 1.0870 (1.0873) gate/usage_max 0.3999 (0.3995) gate/usage_min 0.2759 (0.2777) gate/usage_std 0.0510 (0.0503) teacher/entropy 0.0380 (0.0582) teacher/usage_max 0.5010 (0.5390) teacher/usage_min 0.0118 (0.0500) teacher/usage_std 0.2274 (0.2105) nleep/row_max_mean 1494.8179 (1495.2257) nleep/row_max_std 61.9146 (62.9708) nleep/row_min_mean 1470.4075 (1472.2182) lr 1.9686e-03 eta 0:15:43
epoch [6/50] batch [160/203] time 0.108 (0.104) data 0.000 (0.002) loss 1.0951 (0.9577) teacher_loss 0.2789 (0.1631) loss_zs_kd 0.0620 (0.0723) loss_oracle 0.5953 (0.5327) kd_loss 0.9751 (0.9841) acc 90.6250 (95.4102) gate/entropy 1.0870 (1.0873) gate/usage_max 0.3998 (0.3996) gate/usage_min 0.2759 (0.2775) gate/usage_std 0.0510 (0.0504) teacher/entropy 0.0671 (0.0586) teacher/usage_max 0.4737 (0.5396) teacher/usage_min 0.0961 (0.0511) teacher/usage_std 0.1687 (0.2101) nleep/row_max_mean 1476.9722 (1494.4050) nleep/row_max_std 76.1054 (63.2646) nleep/row_min_mean 1453.4811 (1471.3563) lr 1.9686e-03 eta 0:15:33
epoch [6/50] batch [180/203] time 0.083 (0.107) data 0.000 (0.002) loss 0.8848 (0.9579) teacher_loss 0.0624 (0.1623) loss_zs_kd 0.0697 (0.0721) loss_oracle 0.6190 (0.5355) kd_loss 0.9561 (0.9836) acc 100.0000 (95.4688) gate/entropy 1.0868 (1.0873) gate/usage_max 0.4003 (0.3996) gate/usage_min 0.2754 (0.2773) gate/usage_std 0.0514 (0.0505) teacher/entropy 0.0952 (0.0594) teacher/usage_max 0.4450 (0.5364) teacher/usage_min 0.1137 (0.0529) teacher/usage_std 0.1553 (0.2082) nleep/row_max_mean 1499.7761 (1493.8073) nleep/row_max_std 66.9110 (63.3539) nleep/row_min_mean 1475.6233 (1470.7055) lr 1.9686e-03 eta 0:15:53
epoch [6/50] batch [200/203] time 0.159 (0.110) data 0.000 (0.002) loss 1.0340 (0.9636) teacher_loss 0.2510 (0.1691) loss_zs_kd 0.0556 (0.0723) loss_oracle 0.5016 (0.5339) kd_loss 1.0088 (0.9829) acc 93.7500 (95.2344) gate/entropy 1.0866 (1.0872) gate/usage_max 0.4007 (0.3997) gate/usage_min 0.2749 (0.2771) gate/usage_std 0.0518 (0.0506) teacher/entropy 0.0251 (0.0603) teacher/usage_max 0.4992 (0.5361) teacher/usage_min 0.0385 (0.0537) teacher/usage_std 0.2090 (0.2080) nleep/row_max_mean 1499.6610 (1493.5601) nleep/row_max_std 41.9896 (62.7891) nleep/row_min_mean 1474.2346 (1470.3454) lr 1.9686e-03 eta 0:16:24
Evaluate on the *val* set
=> result
* total: 2,795
* correct: 2,307
* accuracy: 82.5%
* error: 17.5%
* macro_f1: 86.6%
Evaluate on the *test* set
=> result
* total: 1,415
* correct: 1,414
* accuracy: 99.9%
* error: 0.1%
* macro_f1: 99.8%
******* Domain c best val acc:      83.3%, epoch: 5 *******
******* Domain c best val test acc: 100.0%, epoch: 5 *******
******* Domain c best test acc:     100.0%, epoch: 1 *******
epoch [7/50] batch [20/203] time 0.091 (0.119) data 0.000 (0.018) loss 0.8718 (0.9509) teacher_loss 0.1192 (0.1821) loss_zs_kd 0.0702 (0.0683) loss_oracle 0.5223 (0.4931) kd_loss 0.9127 (0.9762) acc 96.8750 (94.3750) gate/entropy 1.0863 (1.0865) gate/usage_max 0.4015 (0.4011) gate/usage_min 0.2739 (0.2743) gate/usage_std 0.0525 (0.0522) teacher/entropy 0.0793 (0.0553) teacher/usage_max 0.6446 (0.5349) teacher/usage_min 0.0238 (0.0387) teacher/usage_std 0.2534 (0.2168) nleep/row_max_mean 1505.8418 (1492.6059) nleep/row_max_std 65.0504 (63.2123) nleep/row_min_mean 1482.8215 (1468.3434) lr 1.9511e-03 eta 0:17:36
epoch [7/50] batch [40/203] time 0.097 (0.105) data 0.000 (0.009) loss 0.8692 (0.9388) teacher_loss 0.0751 (0.1604) loss_zs_kd 0.0676 (0.0683) loss_oracle 0.5245 (0.5043) kd_loss 0.9961 (0.9840) acc 96.8750 (95.3125) gate/entropy 1.0861 (1.0863) gate/usage_max 0.4020 (0.4015) gate/usage_min 0.2732 (0.2739) gate/usage_std 0.0529 (0.0524) teacher/entropy 0.0387 (0.0477) teacher/usage_max 0.5440 (0.5436) teacher/usage_min 0.0207 (0.0371) teacher/usage_std 0.2255 (0.2184) nleep/row_max_mean 1491.4521 (1493.2366) nleep/row_max_std 68.5878 (61.1580) nleep/row_min_mean 1464.7798 (1468.4619) lr 1.9511e-03 eta 0:15:31
epoch [7/50] batch [60/203] time 0.089 (0.102) data 0.001 (0.006) loss 0.8912 (0.9456) teacher_loss 0.0956 (0.1669) loss_zs_kd 0.0635 (0.0673) loss_oracle 0.5455 (0.5004) kd_loss 0.9820 (0.9896) acc 100.0000 (95.4688) gate/entropy 1.0859 (1.0862) gate/usage_max 0.4024 (0.4017) gate/usage_min 0.2724 (0.2735) gate/usage_std 0.0534 (0.0527) teacher/entropy 0.0552 (0.0438) teacher/usage_max 0.5369 (0.5457) teacher/usage_min 0.0317 (0.0367) teacher/usage_std 0.2176 (0.2194) nleep/row_max_mean 1500.4915 (1493.6388) nleep/row_max_std 48.8811 (60.1327) nleep/row_min_mean 1472.0906 (1468.6611) lr 1.9511e-03 eta 0:15:05
epoch [7/50] batch [80/203] time 0.094 (0.100) data 0.000 (0.005) loss 1.0217 (0.9493) teacher_loss 0.2110 (0.1658) loss_zs_kd 0.0849 (0.0678) loss_oracle 0.5374 (0.5072) kd_loss 0.9991 (0.9920) acc 96.8750 (95.4297) gate/entropy 1.0857 (1.0861) gate/usage_max 0.4027 (0.4020) gate/usage_min 0.2718 (0.2731) gate/usage_std 0.0537 (0.0529) teacher/entropy 0.0104 (0.0430) teacher/usage_max 0.5304 (0.5511) teacher/usage_min 0.0014 (0.0376) teacher/usage_std 0.2361 (0.2203) nleep/row_max_mean 1493.4664 (1493.0654) nleep/row_max_std 54.4754 (60.2756) nleep/row_min_mean 1467.9917 (1467.9986) lr 1.9511e-03 eta 0:14:41
epoch [7/50] batch [100/203] time 0.099 (0.099) data 0.000 (0.004) loss 0.9507 (0.9582) teacher_loss 0.1416 (0.1723) loss_zs_kd 0.0547 (0.0677) loss_oracle 0.5586 (0.5092) kd_loss 1.0049 (0.9948) acc 96.8750 (95.4062) gate/entropy 1.0856 (1.0860) gate/usage_max 0.4025 (0.4021) gate/usage_min 0.2712 (0.2728) gate/usage_std 0.0539 (0.0531) teacher/entropy 0.0471 (0.0402) teacher/usage_max 0.4917 (0.5494) teacher/usage_min 0.0991 (0.0382) teacher/usage_std 0.1690 (0.2193) nleep/row_max_mean 1495.7917 (1493.4293) nleep/row_max_std 59.2302 (59.5932) nleep/row_min_mean 1471.8943 (1468.1191) lr 1.9511e-03 eta 0:14:31
epoch [7/50] batch [120/203] time 0.090 (0.097) data 0.000 (0.003) loss 0.8398 (0.9528) teacher_loss 0.0718 (0.1665) loss_zs_kd 0.0598 (0.0669) loss_oracle 0.4735 (0.5086) kd_loss 1.0026 (0.9970) acc 96.8750 (95.5729) gate/entropy 1.0855 (1.0859) gate/usage_max 0.4024 (0.4022) gate/usage_min 0.2703 (0.2724) gate/usage_std 0.0541 (0.0533) teacher/entropy 0.0517 (0.0377) teacher/usage_max 0.6594 (0.5521) teacher/usage_min 0.0208 (0.0358) teacher/usage_std 0.2609 (0.2213) nleep/row_max_mean 1497.3500 (1493.2834) nleep/row_max_std 53.3730 (58.9407) nleep/row_min_mean 1471.3765 (1467.7691) lr 1.9511e-03 eta 0:14:15
epoch [7/50] batch [140/203] time 0.151 (0.098) data 0.000 (0.003) loss 0.8952 (0.9491) teacher_loss 0.0525 (0.1599) loss_zs_kd 0.0738 (0.0670) loss_oracle 0.6282 (0.5145) kd_loss 0.9834 (0.9968) acc 100.0000 (95.7589) gate/entropy 1.0852 (1.0858) gate/usage_max 0.4027 (0.4022) gate/usage_min 0.2694 (0.2721) gate/usage_std 0.0546 (0.0534) teacher/entropy 0.0467 (0.0364) teacher/usage_max 0.4850 (0.5503) teacher/usage_min 0.0775 (0.0348) teacher/usage_std 0.1819 (0.2216) nleep/row_max_mean 1502.0603 (1493.4678) nleep/row_max_std 50.5767 (58.4222) nleep/row_min_mean 1474.3171 (1467.7122) lr 1.9511e-03 eta 0:14:22
epoch [7/50] batch [160/203] time 0.159 (0.101) data 0.000 (0.002) loss 1.2556 (0.9530) teacher_loss 0.4410 (0.1617) loss_zs_kd 0.0749 (0.0677) loss_oracle 0.5380 (0.5178) kd_loss 1.0162 (0.9972) acc 90.6250 (95.7812) gate/entropy 1.0850 (1.0858) gate/usage_max 0.4028 (0.4023) gate/usage_min 0.2685 (0.2717) gate/usage_std 0.0549 (0.0536) teacher/entropy 0.0006 (0.0350) teacher/usage_max 0.5312 (0.5502) teacher/usage_min 0.0001 (0.0331) teacher/usage_std 0.2370 (0.2226) nleep/row_max_mean 1495.6064 (1494.1979) nleep/row_max_std 71.4071 (57.9073) nleep/row_min_mean 1465.4080 (1468.1380) lr 1.9511e-03 eta 0:14:45
epoch [7/50] batch [180/203] time 0.109 (0.104) data 0.000 (0.002) loss 1.0497 (0.9536) teacher_loss 0.2477 (0.1612) loss_zs_kd 0.0540 (0.0676) loss_oracle 0.5328 (0.5184) kd_loss 1.0172 (0.9987) acc 93.7500 (95.8160) gate/entropy 1.0850 (1.0857) gate/usage_max 0.4023 (0.4023) gate/usage_min 0.2680 (0.2713) gate/usage_std 0.0549 (0.0537) teacher/entropy 0.0189 (0.0332) teacher/usage_max 0.6334 (0.5562) teacher/usage_min 0.0001 (0.0306) teacher/usage_std 0.2596 (0.2254) nleep/row_max_mean 1493.6287 (1494.1667) nleep/row_max_std 58.6823 (58.4515) nleep/row_min_mean 1460.2458 (1467.7531) lr 1.9511e-03 eta 0:15:11
epoch [7/50] batch [200/203] time 0.096 (0.104) data 0.000 (0.002) loss 1.0613 (0.9551) teacher_loss 0.2677 (0.1619) loss_zs_kd 0.0712 (0.0681) loss_oracle 0.5020 (0.5200) kd_loss 1.0141 (0.9984) acc 93.7500 (95.7500) gate/entropy 1.0848 (1.0856) gate/usage_max 0.4025 (0.4023) gate/usage_min 0.2670 (0.2709) gate/usage_std 0.0554 (0.0539) teacher/entropy 0.0062 (0.0326) teacher/usage_max 0.5614 (0.5562) teacher/usage_min 0.0002 (0.0292) teacher/usage_std 0.2408 (0.2262) nleep/row_max_mean 1496.5598 (1494.2854) nleep/row_max_std 60.4300 (58.5973) nleep/row_min_mean 1466.8636 (1467.6537) lr 1.9511e-03 eta 0:15:11
Evaluate on the *val* set
=> result
* total: 2,795
* correct: 2,261
* accuracy: 80.9%
* error: 19.1%
* macro_f1: 85.7%
Evaluate on the *test* set
=> result
* total: 1,415
* correct: 1,415
* accuracy: 100.0%
* error: 0.0%
* macro_f1: 100.0%
******* Domain c best val acc:      83.3%, epoch: 5 *******
******* Domain c best val test acc: 100.0%, epoch: 5 *******
******* Domain c best test acc:     100.0%, epoch: 1 *******
epoch [8/50] batch [20/203] time 0.113 (0.117) data 0.000 (0.013) loss 1.1305 (0.9570) teacher_loss 0.2714 (0.1468) loss_zs_kd 0.0881 (0.0701) loss_oracle 0.6090 (0.5554) kd_loss 1.0212 (0.9950) acc 90.6250 (95.0000) gate/entropy 1.0846 (1.0847) gate/usage_max 0.4027 (0.4026) gate/usage_min 0.2664 (0.2667) gate/usage_std 0.0556 (0.0555) teacher/entropy 0.0049 (0.0227) teacher/usage_max 0.5310 (0.5469) teacher/usage_min 0.0320 (0.0241) teacher/usage_std 0.2165 (0.2264) nleep/row_max_mean 1476.9524 (1491.5799) nleep/row_max_std 84.5617 (62.9292) nleep/row_min_mean 1451.5449 (1463.2796) lr 1.9298e-03 eta 0:16:55
epoch [8/50] batch [40/203] time 0.101 (0.113) data 0.000 (0.006) loss 0.8163 (0.9678) teacher_loss 0.0490 (0.1574) loss_zs_kd 0.0634 (0.0730) loss_oracle 0.4411 (0.5478) kd_loss 1.0303 (1.0000) acc 100.0000 (95.3125) gate/entropy 1.0847 (1.0847) gate/usage_max 0.4021 (0.4026) gate/usage_min 0.2663 (0.2665) gate/usage_std 0.0554 (0.0556) teacher/entropy 0.0122 (0.0184) teacher/usage_max 0.6246 (0.5637) teacher/usage_min 0.0285 (0.0169) teacher/usage_std 0.2436 (0.2344) nleep/row_max_mean 1484.7672 (1494.3614) nleep/row_max_std 64.5409 (59.6554) nleep/row_min_mean 1457.2893 (1465.8063) lr 1.9298e-03 eta 0:16:22
epoch [8/50] batch [60/203] time 0.108 (0.112) data 0.001 (0.004) loss 0.9336 (0.9701) teacher_loss 0.1419 (0.1648) loss_zs_kd 0.0641 (0.0751) loss_oracle 0.5069 (0.5356) kd_loss 1.0124 (0.9998) acc 93.7500 (95.3646) gate/entropy 1.0845 (1.0846) gate/usage_max 0.4023 (0.4025) gate/usage_min 0.2653 (0.2662) gate/usage_std 0.0559 (0.0556) teacher/entropy 0.0195 (0.0184) teacher/usage_max 0.6247 (0.5692) teacher/usage_min 0.0069 (0.0128) teacher/usage_std 0.2534 (0.2381) nleep/row_max_mean 1489.6321 (1494.4003) nleep/row_max_std 74.1728 (59.8019) nleep/row_min_mean 1460.7913 (1465.5254) lr 1.9298e-03 eta 0:16:10
epoch [8/50] batch [80/203] time 0.111 (0.111) data 0.000 (0.003) loss 0.9735 (0.9638) teacher_loss 0.1595 (0.1617) loss_zs_kd 0.0832 (0.0737) loss_oracle 0.5617 (0.5327) kd_loss 0.9831 (0.9978) acc 96.8750 (95.7812) gate/entropy 1.0843 (1.0846) gate/usage_max 0.4023 (0.4024) gate/usage_min 0.2644 (0.2659) gate/usage_std 0.0563 (0.0558) teacher/entropy 0.0199 (0.0183) teacher/usage_max 0.5078 (0.5635) teacher/usage_min 0.0001 (0.0120) teacher/usage_std 0.2357 (0.2373) nleep/row_max_mean 1492.6565 (1495.6311) nleep/row_max_std 61.6572 (58.8324) nleep/row_min_mean 1463.2817 (1466.7138) lr 1.9298e-03 eta 0:15:57
epoch [8/50] batch [100/203] time 0.174 (0.112) data 0.000 (0.003) loss 0.9177 (0.9573) teacher_loss 0.0668 (0.1521) loss_zs_kd 0.1013 (0.0737) loss_oracle 0.5804 (0.5372) kd_loss 1.0201 (0.9995) acc 96.8750 (95.9375) gate/entropy 1.0842 (1.0845) gate/usage_max 0.4020 (0.4024) gate/usage_min 0.2638 (0.2656) gate/usage_std 0.0564 (0.0559) teacher/entropy 0.0004 (0.0172) teacher/usage_max 0.5937 (0.5684) teacher/usage_min 0.0000 (0.0107) teacher/usage_std 0.2478 (0.2393) nleep/row_max_mean 1484.6572 (1495.2422) nleep/row_max_std 63.9541 (59.6522) nleep/row_min_mean 1454.0269 (1466.1464) lr 1.9298e-03 eta 0:16:03
epoch [8/50] batch [120/203] time 0.150 (0.115) data 0.000 (0.002) loss 0.9743 (0.9576) teacher_loss 0.1443 (0.1513) loss_zs_kd 0.0720 (0.0749) loss_oracle 0.5636 (0.5379) kd_loss 1.0244 (1.0000) acc 96.8750 (95.9375) gate/entropy 1.0841 (1.0844) gate/usage_max 0.4016 (0.4023) gate/usage_min 0.2632 (0.2652) gate/usage_std 0.0566 (0.0560) teacher/entropy 0.0080 (0.0162) teacher/usage_max 0.5936 (0.5683) teacher/usage_min 0.0316 (0.0097) teacher/usage_std 0.2313 (0.2398) nleep/row_max_mean 1487.0439 (1495.2410) nleep/row_max_std 70.6579 (60.2609) nleep/row_min_mean 1455.4053 (1466.0172) lr 1.9298e-03 eta 0:16:29
epoch [8/50] batch [140/203] time 0.096 (0.119) data 0.001 (0.002) loss 0.9441 (0.9560) teacher_loss 0.1482 (0.1510) loss_zs_kd 0.0903 (0.0743) loss_oracle 0.4900 (0.5354) kd_loss 1.0116 (1.0003) acc 96.8750 (96.0045) gate/entropy 1.0840 (1.0844) gate/usage_max 0.4015 (0.4022) gate/usage_min 0.2626 (0.2649) gate/usage_std 0.0567 (0.0561) teacher/entropy 0.0008 (0.0155) teacher/usage_max 0.5625 (0.5701) teacher/usage_min 0.0001 (0.0099) teacher/usage_std 0.2411 (0.2402) nleep/row_max_mean 1479.7151 (1494.9801) nleep/row_max_std 85.7772 (60.3377) nleep/row_min_mean 1451.0765 (1465.7157) lr 1.9298e-03 eta 0:16:57
epoch [8/50] batch [160/203] time 0.100 (0.116) data 0.000 (0.002) loss 0.9054 (0.9538) teacher_loss 0.1070 (0.1495) loss_zs_kd 0.0625 (0.0734) loss_oracle 0.5406 (0.5350) kd_loss 0.9936 (1.0000) acc 96.8750 (96.0742) gate/entropy 1.0837 (1.0843) gate/usage_max 0.4016 (0.4021) gate/usage_min 0.2617 (0.2646) gate/usage_std 0.0571 (0.0562) teacher/entropy 0.0072 (0.0150) teacher/usage_max 0.5000 (0.5716) teacher/usage_min 0.0018 (0.0094) teacher/usage_std 0.2344 (0.2409) nleep/row_max_mean 1505.2568 (1494.6830) nleep/row_max_std 64.0367 (60.8677) nleep/row_min_mean 1476.1498 (1465.4109) lr 1.9298e-03 eta 0:16:35
epoch [8/50] batch [180/203] time 0.101 (0.114) data 0.000 (0.002) loss 0.8861 (0.9514) teacher_loss 0.0948 (0.1485) loss_zs_kd 0.0545 (0.0731) loss_oracle 0.5594 (0.5322) kd_loss 0.9687 (1.0005) acc 96.8750 (96.0243) gate/entropy 1.0837 (1.0843) gate/usage_max 0.4013 (0.4020) gate/usage_min 0.2613 (0.2642) gate/usage_std 0.0572 (0.0563) teacher/entropy 0.0184 (0.0145) teacher/usage_max 0.5705 (0.5760) teacher/usage_min 0.0001 (0.0085) teacher/usage_std 0.2426 (0.2427) nleep/row_max_mean 1515.2799 (1494.4229) nleep/row_max_std 47.6556 (61.2860) nleep/row_min_mean 1484.0387 (1465.1328) lr 1.9298e-03 eta 0:16:15
epoch [8/50] batch [200/203] time 0.090 (0.112) data 0.000 (0.002) loss 0.8361 (0.9504) teacher_loss 0.0805 (0.1481) loss_zs_kd 0.0719 (0.0729) loss_oracle 0.4346 (0.5314) kd_loss 1.0047 (1.0001) acc 96.8750 (96.0781) gate/entropy 1.0836 (1.0842) gate/usage_max 0.4010 (0.4019) gate/usage_min 0.2608 (0.2639) gate/usage_std 0.0573 (0.0564) teacher/entropy 0.0044 (0.0141) teacher/usage_max 0.5632 (0.5752) teacher/usage_min 0.0001 (0.0085) teacher/usage_std 0.2412 (0.2424) nleep/row_max_mean 1500.6375 (1494.4874) nleep/row_max_std 58.4025 (61.2316) nleep/row_min_mean 1472.7233 (1465.2702) lr 1.9298e-03 eta 0:15:55
Evaluate on the *val* set
=> result
* total: 2,795
* correct: 2,318
* accuracy: 82.9%
* error: 17.1%
* macro_f1: 86.8%
Evaluate on the *test* set
=> result
* total: 1,415
* correct: 1,415
* accuracy: 100.0%
* error: 0.0%
* macro_f1: 100.0%
******* Domain c best val acc:      83.3%, epoch: 5 *******
******* Domain c best val test acc: 100.0%, epoch: 5 *******
******* Domain c best test acc:     100.0%, epoch: 1 *******
